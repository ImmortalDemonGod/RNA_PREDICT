ğŸ§™ğŸ¾â€: â€œHereâ€™s an overview of how AlphaFold 3â€™s 48-block Pairformer stack worksâ€”covering the triangular updates and the single-representation-with-pair-bias pieceâ€”plus how it all supports â€˜pairwise distanceâ€™ prediction.â€

[emoji]: Detailed Explanation

Below is a step-by-step look at the â€œPairformerâ€ trunk that AF3 uses after the MSA Module. Each of the 48 Pairformer blocks transforms an (N \times N) pair-representation array (\{z_{ij}\}) plus a single-representation vector for each token (\{s_i\}). In total:
	1.	Pair Representation:
	â€¢	An (N \times N)-shaped representation where each entry \mathbf{z}_{ij} (the â€œpair embeddingâ€) encodes how token i (residue or ligand atom) relates to token j.
	â€¢	These pair embeddings are continuously refined with â€œtriangle updatesâ€ (both multiplicative and attention-based).
	2.	Single Representation:
	â€¢	A length-N array \{\mathbf{s}_i\} whose elements store information about each token in isolation (e.g., summarizing the local environment, MSA context, etc.).
	â€¢	AF3 processes that single representation side-by-side with the pair representation, letting them exchange information.

â¸»

(A) Triangular Multiplication (Outgoing & Incoming)

Each block begins with two multiplicative updates on the pair array:
	1.	TriangleMultiplicationOutgoing (\text{â€œoutgoingâ€ edges})
	2.	TriangleMultiplicationIncoming (\text{â€œincomingâ€ edges})

Conceptually, the pair embedding \mathbf{z}_{ij} is viewed as an â€œedgeâ€ connecting node i and node j. The â€œtriangleâ€ part means each edge can be updated by looking at a shared vertex k.
	â€¢	Outgoing means: â€œUpdate \mathbf{z}_{ij} by considering edges from i to k and j to k.â€
	â€¢	Incoming means: â€œUpdate \mathbf{z}_{ij} by considering edges from k to i and k to j.â€

Each such update is a learned transformation that ensures geometric consistency among triplets (i,j,k). In simpler terms, it fuses information from the â€˜third nodeâ€™ k, telling \mathbf{z}_{ij} how i\rightarrow k and j\rightarrow k compare or multiply.

â¸»

(B) Triangular Self-Attention (Starting & Ending Node)

Next come two specialized attention layers:
	1.	TriangleAttentionStartingNode
	2.	TriangleAttentionEndingNode

They again treat \mathbf{z}{ij} like edges in a complete graph. One triangular attention uses i as the â€œfocalâ€ node, scanning across possible k to update \mathbf{z}{ij}. The other uses j as the focus.
	â€¢	â€œStarting nodeâ€ means each edge (i \to j) pays attention to edges (i \to k).
	â€¢	â€œEnding nodeâ€ means the edge (i \to j) pays attention to edges (k \to j).

Hence it is a form of pairwise attention that ensures each pair embedding \mathbf{z}_{ij} is consistent with adjacency edges sharing one vertex. This further enforces consistent â€œtriangle geometry.â€

â¸»

(C) Single Representation Update via Pair-Bias Attention

After the triangular updates, the model updates the single representation \mathbf{s}_i. In AF3, each tokenâ€™s single vector \mathbf{s}i does a self-attention step in which the attention logits are â€œbiasedâ€ by the pair representation \mathbf{z}{ij}.
	â€¢	Concretely, we run an attention over \{\mathbf{s}i\} with something like \text{logit}(i,j) \,+=\, \text{Linear}(\mathbf{z}{ij}).
	â€¢	This means: if pair (i,j) is known (via \mathbf{z}_{ij}) to be highly interacting or geometrically close, the attention between single vectors \mathbf{s}_i and \mathbf{s}_j is boosted.
	â€¢	The end result is that single vectors incorporate pairwise knowledge.
	â€¢	After attention, we pass \{\mathbf{s}_i\} through a small transition (feed-forward) block.

Hence each Pairformer block effectively does:
	1.	Triangular multiplication (outgoing + incoming) on \mathbf{z}_{ij}
	2.	Triangular attention (starting + ending node) on \mathbf{z}_{ij}
	3.	A â€œtransitionâ€ on \mathbf{z}_{ij}
	4.	Single attention with pair bias on \mathbf{s}_i
	5.	A â€œtransitionâ€ on \mathbf{s}_i

â¸»

(D) Ties to â€˜Pairwise Distanceâ€™â€“Based Prediction

Because each \mathbf{z}{ij} is a learned representation of how token i and j relate, it is naturally well-suited for predicting distances between them. Indeed, AF3â€™s final distogram head uses the last \{\mathbf{z}{ij}\} to produce a binned distribution of distances.
	â€¢	The triangular updates effectively ensure that any triplet (i,j,k) sees consistent constraints. This is crucial for capturing geometry.
	â€¢	Unlike a simple binary contact map, the pair representation is a continuous, multi-channel embedding that eventually yields a distribution over distances (the â€œdistogramâ€)â€”thus under-the-hood, it is still a â€œpairwise distanceâ€ predictor.

Hence, the 48-block Pairformer architecture is the engine for learning geometry from pairwise tokens, letting the model resolve local/long-range distances. Finally, the Distogram headâ€”reading from \{\mathbf{z}_{ij}\}â€”produces the discrete distance bins that underlie AF3â€™s structural accuracy.

Would you like more on how the diffusion module consumes these pair embeddings to generate coordinates?