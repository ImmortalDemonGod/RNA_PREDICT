# ğŸš€ Stage B: Comprehensive RNA Torsion Angle Predictor

---

## ğŸ“Œ Domain, Inputs & Outputs

### ğŸ“¥ Inputs

1. **RNA Sequence**
   - Length: **N** residues
   - Residues: `{A, C, G, U}` or modification tokens

2. **2D Adjacency (Base-Pair Matrix)** (from Stage A)
   - Matrix size: **N Ã— N**
   - Values:
     - Binary (`1` paired, `0` unpaired), or
     - Real-valued probabilities `[0,1]`

3. **Optional Node Features**
   - MSA-based evolutionary profiles
   - Secondary-structure metadata (e.g., hairpin loops, non-canonical pairs)

---

## ğŸ¯ Outputs

For each residue **i**, predict backbone torsion angles:
- Angles: **Î±, Î², Î³, Î´, Îµ, Î¶, Ï‡**
- Optional: **Sugar pucker angle or pseudorotation (Páµ¢)**
  - Range: `[-Ï€, Ï€]`

### ğŸ”§ Constraints & Goals
- **Angle periodicity**: Use sine/cosine representation to manage wraparound
- **Secondary-structure constraints**: base-pairing, backbone continuity, possible pseudoknots
- **Geometric consistency**: influenced by local and distant residues

---

## ğŸŒ Graph Representation & GDL Principles

Represent RNA as graph **G=(V,E)**:
- Nodes (**V**): Residues `{1, 2, ..., N}`
- Edges (**E**):
  1. Backbone edges: **i â†” i+1**
  2. Base-pair edges from adjacency: **i â†” j**
  3. Optional short-range edges: **i â†” i+2**, **i â†” i+3** (enhanced local context)

ğŸ“Œ Equivariant under node permutations (adjacency fixed by indexing & base pairing).

---

## ğŸ§© Node & Edge Feature Construction

### ğŸ”¹ Node Features (**náµ¢**)
- Sequence One-Hot (`A/C/G/U`)
- Base-Pair Stats: Sum row from adjacency; indicator "unpaired"
- Optional: MSA evolutionary profiles

Concatenate and embed:
```
ğ’‰áµ¢â½â°â¾ = Linear(náµ¢)
```

### ğŸ”¸ Edge Features (**eáµ¢â±¼**)
- Base-Pair Probability (`adj[i,j]`)
- Type: **Backbone** vs. **Long-range** (canonical/non-canonical)
- Sequence distance: `|i-j|` (binned/clipped)

Embed edges:
```
ğ’ˆáµ¢â±¼â½â°â¾ = LinearEdge(eáµ¢â±¼)
```

---

## âš™ï¸ Graph Transformer Architecture

Employ **Multi-Head Attention + Message Passing**:

### ğŸ“‹ Detailed Pseudocode

```python
def GraphTransformer(nodes, edges, adjacency, L=6, c_hidden=128):
    h = Linear(nodes)  # [N, c_hidden]
    g = {(i,j): LinearEdge(edges[(i,j)]) for (i,j) in adjacency}

    for layer in range(L):
        # Nodeâ†’Edge update
        for (i,j) in adjacency:
            x_ij = concat(h[i], h[j], g[(i,j)])
            g[(i,j)] += MLP_edge[layer](x_ij)

        # Edgeâ†’Node update (Multi-head Attention)
        new_h = zeros_like(h)
        for i in range(N):
            neighbors = adjacency.neighbors(i)
            attn_scores, vs = [], []
            for j in neighbors:
                score = dot(q_proj(h[i]), k_proj(h[j])) + bias_proj(g[(i,j)])
                attn_scores.append(score)
                vs.append(v_proj(h[j]))
            weights = softmax(attn_scores)
            new_h[i] = sum(weight * vs[j] for j, weight in enumerate(attn_scores))
        h = LayerNorm(h + new_h)

    return h, g
```
- Use edge embedding biases in attention.
- Optional: integrate "pairformer" or AF triangle multiplication.

---

## ğŸ² Angle Prediction Head & Loss

### ğŸ¯ Angle Output
Final node embedding (`háµ¢â½finalâ¾`) via MLP:
```
anglesáµ¢ = MLP_final(h[i])  # [7Ã—2] (sin/cos)
```
Then:
```
Î±áµ¢ = atan2(sin_Î±áµ¢, cos_Î±áµ¢), etc.
```

### ğŸ“ Loss Function
```
L = (1/(NÃ—7)) âˆ‘áµ¢ âˆ‘Ï† [wrap(Î¸Ì‚áµ¢ - Î¸áµ¢)]Â²
```
- Optional:
  - Angle prior (A-form RNA distributions)
  - 3D coordinate-based regularization (Stage C)

---

## ğŸ“š Training Data & Procedure

### ğŸ—ƒï¸ Data Preparation
- Curate RNA structures (PDB)
- Compute torsion angles & adjacencies
- Assemble node & edge features

### ğŸ“ Training Steps
- Forward pass: GraphTransformer + angle head
- Loss: angle-based MSE or `(sin, cos)` differences
- Optimization: Adam/AdamW, learning rate scheduler
- Validation: angle-level MSE, optional Stageâ€¯C 3D RMSD check

---

## ğŸ Full Python Implementation
Refer to the complete Python implementation provided above, which includes:
- `TorsionPredictor`, `GraphTransformerBlock`, `NodeEdgeAttention`
- Detailed inline comments for clarity
- Fully modular implementation suitable for PyTorch

---

## ğŸ’¡ Why "Nearly Foolproof"?
1. ğŸ“ **Graph Representation**: Explicit pairing & backbone adjacency.
2. ğŸŒ **Local + Global**: Transformer captures interactions at all scales.
3. ğŸ”„ **Angle Periodicity**: Stable sineâ€“cosine encoding.
4. âš¡ **Scalable & Efficient**: Suitable for large datasets.
4. ğŸ“ **Physics-based Priors**: Integrate RNA angle distributions.
5. ğŸ”— **Stageâ€¯C Compatible**: Easily feeds into 3D coordinate reconstruction.

---

## ğŸ–ï¸ Final Thoughts
- Methodically addresses RNA torsion prediction complexities.
- Graph Transformer is flexible, powerful, and scalable.
- Predictive output stable through sine/cosine angle pairs.
- Structured loss function and validation improve robustness.

ğŸ“Œ **Outcome**: Robust, reliable RNA torsion predictor leveraging proven GDL principles.

