üßôüèæ‚Äç‚ôÇÔ∏è: Aligning on your goal: You need a self-contained reference document that guides a junior developer to incorporate Hydra into an ML project, using patterns drawn from the ‚ÄúProjectEquiSurv‚Äù code base.

[emoji]: Below is a comprehensive tutorial-style guide. A junior dev can follow it step-by-step to set up Hydra for configuration management, referencing real code snippets from ProjectEquiSurv.

‚∏ª

Hydra Usage Guide with ML Project Patterns (Based on Project EquiSurv)

This document introduces the Hydra framework, a powerful Python library for composable configurations. We will walk through:
	1.	Why Hydra?
	2.	Project Structure & Key Files
	3.	Basic Hydra Concepts
	4.	Setting Up Hydra
	5.	Config Directory & YAML Organization
	6.	Composing Configs
	7.	Runtime Overrides & CLI Examples
	8.	Using Hydra for HPC vs. Local
	9.	(Optional) Hydra Sweeping vs. Custom Search
	10.	FAQ & Common Pitfalls

Throughout, we‚Äôll reference patterns from the ProjectEquiSurv code‚Äîparticularly the quick_fixes/conf/ directory and usage in pipeline/main.py.

‚∏ª

1. Why Hydra?

Machine learning projects often require multiple environment-dependent configurations:
	‚Ä¢	Different hyperparameters for training
	‚Ä¢	Different dataset paths, logging settings, HPC vs. local
	‚Ä¢	Variation in experiment modes (e.g. ‚Äútrain‚Äù, ‚Äúeval‚Äù, ‚Äúhyperparam sweep‚Äù)

Hydra standardizes how these configurations are defined, merged, and overridden via command-line. Instead of large ad-hoc config Python scripts or a single giant YAML, Hydra encourages modular sub-configs that Hydra merges at runtime, letting you override any field on the command-line if desired.

‚∏ª

2. Project Structure & Key Files

Below is an example structure (mirroring how ProjectEquiSurv organizes quick_fixes/conf), focusing on the Hydra config pieces:

my_ml_project/
‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml               # The main or "top-level" config
‚îÇ   ‚îú‚îÄ‚îÄ model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default.yaml          # Model hyperparams
‚îÇ   ‚îú‚îÄ‚îÄ dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default.yaml          # Dataset references, data paths
‚îÇ   ‚îú‚îÄ‚îÄ experiment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local.yaml            # Local dev environment overrides
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hpc.yaml              # HPC environment overrides
‚îÇ   ‚îî‚îÄ‚îÄ search_space
‚îÇ       ‚îî‚îÄ‚îÄ default.yaml          # Hyperparam search ranges
‚îú‚îÄ‚îÄ pipeline
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Hydra entrypoint (@hydra.main)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_train_single.py  # Train logic referencing cfg
‚îÇ   ‚îî‚îÄ‚îÄ ... (other scripts)
‚îî‚îÄ‚îÄ ...

Quick-Glance at Key Files
	‚Ä¢	conf/config.yaml: Declares top-level defaults and merges sub-configs:

defaults:
  - model: default
  - dataset: default
  - experiment: local
  - search_space: default
  - _self_

mode: "train"
some_global_setting: true

# Logging dir or run settings, e.g.:
hydra:
  run:
    dir: ./outputs  # or "outputs/${now:%Y-%m-%d_%H-%M-%S}"


	‚Ä¢	pipeline/main.py: The Python script that calls:

import hydra
from omegaconf import DictConfig

@hydra.main(version_base=None, config_path="../conf", config_name="config")
def main(cfg: DictConfig) -> None:
    # Hydra merges everything into cfg
    mode = cfg.mode
    if mode == "train":
        ...
    elif mode == "hyperparam_sweep":
        ...
    else:
        print(f"Unknown mode: {mode}")

if __name__ == "__main__":
    main()



‚∏ª

3. Basic Hydra Concepts
	‚Ä¢	Structured YAML: Hydra merges multiple YAMLs declared in the defaults list (or via overrides).
	‚Ä¢	DictConfig: The resulting object inside main.py. Access config fields with cfg.something.
	‚Ä¢	Compositional configs: Each sub-config file can hold a portion of the overall settings (e.g. model vs. dataset).
	‚Ä¢	CLI Overrides: You can override any setting from the command-line, e.g. python main.py mode=train model.hidden_dim=512.

‚∏ª

4. Setting Up Hydra
	1.	Install Hydra

pip install hydra-core>=1.2  # Or match your environment


	2.	Create a conf/ directory
Inside your project, place a root YAML, e.g. config.yaml.
	3.	In your main script:

import hydra
from omegaconf import DictConfig

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    print(cfg)   # For debugging
    # rest of your code

if __name__ == "__main__":
    main()



With that minimal approach, Hydra loads conf/config.yaml. Next, you add sub-configs to define specific settings.

‚∏ª

5. Config Directory & YAML Organization

Following ProjectEquiSurv best practices:
	‚Ä¢	conf/config.yaml (top-level):

defaults:
  - model: default
  - dataset: default
  - experiment: local
  - search_space: default
  - _self_

# Hydra or top-level fields
mode: "train"
device: "cpu"
# (You can store pipeline settings, e.g. logs, output_dir, etc.)


	‚Ä¢	conf/model/default.yaml:

# model/default.yaml

hidden_dim: 256
num_bins: 10
survival_mode: "cox"  # or "discrete"

dropout: 0.1
# Additional model hyperparams


	‚Ä¢	conf/dataset/default.yaml:

# dataset/default.yaml

train_csv_path: "data/raw/train.csv"
test_csv_path: "data/raw/test.csv"

val_fraction: 0.2
split_strategy: "random"


	‚Ä¢	conf/experiment/local.yaml:

# local.yaml

epochs: 5
train_batch_size: 8
some_local_override: true


	‚Ä¢	conf/experiment/hpc.yaml (for HPC):

# hpc.yaml

epochs: 50
train_batch_size: 64
some_local_override: false
# HPC cluster settings


	‚Ä¢	conf/search_space/default.yaml (for hyperparam search definitions):

# search_space/default.yaml

hyperparam_search:
  # e.g. define param ranges, or store them for a custom search routine
  hidden_dim:
    min: 128
    max: 512



Key: Hydra sees the defaults list in conf/config.yaml and merges each sub-config. The final result is accessible as cfg.model.hidden_dim, cfg.dataset.train_csv_path, etc.

‚∏ª

6. Composing Configs

Inside conf/config.yaml:

defaults:
  - model: default
  - dataset: default
  - experiment: local
  - search_space: default
  - _self_

	‚Ä¢	The order matters: Hydra merges them top to bottom. _self_ means ‚Äúfinally merge the contents of this file (config.yaml) last.‚Äù

If you want to switch from experiment: local to experiment: hpc, you can do one of two approaches:
	1.	Edit config.yaml:

defaults:
  - model: default
  - dataset: default
  - experiment: hpc  # swapped local => hpc
  ...


	2.	Command-line Override:

python main.py experiment=hpc

Hydra sees experiment=hpc and merges hpc.yaml instead of local.yaml.

‚∏ª

7. Runtime Overrides & CLI Examples

A big advantage of Hydra is on-the-fly overrides. For example:

# 1) Switch environment
python main.py experiment=hpc

# 2) Override a single field
python main.py model.hidden_dim=512

# 3) Combine multiple overrides
python main.py mode=train dataset.split_strategy=race_time_stratified model.survival_mode=discrete

# 4) Overriding path for train_csv
python main.py dataset.train_csv_path="/mnt/large_train.csv"

All those changes apply without rewriting config YAMLs. Hydra merges them at runtime.

‚∏ª

8. Using Hydra for HPC vs. Local

ProjectEquiSurv uses separate YAMLs for HPC and local:
	‚Ä¢	experiment/local.yaml:

epochs: 5
train_batch_size: 8
# ...


	‚Ä¢	experiment/hpc.yaml:

epochs: 50
train_batch_size: 64
# HPC-specific fields



When you specify experiment:hpc, Hydra merges the HPC settings. If your code references cfg.epochs or cfg.train_batch_size, it automatically picks the HPC values.

Additionally, in HPC contexts, you might define some_local_override: false, or cluster-based resource strings (like slurm scripts). This keeps your code environment-agnostic, because it always reads from cfg....

‚∏ª

9. (Optional) Hydra Sweeping vs. Custom Search

In ProjectEquiSurv, the hyperparameter search is done with a custom Optuna approach. This is completely valid. Alternatively, Hydra has built-in sweepers that can loop through sets of config overrides in multiple runs. For instance:

python main.py -m model.hidden_dim=128,256,512 optimizer.lr=0.001,0.01

Hydra would produce 6 runs (3 √ó 2). If you prefer your custom approach, just keep it. You still store search ranges in, e.g., conf/search_space/default.yaml.

‚∏ª

10. FAQ & Common Pitfalls
	1.	‚ÄúMy directory structure is different‚Äù
	‚Ä¢	Hydra doesn‚Äôt require an exact naming convention or ‚Äúconf/‚Äù location. You can define @hydra.main(config_path="path/to/my_configs", config_name="base"). Just keep a subdirectory of YAMLs and be consistent.
	2.	‚ÄúValueError: Could not merge config‚Äù
	‚Ä¢	Usually you have conflicting keys or typed fields. Double check that your sub-configs have consistent naming or mark items as optional.
	3.	Logging & Output Directory
	‚Ä¢	By default, Hydra can create per-run subdirectories like ./outputs/2023-07-10_15-03-22 to isolate logs. If you prefer your own directory creation logic (like ProjectEquiSurv does in path_utils.py), that‚Äôs also fine.
	4.	Common Patterns
	‚Ä¢	Use cfg.mode to route to different pipelines (train, eval, etc.).
	‚Ä¢	Keep environment overrides (like HPC, local, dev) in separate YAMLs.
	‚Ä¢	Keep large code blocks out of the config: the config is for hyperparams, paths, toggles, not big scripts.

‚∏ª

Example: Putting It All Together

Below is a minimal working snippet that echoes the style of ProjectEquiSurv:
	1.	Directory:

my_project/
‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default.yaml
‚îÇ   ‚îî‚îÄ‚îÄ experiment
‚îÇ       ‚îú‚îÄ‚îÄ local.yaml
‚îÇ       ‚îî‚îÄ‚îÄ hpc.yaml
‚îî‚îÄ‚îÄ main.py


	2.	conf/config.yaml:

defaults:
  - model: default
  - experiment: local
  - _self_

mode: "train"


	3.	conf/model/default.yaml:

hidden_dim: 256
survival_mode: "cox"


	4.	conf/experiment/local.yaml:

epochs: 5
train_batch_size: 8


	5.	conf/experiment/hpc.yaml:

epochs: 50
train_batch_size: 64


	6.	main.py:

import hydra
from omegaconf import DictConfig

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    print("MODE:", cfg.mode)
    print("MODEL:", cfg.model)
    print("EPOCHS:", cfg.epochs)
    if cfg.mode == "train":
        run_training(cfg)
    else:
        print("No valid mode selected.")

def run_training(cfg):
    # Example usage:
    print(f"Training for {cfg.epochs} epochs with batch_size={cfg.train_batch_size}")
    # Access model config
    print(f"Hidden dim: {cfg.model.hidden_dim}, survival_mode: {cfg.model.survival_mode}")
    # ... do your training ...

if __name__ == "__main__":
    main()



Running it:

# Default local config
python main.py

# HPC override
python main.py experiment=hpc

# Additional override
python main.py experiment=hpc model.hidden_dim=512



‚∏ª

Final Remarks

By adopting Hydra:
	1.	You keep your Python code simpler‚Äîjust read cfg.
	2.	You scale easily to multiple environments or hyperparam sets.
	3.	You can embed advanced Hydra features (e.g. plugin-based sweepers, custom search space merges, dynamic imports of partial configs, etc.).

For deeper reference, see official Hydra docs or the more advanced usage in ProjectEquiSurv‚Äôs quick_fixes/advanced/pipeline/*.py scripts, where HPC, local, meta-splits, and more specialized sub-configs are managed elegantly.

‚∏ª

üßôüèæ‚Äç‚ôÇÔ∏è: Next Steps,
[emoji]: Now that you have a comprehensive Hydra setup guide referencing ProjectEquiSurv, try implementing your own conf/ structure, define sub-configs for your model and dataset, and integrate them in a main.py Hydra entry script. If you run into any questions, feel free to ask‚Äîwhat specific aspect of Hydra integration can we clarify next?