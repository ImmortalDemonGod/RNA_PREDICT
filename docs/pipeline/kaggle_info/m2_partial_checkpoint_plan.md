
Okay, performing a deep dive into the `RNA_PREDICT` codebase provided.

**Deep Dive Analysis of `RNA_PREDICT` Codebase (Focus on M2 Checkpoint Cycle):**

1.  **Model Definitions & Training Structure:**
    *   **Core Models:** The primary pipeline components (`StageARFoldPredictor`, `StageBTorsionBertPredictor`, `PairformerWrapper`, `StageCReconstruction`, `DiffusionModule`, `ProtenixDiffusionManager`, various attention/embedding modules) are implemented as standard `torch.nn.Module` subclasses.
    *   **Lightning Usage:** There is **no evidence** of `pytorch_lightning.LightningModule` being used for the main pipeline models. Lightning might be used in isolated tests or utilities, but the core training/inference flow doesn't rely on it.
    *   **Training/Inference Execution:** Orchestration happens primarily through scripts:
        *   `rna_predict/run_full_pipeline.py`: Calls individual stage functions.
        *   Stage-specific runners: `rna_predict/pipeline/stageA/run_stageA.py`, `rna_predict/pipeline/stageB/main.py` (calls `run_stageB_combined`), `rna_predict/pipeline/stageC/stage_c_reconstruction.py` (calls `run_stageC_rna_mpnerf`), `rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py`.
        *   These scripts manage the flow but **do not contain standard training loops** (forward, loss, backward, step). The M2 plan implies a new `train.py` script, likely using Lightning, needs to be created.
    *   **Implication:** For the M2 partial checkpoint *test*, we cannot rely on `Lightning Trainer`. The test itself will need to implement a *manual*, minimal PyTorch training loop.

2.  **Checkpointing Mechanisms:**
    *   **Gradient Checkpointing:** `rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py` exists but handles `torch.utils.checkpoint` for memory saving during forward/backward, **not** model state saving/loading.
    *   **State Saving/Loading:** No centralized utility (like `checkpoint.py`) or function (`partial_load_state_dict`) was found in `rna_predict/utils/`. Checkpoint paths in configs (`stageA.yaml`, `stageB_torsion.yaml`) refer to loading *pre-trained models*, not saving/loading training states.
    *   **Implication:** The `partial_load_state_dict` function is a **critical missing piece** and must be implemented from scratch. The test will need to manually save the relevant state dict using `torch.save`.

3.  **LoRA/Custom Trainable Modules:**
    *   **Configuration:** LoRA configuration sections (`lora: enabled: ...`) are present in `rna_predict/conf/config_schema.py` (for `TorsionBertConfig`, `PairformerConfig`) and the corresponding YAML files (`stageB_torsion.yaml`, `stageB_pairformer.yaml`). This indicates the *design intent* to use LoRA.
    *   **Implementation:** A search for `LoRA`, `peft`, `adapter` within the primary model code (`rna_predict/pipeline/`) did not reveal active LoRA layers or integration using libraries like `peft`. `StageBTorsionBertPredictor` and `PairformerWrapper` load base models but don't appear to apply LoRA adapters dynamically based on the config *yet*.
    *   **Implication:** The M2 test needs a *dummy model* that explicitly simulates the intended structure: a frozen "base" part and a distinct "trainable/adapter" part. The test will focus on the *mechanics* of saving/loading only the trainable part, assuming LoRA integration will happen separately in the main codebase.

4.  **Hydra Configuration:**
    *   **Usage:** Hydra is deeply integrated. Structured configs are defined in `rna_predict/conf/config_schema.py`, YAMLs are in `rna_predict/conf/`, and scripts use `@hydra.main`.
    *   **Config Path:** The absolute path `/Users/tomriddle1/RNA_PREDICT/rna_predict/conf` is the correct one to use for initialization within tests.
    *   **Test Configs:** `rna_predict/conf/test/data.yaml` exists.
    *   **Implication:** Tests must initialize Hydra correctly using the absolute path. A minimal test-specific config might be useful but not strictly necessary if the test defines its parameters directly.

5.  **Testing Infrastructure:**
    *   **Directory:** `tests/` is the root.
    *   **Structure:** Well-organized with `integration/`, `unit/`, stage-specific folders (`tests/pipeline/stageD/`, etc.), and common utilities (`tests/common/`).
    *   **Existing Tests:** Follow standard patterns (e.g., `tests/integration/test_full_pipeline.py`). Dummy components are sometimes defined within tests or in `tests/common/`.
    *   **Implication:** New tests should be placed appropriately: `tests/integration/test_partial_checkpoint_cycle.py` and `tests/unit/test_partial_load_state_dict.py` (or `tests/unit/utils/test_checkpoint.py`).

**Revised & Concrete Implementation Plan for M2 Partial Checkpoint Cycle Test (RNA_PREDICT Context):**

1.  **Implement `partial_load_state_dict` Utility:**
    *   **File:** Create `rna_predict/utils/checkpoint.py`.
    *   **Function:**
        ```python
        # rna_predict/utils/checkpoint.py
        import torch
        import logging
        from collections import OrderedDict

        logger = logging.getLogger(__name__)

        def partial_load_state_dict(model: torch.nn.Module, state_dict: dict, strict: bool = False):
            """
            Loads parameters from state_dict into model, skipping mismatched keys
            and logging information about missing/unexpected keys.

            Args:
                model: The PyTorch model to load parameters into.
                state_dict: The dictionary containing parameters to load.
                strict: If True, raise an error for missing or unexpected keys (default: False).

            Returns:
                Tuple[List[str], List[str]]: missing_keys, unexpected_keys
            """
            missing_keys = []
            unexpected_keys = []
            error_msgs = []

            # Convert state_dict to OrderedDict if needed
            if not isinstance(state_dict, OrderedDict):
                 state_dict = OrderedDict(state_dict)

            metadata = getattr(state_dict, '_metadata', None)
            if metadata is not None:
                state_dict = state_dict.copy()
                state_dict._metadata = metadata

            own_state = model.state_dict()

            # Check for unexpected keys first if strict=False
            if not strict:
                for name in state_dict:
                    if name not in own_state:
                        unexpected_keys.append(name)

            # Load matching keys
            for name, param in state_dict.items():
                if name in own_state:
                    if isinstance(param, torch.nn.Parameter):
                        # backwards compatibility for serialized parameters
                        param = param.data
                    try:
                        own_state[name].copy_(param)
                    except Exception as e:
                        error_msgs.append(f'While copying the parameter named "{name}", '
                                          f'whose dimensions in the model are {own_state[name].size()} and '
                                          f'whose dimensions in the checkpoint are {param.size()}: {str(e)}')
                # If strict=True, unexpected keys are handled below

            # Check for missing keys
            loaded_keys = set(state_dict.keys())
            for name in own_state:
                if name not in loaded_keys:
                    missing_keys.append(name)

            # Handle strict mode errors
            if strict:
                 unexpected_keys = [k for k in state_dict if k not in own_state] # Recalculate for strict
                 if unexpected_keys:
                     error_msgs.insert(
                         0, 'Unexpected key(s) in state_dict: {}. '.format(
                             ', '.join(f'"{k}"' for k in unexpected_keys)))
                 if missing_keys:
                     error_msgs.insert(
                         0, 'Missing key(s) in state_dict: {}. '.format(
                             ', '.join(f'"{k}"' for k in missing_keys)))

            if error_msgs:
                raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
                                   model.__class__.__name__, "\n\t".join(error_msgs)))

            # Log warnings if not strict
            if not strict:
                 if unexpected_keys:
                     logger.warning(f"Unexpected key(s) in state_dict: {', '.join(unexpected_keys)}")
                 if missing_keys:
                     logger.warning(f"Missing key(s) in state_dict: {', '.join(missing_keys)}")

            logger.info(f"Loaded {len(own_state) - len(missing_keys)} keys from checkpoint into {model.__class__.__name__}.")
            return missing_keys, unexpected_keys # Return lists for inspection in tests
        ```
    *   **Unit Test:** Create `tests/unit/utils/test_checkpoint.py`.
        *   Define simple dummy `nn.Module`s (`BaseModel`, `AdapterModel` inheriting or composing `BaseModel`).
        *   Test loading full state into full model (`strict=True`).
        *   Test loading partial (adapter-only) state into full model (`strict=False` -> succeed, `strict=True` -> fail).
        *   Test loading state with extra keys (`strict=False` -> succeed with warning, `strict=True` -> fail).

2.  **Create Test Infrastructure:**
    *   **File:** Create `tests/integration/test_partial_checkpoint_cycle.py`.
    *   **Dummy Model:** Define within the test file:
        ```python
        # tests/integration/test_partial_checkpoint_cycle.py
        import torch
        import torch.nn as nn

        class DummyCheckpointModel(nn.Module):
            # Renamed to avoid potential conflicts if imported elsewhere
            def __init__(self, base_dim=16, adapter_dim=8):
                super().__init__()
                # Simulate a frozen base
                self.base_layer = nn.Linear(base_dim, base_dim)
                # Simulate a trainable adapter/head
                self.adapter_layer = nn.Linear(base_dim, adapter_dim)

                # Freeze base layer by default for the test's purpose
                for param in self.base_layer.parameters():
                    param.requires_grad = False

            def forward(self, x):
                x = self.base_layer(x)
                x = self.adapter_layer(x)
                return x

            def get_adapter_state_dict(self):
                # Gets ONLY the state dict of the part we intend to train/save partially
                return self.adapter_layer.state_dict()
        ```
    *   **Synthetic Data:**
        ```python
        # tests/integration/test_partial_checkpoint_cycle.py
        from torch.utils.data import TensorDataset, DataLoader

        def create_dummy_dataloader(batch_size=4, n_samples=12, base_dim=16, adapter_dim=8):
            X = torch.randn(n_samples, base_dim)
            y = torch.randn(n_samples, adapter_dim) # Target matches adapter output dim
            dataset = TensorDataset(X, y)
            return DataLoader(dataset, batch_size=batch_size)
        ```
    *   **Hydra Init (if needed):**
        ```python
        # tests/integration/test_partial_checkpoint_cycle.py
        import hydra

        def setup_hydra():
            # Only initialize if not already initialized (important for pytest)
            if not hydra.core.global_hydra.GlobalHydra.instance().is_initialized():
                hydra.initialize(config_path="/Users/tomriddle1/RNA_PREDICT/rna_predict/conf", version_base=None)
            # cfg = hydra.compose(config_name="test/your_test_config") # If needed
            # return cfg
        ```

3.  **Implement Integration Test Logic:**
    *   **File:** `tests/integration/test_partial_checkpoint_cycle.py`
    *   **Test Function (`test_train_save_partial_load_infer`):**
        *   **Setup:** Call `setup_hydra()`, create dataloader, instantiate `model = DummyCheckpointModel()`. Use `tmp_path` fixture.
        *   **Optimizer:** `optimizer = torch.optim.Adam(model.adapter_layer.parameters(), lr=1e-3)` (Crucially, optimize *only* adapter).
        *   **Manual Training Loop:**
            ```python
            model.train()
            criterion = nn.MSELoss()
            for epoch in range(2): # Train for 2 epochs
                for batch_x, batch_y in dataloader:
                    optimizer.zero_grad()
                    output = model(batch_x)
                    loss = criterion(output, batch_y)
                    loss.backward()
                    optimizer.step()
            ```
        *   **Save Partial Checkpoint:**
            ```python
            adapter_state_dict = model.get_adapter_state_dict()
            checkpoint = {
                'model_adapter_state_dict': adapter_state_dict,
                'optimizer_state_dict': optimizer.state_dict(),
                # Add epoch or other metadata if needed
            }
            partial_ckpt_path = tmp_path / "partial_adapter_checkpoint.pt"
            torch.save(checkpoint, partial_ckpt_path)
            ```
        *   **(Optional) Save Full State:** `full_ckpt_path = tmp_path / "full_model.pt"; torch.save(model.state_dict(), full_ckpt_path)`
        *   **Reload Stage:**
            ```python
            from rna_predict.utils.checkpoint import partial_load_state_dict # Import utility

            new_model = DummyCheckpointModel() # Fresh model instance
            loaded_checkpoint = torch.load(partial_ckpt_path)

            # Store initial weights for comparison
            initial_adapter_weight = new_model.adapter_layer.weight.clone().detach()
            initial_base_weight = new_model.base_layer.weight.clone().detach()

            missing, unexpected = partial_load_state_dict(
                new_model, loaded_checkpoint['model_adapter_state_dict'], strict=False
            )

            # Assert loading results
            assert not unexpected # Should be no keys in ckpt not in the full model structure
            assert all('base_layer' in k for k in missing) # Base layer keys should be missing
            ```
        *   **Inference & Assertions:**
            ```python
            new_model.eval()
            test_input = torch.randn(1, 16) # Use dims from model
            with torch.no_grad():
                inference_output = new_model(test_input)

            assert inference_output.shape == (1, 8) # Matches adapter_dim
            assert not torch.isnan(inference_output).any()
            assert not torch.isinf(inference_output).any()

            # Verify weights loaded correctly
            assert not torch.equal(new_model.adapter_layer.weight, initial_adapter_weight) # Adapter weights should have changed
            assert torch.equal(new_model.base_layer.weight, initial_base_weight) # Base weights should NOT have changed
            ```
        *   **(Optional) Size Check:**
            ```python
            partial_size = partial_ckpt_path.stat().st_size
            full_size = full_ckpt_path.stat().st_size
            assert partial_size < full_size
            print(f"Partial ckpt size: {partial_size}, Full ckpt size: {full_size}")
            ```

**Summary Table (Final RNA_PREDICT Context):**

| Step                | File/Location                                                         | Key Actions & Notes                                                              | Status   |
| :------------------ | :-------------------------------------------------------------------- | :------------------------------------------------------------------------------- | :------- |
| Implement Utility   | `rna_predict/utils/checkpoint.py`                                     | Create `partial_load_state_dict`.                                                | **TODO** |
| Unit Test Utility   | `tests/unit/utils/test_checkpoint.py`                                 | Test loader thoroughly with dummy modules/dicts.                                 | **TODO** |
| Create Test Infra   | `tests/integration/test_partial_checkpoint_cycle.py`                  | Define `DummyCheckpointModel`, `create_dummy_dataloader`, `setup_hydra`.        | **TODO** |
| Manual Training     | `tests/integration/test_partial_checkpoint_cycle.py`                  | Manual loop, optimize *only adapter* params.                                     | **TODO** |
| Save Partial Ckpt   | `tests/integration/test_partial_checkpoint_cycle.py`                  | Manually get *adapter* state dict, `torch.save`.                                 | **TODO** |
| Reload & Infer      | `tests/integration/test_partial_checkpoint_cycle.py`                  | Instantiate new model, call *implemented* `partial_load_state_dict`.           | **TODO** |
| Assert Correctness  | `tests/integration/test_partial_checkpoint_cycle.py`                  | Check load success, inference output, *verify which weights changed/didn't*.     | **TODO** |
| (Optional) Size Check| `tests/integration/test_partial_checkpoint_cycle.py`                  | Compare partial vs. full state dict file sizes.                                  | Optional |

This detailed, context-aware plan provides the necessary steps, code structure, and verification logic to implement and test the M2 partial checkpoint cycle within the specific environment of the `RNA_PREDICT` project. The next immediate step is to implement the `partial_load_state_dict` function and its unit tests.

# üßôüèæ‚Äç‚ôÇÔ∏è: [aligning on my goal] 
Certainly! Below is an updated ‚Äúdeep dive‚Äù analysis of your codebase, focusing on how to implement the M2 partial checkpoint cycle. I‚Äôve structured it around (1) your existing code architecture, (2) where partial checkpoint logic best fits, (3) how to design the dummy model and partial loader, and (4) an actionable step-by-step plan.

[emoji]: [actionable response]

---

## 1. Code Architecture Overview

### Model Definitions & Training Patterns
- **`torch.nn.Module`** is the primary inheritance pattern for your models (e.g. `StageBTorsionBertPredictor`, `PairformerWrapper`, `DiffusionModule`), not `pytorch_lightning.LightningModule`.
- **Training loops** in your code are mostly ‚Äúmanual‚Äù‚Äîscripts like `run_full_pipeline.py` or `run_stageB_combined` orchestrate forward passes but do not do full epoch-based training or checkpoint saving as PyTorch Lightning would.  

**Implication**: For the M2 partial checkpoint test, you‚Äôll likely create a **minimal** training loop (manual forward/backward/step) rather than using a Lightning `Trainer`.

### LoRA or Custom Trainable Modules
- Your config (`rna_predict/conf/model/stageB_torsion.yaml`, etc.) references `lora:` parameters, but the actual injection of LoRA modules into a base model (e.g. `peft.get_peft_model`) isn‚Äôt clearly implemented. 
- **Implication**: For the partial checkpoint cycle test, you need a **dummy** model that simulates ‚Äúfrozen base‚Äù + ‚Äútrainable adapter.‚Äù Then you only save the trainable adapter state to simulate ‚Äúpartial checkpoint.‚Äù

### Checkpointing
- **No dedicated partial loader**: The code has references to gradient checkpointing but not to saving or partial loading the model‚Äôs state. 
- **Implication**: You‚Äôll need to create something like `partial_load_state_dict(model, checkpoint_dict, strict=False)` in a new file, e.g. `rna_predict/utils/checkpoint.py`. That function will:
  1. Loop over `checkpoint_dict` keys 
  2. Compare them to `model.state_dict()` keys 
  3. Load the intersection 
  4. Possibly warn/ignore mismatches.

### Testing & Hydra
- You have **`tests/`** with subfolders for `integration/`, `unit/`, and stage-specific tests. 
- Hydra is used widely. If you need config parameters for the test, you can `hydra.initialize(config_path=...)` in your test. Otherwise, you might define everything inline for simplicity.

---

## 2. Where to Place Partial Checkpoint Logic

1. **New Utility**: 
   - File: `rna_predict/utils/checkpoint.py`
   - Function: `partial_load_state_dict(model: nn.Module, state_dict: dict, strict: bool = False) -> None`
   - Purpose: Let you load only certain matching parameters into the model, ignoring or warning about extras.

2. **Integration Test**:
   - File: `tests/integration/test_partial_checkpoint_cycle.py`
   - Purpose:  
     1. Build a **dummy** model with a ‚Äúfrozen base layer‚Äù + ‚Äútrainable adapter layer.‚Äù  
     2. Manually train for a couple steps (so the adapter changes).  
     3. Save only the adapter‚Äôs weights as a partial checkpoint.  
     4. Reload them into a fresh model.  
     5. Check that the base remains default, the adapter is updated, and inference works.

3. **Unit Test** (Optional):
   - File: `tests/unit/test_partial_load_state_dict.py`
   - Purpose: Validate `partial_load_state_dict(...)` in isolation with smaller scenario: missing keys, unexpected keys, etc.

---

## 3. Designing the Dummy Model & Minimal Training Loop

Below is a conceptual outline (not full code) of what the dummy model and minimal training loop might look like:

1. **Dummy Model**:
   ```python
   import torch
   import torch.nn as nn

   class DummyLoRAModel(nn.Module):
       def __init__(self):
           super().__init__()
           self.base_layer = nn.Linear(10, 10)     # Frozen
           self.adapter_layer = nn.Linear(10, 2)   # Trainable (like LoRA)

           # Freeze base layer
           for p in self.base_layer.parameters():
               p.requires_grad = False

       def forward(self, x):
           x = self.base_layer(x)
           return self.adapter_layer(x)

       def get_trainable_params(self):
           return self.adapter_layer.parameters()
   ```
   - The ‚Äúbase_layer‚Äù simulates ‚Äúfrozen base,‚Äù 
   - The ‚Äúadapter_layer‚Äù simulates ‚ÄúLoRA or new module.‚Äù

2. **Training Step** (in your integration test):
   ```python
   model = DummyLoRAModel()
   optimizer = torch.optim.Adam(model.get_trainable_params(), lr=1e-3)

   # Synthetic data
   X = torch.randn(4, 10)  # batch=4, input_dim=10
   Y = torch.randn(4, 2)   # match adapter_layer output dim=2
   loss_fn = nn.MSELoss()

   # Simple training loop
   for step in range(2):
       optimizer.zero_grad()
       preds = model(X)
       loss = loss_fn(preds, Y)
       loss.backward()
       optimizer.step()
   ```
   - After these 2 steps, the adapter layer‚Äôs weights have changed.  
   - Then you‚Äôd do `torch.save(...)` for the partial checkpoint.

3. **Saving Partial Checkpoint**:
   ```python
   # Only the adapter layer's weights
   partial_sd = model.adapter_layer.state_dict()
   torch.save(partial_sd, "partial_checkpoint.pt")
   ```
4. **Reloading**:
   ```python
   new_model = DummyLoRAModel()   # fresh instance
   loaded_sd = torch.load("partial_checkpoint.pt")
   partial_load_state_dict(new_model.adapter_layer, loaded_sd, strict=False)
   ```

---

## 4. Step-by-Step Implementation Plan

Below is the final recommended plan for the M2 partial checkpoint cycle:

1. **Create Partial Loader Utility**  
   - **File**: `rna_predict/utils/checkpoint.py`  
   - **Function**: 
     ```python
     def partial_load_state_dict(module: nn.Module, state_dict: dict, strict: bool = False):
         # Compare keys with module.state_dict().
         # Load only matching keys, skip or warn about mismatches.
         # Return any missing/unexpected keys if needed.
     ```
   - This should handle e.g. `module.adapter_layer.weight` and `module.adapter_layer.bias`.

2. **Unit Test** (Optional but recommended)  
   - **File**: `tests/unit/test_partial_load_state_dict.py`  
   - **Scenarios**:
     1. Perfect match.
     2. Missing keys in checkpoint.
     3. Extra keys in checkpoint.
     4. `strict=True` vs `strict=False` behavior.

3. **Integration Test**  
   - **File**: `tests/integration/test_partial_checkpoint_cycle.py`  
   - **Implementation**:
     1. **Define** `DummyLoRAModel`.
     2. **Create** a small synthetic dataset (like `(X, Y)` random).
     3. **Train** for a few steps (2‚Äì3) with only the adapter‚Äôs parameters in the optimizer.
     4. **Save** partial checkpoint with `adapter_layer.state_dict()`.
     5. **Reload** into a fresh `DummyLoRAModel` instance using `partial_load_state_dict(...)`.
     6. **Run** a forward pass on the new model, checking output shape is correct, no NaNs, etc.
     7. (Optional) Compare partial checkpoint file size to a full `model.state_dict()` to confirm it‚Äôs smaller.

4. **Hydra Usage**:
   - If needed for specifying device or hyperparameters, you can wrap the test in `hydra.initialize(...)`.  
   - Many times you can keep the test simple with inline definitions for the dataset, model, etc.

5. **Add Documentation**:
   - In `checkpoint.py`, docstring to explain partial load logic.  
   - In the integration test, short docstrings about the partial checkpoint approach for M2.

---

## 5. Potential Edge Cases & Notes

- **Key naming**: If your actual LoRA code modifies param names (e.g. ‚Äúbase_layer.lora_A‚Äù), your partial loader must handle that. For the dummy test, you can keep it straightforward.  
- **Multiple adapters**: If in future you have multiple adapters, you can save them each to separate partial checkpoints or combine them in one dictionary.  
- **Frozen base mismatch**: Ensure the base parameters remain identical to their initialization in the new model (the test can assert that).  

---

## Next Steps

1. **Implement** `partial_load_state_dict` in `rna_predict/utils/checkpoint.py`.
2. **Write** the integration test in `tests/integration/test_partial_checkpoint_cycle.py`.
3. **Optionally** add a unit test for partial loading edge cases.
4. **Validate** the M2 partial checkpoint cycle by running `pytest -k partial_checkpoint_cycle`.

**Outcome**: You‚Äôll have a minimal, workable demonstration of saving only the trainable adapter‚Äôs weights and reloading them into a fresh model instance‚Äîfulfilling the M2 partial checkpoint requirement.

---
[emoji]: 
Would you like any further clarifications or example code patterns? Feel free to let me know which section you‚Äôd like me to detail more.

Here‚Äôs a systematic, actionable plan for implementing and testing the M2 partial checkpoint cycle, based on the requirements and best practices you provided:

1. Dummy Model and LoRA/New Modules
Implement a minimal PyTorch (Lightning) model that includes at least one LoRA or new module to ensure non-triviality.
The model should be simple (e.g., a linear layer with LoRA adaptation) but must have trainable parameters.
2. Synthetic Data Loader
Create a DataLoader that yields synthetic (random or constant) data and labels.
Keep batch size and number of batches small for speed (e.g., batch size = 2, num_batches = 3).
3. Minimal Training Loop
Use PyTorch Lightning Trainer (or a minimal manual loop if Lightning is not available).
Train for a minimal number of steps/epochs (e.g., 1-2 steps).
Ensure at least one optimizer step is taken so parameters are updated.
Use ModelCheckpoint (or equivalent) to save checkpoints containing only trainable parameters and optimizer state.
4. Partial Checkpoint Loader
Implement or use an existing partial_load_state_dict function.
This function must:
Load only matching keys from the checkpoint into the model.
Gracefully handle missing/unexpected keys (no crash).
Write a unit test with dummy modules and state dicts to verify this logic.
5. Integration Test: Checkpoint Cycle
Steps:
Train the dummy model on synthetic data and save a checkpoint.
Instantiate a new model instance.
Load the checkpoint using partial_load_state_dict.
Run inference on synthetic data.
Assert:
No errors/crashes.
Output shape is correct.
Output is not all-zeros or NaNs.
Manually or programmatically check that the checkpoint file is smaller than a full model checkpoint (if possible).
6. Test Coverage
Ensure both:
Unit test for the partial loader (dummy modules/state dicts).
Integration test for the full cycle (train ‚Üí save ‚Üí reload ‚Üí inference).
7. Hydra/Config
If config is needed, use minimal, hardcoded or Hydra-based config as appropriate.
Avoid unnecessary dependencies‚Äîkeep the test self-contained.
8. Documentation and Manual Inspection
Add docstrings/comments explaining the test logic.
Optionally print/log checkpoint file size for manual inspection.
Summary Table
| Step | What to Implement/Test | Notes | |---------------------|--------------------------------------------------------|-------------------------------------------| | Dummy Model | Minimal Lightning model with LoRA/new module | Should have trainable params | | Synthetic Data | DataLoader with random/constant data | Small batch/epoch for speed | | Training Loop | Minimal training to produce checkpoint | Use ModelCheckpoint | | Partial Loader | partial_load_state_dict + unit test | Handles missing/unexpected keys | | Integration Test | Full cycle: train ‚Üí save ‚Üí reload ‚Üí inference | Check shape, no crash, no NaNs/all-zeros | | Checkpoint Size | Optional: assert/print that checkpoint is ‚Äúpartial‚Äù | Manual or programmatic check | | Documentation | Comments/docstrings/logs | For clarity and reproducibility |

Next Steps
Implement the dummy model and LoRA/new module.
Write the synthetic DataLoader.
Set up the minimal training loop with checkpoint saving.
Implement and unit test the partial loader.
Write the integration test for the checkpoint cycle.
Add documentation and (optionally) checkpoint size check.
If you‚Äôd like, I can scaffold the code structure or write the first component for you‚Äîjust let me know which step you want to start with!
To adapt the M2 partial checkpoint cycle plan to your current code structure, here‚Äôs a systematic approach tailored to the RNA_PREDICT project:

1. Understand Current Code Structure and Integration Points
Model Location: Your main models and modules (including LoRA or custom modules) are likely under rna_predict/pipeline/ (e.g., StageB, StageD).
Training/Inference Entrypoints: Training and inference are typically managed by scripts like run_all_pipeline.py or stage-specific scripts (e.g., run_stageD.py).
Config Management: You use Hydra for configuration, with schemas in rna_predict/conf/config_schema.py and configs in rna_predict/conf/.
Testing: Tests are organized in the test/ directory, following best practices for modular and integration testing.
2. Where to Place the Partial Checkpoint Cycle Test
Integration Test:
Place the full-cycle integration test in test/integration/ (or create this directory if it doesn‚Äôt exist).
Name it something like test_partial_checkpoint_cycle.py.
Unit Test for Loader:
Place the partial loader unit test in test/unit/ or alongside the loader implementation if it‚Äôs in a utility module.
Dummy Model/Data:
Define these within the test file itself or in a test/utils/ module to avoid polluting production code.
3. How to Use Existing Infrastructure
Hydra:
Use the correct config path (/Users/tomriddle1/RNA_PREDICT/rna_predict/conf) for all initialization in tests.
Create a minimal or test-specific Hydra config if needed.
Lightning/Training Loop:
If you have Lightning modules, subclass a minimal one for the dummy model.
Use the same checkpointing mechanism as the main training loop (ModelCheckpoint or custom).
Partial Loader:
If you have an existing partial_load_state_dict, import and use it. Otherwise, implement it in a utility module (e.g., rna_predict/utils/checkpoint_utils.py).
4. Minimize Coupling and Keep Tests Self-Contained
Avoid using full pipeline scripts (like run_all_pipeline.py) for this test‚Äîkeep the test focused and isolated.
Use synthetic data and a minimal model defined within the test, not production data/models.
Mock or stub Hydra configs as needed, but ensure correct initialization.
5. Proposed Directory and File Structure
CopyInsert
rna_predict/
  pipeline/
    ... (existing code)
  conf/
    ... (existing Hydra configs)
  utils/
    checkpoint_utils.py  # (if partial_load_state_dict is here)
test/
  integration/
    test_partial_checkpoint_cycle.py
  unit/
    test_partial_load_state_dict.py
  utils/
    dummy_model.py
    dummy_data.py
6. Example: Integration Test Skeleton
python
CopyInsert
# test/integration/test_partial_checkpoint_cycle.py

import torch
import pytorch_lightning as pl
from rna_predict.utils.checkpoint_utils import partial_load_state_dict

# Dummy model and dataloader defined here or imported from test/utils

def test_partial_checkpoint_cycle(tmp_path):
    # 1. Instantiate dummy model
    # 2. Create synthetic dataloader
    # 3. Train for a few steps, save checkpoint to tmp_path
    # 4. Instantiate new model, load with partial_load_state_dict
    # 5. Run inference, assert output shape, no NaNs/all-zeros
    # 6. (Optional) Check checkpoint file size
    pass
7. Hydra Integration in Tests
Use the absolute config path for Hydra initialization in tests:
python
CopyInsert
with hydra.initialize(config_path="/Users/tomriddle1/RNA_PREDICT/rna_predict/conf", version_base=None):
    cfg = hydra.compose(config_name="test_config")
Create a minimal config file for the test if needed.
8. Next Steps
Confirm where your partial_load_state_dict lives (or if it needs to be implemented).
Set up the test directory structure if not present.
Scaffold the dummy model and data loader in test utils or directly in the integration test.
Implement the integration and unit tests as outlined.
Would you like me to:

Locate or scaffold the partial_load_state_dict utility?
Scaffold the integration test file and dummy model/data?
Suggest a minimal test Hydra config?
Let me know your preferred starting point!

Feedback submitted
Me

can you actually do a deep dive into the code and show me your updated analysis similar to what you just made

Absolutely! I‚Äôll do a deep dive into your codebase to provide a concrete, context-aware analysis for implementing the M2 partial checkpoint cycle. I‚Äôll focus on:

Where models, training loops, and checkpoints are implemented
Where/how LoRA or custom modules are integrated
How configuration and Hydra are used
Where checkpoint saving/loading happens
What test and utility infrastructure exists
How/where to best add the integration/unit tests for the checkpoint cycle
Let‚Äôs begin by mapping out the relevant code structure and identifying key implementation points. I‚Äôll start by searching for model definitions, checkpoint logic, LoRA usage, and test infrastructure.

I will:

Search for model and LightningModule definitions.
Locate checkpoint saving/loading code.
Identify any LoRA or ‚Äúnew module‚Äù integrations.
Examine config/Hydra usage.
Map the test directory and existing test patterns/utilities.
Let‚Äôs start with these searches.

Semantic searched codebase
669 results
Semantic searchedrna_predict/pipelinewith queryLightningModule
Semantic searched codebase
755 results
Semantic searchedrna_predict/pipelinewith querycheckpoint
Semantic searched codebase
780 results
Semantic searchedrna_predict/pipelinewith queryLoRA
Semantic searched codebase
775 results
Semantic searchedrna_predict/pipelinewith queryhydra
Searched filesystem
2 results
Searchedfordirectoriesin~/RNA_PREDICTwith depth3with patterntest*
Searched filesystem
99 results
Searchedforfilesintestswith depth3with pattern*.py
Searched filesystem
0 results
Searchedforfilesinrna_predict/utilswith depth2with patterncheckpoint*
Here is a deep, codebase-specific analysis for implementing the M2 partial checkpoint cycle in the context of RNA_PREDICT, based on the latest code structure and findings:

1. Model and Training Loop Structure
Model Definitions:
Most models are implemented as nn.Module (not PyTorch LightningModule), e.g. PairformerStack, PairformerBlock, PairformerWrapper (in stageB/pairwise/), and various predictors.
There is a DummyPairformerModel in stageB/pairwise/dummy_pairformer.py for testing.
No direct evidence of LightningModule usage for main pipeline models, but some test and utility code may use Lightning for convenience.
Training Loops:
Training and inference are managed by scripts like run_stageB_combined, run_pipeline (in stageB/main.py), and run_stageD (in stageD/run_stageD.py).
These functions are not Lightning-native but could be adapted for minimal training/testing.
2. Checkpointing Logic
Checkpoint Saving/Loading:
No dedicated checkpoint_utils.py or clear utility for partial loading in rna_predict/utils/.
Checkpointing is referenced in some configs and model wrappers (e.g. use_checkpoint in PairformerWrapper), but actual save/load logic is not centralized.
No existing partial_load_state_dict found‚Äîthis will need to be implemented, likely in a new utility file (e.g., rna_predict/utils/checkpoint.py).
3. LoRA/Adapter Modules
LoRA Integration:
No explicit LoRA modules found in the main pipeline code.
The test plan should use a dummy model with a simple trainable submodule to simulate LoRA/new module behavior.
4. Hydra Configuration
Hydra Usage:
All main scripts and many tests use Hydra, with configs in /Users/tomriddle1/RNA_PREDICT/rna_predict/conf.
For all tests and initialization, always use the absolute config path (/Users/tomriddle1/RNA_PREDICT/rna_predict/conf).
Test configs exist under conf/test/.
5. Test Infrastructure
Test Directory:
Main test directory is tests/ (not test/), with subfolders for integration/, unit/, and stage-specific tests.
Integration tests like integration/test_full_pipeline.py and integration/test_pipeline_integration.py exist.
Dummy models and data are sometimes defined in test files or in tests/common/ or tests/utils/.
6. Proposed Implementation Plan (RNA_PREDICT-Specific)
A. Utility Implementation
Create rna_predict/utils/checkpoint.py with:
partial_load_state_dict (loads only matching keys, ignores missing/unexpected).
Unit test for this function in tests/unit/test_partial_load_state_dict.py.
B. Dummy Model and Data
Define a minimal nn.Module (with a dummy trainable submodule, simulating LoRA) in the integration test or in tests/utils/dummy_model.py.
Use a synthetic DataLoader (random tensors).
C. Minimal Training Loop
In the integration test (tests/integration/test_partial_checkpoint_cycle.py):
Instantiate the dummy model.
Run a minimal manual training loop (since main models are not LightningModules).
Save a checkpoint (using torch.save(model.state_dict(), ...) and torch.save(optimizer.state_dict(), ...)).
D. Partial Checkpoint Cycle Test
In the same integration test:
Instantiate a new dummy model.
Load checkpoint using partial_load_state_dict.
Run inference, check output shape, ensure no all-zeros/NaNs.
Optionally, compare checkpoint file sizes.
E. Hydra Integration
Use hydra.initialize(config_path="/Users/tomriddle1/RNA_PREDICT/rna_predict/conf", ...) in all tests/configs.
If needed, create a minimal config in conf/test/.
7. Directory and File Structure
CopyInsert
rna_predict/
  utils/
    checkpoint.py  # (new, for partial_load_state_dict)
tests/
  integration/
    test_partial_checkpoint_cycle.py
  unit/
    test_partial_load_state_dict.py
  utils/
    dummy_model.py  # (if needed)
8. Summary Table (RNA_PREDICT Context)
| Step | File/Location | Notes | |---------------------|----------------------------------------------------|---------------------------------| | Partial Loader | utils/checkpoint.py, unit/test_partial_load_state_dict.py | New utility + unit test | | Dummy Model/Data | integration/test_partial_checkpoint_cycle.py or tests/utils/dummy_model.py | Use nn.Module, not Lightning | | Training Loop | integration/test_partial_checkpoint_cycle.py | Manual loop, torch.save | | Checkpoint Cycle | integration/test_partial_checkpoint_cycle.py | Save, partial load, infer, check| | Hydra Config | Use absolute path in all tests | Use conf/test/ if needed |

9. Next Steps
Implement partial_load_state_dict in rna_predict/utils/checkpoint.py.
Write its unit test in tests/unit/test_partial_load_state_dict.py.
Scaffold the integration test in tests/integration/test_partial_checkpoint_cycle.py using a dummy model and manual training loop.
Use Hydra with the correct config path in all test code.
Would you like me to scaffold any of these files or show concrete code snippets for any step? Let me know your preferred next step!

# Task Master Data for M2 Partial Checkpoint Cycle (as of 2025-04-21)

## Project Dashboard
- Tasks Progress: 60% (25 done, 17 pending)
- Subtasks Progress: 43% (15/35 completed)
- High priority: 14, Medium: 23, Low: 5

## Next Task to Work On
- **ID 25**: M2-Test Verify Partial Checkpoint Cycle (high priority)
- Dependencies: 40

## Task #25 - M2-Test Verify Partial Checkpoint Cycle
**Description:**
We save only trainable parameters (LoRA + new modules). This test ensures partial checkpoints reload cleanly without missing/unexpected key errors and that inference still works.

### Tasks
1. **New test**: `tests/integration/test_checkpoint_cycle.py`.
2. **Dummy training**: A few steps, produce partial checkpoint.
3. **Reload**: Use `partial_load_state_dict(...)` in a fresh model.
4. **Inference**: Confirm no key mismatch, shape is correct.

### Acceptance Criteria
- Partial checkpoint is smaller than a full model.
- Reload is error-free.
- Forward pass works post-load.

### Potential Problems
- Lightning‚Äôs default checkpoint might still save the entire state.
- Param naming differences can cause mismatch warnings.
- Distinguishing intentionally excluded vs. truly missing keys can be tricky.

### Test Strategy
In `test_checkpoint_cycle.py`, train a minimal model for a few steps, save partial checkpoint, reload into a fresh model, run forward pass to confirm no shape mismatch or missing keys.

### Subtasks
| ID    | Status   | Title                                 | Dependencies        |
|-------|----------|---------------------------------------|---------------------|
| 25.1  | pending  | Create Test Infrastructure and Dummy Model | None           |
| 25.2  | pending  | Implement Partial Checkpoint Generation    | 25.1           |
| 25.3  | pending  | Implement Partial State Dict Loading       | 25.1, 25.2     |
| 25.4  | pending  | Validate Model Inference Post-Loading      | 25.1, 25.2, 25.3|

Subtask Progress: 0/4 (0%)

### Suggested Actions
- Mark as in-progress: `task-master set-status --id=25 --status=in-progress`
- Mark as done when completed: `task-master set-status --id=25 --status=done`
- Update subtask status: `task-master set-status --id=25.1 --status=done`

---

*This section was generated by exporting current Task Master data for the M2 partial checkpoint cycle feature. For updates, rerun `task-master list` and `task-master show 25`.*