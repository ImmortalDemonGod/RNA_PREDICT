
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Lora - rna_predict</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#low-rank-adaptation-lora-for-transformers-in-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="rna_predict" class="md-header__button md-logo" aria-label="rna_predict" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            rna_predict
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Lora
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="rna_predict" class="md-nav__button md-logo" aria-label="rna_predict" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    rna_predict
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Guides
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Guides
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Getting Started
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Getting Started
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/getting_started/windows_compatibility_plan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Windows Compatibility
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Best Practices
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Best Practices
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/best_practices/code_quality_best_practices/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code Quality
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2_2" id="__nav_2_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Testing
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Testing
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/best_practices/testing/test_coverage_strategy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Coverage Strategy
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/best_practices/testing/progressive_coverage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Progressive Coverage
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/best_practices/testing/test_generation_prompt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Test Generation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_2_3" id="__nav_2_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Debugging
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Debugging
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../guides/best_practices/debugging/comprehensive_debugging_guide/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Comprehensive Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Pipeline
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Pipeline
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Overview
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/overview/core_framework/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Core Framework
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/overview/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Comprehensive Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/overview/full_pipeline_specification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Full Specification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/overview/Multi_Stage_Implementation_Plan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multi-Stage Plan
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Integration
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Integration
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/full_pipeline_specification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Full Specification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_3" >
        
          
          <label class="md-nav__link" for="__nav_3_2_3" id="__nav_3_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Hydra Integration
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Hydra Integration
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/hydra_tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/hydra_integration_gap_analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gap Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/hydra_integration_master_document/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Master Document
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_2_3_4" id="__nav_3_2_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Components
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_2_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_3_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Components
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stage A
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stage B
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stage C
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stage D
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Unified Latent
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage A
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage A
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageA/StageA_RFold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageA/RFold_code/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RFold Code
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageA/RFold_paper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RFold Paper
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageA/rfold-demo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Demo
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageA/stage_a_extra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Extra Notes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage B
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage B
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageB/Stage_B/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_2" >
        
          
          <label class="md-nav__link" for="__nav_3_4_2" id="__nav_3_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TorsionBERT
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    TorsionBERT
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageB/torsionBert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageB/torsionbert_code/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageB/torsionBert_full_paper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Paper
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage C
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage C
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageC/Stage_C/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageC/Integrated_RNA_Geometry_and_3D_Reconstruction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Geometry & Reconstruction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageC/Unified, Comprehensive Plan for Integrating MP-NeRF into Stage C.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MP-NeRF Integration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/stageC/mp_nerf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MP-NeRF Details
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Unified Latent
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Unified Latent
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/unified_latent/Perceiver_IO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Perceiver IO
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/Energy_Minimization_%26_Molecular_Dynamics_%28MD%29_for_RNA_Structure_Refinement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Energy & MD
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Testing
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Testing
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/test_time_scaling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Time Scaling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Kaggle
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    Kaggle
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/kaggle_info/kaggle_competition/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Competition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pipeline/kaggle_info/M2_Plan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    M2 Plan
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced Methods
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced Methods
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    AlphaFold 3
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    AlphaFold 3
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../af3/AF3_paper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Paper
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../af3/AlphaFold3_progress/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Progress
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../af3/Pairwise_Distance_Based_Prediction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Distance Prediction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_2" >
        
          
          <label class="md-nav__link" for="__nav_4_1_2" id="__nav_4_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Diffusion
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Diffusion
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../diffusion/s4_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    S4 Diffusion
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../diffusion/test_time_scaling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Time Scaling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../isosteric_substitutions/RNA_isostericity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Isosteric Substitutions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    External Literature
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    External Literature
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../external_lit/2d_structure_prediction_papers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2D Structure
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../external_lit/RNA_papers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RNA Papers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Residue Atom Bridging
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Residue Atom Bridging
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/audit_report/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Audit Report
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/design_spec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Design Spec
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/documentation_draft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Documentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/implementation_notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/refactoring_plan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Refactoring
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../residue_atom_bridging/bridging_caveats_guidelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Guidelines
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
        
          
          <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Torsion Calculations
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Torsion Calculations
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torsion_calculations/torsion_angles_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torsion_calculations/Standard_Bond_Lengths_and_Angles_in_RNA_Nucleotides/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Bond Lengths & Angles
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torsion_calculations/Torsion_Angles_3D_Coordinates_RNA_Structure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3D Coordinates
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../torsion_calculations/torsion_angle_Latent_Manifold_Representation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Latent Manifold
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-lora-and-why-use-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        What is LoRA and Why Use It?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-lora-works-low-rank-adaptation-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      
        How LoRA Works (Low-Rank Adaptation Mechanism)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#popular-libraries-for-lora-in-pytorch-training-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        Popular Libraries for LoRA in PyTorch (Training &amp; Inference)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-workflow-with-lora" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Workflow with LoRA
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-workflow-with-lora" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inference Workflow with LoRA
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-and-tips-for-using-lora" class="md-nav__link">
    <span class="md-ellipsis">
      
        Best Practices and Tips for Using LoRA
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<p>Great. Ill look into the most up-to-date libraries and practices for implementing LoRA (Low-Rank Adaptation) with transformer models in PyTorch. This will include training and inference workflows, CPU/GPU support, and integration with widely used libraries such as Hugging Face and PEFT.</p>
<p>Ill get back to you shortly with a breakdown of the foundational concepts, recommended libraries, code examples, and best practices.</p>
<h1 id="low-rank-adaptation-lora-for-transformers-in-pytorch">Low-Rank Adaptation (LoRA) for Transformers in PyTorch</h1>
<h2 id="what-is-lora-and-why-use-it">What is LoRA and Why Use It?</h2>
<p><strong>Low-Rank Adaptation (LoRA)</strong> is a technique for efficiently fine-tuning large neural networks  especially Transformer models  by introducing a small number of trainable parameters instead of updating the entire model. In the standard fine-tuning of a transformer like GPT-3 or BERT, all model weights (potentially billions of parameters) are adjusted, which is memory-intensive and slow. LoRA addresses this by <strong>freezing the original pretrained weights and injecting small low-rank weight matrices (adapters) into each layer</strong> of the Transformer (<a href="https://arxiv.org/abs/2106.09685#:~:text=example%20,trainable%20parameters%2C%20a%20higher%20training">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=Instead%20of%20retraining%20the%20whole,match%20the%20desired%20use%20case">What is LoRA (Low-Rank Adaption)? | IBM</a>). These adapters represent the weight updates in a compressed form. During training, only the low-rank adapter weights are updated, which drastically reduces the number of trainable parameters and required GPU memory:</p>
<ul>
<li><strong>Dramatic Reduction in Trainable Parameters:</strong> For example, applying LoRA to GPT-3 (175B parameters) can reduce the number of trainable parameters by up to <strong>10,000</strong> (from 175 billion to only millions) and cut memory usage by ~3 (<a href="https://arxiv.org/abs/2106.09685#:~:text=example%20,trainable%20parameters%2C%20a%20higher%20training">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>). A full fine-tune of GPT-3 would adjust all 175B parameters, but LoRA can tune roughly 18M parameters (rank-dependent) for an equivalent adaptation (<a href="https://www.ibm.com/think/topics/lora#:~:text=way%20to%20adapt%20the%20model,without%20retraining%20it">What is LoRA (Low-Rank Adaption)? | IBM</a>). This enables fine-tuning huge models on much smaller hardware.</li>
<li><strong>Minimal Impact on Performance:</strong> Despite updating far fewer parameters, LoRA-based fine-tuning achieves on-par (or even better) model quality compared to full fine-tuning on a variety of models (RoBERTa, GPT-2, GPT-3, etc.) (<a href="https://arxiv.org/abs/2106.09685#:~:text=which%20freezes%20the%20pre,We%20also">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>). It retains model accuracy while being more efficient.</li>
<li><strong>No Added Inference Latency:</strong> Unlike some other adapter methods that add extra network layers, LoRA doesnt slow down inference. The low-rank updates can be merged into the original weights for deployment, so the model runs just as fast as the fully fine-tuned model (<a href="https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,trainable%20parameters%2C%20a%20higher%20training">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=much%20smaller%20low">What is LoRA (Low-Rank Adaption)? | IBM</a>). By design, LoRAs adjustments are linear and can be integrated into the existing layers without additional computational steps at inference time.</li>
<li><strong>Reusable Base Models &amp; Modular Adaptation:</strong> A single frozen pretrained model can support many LoRA adapters for different tasks. You can keep the original model weights fixed and <strong>swap in different LoRA weight modules</strong> for each new task or domain (<a href="https://www.ibm.com/think/topics/lora#:~:text=One%20of%20the%20key%20advantages,tuning">What is LoRA (Low-Rank Adaption)? | IBM</a>). This means you dont need to maintain multiple full copies of a model for different fine-tuned variants  you just maintain small adapter files (often only tens of megabytes each) for each task, which is far more storage-efficient (<a href="https://github.com/huggingface/peft#:~:text=The%20bigscience%2FT0_3B%20model%20performance%20isn%27t,PEFT%20in%20this%20blog%20post">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>).</li>
</ul>
<p>In summary, LoRA is a form of <strong>parameter-efficient fine-tuning</strong> that injects a few trainable parameters per layer to achieve results comparable to traditional fine-tuning, with massive savings in compute and memory (<a href="https://arxiv.org/abs/2106.09685#:~:text=example%20,trainable%20parameters%2C%20a%20higher%20training">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>) (<a href="https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,share%2C%20store%2C%20and%20load%20them">PEFT</a>). This method has become popular for adapting large language models (LLMs) and other transformer-based models on consumer-grade hardware.</p>
<h2 id="how-lora-works-low-rank-adaptation-mechanism">How LoRA Works (Low-Rank Adaptation Mechanism)</h2>
<p>LoRA is based on the idea that the changes needed to fine-tune a large model lie in a <strong>low-dimensional subspace</strong>. In practical terms, LoRA represents the weight updates for a given layer as a <strong>low-rank decomposition</strong>. Consider a weight matrix <span class="arithmatex">\(W\)</span> (from the pretrained model) of size <span class="arithmatex">\(d \times k\)</span> in a Transformer layer. Standard fine-tuning would compute an update matrix <span class="arithmatex">\(W\)</span> of the same size. LoRA instead assumes <strong><span class="arithmatex">\(W = A \times B\)</span></strong>, where <span class="arithmatex">\(A\)</span> is a <span class="arithmatex">\(d \times r\)</span> matrix and <span class="arithmatex">\(B\)</span> is an <span class="arithmatex">\(r \times k\)</span> matrix with <span class="arithmatex">\(r \ll \min(d,k)\)</span> (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=The%20LoRA%20method%20proposed%20by,shown%20in%20the%20figure%20below">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=Instead%20of%20retraining%20the%20whole,match%20the%20desired%20use%20case">What is LoRA (Low-Rank Adaption)? | IBM</a>). The rank <span class="arithmatex">\(r\)</span> is a tunable hyperparameter (e.g. 4, 8, 16) that controls the dimensionality of the adaptation. During training, <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> are the <em>only</em> weights that get updated (while the original <span class="arithmatex">\(W\)</span> remains frozen).</p>
<p>(<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>) <em>Visualization of weight updates in regular fine-tuning vs. LoRA. In LoRA (right), the full weight update matrix W is approximated by the product of two low-rank matrices <span class="arithmatex">\(B\)</span> and <span class="arithmatex">\(A\)</span> (of inner dimension <span class="arithmatex">\(r\)</span>). These low-rank matrices are learned during fine-tuning instead of modifying the large pretrained weight <span class="arithmatex">\(W\)</span> (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=The%20LoRA%20method%20proposed%20by,shown%20in%20the%20figure%20below">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>).</em></p>
<p>At inference time, the pretrained weight <span class="arithmatex">\(W\)</span> and the learned LoRA matrices <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(B\)</span> are combined to get the adapted weight <span class="arithmatex">\(W_{\text{adapted}} = W + \Delta W = W + A B\)</span>. In practice, the models forward pass for a LoRA-augmented layer computes:</p>
<div class="arithmatex">\[\text{output} = W \cdot x + (A B) \cdot x,\]</div>
<p>where <span class="arithmatex">\(x\)</span> is the input to that layer. Because <span class="arithmatex">\(A B x = A (B x)\)</span>, this can be implemented efficiently by first projecting <span class="arithmatex">\(x\)</span> into a smaller <span class="arithmatex">\(r\)</span>-dimensional space (with <span class="arithmatex">\(B\)</span>) and then back to <span class="arithmatex">\(d\)</span> (with <span class="arithmatex">\(A\)</span>), and adding it to the original <span class="arithmatex">\(W x\)</span> output. The low-rank bottleneck (dimension <span class="arithmatex">\(r\)</span>) ensures the update has far fewer parameters than <span class="arithmatex">\(W\)</span> itself. In effect, LoRA <strong>adds a small trainable branch to each weight matrix's output</strong> that nudges the layer's behavior, instead of changing the main weight directly (<a href="https://www.ibm.com/think/topics/lora#:~:text=Instead%20of%20retraining%20the%20whole,match%20the%20desired%20use%20case">What is LoRA (Low-Rank Adaption)? | IBM</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=LoRA%20adds%20low,computing%20power%20and%20training%20time">What is LoRA (Low-Rank Adaption)? | IBM</a>). Key points about how LoRA works:</p>
<ul>
<li><strong>LoRA injection points:</strong> LoRA adapters are typically applied to the <em>linear projection layers</em> inside transformer architectures. For example, in a self-attention block, one might add LoRA adapters to the query and value projection matrices (as was common in some LoRA fine-tuning setups) or to <em>all</em> attention and feed-forward linear layers. The choice of which modules to target (often specified by name, e.g. all <code>"q_proj"</code> and <code>"v_proj"</code> layers in the model) is a hyperparameter (<a href="https://www.ibm.com/think/topics/lora#:~:text=results%20in%20smaller%20update%20matrices,with%20fewer%20trainable%20parameters">What is LoRA (Low-Rank Adaption)? | IBM</a>). Targeting more layers can improve adaptability at the cost of more trainable parameters. (Originally, LoRA was demonstrated mainly on attention layers, but recent practice suggests applying LoRA broadly across many or all transformer layers yields the best performance (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=6,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>).)</li>
<li><strong>Rank (<span class="arithmatex">\(r\)</span>) and Scaling (<span class="arithmatex">\(\alpha\)</span>):</strong> The rank <span class="arithmatex">\(r\)</span> controls the size of the LoRA adapters. A higher <span class="arithmatex">\(r\)</span> means more capacity to learn complex adaptations, but also more parameters to train. A common setting is <span class="arithmatex">\(r=8\)</span> or <span class="arithmatex">\(r=16\)</span> for large models, though it can vary. LoRA also often uses a scaling factor <span class="arithmatex">\(\alpha\)</span> (sometimes called <code>lora_alpha</code> in code) to adjust the magnitude of the LoRA update. In many implementations, the actual update added is scaled as <span class="arithmatex">\((\alpha/r) \cdot (A B x)\)</span>  effectively, <span class="arithmatex">\(\alpha\)</span> controls an initial weight for the LoRA branch. A heuristic is to set <span class="arithmatex">\(\alpha\)</span> to about twice the rank (e.g. if <span class="arithmatex">\(r=8\)</span>, <span class="arithmatex">\(\alpha=16\)</span>) (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=just%20to%20the%20Key%20and,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>), although this can be tuned. This scaling helps ensure the LoRA update is initially small and grows to the right scale during training.</li>
<li><strong>LoRA Dropout:</strong> Some implementations include a dropout on the LoRA branch (e.g. <code>lora_dropout</code> probability) (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=r%3D16%2C%20lora_alpha%3D32%2C%20target_modules%3D%5B,model">Efficient Large Language Model training with LoRA and Hugging Face</a>). This means during training, the LoRA adapter is sometimes stochastically disabled for regularization. LoRA dropout can help prevent overfitting of the small adapter, especially if <span class="arithmatex">\(r\)</span> is relatively large or the fine-tuning dataset is small.</li>
<li><strong>Merging and Unmerging:</strong> Because <span class="arithmatex">\(W\)</span> remains unchanged and all adjustments are in <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span>, one can choose after training to <strong>merge</strong> the LoRA weights into <span class="arithmatex">\(W\)</span> (simply by computing <span class="arithmatex">\(W \leftarrow W + A B\)</span> once) for deployment, or keep them separate. Merging produces a standalone model identical in inference behavior to using LoRA, whereas keeping them separate allows flexibility to swap adapters. Merging is straightforward since its just matrix addition  and doing so results in a model that is equivalent to fully fine-tuned weights (<a href="https://www.ibm.com/think/topics/lora#:~:text=The%20diagram%20shows%20how%20LoRA,weights%20of%20the%20pretrained%20model">What is LoRA (Low-Rank Adaption)? | IBM</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=LoRA%20adds%20low,computing%20power%20and%20training%20time">What is LoRA (Low-Rank Adaption)? | IBM</a>).</li>
</ul>
<p>LoRA builds on the insight that large neural networks are often highly over-parameterized (many weights are redundant). By training only a small low-rank subset of weights, we exploit the models inherent low-dimensional structure (<a href="https://www.ibm.com/think/topics/lora#:~:text=LoRA%20is%20built%20on%20the,the%20adaptation%20process%20more%20efficient">What is LoRA (Low-Rank Adaption)? | IBM</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=While%20LoRA%20provides%20a%20significant,There%20is%20redundancy%2C%20robustness%20and">What is LoRA (Low-Rank Adaption)? | IBM</a>). This not only saves memory and compute, but can even act as a form of regularization  the model cant overfit too badly when it only has a few million parameters to tweak, though one must still be mindful of overfitting on small data (e.g., multiple epochs on a small dataset can still overfit even with LoRA). The rank <span class="arithmatex">\(r\)</span> may need to be adjusted depending on task complexity: too low a rank might under-fit (losing some information by oversimplifying the weight update), while too high a rank diminishes the efficiency gains and could overfit (<a href="https://www.ibm.com/think/topics/lora#:~:text=While%20LoRA%20provides%20a%20significant,There%20is%20redundancy%2C%20robustness%20and">What is LoRA (Low-Rank Adaption)? | IBM</a>). In practice, values in the single digits to a few tens are common, and one can start with a moderate rank (like 8 or 16) and tune from there.</p>
<h2 id="popular-libraries-for-lora-in-pytorch-training-inference">Popular Libraries for LoRA in PyTorch (Training &amp; Inference)</h2>
<p>LoRA was first introduced in 2021 alongside an open-source package called <strong><code>loralib</code></strong> (by Microsoft) that implemented LoRA layers for PyTorch (<a href="https://github.com/microsoft/LoRA#:~:text=This%20repo%20contains%20the%20source,a%20detailed%20description%20of%20LoRA">GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"</a>). However, the most widely adopted tools for using LoRA today are part of the Hugging Face ecosystem, which provides high-level integration for transformers models. Below are the main libraries and frameworks used for LoRA with PyTorch:</p>
<ul>
<li><strong>Hugging Face Transformers + PEFT:</strong> Hugging Faces Transformers library (for model architectures and weights) combined with the <strong>PEFT</strong> library (<code>peft</code>  <em>Parameter-Efficient Fine-Tuning</em>) is the de facto standard for using LoRA in 2024/2025. The PEFT library includes LoRA as one of its methods, along with other techniques like prefix-tuning and prompt tuning (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=Quick%20intro%3A%20PEFT%20or%20Parameter,tuning">Efficient Large Language Model training with LoRA and Hugging Face</a>). Using PEFT, applying LoRA to a model is very straightforward  you load a pretrained transformer model with  Transformers, then wrap it with a LoRA configuration via PEFT. For example, with PEFT one can do:</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/mt0-large&quot;</span><span class="p">)</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_2_SEQ_LM</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
</code></pre></div>
<p>This would add LoRA adapters of rank 8 to the seq2seq model and freeze the rest. The <code>print_trainable_parameters()</code> method would show that only a tiny fraction of parameters (on the order of 0.10.2%) are now trainabl (<a href="https://github.com/huggingface/peft#:~:text=model%20%3D%20AutoModelForSeq2SeqLM,0.19151053100118282">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>). For instance, wrapping a 1.2 billion parameter MT0 model with LoRA (r=8) results in only ~2.36 million trainable params (~0.19% of the model (<a href="https://github.com/huggingface/peft#:~:text=model%20%3D%20AutoModelForSeq2SeqLM,0.19151053100118282">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>). Hugging Faces implementation automatically handles freezing the original weights and marking the LoRA parameters as trainable.</p>
<p>Hugging Face Transformers also has built-in support for <strong>loading and merging LoRA adapters at inference</strong>. Thanks to the integration of PEFT with Transformers, if you save a LoRA adapter, you can load it by itself or along with the base model. For example, <code>AutoPeftModelForCausalLM.from_pretrained("username/model-lora")</code> will download the LoRA adapter from the Hub and <strong>automatically load the base model and apply the adapter weights</strong> (the adapters config file contains a reference to the <code>base_model_name</code> (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=Thanks%20to%20the%20PEFT%20integration,key">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>). This makes using LoRA in pipelines or deployment very convenient  the library will handle the composition of base weights + LoRA weights under the hood. In practice, you can also manually load a base model and then attach a LoRA adapter via <code>PeftModel.from_pretrained</code>: for example, load the base <code>facebook/opt-350m</code> and then apply a LoRA adapter on to (<a href="https://github.com/huggingface/peft#:~:text=from%20peft%20import%20AutoPeftModelForCausalLM%20from,transformers%20import%20AutoTokenizer%20import%20torch">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>). Both training and inference are supported on CPU and GPU  the <code>.to("cuda")</code> call can move the combined model to GPU for fast inference, or you can keep it on CPU for smaller models or testing. (Large LoRA-adapted models typically require a GPU to run efficiently, but the adapters themselves dont change the device requirements beyond what the base model needs.)</p>
<ul>
<li>
<p><strong>Original <code>loralib</code> (Microsoft):</strong> The initial implementation of LoRA was released as an open-source PyTorch library (<code>pip install loralib</code>). It provides LoRA-enabled layers (e.g., a custom <code>lora.Linear</code> module) that replace or augment <code>nn.Linear</code> layers in a mode (<a href="https://github.com/microsoft/LoRA#:~:text=implemented%20in%20,see%20Additional%20Notes%20for%20more">GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"</a>). Under the hood, these layers perform the <span class="arithmatex">\(W x + BA x\)</span> computation (with <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(B\)</span> as the low-rank matrices). The <code>loralib</code> package supports Linear, Embedding, and Conv2D layers, and even a merged Linear for cases where multiple weight matrices are packed together (as in some Transformer implementations (<a href="https://github.com/microsoft/LoRA#:~:text=match%20at%20L344%20implemented%20in,see%20Additional%20Notes%20for%20more">GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"</a>). While one can still use <code>loralib</code> to manually modify a model, its somewhat low-level. In practice, the Hugging Face PEFT approach has largely supplanted direct use of <code>loralib</code> by offering a higher-level, easier API (and support for saving/loading adapters, etc.). However, <code>loralib</code> was instrumental in demonstrating integration of LoRA with Hugging Face models early o (<a href="https://github.com/microsoft/LoRA#:~:text=This%20repo%20contains%20the%20source,a%20detailed%20description%20of%20LoRA">GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"</a>), and it remains a lightweight alternative if needed.</p>
</li>
<li>
<p><strong>PyTorch Lightning and Others:</strong> For users of PyTorch Lightning or other training frameworks, LoRA can also be integrated. Lightning, for example, has guides on how to replace a models linear layers with LoRA layers (using either <code>loralib</code> or custom modules) to allow training with LoRA while still using the Lightning Trainer for the loo (<a href="https://lightning.ai/docs/overview/finetune-models/llm-low-rank-adaption-of-large-language-models-lora#:~:text=To%20apply%20LoRA%2C%20we%20replace,both%20the%20original%20Linear">LoRA finetuning Lightning AI - Docs</a>). There are also community projects like <strong>Lit-LLaMA (Lit-GPT)</strong> and others that have LoRA support built-in, since LoRA is vital for fine-tuning large models without enormous resources. Additionally, minimal reimplementations (such as <strong><code>minLoRA</code></strong>, ~100 lines of code) exist to illustrate LoRAs simplicit (<a href="https://github.com/cccntu/minLoRA#:~:text=minLoRA%3A%20a%20minimal%20PyTorch%20library,minLoRA%20supports%20the%20following%20features">minLoRA: a minimal PyTorch library that allows you to apply LoRA ...</a>). These alternatives can be useful for understanding or customizing LoRA, but when it comes to widely adopted solutions, Hugging Faces Transformer+PEFT combo is by far the most popular and well-maintained route. It benefits from community support and continued development (e.g., Hugging Face has extended PEFT with variants like AdaLoRA, and even domain-specific LoRA for diffusion models, though those are beyond our scope).</p>
</li>
</ul>
<p><strong>Hardware support (CPU/GPU):</strong> All the libraries above are built on PyTorch, which means LoRA-enhanced models can run on CPU or GPU. In practice, training large models with LoRA is typically done on GPUs for speed, but because youre training far fewer parameters, the <strong>memory footprint is greatly reduced</strong>. This can allow training on a single GPU what would normally require multiple GPUs or not be feasible at all. For example, with LoRA, a 12-billion-parameter model that would normally <strong>OOM on an 80GB GPU</strong> could be fine-tuned in 22GB of GPU memory (with some offloading (<a href="https://github.com/huggingface/peft#:~:text=Model%20Full%20Finetuning%20PEFT,LoRA%20DeepSpeed%20with%20CPU%20Offloading">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>). Phil Schmid demonstrated fine-tuning an 11B parameter T5 model (FLAN-T5-XXL) on a single 24GB GPU by using LoRA with 8-bit quantization, whereas full fine-tuning the same model required 8 40GB GPU (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=The%20training%20took%20,322">Efficient Large Language Model training with LoRA and Hugging Face</a>). Inference can similarly be done on GPU for speed or on CPU if needed (for smaller models or where real-time is not required). The model + LoRA weights combined take only slightly more RAM than the base model alone (since the LoRA matrices are tiny). Therefore, if a base model fits in CPU RAM, the LoRA-adapted model will as well. The <strong>PEFT/Transformers integration handles device placement</strong>  you can move the model to CPU or GPU with standard PyTorch <code>.to()</code> calls or use Hugging Face Accelerate for more complex multi-device setups. Additionally, Hugging Faces inference tooling (like <a href="https://github.com/huggingface/text-generation-inference">Text Generation Inference server</a>) has added explicit support for LoRA adapters, including the ability to serve multiple LoRA adapters on one base model concurrently (multi-adapter serving (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=TGI%20for%20now%20only%20supports,1602174068">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>). This shows the ecosystem is evolving to make deploying LoRA-modified models as convenient as possible.</p>
<h2 id="training-workflow-with-lora">Training Workflow with LoRA</h2>
<p>Using LoRA for training a Transformer model typically follows these steps:</p>
<ol>
<li>
<p><strong>Load a Pretrained Base Model:</strong> Start with a pretrained Transformer (e.g., a Hugging Face model checkpoint for BERT, GPT-2, T5, GPT-NeoX, etc.). This will be your base model that provides all the knowledge that LoRA will adapt. For example: <code>model = AutoModelForCausalLM.from_pretrained("gpt2-large")</code>. Its often useful to use half-precision (<code>torch.float16</code>) or 8-bit loading for very large models to reduce memory, but this is optional. Ensure you have the tokenizer and model config as well if needed.</p>
</li>
<li>
<p><strong>Configure LoRA:</strong> Decide which parts of the model to adapt and set the LoRA hyperparameters. Using the PEFT library, you create a <code>LoraConfig</code> specifying:</p>
</li>
<li><code>r</code>: the rank of the LoRA matrices (small integer, e.g. 4, 8, 16 (<a href="https://www.ibm.com/think/topics/lora#:~:text=r%3A%20the%20rank%20of%20the,matrices%20with%20fewer%20trainable%20parameters">What is LoRA (Low-Rank Adaption)? | IBM</a>).</li>
<li><code>target_modules</code>: a list or pattern specifying which submodules to apply LoRA to (for example, <code>["q_proj", "v_proj"]</code> for attention query and value projection matrices, or <code>"all.linear"</code> to target all linear layers (<a href="https://www.ibm.com/think/topics/lora#:~:text=results%20in%20smaller%20update%20matrices,with%20fewer%20trainable%20parameters">What is LoRA (Low-Rank Adaption)? | IBM</a>). For many Transformer architectures, a common choice is all the W<sub>q</sub> and W<sub>v</sub> weight matrices in self-attention blocks, since those are big matrices and important for the models behavior. However, you can target feed-forward layers or others as well. The Hugging Face PEFT library provides some defaults per <code>TaskType</code> (e.g., for causal language modeling it might default to the attention projections).</li>
<li><code>lora_alpha</code>: the scaling factor  (controls initial update magnitude (<a href="https://www.ibm.com/think/topics/lora#:~:text=lora_alpha%3A%20LoRA%20scaling%20factor">What is LoRA (Low-Rank Adaption)? | IBM</a>).</li>
<li><code>lora_dropout</code>: dropout probability for LoRA layers (e.g. 0.05 or 0.1 to regularize (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=r%3D16%2C%20lora_alpha%3D32%2C%20target_modules%3D%5B,model">Efficient Large Language Model training with LoRA and Hugging Face</a>).</li>
<li><code>bias</code>: whether to also allow biases to be trained or not (often kept <code>"none"</code> or <code>"lora_only"</code> meaning do not update any bias terms except maybe those within LoRA layers). By default, LoRA doesnt add trainable bias unless specified.</li>
</ol>
<p>For example, a config might be: <em>rank=16, alpha=32, target_modules=["q", "v"], dropout=0.1</em>, for a seq2seq LM tas (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=,task_type%3DTaskType.SEQ_2_SEQ_LM">Efficient Large Language Model training with LoRA and Hugging Face</a>). This would insert LoRA adapters in the Q and V linear layers of each Transformer block, each of rank 16, and scale their effect by 32 with a 10% dropout.</p>
<ol>
<li>
<p><strong>Inject LoRA Adapters into the Model:</strong> Using the configuration, apply LoRA to your model. With PEFT, this is one line: <code>model = get_peft_model(model, lora_config)</code>. This function will modify the model <strong>in-place</strong>, adding the new LoRA layers and freezing the original weights. After this, <code>model</code> is a <code>PeftModel</code> wrapper around the original model. You can verify the setup by printing trainable params as shown earlier  e.g., <em>trainable params: 0.17%</em> of the mode (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=model">Efficient Large Language Model training with LoRA and Hugging Face</a>), confirming that only LoRA layers are unfrozen. Under the hood, each targeted <code>nn.Linear</code> module (or whatever target) is replaced or augmented by a LoRA-equipped counterpart. (If using another approach like <code>loralib</code>, at this step you would manually replace modules or call something like <code>lora.Linear(..., r=...)</code> to create layers with LoRA.)</p>
</li>
<li>
<p><strong>Prepare for Training:</strong> Since the vast majority of the models weights are frozen (non-trainable), training with LoRA often allows a higher learning rate on the adapters. Its not uncommon to use learning rates in the range 1e-3 to 1e-4 for LoRA, whereas full-model fine-tuning might use 1e-5 or lower. This is because only the small adapter needs to be learned, and it can typically be learned faster without divergin (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=output_dir%3Doutput_dir%2C%20auto_find_batch_size%3DTrue%2C%20learning_rate%3D1e,no">Efficient Large Language Model training with LoRA and Hugging Face</a>). You can use any PyTorch optimizer (AdamW is standard)  by default it will only update the LoRA parameters because others have <code>requires_grad=False</code>. If using the Hugging Face <code>Trainer</code> API, it will automatically ignore frozen params (no grads computed for them). You might also want to set <code>model.config.use_cache = False</code> during training if using certain models (e.g., T5, GPT-2) to avoid caching issues  this is a minor detail that the PEFT docs sometimes note for training with LoRA.</p>
</li>
</ol>
<p>Also ensure your training data is ready (tokenized, wrapped in a Dataset or DataLoader). With LoRA, you can usually afford a larger batch size than full fine-tuning because memory usage per batch is lower (optimizer states for frozen weights arent kept). Still, the total memory is dominated by the forward/backward activations, so you wont get <em>massive</em> batch size increases unless you also utilize gradient accumulation or a memory-saver like DeepSpeed. If needed, you can combine LoRA with techniques like gradient checkpointing to further reduce memory.</p>
<ol>
<li>
<p><strong>Train the Model:</strong> Run the training loop or HuggingFace Trainer. The training will focus on the LoRA adapter parameters. Thanks to the reduced size, training is typically much faster and uses less memory per step. For example, one report showed fine-tuning a 7B parameter model with LoRA on a single GPU in a few hours (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=8,not%20be%20the%20ideal%20tool">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>). Another example: fine-tuning FLAN-T5-XXL (11B) with LoRA + 8-bit took ~10 hours on a single A10 GPU, whereas full fine-tuning the same model for 10 hours would have required multi-GPU and cost significantly mor (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=The%20training%20took%20,322">Efficient Large Language Model training with LoRA and Hugging Face</a>). Throughout training, monitor your validation metrics as usual to ensure the model is improving on your task.</p>
</li>
<li>
<p><strong>Save the Adapter (and perhaps the merged model):</strong> After fine-tuning, you have two choices:</p>
</li>
<li><strong>Save only the LoRA adapter</strong>  i.e. the small matrices and config. If you used PEFTs <code>Trainer</code> or the <code>PeftModel</code>, calling <code>model.save_pretrained("my-lora-adapter")</code> will save files like <code>adapter_model.bin</code> or <code>.safetensors</code> and an <code>adapter_config.json</code>, rather than the full model weights. This is very storage-efficient (often just a few MB). You would use this if you want to publish or reuse the adapter on top of the original base model. For instance, users can then load <em>your</em> adapter and combine it with the public base model to get the fine-tuned model. This is how many LoRA fine-tuned LLMs are shared on Hugging Face Hub  the adapter might be ~20MB versus a 10+GB base mode (<a href="https://github.com/huggingface/peft#:~:text=The%20bigscience%2FT0_3B%20model%20performance%20isn%27t,PEFT%20in%20this%20blog%20post">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>).</li>
<li><strong>Merge and save the full model</strong>  you can merge the LoRA weights into the base model weights to create a new set of model weights that directly incorporate the fine-tuning. In code, PEFT provides <code>model.merge_and_unload()</code> which will add <span class="arithmatex">\(A B\)</span> into <span class="arithmatex">\(W\)</span> for all LoRA layers (and remove the LoRA hooks). After that, you can save the <code>model</code> as a normal <code>transformers</code> model (which will be the size of the full model). Merging is useful if you need a single file for deployment (for example, if the deployment environment doesnt support PEFT adapters) or want to do further tasks like quantizing the combined model. However, if your deployment can handle LoRA adapters, its often preferable to keep them separate for flexibility.</li>
</ol>
<p><strong>Example:</strong> Suppose we fine-tune a BERT-like model for a classification task with LoRA. We choose <span class="arithmatex">\(r=8\)</span>, target the attention projections, and train. At the end, we might have an adapter with ~1M parameters (just an example). We save this adapter. The base model (with, say, 110M parameters) remains untouched. If we want to fine-tune for another task, we can reuse the same 110M base and train another small adapter. Each tasks adapter is small, and we avoid duplicating the whole BERT for each task. This modular approach is a key benefit of LoR (<a href="https://www.ibm.com/think/topics/lora#:~:text=One%20of%20the%20key%20advantages,tuning">What is LoRA (Low-Rank Adaption)? | IBM</a>)  many adapters can plug into one model.</p>
<h2 id="inference-workflow-with-lora">Inference Workflow with LoRA</h2>
<p>After training, using a LoRA-adapted model for inference is very flexible. You can either load the base model and the LoRA adapter separately or use a combined approach. Here are common workflows for inference (applicable to both CPU and GPU execution):</p>
<ul>
<li>
<p><strong>Loading Base + LoRA Adapter:</strong> This is the most typical approach when you have saved a LoRA adapter. In code, you load the base pretrained model, then load the LoRA weights on top:
  <div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">PeftModel</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-large&quot;</span><span class="p">)</span>        <span class="c1"># load base</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="s2">&quot;path/or/hub-id-of-my-lora-adapter&quot;</span><span class="p">)</span>
</code></pre></div>
  The <code>from_pretrained</code> of <code>PeftModel</code> will read the adapter files (which include a config that points to the base model name, in case you didnt pass an already loaded base), then inject the LoRA weights into the model. After this, <code>model</code> behaves just like the fine-tuned model. You can then do <code>model.eval()</code> and use it for generating predictions or embeddings or whatever the task is. This method works on CPU or GPU  if you want it on GPU, either move it with <code>model.to('cuda')</code> after loading, or use the <code>device_map</code> argument to load directly to CUDA. The memory overhead for the LoRA is negligible (a few MB of weights plus some extra small computations).</p>
</li>
<li>
<p><strong>Direct Loading from Hub (Base + LoRA together):</strong> If the LoRA adapter is hosted on the Hugging Face Hub and was saved with the proper <code>adapter_config.json</code> (containing <code>"base_model_name_or_path"</code>), you can <strong>directly load the merged model via the Transformers API</strong>. As noted earlier, Hugging Face Transformers &gt;= v4.30 integrates PEFT, so even using <code>AutoModelForCausalLM.from_pretrained()</code> can automatically combine LoRA if it finds adapter weights:
  <div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;username/my-lora-adapter-hubrepo&quot;</span><span class="p">)</span>
</code></pre></div>
  Under the hood, this will see that the repo contains an adapter config and weights, and then load the base model (it will download the base model if not cached) and apply the LoRA. Similarly, <code>pipeline()</code> will work if pointed to the adapter repo. This one-liner usage is enabled by the PEFT integration and makes it dead simple to use LoRA fine-tuned models  *<em>the user doesnt even have to manually load base model</em> (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=Thanks%20to%20the%20PEFT%20integration,key">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>). For example, if someone shares a LoRA fine-tuned OPT-350M model on the hub, you can load it in one command and it will give you the ready-to-use mode (<a href="https://github.com/huggingface/peft#:~:text=from%20peft%20import%20AutoPeftModelForCausalLM%20from,transformers%20import%20AutoTokenizer%20import%20torch">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>). (Make sure to use a recent version of Transformers that has this capability.)</p>
</li>
<li>
<p><strong>Merged Model:</strong> In cases where you have merged the LoRA into the base weights and saved a full model, you just load it like any other model checkpoint (no special steps needed, since its now a standard model). This might be done if, for instance, you export the model to a production environment that doesnt know about LoRA. However, note that by merging you lose the ability to easily swap the adapter off  its now baked into the weights.</p>
</li>
<li>
<p><strong>Multi-Adapter Inference:</strong> An advanced scenario is having multiple LoRA adapters for different tasks and selecting between them at inference. The PEFT library allows loading multiple adapters and either merging them or switching which is active. For example, you could maintain one base model instance in memory and load two different LoRA adapters (say one for legal text generation and one for medical text). By toggling which adapter is applied, the model can serve different styles or tasks without loading a whole new model. This is especially useful in serving scenarios  as mentioned, Hugging Faces TGI server can even host dozens of LoRAs on one base model and route requests to the appropriate one with virtually no overhead of loading/unloadin (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=Update%2C%20TGI%20now%20also%20supports,LoRa%20into%20the%20base%20model">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>). This kind of serve many models with one base is made possible only because the adapters are so lightweight.</p>
</li>
</ul>
<p>Regardless of how you load the model, <strong>inference speed using LoRA-adapted models is essentially the same as the original model</strong>. When separate, the model does an extra add operation (<code>+ A(Bx)</code>), but this is a simple matrix multiplication addition which is negligible compared to the large matrix multiplications the Transformer is already doing. The original LoRA paper emphasized that <em>LoRA adds no additional latency at inference by construction (<a href="https://www.ibm.com/think/topics/lora#:~:text=much%20smaller%20low">What is LoRA (Low-Rank Adaption)? | IBM</a>). If you merged the weights, it </em>literally* is the same as a fully fine-tuned model in terms of operations. Thus, you dont pay a runtime penalty for the parameter-efficient approach  a key advantage over some other adapter methods that, for example, insert additional layers and do increase compute.</p>
<p><strong>CPU vs GPU at inference:</strong> If the model is small (or quantized), you might run inference on CPU. LoRA doesnt change the feasibility of this  you still need enough RAM for the base model. For example, if a base model is 2GB on disk (FP16 ~4GB in RAM), and the LoRA adapter is 30MB, the total at runtime might be ~4.03GB which is essentially the same requirement. The extra matrix multiplies for LoRA will use some CPU cycles, but if the model was runnable on CPU before, it will still be after (with only a slight overhead thats usually dwarfed by the rest of the models computation). In many cases, though, large models are run on GPU for speed, and LoRA is fully GPU-compatible. Just ensure you load or move both base and LoRA weights to the GPU.</p>
<p>Finally, a note on <strong>quantization</strong> at inference: LoRA can be combined with model weight quantization (int8 or int4) to further shrink memory usage. A popular strategy is <strong>QLoRA (Quantized LoRA)</strong>, where the base model is loaded in 4-bit precision and LoRA adapters are used to fine-tune. During inference, you can keep the base model in 4-bit and the LoRA in normal precision  this yields huge memory savings (for example, 33% less memory at cost of 30-40% more compute time in one stud (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=runs">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>)). The Hugging Face PEFT documentation even provides utilities to apply LoRA on top of 4-bit models (with the <code>BitsAndBytesConfig</code> from <code>transformers</code> and methods to handle LoRA initialization in quantized scenario (<a href="https://huggingface.co/docs/peft/main/en/developer_guides/lora#:~:text=In%20general%2C%20for%20LoftQ%20to,nf4">LoRA</a>) (<a href="https://huggingface.co/docs/peft/main/en/developer_guides/lora#:~:text=from%20peft%20import%20replace_lora_weights_loftq%20from,transformers%20import%20BitsAndBytesConfig">LoRA</a>)). This is quite advanced, but its worth mentioning that LoRA is <strong>compatible with quantization</strong>  you can load an 8-bit or 4-bit base model on GPU, attach LoRA, and still get the benefits of both techniques. In short, LoRA does not conflict with inference optimization methods; it plays nicely with half-precision, mixed-precision, and quantized models.</p>
<h2 id="best-practices-and-tips-for-using-lora">Best Practices and Tips for Using LoRA</h2>
<p>When integrating LoRA into modern training and inference pipelines, here are some <strong>best practices and considerations</strong> to keep in mind:</p>
<ul>
<li>
<p><strong>Use Established Libraries:</strong> Prefer using well-maintained libraries like Hugging Faces PEFT for LoRA, which abstract away the tricky parts of implementation. They handle naming, freezing, saving, and loading seamlessly. This reduces errors compared to writing your own LoRA logic. The Hugging Face implementation is widely tested and kept up-to-date with new features, which makes your life easier.</p>
</li>
<li>
<p><strong>Choose the Right Layers (target_modules):</strong> Decide which parts of the model to adapt based on a trade-off between flexibility and efficiency. Applying LoRA to <strong>all attention and feed-forward layers</strong> will give the adapter more reach to change the models behavior (improving task performance), but means more parameters to train. Focusing on a subset (like just the attention projections) limits trainable params and memory. Empirically, many have found that applying LoRA across <em>more</em> layers (even all linear layers) tends to maximize performance on difficult task (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=6,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>). If you are fine-tuning for something that requires altering a lot of the models knowledge, consider a higher coverage of layers. If you only need slight tweaks (or need extreme efficiency), target key layers (e.g. attention Q and V matrices). The PEFT library allows targeting by module name patterns, which you can customize for your architecture.</p>
</li>
<li>
<p><strong>Set an Appropriate Rank:</strong> The rank <span class="arithmatex">\(r\)</span> is a crucial hyperparameter. Common default values are 8 or 16 for large language models; smaller models or simpler tasks might even use <span class="arithmatex">\(r=4\)</span>, whereas very complex tasks might benefit from <span class="arithmatex">\(r=32\)</span> or more. A higher rank gives the LoRA adapter more capacity to learn nuanced changes (at the cost of more GPU memory and risk of overfitting). Its often recommended to start with a moderate rank (e.g. 8) and see if performance is satisfactory. If the model underfits (cant reach the desired accuracy), increasing the rank can help. <strong>Tune <span class="arithmatex">\(r\)</span> in conjunction with the scaling factor <span class="arithmatex">\(\alpha\)</span></strong>  a rule of thumb is $\alpha = 2 \times r (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=just%20to%20the%20Key%20and,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>), which keeps the initial scale of updates roughly balanced. The IBM guidance notes that reducing the rank too much can cause some information los (<a href="https://www.ibm.com/think/topics/lora#:~:text=While%20LoRA%20provides%20a%20significant,There%20is%20redundancy%2C%20robustness%20and">What is LoRA (Low-Rank Adaption)? | IBM</a>), but because transformers are very overparameterized, they often tolerate surprisingly low ranks without much drop in performance.</p>
</li>
<li>
<p><strong>LoRA Alpha and Dropout:</strong> As mentioned, setting <span class="arithmatex">\(\alpha\)</span> (lora_alpha) to a couple times the rank is a reasonable heuristic. This doesnt usually need heavy tuning beyond that; it mainly affects training dynamics early on. For LoRA dropout, if your dataset is small or you notice overfitting, adding a bit of dropout (e.g. 0.050.1) on the LoRA weights can improve generalization. If your dataset is large, dropout may not be necessary.</p>
</li>
<li>
<p><strong>Monitor Trainable Parameter %:</strong> One nice feature of LoRA is the easy calculation of how many parameters youre actually training. Use <code>model.print_trainable_parameters()</code> (in PEFT) or manually sum <code>numel()</code> of parameters with <code>requires_grad=True</code>. This helps you verify that LoRA is applied correctly and gauge the size of the adapter. For instance, seeing <em>trainable params: 0.2%</em> gives confidence you achieved a big efficiency gai (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=model">Efficient Large Language Model training with LoRA and Hugging Face</a>). You can also estimate memory needs: if you train 2 million params with AdamW, thats on the order of ~16 MB of optimizer states, which is trivial compared to many GBs for the full models states.</p>
</li>
<li>
<p><strong>Training Regimen:</strong> Because LoRA reduces the risk of overfitting by reducing parameters, you might be tempted to train for many epochs. However, be cautious  <strong>overfitting can still occur</strong>. In fact, with very small training sets, a powerful model might overfit the few million adapter params quickly. Its often observed that a few epochs (or even just a single epoch with a large dataset) is sufficient. Keep an eye on validation loss and use early stopping if possible. Also, consider that <em>not all tasks benefit from multi-epoch training</em> with LoRA; sometimes a single pass is enough to imprint the new informatio (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=5,results%2C%20probably%20due%20to%20overfitting">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>).</p>
</li>
<li>
<p><strong>Optimizer and Precision:</strong> LoRA doesnt mandate a specific optimizer. AdamW is commonly used and works well. One finding from LoRA practitioners is that fancy optimizers arent strictly necessary  since youre training a small part of the model, even SGD can reach a good solution if used properly, though adaptive optimizers converge faste (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=2.%20QLoRA%20presents%20a%20trade,increase%20in%20runtime">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>). As for precision, you can train LoRA adapters in full fp32 or mixed precision. Mixed precision (fp16 or bf16) works fine for the gradients of LoRA weights and will speed up training on GPUs that support it. If using an 8-bit base model (via bitsandbytes), ensure you follow PEFT guidelines (e.g., use <code>prepare_model_for_int8_training</code> to keep certain layers in float32 for stabilit (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=%29%20%23%20prepare%20int,model">Efficient Large Language Model training with LoRA and Hugging Face</a>)).</p>
</li>
<li>
<p><strong>Combine with Other Techniques Cautiously:</strong> LoRA can be combined with other parameter-efficient methods (like prefix tuning, or even adding adapters on top of LoRA) but this gets complex. In the Hugging Face PEFT library you can actually stack adapters (they support e.g. LoRA + prompt tuning together). Generally, LoRA alone is powerful enough for most fine-tuning needs. If you do combine methods, ensure you understand how they interact (for example, prefix tuning adds tokens to inputs, while LoRA changes weights  they wont conflict, but the benefits might not be strictly additive). A more common combination is LoRA with model compression (quantization/pruning) as discussed  e.g. QLoRA for training and then maybe distilling the model afterward. These pipelines can get sophisticated, but the community has shown success with them (like fine-tuning 65B LLMs with 4-bit QLoRA on a single GP (<a href="https://github.com/huggingface/peft#:~:text=methods%20to%20make%20it%20even,and%20load%20LLMs%20for%20inference">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>)).</p>
</li>
<li>
<p><strong>Inference Deployment:</strong> When deploying a LoRA-adapted model, decide whether to use a framework that supports adapters or to merge the weights. If youre using the Hugging Face <code>transformers</code> library in your deployment, you can simply load the adapter as we described. If youre exporting to ONNX or using a C++ runtime, you might prefer to merge weights beforehand since those runtimes wont know about the separate LoRA weights. Merging is lossless, so its a safe operation. As a best practice, keep a copy of the original adapter weights even if you merge  this way you can later update or swap adapters without needing to re-run fine-tuning. And if serving multiple tasks, consider a setup that can hot-swap adapters (Hugging Faces <code>TextGenerationInference</code> server or a custom PyTorch service) to maximize the benefits of LoRAs modularity.</p>
</li>
<li>
<p><strong>Community Resources and Models:</strong> Since LoRA is widely used, there are many existing LoRA adapters shared online for popular models (especially in the realm of large language models like LLaMA, GPT-J, etc.). You can often find a LoRA for a certain domain or task on Hugging Face Hub  using it might save you training time. These adapters can sometimes be combined or sequentially applied if they are compatible (research into merging LoRA weights from different fine-tunings is ongoin (<a href="https://github.com/huggingface/peft/issues/1836#:~:text=Different%20results%20when%20merging%20LORA,when%20they%20are%20not">Different results when merging LORA weights into the base model ...</a>)). Keep an eye on the latest PEFT library features for merging multiple LoRAs (there are methods like <code>add_weighted_adapter</code> to combine adapters, useful in model merging scenario (<a href="https://huggingface.co/docs/diffusers/en/using-diffusers/merge_loras#:~:text=Merge%20LoRAs%20,and%20add_weighted_adapter%20methods">Merge LoRAs - Hugging Face</a>)). As of 2025, techniques like TIES and DARE are being explored to intelligently merge LoRA adapters from different models/task (<a href="https://huggingface.co/docs/peft/en/developer_guides/model_merging#:~:text=PEFT%20provides%20several%20methods%20for,adapters%20by%20eliminating%20redundant%20parameters">Model merging</a>) (<a href="https://huggingface.co/docs/peft/en/developer_guides/model_merging#:~:text=%2A%20TIES%20,This">Model merging</a>)  interesting for multi-skill models.</p>
</li>
</ul>
<p>In conclusion, LoRA has become a <strong>key component of modern model fine-tuning pipelines</strong> due to its efficiency and simplicity. Libraries like Hugging Face PEFT make it easy to apply LoRA to transformers with just a few lines of code, and to integrate the resulting adapters into both training and inference workflows on CPU or GPU. By following best practices  choosing proper hyperparameters, monitoring training, and utilizing the robust tooling available  developers can adapt large pretrained models to new tasks <strong>quickly and cost-effectively</strong>. LoRA enables experimentation with large models that would otherwise be inaccessible to most, and it does so while maintaining performance and fast inference speed, making it a widely adopted technique in the NLP communit (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=Low,technique%20worth%20familiarizing%20oneself%20with">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>) (<a href="https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,share%2C%20store%2C%20and%20load%20them">PEFT</a>).</p>
<p><strong>Sources:</strong></p>
<ul>
<li>Hu et al., <em>LoRA: Low-Rank Adaptation of Large Language Models</em>, arXiv 2021 (<a href="https://arxiv.org/abs/2106.09685#:~:text=example%20,trainable%20parameters%2C%20a%20higher%20training">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>) (<a href="https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,PyTorch%20models%20and%20provide%20our">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</a>).</li>
<li>IBM Cloud Education, <em>What is LoRA (Low-Rank Adaptation)?</em>  Think Blog, 2023 (<a href="https://www.ibm.com/think/topics/lora#:~:text=Instead%20of%20retraining%20the%20whole,match%20the%20desired%20use%20case">What is LoRA (Low-Rank Adaption)? | IBM</a>) (<a href="https://www.ibm.com/think/topics/lora#:~:text=One%20of%20the%20key%20advantages,tuning">What is LoRA (Low-Rank Adaption)? | IBM</a>).</li>
<li> Hugging Face PEFT Documentation and GitHub READM (<a href="https://github.com/huggingface/peft#:~:text=peft_config%20%3D%20LoraConfig%28%20task_type%3DTaskType,1">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>) (<a href="https://github.com/huggingface/peft#:~:text=from%20peft%20import%20AutoPeftModelForCausalLM%20from,transformers%20import%20AutoTokenizer%20import%20torch">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>) (<a href="https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,share%2C%20store%2C%20and%20load%20them">PEFT</a>)0.</li>
<li>P. Schmid, <em>Efficient Large Language Model training with LoRA</em>, 202 (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=,task_type%3DTaskType.SEQ_2_SEQ_LM">Efficient Large Language Model training with LoRA and Hugging Face</a>) (<a href="https://www.philschmid.de/fine-tune-flan-t5-peft#:~:text=model">Efficient Large Language Model training with LoRA and Hugging Face</a>)4.</li>
<li>S. Raschka, <em>Practical Tips for Finetuning LLMs Using LoRA</em>, Ahead-of-AI Substack, 202 (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=just%20to%20the%20Key%20and,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>) (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=6,matrices%2C%20to%20maximize%20model%20performance">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>)4.</li>
<li>Hugging Face Forums  discussions on LoRA usage and deploymen (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=Thanks%20to%20the%20PEFT%20integration,key">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>) (<a href="https://discuss.huggingface.co/t/using-text-generation-inference-with-lora-adapter/73286#:~:text=TGI%20for%20now%20only%20supports,1602174068">Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums</a>)7.</li>
<li>Hugging Face blog and resources on PEFT/QLoRA (e.g., 4-bit LoRA on consumer GPUs (<a href="https://github.com/huggingface/peft#:~:text=Quantization">GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a>) (<a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=runs">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>)2.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>