
=== CODESCENE ANALYSIS ===
Please systematically review the following CodeScene analysis results and provide a detailed summary of the findings. Then plan out the necessary refactoring steps based on the CodeScene analysis.

{"score":9.38,"review":[{"category":"Overall Code Complexity","description":"Overall Code Complexity is measured by the mean cyclomatic complexity across all functions in the file. The lower the number, the better.\n\nCyclomatic complexity is a function level metric that measures the number of logical branches (if-else, loops, etc.). Cyclomatic complexity is a rough complexity measure, but useful as a way of estimating the minimum number of unit tests you would need. As such, prefer functions with low cyclomatic complexity (2-3 branches).","indication":2}]}
=== MYPY ANALYSIS ===

<?xml-stylesheet type="text/xsl" href="mypy-html.xslt"?><mypy-report-index name="index"><file module="rna_predict.pipeline.stageA.input_embedding.current.utils.general" name="rna_predict/pipeline/stageA/input_embedding/current/utils/general.py" total="233" any="28" empty="156" imprecise="21" precise="28" unanalyzed="0"/></mypy-report-index>
=== RUFF FIX OUTPUT ===

All checks passed!

=======
PROMPT:
**=======**
REFACTOR:
=======
The major types of code refactoring mentioned include:

1. **Extract Function**: Extracting code into a function or method (also referred to as Extract Method).
2. **Extract Variable**: Extracting code into a variable.
3. **Inline Function**: The inverse of Extract Function, where a function is inlined back into its calling code.
4. **Inline Variable**: The inverse of Extract Variable, where a variable is inlined back into its usage.
5. **Change Function Declaration**: Changing the names or arguments of functions.
6. **Rename Variable**: Renaming variables for clarity.
7. **Encapsulate Variable**: Encapsulating a variable to manage its visibility.
8. **Introduce Parameter Object**: Combining common arguments into a single object.
9. **Combine Functions into Class**: Grouping functions with the data they operate on into a class.
10. **Combine Functions into Transform**: Merging functions particularly useful with read-only data.
11. **Split Phase**: Organizing modules into distinct processing phases.

These refactorings focus on improving code clarity and maintainability without altering its observable behavior.

For more detailed information, you might consider using tools that could provide further insights or examples related to these refactoring types.


====
FULL CODE:
====
show the full file dont drop comments or existing functionality

**====**
FULL CODE:
**====**
# protenix/model/utils.py
# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
General utility functions for RNA structure prediction.
"""

from typing import Dict, Optional

import numpy as np
import torch


def sample_indices(
    n: int,
    sample_size: int,
    strategy: str = "random",
    device: Optional[torch.device] = None,
) -> torch.Tensor:
    """
    Sample indices using specified strategy.

    Args:
        n: Total number of indices to sample from
        sample_size: Number of indices to sample
        strategy: Sampling strategy ('random' or 'topk')
        device: Device to place the output tensor on

    Returns:
        Tensor of sampled indices
    """
    assert strategy in ["random", "topk"], f"Invalid sampling strategy: {strategy}"
    assert sample_size <= n, f"Cannot sample {sample_size} items from {n} items"

    if strategy == "random":
        # Ensure n is positive for randperm
        if n <= 0:
            return torch.tensor([], dtype=torch.long, device=device)
        indices = torch.randperm(n=n, device=device)[:sample_size]
    elif strategy == "topk":
        indices = torch.arange(sample_size, device=device)
    else:
        raise ValueError(f"Invalid sampling strategy: {strategy}")
    return indices


def sample_msa_feature_dict_random_without_replacement(
    feat_dict: Dict[str, torch.Tensor],
    sample_size: int,
    device: Optional[torch.device] = None,
) -> Dict[str, torch.Tensor]:
    """
    Sample MSA features randomly without replacement.
    """
    n_seq = next(iter(feat_dict.values())).shape[0]
    indices = sample_indices(n_seq, sample_size, strategy="random", device=device)

    return {k: v[indices] for k, v in feat_dict.items()}


def _convert_value_to_numpy(value) -> np.ndarray:
    """Convert a value to a numpy array.

    Args:
        value: Value to convert (float, int, torch.Tensor, or np.ndarray)

    Returns:
        np.ndarray: Converted value

    Raises:
        ValueError: If the value type is not supported
    """
    if isinstance(value, (float, int)):
        return np.array([value])
    elif isinstance(value, torch.Tensor):
        if value.dim() == 0:
            return np.array([value.item()])
        else:
            return value.detach().cpu().numpy()
    elif isinstance(value, np.ndarray):
        return value
    else:
        raise ValueError(f"Unsupported type for metric data: {type(value)}")


def _collect_values_from_dicts(dict_list: list[dict]) -> dict:
    """Collect values from a list of dictionaries.

    Args:
        dict_list: List of dictionaries to collect values from

    Returns:
        dict: Dictionary with collected values as lists
    """
    merged_dict: dict = {}

    for x in dict_list:
        for k, v in x.items():
            merged_dict.setdefault(k, [])
            merged_dict[k].append(_convert_value_to_numpy(v))

    return merged_dict


def _is_compatible_shape(shape1, shape2) -> bool:
    """Check if two shapes are compatible for reshaping.

    Args:
        shape1: First shape
        shape2: Second shape

    Returns:
        bool: True if shapes are compatible, False otherwise
    """
    return (
        shape1 == shape2
        or (shape1 == () and shape2 == (1,))
        or (shape1 == (1,) and shape2 == ())
    )


def _reshape_item_to_match(item: np.ndarray, target_shape) -> np.ndarray:
    """Reshape an item to match the target shape if possible.

    Args:
        item: Array to reshape
        target_shape: Target shape

    Returns:
        np.ndarray: Reshaped array
    """
    if item.shape == target_shape:
        return item
    elif item.shape == () and target_shape == (1,):
        return item.reshape(1)
    elif item.shape == (1,) and target_shape == ():
        return item.reshape(())
    else:
        return item  # Will be caught by compatibility check later


def _reshape_arrays_if_needed(arrays: list, key: str) -> list:
    """Reshape arrays to make them compatible for concatenation.

    Args:
        arrays: List of arrays to reshape
        key: Dictionary key for error reporting

    Returns:
        list: List of reshaped arrays

    Raises:
        ValueError: If arrays have incompatible shapes
    """
    if not arrays:
        return arrays

    first_shape = arrays[0].shape
    if all(item.shape == first_shape for item in arrays):
        return arrays

    # Attempt to reshape if possible
    reshaped_arrays = []
    for item in arrays:
        # Check if shapes are compatible before reshaping
        if not _is_compatible_shape(item.shape, first_shape):
            raise ValueError(
                f"Incompatible shapes for key '{key}': {first_shape} vs {item.shape}"
            )

        # Reshape the item
        reshaped_item = _reshape_item_to_match(item, first_shape)
        reshaped_arrays.append(reshaped_item)

    return reshaped_arrays


def _concatenate_arrays(arrays: list, key: str) -> np.ndarray:
    """Concatenate arrays into a single array.

    Args:
        arrays: List of arrays to concatenate
        key: Dictionary key for error reporting

    Returns:
        np.ndarray: Concatenated array

    Raises:
        ValueError: If arrays cannot be concatenated
    """
    try:
        return np.concatenate(arrays)
    except ValueError as e:
        print(f"Error concatenating key '{key}': {e}")
        raise e


def simple_merge_dict_list(dict_list: list[dict]) -> dict:
    """
    Merge a list of dictionaries into a single dictionary.

    Args:
        dict_list (list[dict]): List of dictionaries to merge.

    Returns:
        dict: Merged dictionary where values are concatenated arrays.
    """
    # Collect values from dictionaries
    merged_dict = _collect_values_from_dicts(dict_list)

    # Process each key-value pair
    for k, v in list(merged_dict.items()):
        # Skip empty lists
        if not v:
            continue

        # Reshape arrays if needed
        reshaped_v = _reshape_arrays_if_needed(v, k)

        # Concatenate arrays
        merged_dict[k] = _concatenate_arrays(reshaped_v, k)

    return merged_dict
