{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":5942070,"sourceType":"datasetVersion","datasetId":3410079},{"sourceId":11026565,"sourceType":"datasetVersion","datasetId":6866703},{"sourceId":11762635,"sourceType":"datasetVersion","datasetId":7384470},{"sourceId":11787394,"sourceType":"datasetVersion","datasetId":7401070},{"sourceId":11787916,"sourceType":"datasetVersion","datasetId":7401477},{"sourceId":11788332,"sourceType":"datasetVersion","datasetId":7401777},{"sourceId":11788337,"sourceType":"datasetVersion","datasetId":7401782},{"sourceId":11788349,"sourceType":"datasetVersion","datasetId":7401791},{"sourceId":11788362,"sourceType":"datasetVersion","datasetId":7401801},{"sourceId":11788374,"sourceType":"datasetVersion","datasetId":7401810},{"sourceId":11788377,"sourceType":"datasetVersion","datasetId":7401813},{"sourceId":11788382,"sourceType":"datasetVersion","datasetId":7401817},{"sourceId":11788386,"sourceType":"datasetVersion","datasetId":7401820},{"sourceId":11788388,"sourceType":"datasetVersion","datasetId":7401822},{"sourceId":11788390,"sourceType":"datasetVersion","datasetId":7401824},{"sourceId":11788491,"sourceType":"datasetVersion","datasetId":7401888},{"sourceId":11788496,"sourceType":"datasetVersion","datasetId":7401891},{"sourceId":11788500,"sourceType":"datasetVersion","datasetId":7401893},{"sourceId":11788503,"sourceType":"datasetVersion","datasetId":7401896},{"sourceId":11788505,"sourceType":"datasetVersion","datasetId":7401898},{"sourceId":11788513,"sourceType":"datasetVersion","datasetId":7401905},{"sourceId":11788517,"sourceType":"datasetVersion","datasetId":7401907},{"sourceId":11788524,"sourceType":"datasetVersion","datasetId":7401913},{"sourceId":11788527,"sourceType":"datasetVersion","datasetId":7401915},{"sourceId":11788542,"sourceType":"datasetVersion","datasetId":7401925},{"sourceId":11788545,"sourceType":"datasetVersion","datasetId":7401927},{"sourceId":11788551,"sourceType":"datasetVersion","datasetId":7401929},{"sourceId":11788558,"sourceType":"datasetVersion","datasetId":7401931},{"sourceId":11788622,"sourceType":"datasetVersion","datasetId":7401980},{"sourceId":11788630,"sourceType":"datasetVersion","datasetId":7401890},{"sourceId":11788641,"sourceType":"datasetVersion","datasetId":7401995},{"sourceId":11788656,"sourceType":"datasetVersion","datasetId":7401827},{"sourceId":11814119,"sourceType":"datasetVersion","datasetId":7420372},{"sourceId":11814128,"sourceType":"datasetVersion","datasetId":7420378},{"sourceId":11814137,"sourceType":"datasetVersion","datasetId":7401990},{"sourceId":11814142,"sourceType":"datasetVersion","datasetId":7420389},{"sourceId":11814146,"sourceType":"datasetVersion","datasetId":7420392},{"sourceId":11814150,"sourceType":"datasetVersion","datasetId":7420395},{"sourceId":11814170,"sourceType":"datasetVersion","datasetId":7420409},{"sourceId":11814175,"sourceType":"datasetVersion","datasetId":7420413},{"sourceId":11814180,"sourceType":"datasetVersion","datasetId":7420416},{"sourceId":11814187,"sourceType":"datasetVersion","datasetId":7420420},{"sourceId":11814259,"sourceType":"datasetVersion","datasetId":7420472},{"sourceId":11814267,"sourceType":"datasetVersion","datasetId":7420479},{"sourceId":11814282,"sourceType":"datasetVersion","datasetId":7420492},{"sourceId":11814553,"sourceType":"datasetVersion","datasetId":7420638},{"sourceId":11831012,"sourceType":"datasetVersion","datasetId":6866398}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell : clean auto-generated requirements file  (run FIRST!)\n# -----------------------------------------------------------\nimport pathlib, shutil, re, textwrap, sys, os\n\nREQ_PATH = pathlib.Path(\"/kaggle/requirements/input_requirements.txt\")\nif REQ_PATH.is_file():\n    cleaned_lines = []\n    for line in REQ_PATH.read_text().splitlines():\n        line = line.strip()\n        if not line or line.startswith(\"#\"):\n            continue                    # â† drop blanks / comments\n        if not line.startswith(\"pip install\"):\n            # keep it but comment it out so the helper ignores it\n            line = f\"# {line}\"\n        cleaned_lines.append(line)\n\n    REQ_PATH.write_text(\"\\n\".join(cleaned_lines) + (\"\\n\" if cleaned_lines else \"\"))\n\n    print(f\"[INFO] requirements cleaned â€“ {len(cleaned_lines)} valid \"\n          f\"pip-install line(s) kept.\")\nelse:\n    print(f\"[INFO] {REQ_PATH} not found â€“ nothing to clean.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:02:29.551618Z","iopub.execute_input":"2025-05-16T19:02:29.552081Z","iopub.status.idle":"2025-05-16T19:02:29.560720Z","shell.execute_reply.started":"2025-05-16T19:02:29.552014Z","shell.execute_reply":"2025-05-16T19:02:29.559195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Cell: show whatâ€™s inside every mounted Kaggle dataset  ğŸ”\n# --------------------------------------------------------\necho -e \"\\nğŸ“‚  Listing the first two levels of /kaggle/input â€¦\\n\"\n\n# Change depth (-maxdepth) if you want more or fewer levels\nfind /kaggle/input -maxdepth 2 -mindepth 1 -print | sed 's|^|  |'\n\necho -e \"\\nâœ…  Done.\\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:02:29.562448Z","iopub.execute_input":"2025-05-16T19:02:29.562808Z","iopub.status.idle":"2025-05-16T19:02:29.661765Z","shell.execute_reply.started":"2025-05-16T19:02:29.562770Z","shell.execute_reply":"2025-05-16T19:02:29.660349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Cell : offline installs that match the *current* wheel set (lean version)\n# -----------------------------------------------------------------------\nset -euo pipefail\n\n# â”€â”€ let pip look inside EVERY sub-folder of /kaggle/input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nWHEEL_ROOT=\"/kaggle/input\"\nFIND_LINKS_ARGS=\"\"\nfor d in \"$WHEEL_ROOT\" \"$WHEEL_ROOT\"/*; do\n  FIND_LINKS_ARGS+=\" --find-links $d\"\ndone\n\np () {                 # quiet install; warn (donâ€™t die) if something fails\n  # shellcheck disable=SC2086\n  pip install --no-index $FIND_LINKS_ARGS --quiet \"$@\" \\\n  || echo \"[WARN] install failed â†’ skipped: $*\"\n}\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1) Core scientific stack\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\np numpy==1.24.3\np pandas==2.2.3\np scipy==1.10.1\np tqdm==4.67.1\np seaborn==0.12.2\np biopython==1.85\np torch               # pre-installed in the Kaggle image\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2)  ML / NLP stack\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\np huggingface_hub==0.31.1      # needs hf-xet (you already uploaded)\np transformers==4.51.3\np pytorch_lightning==2.5.0.post0   # gives us Lightning-core features\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3)  Extra deps *rna_predict* really imports\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\np lightning-utilities==0.11.2  # comes with PL wheel but list explicitly\np datasets==3.6.0\np einops==0.8.1\np hypothesis==6.131.15\np black==25.1.0                # needs pathspec 0.12.1 â†’ you uploaded both\np pathspec==0.12.1\np isort==6.0.1\np ruff==0.11.9\np mss==10.0.0\np mdanalysis==2.9.0\np mmtf-python==1.1.3\np GridDataFormats==1.0.2\np mrcfile==1.5.4\np lxml==5.4.0\np dearpygui==2.0.0\np py-cpuinfo==9.0.0\np Pillow                        # pillow-11-2-1 wheel present\np exit-codes==1.3.0             # small helper used by HF-Hub 0.31+\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 4)  Config utilities\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\np hydra-core==1.3.2\np omegaconf==2.3.0\np ml_collections==1.1.0         # required by Protenix\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 5)  rna-predict itself  (no-deps so nothing reaches PyPI)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npip install --no-index --no-deps --quiet \\\n  /kaggle/input/rna-structure-predict/rna_predict-2.0.3-py3-none-any.whl\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 6)  Protenix 0.4.6  (wheel, but ignore its heavy deps like RDKit)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npip install --no-index --no-deps --quiet \\\n  /kaggle/input/protenix-0-4-6/protenix-0.4.6-py3-none-any.whl \\\n  || echo \"[WARN] Protenix wheel install failed.\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 7)  Runtime shim: make â€œimport lightningâ€ point to pytorch_lightning\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npython - <<'PY'\nimport sys, importlib, types\ntry:\n    import pytorch_lightning as pl\n    sys.modules.setdefault(\"lightning\", pl)\nexcept ImportError:\n    print(\"[WARN] pytorch_lightning missing â€“ shim not created\")\nPY\n\necho \"âœ…  Offline wheel install phase complete.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:02:29.665786Z","iopub.execute_input":"2025-05-16T19:02:29.666432Z","iopub.status.idle":"2025-05-16T19:05:08.009084Z","shell.execute_reply.started":"2025-05-16T19:02:29.666376Z","shell.execute_reply":"2025-05-16T19:05:08.008145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---\n# Cell: ALL-IN-ONE Environment Setup  (no uninstalls, no online pip)\n# ---\n\nimport sys, subprocess, shutil, os, platform\n\ndef run_and_print(cmd):\n    res = subprocess.run(cmd, capture_output=True, text=True)\n    print(res.stdout, end=\"\")\n    if res.stderr:\n        print(res.stderr, end=\"\")\n\n# â•â•â•â•â•â• 1)  System information (unchanged) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"\\n=== [System Information] ===\")\n\nprint(\"\\n[Python Version]\")\nprint(sys.version)\n\nprint(\"\\n[Kernel and OS Information]\")\nrun_and_print([\"uname\", \"-a\"])\n\nprint(\"\\n[CPU Information]\")\nrun_and_print([\"lscpu\"])\n\nprint(\"\\n[Memory Information]\")\nrun_and_print([\"free\", \"-mh\"])\n\nprint(\"\\n[Disk Information]\")\nrun_and_print([\"lsblk\"])\n\nprint(\"\\n=== [End of System Information] ===\\n\")\n\n# â•â•â•â•â•â• 2)  USER CONFIG  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nRNA_PREDICT_VERSION   = \"2.0.3\"\nBLOCK_SPARSE_WHEEL_IN = (\n    \"/kaggle/input/block-sparse-wheels/\"\n    \"block_sparse_attn-0.0.1cu118torch2.0cxx11abiTRUE-\"\n    \"cp310-cp310-linux_x86_64.whl\"\n)\n# PEP 440-compliant rename (Torch version tag trimmed)\nBLOCK_SPARSE_WHEEL_OUT = (\n    \"/kaggle/working/\"\n    \"block_sparse_attn-0.0.1+cu118torch2.0-\"\n    \"cp310-cp310-linux_x86_64.whl\"\n)\n\n# â•â•â•â•â•â• 3)  Environment-fix helper  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\ndef setup_environment():\n    \"\"\"\n    â‘  Ensure Seaborn (and its deps) is present\n    â‘¡ Copy & install block_sparse_attn wheel   â† optional; see note below\n    â‘¢ Install rna_predict\n    â‘£ Install hydra-core from local wheel\n    â‘¤ Show final versions of key packages\n    \"\"\"\n    # â‘  Make sure Seaborn is available â€“\n    #    Kaggle base image already has 0.12.2 but we pin it explicitly:\n    print(\"[INFO] Making sure Seaborn is installedâ€¦\\n\")\n    run_and_print([\"pip\", \"install\", \"--quiet\", \"seaborn==0.12.2\"])\n\n    # â‘¡ Copy & (optionally) install block_sparse_attn\n    if os.path.exists(BLOCK_SPARSE_WHEEL_IN):\n        try:\n            shutil.copyfile(BLOCK_SPARSE_WHEEL_IN, BLOCK_SPARSE_WHEEL_OUT)\n            print(\"\\n[INFO] Copied block-sparse-attn wheel to working dir.\")\n            print(\"[INFO] Installing block-sparse-attn (no deps)â€¦\\n\")\n            run_and_print([\"pip\", \"install\", \"--no-deps\", \"--quiet\", BLOCK_SPARSE_WHEEL_OUT])\n        except Exception as e:\n            print(f\"[WARN] Could not copy/install block-sparse wheel: {e}\")\n            print(\"       Continue without it if your code doesnâ€™t need it.\")\n    else:\n        print(\"[WARN] block-sparse-attn wheel not found in /kaggle/input â€“ skipped.\")\n\n    # â‘¢ Install rna_predict (pure-py, so --no-deps is fine)\n    rnapred_whl = f\"/kaggle/input/rna-structure-predict/\" \\\n                  f\"rna_predict-{RNA_PREDICT_VERSION}-py3-none-any.whl\"\n    if os.path.exists(rnapred_whl):\n        print(f\"\\n[INFO] Installing rna_predict {RNA_PREDICT_VERSION} â€¦\\n\")\n        run_and_print([\"pip\", \"install\", \"--no-deps\", \"--quiet\", rnapred_whl])\n    else:\n        print(f\"[WARN] {rnapred_whl} not found â€“ skipped.\")\n\n    # â‘£ Install hydra-core from local wheel\n    HYDRA_DIR = \"/kaggle/input/hydra-core-132whl\"\n    if os.path.isdir(HYDRA_DIR):\n        wheels = [f for f in os.listdir(HYDRA_DIR) if f.endswith(\".whl\")]\n        if wheels:\n            for whl in wheels:\n                whl_path = os.path.join(HYDRA_DIR, whl)\n                print(f\"\\n[INFO] Installing hydra-core from {whl_path} â€¦\\n\")\n                run_and_print([\"pip\", \"install\", \"--no-deps\", \"--quiet\", whl_path])\n        else:\n            print(f\"[WARN] No .whl files found in {HYDRA_DIR} â€“ skipped.\")\n    else:\n        print(f\"[WARN] {HYDRA_DIR} not found â€“ skipped.\")\n\n    # â‘¤ Show final versions\n    print(\"\\n=== [Final Package Versions] ===\")\n    for pkg in [\n        \"torch\", \"block-sparse-attn\", \"rna-predict\",\n        \"hydra-core\", \"numpy\", \"scipy\", \"scikit-learn\", \"seaborn\"\n    ]:\n        run_and_print([\"pip\", \"show\", pkg])\n    print(\"=== [End of Final Package Versions] ===\\n\")\n\n# â•â•â•â•â•â• 4)  Run it  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsetup_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:08.010711Z","iopub.execute_input":"2025-05-16T19:05:08.011020Z","iopub.status.idle":"2025-05-16T19:05:51.408073Z","shell.execute_reply.started":"2025-05-16T19:05:08.010992Z","shell.execute_reply":"2025-05-16T19:05:51.406853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCell 1: ENVIRONMENT SETUP & LOGGING\n-----------------------------------\n\"\"\"\nimport os\nimport sys\nimport logging\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning Libraries\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n\n# Logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogging.info(\"Cell 1 complete: Libraries imported and logging initialized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:51.409308Z","iopub.execute_input":"2025-05-16T19:05:51.409608Z","iopub.status.idle":"2025-05-16T19:05:51.416335Z","shell.execute_reply.started":"2025-05-16T19:05:51.409579Z","shell.execute_reply":"2025-05-16T19:05:51.414995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 2: DATA IMPORT\n-------------------\nHere, we read in train/validation/test CSVs and a sample submission from the Kaggle environment.\nAdjust the paths if needed for your environment.\n\"\"\"\n\n# Example file paths\nTRAIN_SEQUENCES_PATH = \"/kaggle/input/stanford-rna-3d-folding/train_sequences.csv\"\nTRAIN_LABELS_PATH    = \"/kaggle/input/stanford-rna-3d-folding/train_labels.csv\"\nVALID_SEQUENCES_PATH = \"/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv\"\nVALID_LABELS_PATH    = \"/kaggle/input/stanford-rna-3d-folding/validation_labels.csv\"\nTEST_SEQUENCES_PATH  = \"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\"\nSAMPLE_SUB_PATH      = \"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\"\n\ntry:\n    train_sequences = pd.read_csv(TRAIN_SEQUENCES_PATH)\n    train_labels = pd.read_csv(TRAIN_LABELS_PATH)\n    validation_sequences = pd.read_csv(VALID_SEQUENCES_PATH)\n    validation_labels = pd.read_csv(VALID_LABELS_PATH)\n    test_sequences = pd.read_csv(TEST_SEQUENCES_PATH)\n    sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n\n    logging.info(\"Cell 2 complete: Data loaded successfully.\")\nexcept Exception as e:\n    logging.error(f\"Error loading data: {e}\")\n    sys.exit(1)\n\nlogging.info(f\"train_sequences: {train_sequences.shape}, train_labels: {train_labels.shape}\")\nlogging.info(f\"validation_sequences: {validation_sequences.shape}, validation_labels: {validation_labels.shape}\")\nlogging.info(f\"test_sequences: {test_sequences.shape}, sample_submission: {sample_submission.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:51.417642Z","iopub.execute_input":"2025-05-16T19:05:51.418020Z","iopub.status.idle":"2025-05-16T19:05:51.764659Z","shell.execute_reply.started":"2025-05-16T19:05:51.417969Z","shell.execute_reply":"2025-05-16T19:05:51.763287Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 3: COMBINE TRAIN + VALIDATION & BASIC EDA\n----------------------------------------------\nWe concatenate the train and validation sets to maximize data. \nThen we do a quick EDA check on shapes, missingness, etc.\n\"\"\"\n\n# Combine sequences and labels\ntrainval_sequences = pd.concat([train_sequences, validation_sequences], ignore_index=True)\ntrainval_labels = pd.concat([train_labels, validation_labels], ignore_index=True)\n\nlogging.info(f\"Combined train+validation sequences: {trainval_sequences.shape}, labels: {trainval_labels.shape}\")\n\n# Quick check for missing\nlogging.info(\"Missing in combined sequences:\\n\" + str(trainval_sequences.isnull().sum()))\nlogging.info(\"Missing in combined labels:\\n\" + str(trainval_labels.isnull().sum()))\n\n# Example EDA: sequence length distribution\ntrainval_sequences['sequence_length'] = trainval_sequences['sequence'].str.len()\n\nplt.figure(figsize=(10,4))\nsns.boxplot(x=trainval_sequences['sequence_length'], color='skyblue')\nplt.title(\"Boxplot of Sequence Length (Train + Validation)\")\nplt.xlabel(\"Sequence Length\")\nplt.show()\n\nlogging.info(\"Cell 3 complete: Basic EDA finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:51.766234Z","iopub.execute_input":"2025-05-16T19:05:51.766825Z","iopub.status.idle":"2025-05-16T19:05:52.188571Z","shell.execute_reply.started":"2025-05-16T19:05:51.766785Z","shell.execute_reply":"2025-05-16T19:05:52.187427Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 4: HANDLE MISSING COORDINATES & MERGE\n------------------------------------------\nWe replace '-1e18' with np.nan, then merge sequences with labels on target_id.\n\"\"\"\n\n# Replace -1e18 with np.nan in the labels\nfor col in ['x_1','y_1','z_1']:\n    trainval_labels[col] = trainval_labels[col].replace(-1e18, np.nan)\n\nlogging.info(\"Replaced -1e18 with NaN in trainval_labels for x_1, y_1, z_1.\")\n\n# Extract pdb_id, chain_id from ID\ntrainval_labels['pdb_id']   = trainval_labels['ID'].apply(lambda x: x.split('_')[0])\ntrainval_labels['chain_id'] = trainval_labels['ID'].apply(lambda x: x.split('_')[1])\ntrainval_labels['target_id'] = trainval_labels['pdb_id'] + \"_\" + trainval_labels['chain_id']\n\n# Merge\ntrain_data = pd.merge(trainval_labels, trainval_sequences, on='target_id', how='left')\nlogging.info(f\"Merged train_data shape: {train_data.shape}\")\n\n# Quick check\nlogging.info(f\"Missing in x_1: {train_data['x_1'].isnull().sum()}, \"\n             f\"y_1: {train_data['y_1'].isnull().sum()}, \"\n             f\"z_1: {train_data['z_1'].isnull().sum()}\")\n\nlogging.info(\"Cell 4 complete: Merged train_data, ready for group-based imputation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:52.191954Z","iopub.execute_input":"2025-05-16T19:05:52.192298Z","iopub.status.idle":"2025-05-16T19:05:52.663563Z","shell.execute_reply.started":"2025-05-16T19:05:52.192266Z","shell.execute_reply":"2025-05-16T19:05:52.662206Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 5: FEATURE ENGINEERING\n---------------------------\nCreate numerical/categorical features from the 'sequence'.\nWe'll keep 'resname' from the labels as a valuable feature.\n\"\"\"\n\ndef engineer_features(df):\n    \"\"\"\n    Create numerical & (some) categorical features from raw RNA sequence data.\n    \"\"\"\n    df = df.copy()\n    # Sequence-based\n    df['seq_length'] = df['sequence'].str.len()\n    df['A_cnt'] = df['sequence'].str.count('A')\n    df['C_cnt'] = df['sequence'].str.count('C')\n    df['G_cnt'] = df['sequence'].str.count('G')\n    df['U_cnt'] = df['sequence'].str.count('U')\n    df['begin_seq'] = df['sequence'].str[0]\n    df['end_seq']   = df['sequence'].str[-1]\n    \n    # Di-nucleotide counts (example set)\n    for pair in ['AC','AG','AU','CA','CG','CU','GA','GC','GU','UA','UC','UG',\n                 'AA','CC','GG','UU']:\n        df[f'{pair}_cnt'] = df['sequence'].str.count(pair)\n\n    return df\n\n# Apply feature engineering\ntrain_data = engineer_features(train_data)\n\nlogging.info(\"Feature engineering applied to merged train_data.\")\n\n# We'll show an example of newly added columns\nexample_cols = ['seq_length','A_cnt','C_cnt','G_cnt','U_cnt','begin_seq','end_seq','AC_cnt','AA_cnt']\nlogging.info(f\"Columns after FE sample:\\n{train_data[example_cols].head(3)}\")\n\nlogging.info(\"Cell 5 complete: Feature engineering done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:05:52.665836Z","iopub.execute_input":"2025-05-16T19:05:52.666288Z","iopub.status.idle":"2025-05-16T19:06:35.358204Z","shell.execute_reply.started":"2025-05-16T19:05:52.666252Z","shell.execute_reply":"2025-05-16T19:06:35.356006Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 6: GROUP-BASED IMPUTATION\n------------------------------\nWe impute missing x_1, y_1, z_1 within each (target_id, resname) group.\nFinally, if any NAs remain, we fill them with a global median or drop them.\n\"\"\"\n\n# Perform group-based fill for x_1, y_1, z_1\ntrain_data[['x_1','y_1','z_1']] = (\n    train_data\n    .groupby(['target_id','resname'])[['x_1','y_1','z_1']]\n    .apply(lambda grp: grp.fillna(grp.mean()))\n    .reset_index(level=['target_id','resname'], drop=True)\n)\n\n# In case any remain after group-based mean fill (e.g. group is all NaN), do a global fill\nnum_cols = ['x_1','y_1','z_1']\nglobal_imputer = SimpleImputer(strategy='median')\ntrain_data[num_cols] = global_imputer.fit_transform(train_data[num_cols])\n\n# If you'd prefer to drop any leftover NAs instead:\n# train_data.dropna(subset=['x_1','y_1','z_1'], inplace=True)\n\nlogging.info(\"Group-based imputation + global median fallback complete.\")\n\n# Confirm missing values\nlogging.info(f\"Remaining missing x_1: {train_data['x_1'].isna().sum()}, \"\n             f\"y_1: {train_data['y_1'].isna().sum()}, z_1: {train_data['z_1'].isna().sum()}\")\n\nlogging.info(\"Cell 6 complete: Group-based imputation finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:35.359808Z","iopub.execute_input":"2025-05-16T19:06:35.360289Z","iopub.status.idle":"2025-05-16T19:06:46.697400Z","shell.execute_reply.started":"2025-05-16T19:06:35.360252Z","shell.execute_reply":"2025-05-16T19:06:46.695816Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 7: PREPARE DATA FOR MODELING\n---------------------------------\nWe'll define the columns we won't use, set up X and y for x_1, y_1, z_1, \nand one-hot encode any relevant categorical columns (including resname).\n\"\"\"\n\n# Unused columns\nunused_cols = [\n    'ID','pdb_id','chain_id','resid',\n    'x_1','y_1','z_1',\n    'sequence','description','temporal_cutoff','all_sequences',\n    'target_id'  # key used for merges\n]\n\n# We'll keep resname, begin_seq, end_seq as features this time\nfeature_cols = [col for col in train_data.columns if col not in unused_cols]\n\n# Make a copy\ntrain_df = train_data.copy()\n\n# Convert to categories\nfor cat_col in ['resname','begin_seq','end_seq']:\n    if cat_col in feature_cols:\n        train_df[cat_col] = train_df[cat_col].astype('category')\n\n# One-hot encode\ntrain_df = pd.get_dummies(train_df, columns=['resname','begin_seq','end_seq'], drop_first=True)\n\n# Our final set of features\nX_cols = [col for col in train_df.columns if col not in unused_cols]\n\nX_full = train_df[X_cols]\ny_x_full = train_df['x_1']\ny_y_full = train_df['y_1']\ny_z_full = train_df['z_1']\n\nlogging.info(f\"Feature matrix shape: {X_full.shape}\")\nlogging.info(\"Cell 7 complete: Prepared data for modeling.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:46.698754Z","iopub.execute_input":"2025-05-16T19:06:46.699321Z","iopub.status.idle":"2025-05-16T19:06:47.244815Z","shell.execute_reply.started":"2025-05-16T19:06:46.699153Z","shell.execute_reply":"2025-05-16T19:06:47.243256Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 8: KFold CV for X, Y, Z & Hyperparam Search\n------------------------------------------------\nWe'll do a simplified KFold cross-validation for each coordinate \nto get a sense of good hyperparams, then train final models.\n\"\"\"\n\nfrom sklearn.model_selection import KFold, RandomizedSearchCV\nimport numpy as np\n\n# Example hyperparameter grid (you can expand as needed)\nparam_dist = {\n    'learning_rate': [0.03, 0.05, 0.1],\n    'max_depth': [6, 10, 15],\n    'n_estimators': [500, 800, 1000],\n    'subsample': [0.7, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.9, 1.0]\n}\n\ndef run_random_search(X, y, param_dist, n_iter=5, cv_splits=3):\n    \"\"\"Simple RandomizedSearchCV for an XGBRegressor using GPU in XGBoost >= 2.0.\"\"\"\n    xgb = XGBRegressor(tree_method='hist', device='cuda', random_state=42)\n    rsearch = RandomizedSearchCV(\n        estimator=xgb,\n        param_distributions=param_dist,\n        n_iter=n_iter,\n        scoring='neg_mean_squared_error',\n        cv=cv_splits,\n        verbose=1,\n        random_state=42\n    )\n    rsearch.fit(X, y)\n    best_model = rsearch.best_estimator_\n    logging.info(f\"Best params: {rsearch.best_params_}, Best CV Score: {rsearch.best_score_}\")\n    return best_model, rsearch.best_params_\n\nlogging.info(\"Starting hyperparam search for X coordinate.\")\n#best_model_x, best_params_x = run_random_search(X_full, y_x_full, param_dist, n_iter=5, cv_splits=3)\n\nlogging.info(\"Starting hyperparam search for Y coordinate.\")\n#best_model_y, best_params_y = run_random_search(X_full, y_y_full, param_dist, n_iter=5, cv_splits=3)\n\nlogging.info(\"Starting hyperparam search for Z coordinate.\")\n#best_model_z, best_params_z = run_random_search(X_full, y_z_full, param_dist, n_iter=5, cv_splits=3)\n\nlogging.info(\"Cell 8 complete: RandomizedSearchCV best params found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:47.246482Z","iopub.execute_input":"2025-05-16T19:06:47.246859Z","iopub.status.idle":"2025-05-16T19:06:47.256247Z","shell.execute_reply.started":"2025-05-16T19:06:47.246826Z","shell.execute_reply":"2025-05-16T19:06:47.254737Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 9: FINAL TRAINING ON FULL DATA\n-----------------------------------\nUse the best hyperparams for each coordinate found in CV. \nRetrain each coordinate model on all data (X_full, y_*_full).\n\"\"\"\n\ndef get_best_xgb(params):\n    \"\"\" Return an XGBRegressor with the given params, using GPU. \"\"\"\n    # Here we override or add 'tree_method' to ensure GPU usage\n    # We can also specify predictor='gpu_predictor' to accelerate inference on GPU\n    model = XGBRegressor(\n        **params,\n        tree_method='hist',   # or tree_method=params.get('tree_method', 'hist')\n        device='cuda',        # ensures GPU usage\n        random_state=42\n    )\n    return model\n\nlogging.info(\"Retraining final model for X coordinate...\")\n#model_x = get_best_xgb(best_params_x)\n#model_x.fit(X_full, y_x_full)\n\nlogging.info(\"Retraining final model for Y coordinate...\")\n#model_y = get_best_xgb(best_params_y)\n#model_y.fit(X_full, y_y_full)\n\nlogging.info(\"Retraining final model for Z coordinate...\")\n#model_z = get_best_xgb(best_params_z)\n#model_z.fit(X_full, y_z_full)\n\nlogging.info(\"Cell 9 complete: Final models trained.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:47.257410Z","iopub.execute_input":"2025-05-16T19:06:47.257725Z","iopub.status.idle":"2025-05-16T19:06:47.291639Z","shell.execute_reply.started":"2025-05-16T19:06:47.257695Z","shell.execute_reply":"2025-05-16T19:06:47.289896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 10: PREPARE & ENGINEER TEST DATA\n-------------------------------------\nâ€¢ Expand test_sequences into (ID, resname, resid)\nâ€¢ Merge residueâ€‘level grid with perâ€‘sequence engineered features\nâ€¢ Align with training feature matrix X_full, fill missing values\n\"\"\"\n\n# ---------- 1. Expand residue grid ----------\ntest_expanded = [\n    [row[\"target_id\"], nt, i]\n    for _, row in test_sequences.iterrows()\n    for i, nt in enumerate(row[\"sequence\"], start=1)\n]\ntest_clean_df = pd.DataFrame(test_expanded, columns=[\"ID\", \"resname\", \"resid\"])\nlogging.info(f\"test_clean_df shape: {test_clean_df.shape} (expanded test sequences)\")\n\n# ---------- 2. Perâ€‘sequence engineered features ----------\ntest_feats = engineer_features(test_sequences)\n\n# Merge â€“ one row per residue, sequenceâ€‘level features broadcast to each residue\ntest_merged = pd.merge(\n    test_clean_df,\n    test_feats.drop(columns=[\"seq_length\"]),   # drop if not needed\n    left_on=\"ID\",\n    right_on=\"target_id\",\n    how=\"left\"\n)\nlogging.info(f\"test_merged shape after merging: {test_merged.shape}\")\n\n# ---------- 3. Clean up ----------\n# Replace sentinel values\nfor col in [\"x_1\", \"y_1\", \"z_1\"]:\n    if col in test_merged.columns:\n        test_merged[col] = test_merged[col].replace(-1e18, np.nan)\n\n# Drop columns not used by the model\ndrop_cols = [\"sequence\", \"description\", \"temporal_cutoff\", \"all_sequences\", \"target_id\"]\ntest_merged.drop(columns=[c for c in drop_cols if c in test_merged.columns], inplace=True, errors=\"ignore\")\n\n# ---------- 4. Categorical handling ----------\ncat_cols = {\"resname\", \"begin_seq\", \"end_seq\"} & set(test_merged.columns)\nfor col in cat_cols:\n    test_merged[col] = test_merged[col].astype(\"category\")\ntest_merged = pd.get_dummies(test_merged, columns=list(cat_cols), drop_first=True)\n\n# ---------- 5. Column alignment ----------\n# Single vectorised reindex instead of perâ€‘column insertion â†’ no fragmentation warning\ntest_merged = test_merged.reindex(columns=X_full.columns, fill_value=0)\n\n# ---------- 6. Missingâ€‘value imputation ----------\n# Fit a NEW median imputer on the training feature matrix (numeric cols only)\nnumeric_cols = X_full.select_dtypes(include=np.number).columns\nfeature_imputer = SimpleImputer(strategy=\"median\")\nfeature_imputer.fit(X_full[numeric_cols])\n\ntest_merged[numeric_cols] = feature_imputer.transform(test_merged[numeric_cols])\n\n# ---------- 7. All done ----------\ntest_merged_imputed = test_merged.copy()\nlogging.info(\"Cell 10 complete: Test data prepared, aligned, and imputed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:47.293225Z","iopub.execute_input":"2025-05-16T19:06:47.293726Z","iopub.status.idle":"2025-05-16T19:06:49.855775Z","shell.execute_reply.started":"2025-05-16T19:06:47.293684Z","shell.execute_reply":"2025-05-16T19:06:49.854293Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 HF_HOME=/kaggle/working TRANSFORMERS_CACHE=/kaggle/working ln -sf /kaggle/input/rna-torsion-bert-checkpoint-base/kaggle/working/rna_torsionBERT /kaggle/working/rna_torsionBERT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:49.857717Z","iopub.execute_input":"2025-05-16T19:06:49.858361Z","iopub.status.idle":"2025-05-16T19:06:50.048166Z","shell.execute_reply.started":"2025-05-16T19:06:49.858304Z","shell.execute_reply":"2025-05-16T19:06:50.046315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: RNA Prediction with TorsionBERT  (offline-ready)\n# ------------------------------------------------------\nimport pandas as pd, torch, os, logging, sys, transformers\nfrom omegaconf import OmegaConf\nfrom functools import partial\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 1) LINK LOCAL CHECKPOINTS â–¸ rna_torsionBERT  &  DNA_Bert_3\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nif not os.path.exists(\"/kaggle/working/rna_torsionBERT\"):\n    os.symlink(\n        \"/kaggle/input/rna-torsion-bert-checkpoint-base/kaggle/working/rna_torsionBERT\",\n        \"/kaggle/working/rna_torsionBERT\"\n    )\n\nDNA_BERT_SRC = \"/kaggle/input/dna-bert-rna/DNA_bert_3\"\nDNA_BERT_DST = \"/kaggle/working/zhihan1996/DNA_bert_3\"   # path hard-coded in torsionBERT\nif not os.path.exists(DNA_BERT_DST):\n    os.makedirs(\"/kaggle/working/zhihan1996\", exist_ok=True)\n    os.symlink(DNA_BERT_SRC, DNA_BERT_DST)\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 2) FORCE OFFLINE MODE  +  MAP zhihan1996/* IDs â†’ local folders\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nos.environ.update({\n    \"HF_HUB_OFFLINE\":      \"1\",\n    \"HF_DATASETS_OFFLINE\": \"1\",\n    \"TRANSFORMERS_OFFLINE\":\"1\",\n    \"HF_HOME\":             \"/kaggle/working\",\n    \"TRANSFORMERS_CACHE\":  \"/kaggle/working\"\n})\ndef _localize(repo,*a,**kw):\n    \"\"\"\n    Redirect zhihan1996/DNA_bert_* to local paths and\n    force local_files_only for every HF load.\n    \"\"\"\n    if isinstance(repo,str) and repo.startswith(\"zhihan1996/DNA_bert_\"):\n        repo = \"/kaggle/working/\" + repo\n    kw[\"local_files_only\"] = True\n    return repo,a,kw\n# robust monkey-patch (handles partials, repeated patching, etc.)\nfor _cls in (\"AutoConfig\",\"AutoTokenizer\",\"AutoModel\"):\n    obj      = getattr(transformers, _cls)\n    base_cls = obj.func if isinstance(obj, partial) else obj\n    if not hasattr(base_cls, \"from_pretrained\"):\n        continue\n    _orig = base_cls.from_pretrained\n    def _wrap(repo,*a,__o=_orig,**kw):\n        repo,a,kw = _localize(repo,*a,**kw)\n        return __o(repo,*a,**kw)\n    base_cls.from_pretrained = _wrap\n\n# accept DNA-Bert-3 custom config\ntry:\n    from importlib import import_module\n    custom_conf = import_module(\n        \"transformers_modules.DNA_bert_3.configuration_bert\"\n    ).BertConfig\n    transformers.models.bert.modeling_bert.BertModel.config_class = custom_conf\nexcept Exception as e:\n    logging.warning(f\"[WARN] DNA_Bert_3 config patch skipped: {e}\")\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 3) LOGGING & tiny shell helper\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nlogging.basicConfig(level=logging.INFO,\n                    format=\"%(asctime)s | %(levelname)s | %(message)s\")\ndef run_and_print(cmd):\n    import subprocess, shlex, textwrap\n    res = subprocess.run(cmd if isinstance(cmd,list) else shlex.split(cmd),\n                         capture_output=True, text=True)\n    if res.stdout: print(res.stdout, end=\"\")\n    if res.stderr: print(\"STDERR:\", textwrap.shorten(res.stderr,400), end=\"\")\n    return res\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 4) ENSURE hydra-core (local wheel) â€“ omegaconf already present\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nrun_and_print([\n    \"pip\",\"install\",\"--no-index\",\"--no-deps\",\"--force-reinstall\",\n    \"/kaggle/input/hydra-core-132whl/hydra_core-1.3.2-py3-none-any.whl\"\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:50.049993Z","iopub.execute_input":"2025-05-16T19:06:50.050465Z","iopub.status.idle":"2025-05-16T19:06:51.789565Z","shell.execute_reply.started":"2025-05-16T19:06:50.050419Z","shell.execute_reply":"2025-05-16T19:06:51.788183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 5) RNAPredictor CONFIG (Hydra best practices, stochastic inference)\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nfrom rna_predict.interface import RNAPredictor\n# Import OmegaConf and torch if not already imported in the cell\nfrom omegaconf import OmegaConf\nimport torch\nimport logging # Ensure logging is imported if you use logger.info\n\nTEST_SEQS  = \"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\"\nSAMPLE_SUB = \"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\"\nOUTPUT_CSV = \"submission.csv\"\n\ndef create_predictor():\n    \"\"\"Instantiate RNAPredictor with local checkpoints & GPU/CPU autodetect, matching Hydra config structure.\"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    logging.info(f\"Device: {device}\") # Assuming logging is configured\n    cfg = OmegaConf.create({\n        # Top-level keys consistent with a full Hydra config (e.g., default.yaml)\n        \"device\": device,\n        \"seed\": 42, # Good for reproducibility if used by models\n        \"atoms_per_residue\": 44, # Standard value\n        \"extraction_backend\": \"dssr\", # Or \"mdanalysis\" as needed\n\n        \"pipeline\": { # General pipeline settings\n            \"verbose\": True,\n            \"save_intermediates\": True,\n            # output_dir is usually set by Hydra's run directory or overridden\n        },\n\n        \"prediction\": { # Prediction-specific settings\n            \"repeats\": 5,\n            \"residue_atom_choice\": 0,\n            \"enable_stochastic_inference_for_submission\": True, # CRITICAL: Ensures unique predictions\n            # \"submission_seeds\": [42, 101, 2024, 7, 1991],  # Optional: for reproducible stochastic runs\n        },\n\n        \"model\": {\n            # â”€â”€ Stage B: torsion-angle prediction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n            \"stageB\": {\n                \"torsion_bert\": {\n                    \"model_name_or_path\": \"/kaggle/working/rna_torsionBERT\", # Path to local TorsionBERT model\n                    \"device\": device,\n                    \"angle_mode\": \"degrees\",   # CHANGED: Set to \"degrees\" for consistency with StageC\n                                               # This ensures StageBTorsionBertPredictor outputs angles in degrees.\n                    \"num_angles\": 7,\n                    \"max_length\": 512,\n                    \"checkpoint_path\": None,   # Can be overridden if a specific checkpoint is needed\n                    \"debug_logging\": True,     # Set to False if logs are too verbose\n                    \"init_from_scratch\": False, # Assumes using pretrained TorsionBERT\n                    \"lora\": {                  # LoRA config (currently disabled)\n                        \"enabled\": False,\n                        \"r\": 8,\n                        \"alpha\": 16,\n                        \"dropout\": 0.1,\n                        \"target_modules\": [\"query\", \"value\"],\n                    },\n                }\n                # Pairformer config would go here if used: \"pairformer\": { ... }\n            },\n            # â”€â”€ Stage C: 3D reconstruction (MP-NeRF) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n            \"stageC\": {\n                \"enabled\": True,\n                \"method\": \"mp_nerf\",\n                \"do_ring_closure\": False,       # Consistent with default.yaml; notebook log showed True, adjust if needed.\n                \"place_bases\": True,\n                \"sugar_pucker\": \"C3'-endo\",\n                \"device\": device,\n                \"debug_logging\": True,          # Set to False if logs are too verbose\n                \"angle_representation\": \"degrees\", # StageC expects angles in degrees from StageB\n                \"use_metadata\": False,\n                \"use_memory_efficient_kernel\": False,\n                \"use_deepspeed_evo_attention\": False,\n                \"use_lma\": False,\n                \"inplace_safe\": False,          # Consistent with default.yaml; notebook log showed True.\n                \"chunk_size\": None,\n            },\n            # â”€â”€ Stage D: Diffusion refinement (minimal placeholder) â”€â”€â”€â”€â”€â”€â”€â”€\n            # Add full StageD config if it's actively used in this notebook\n            \"stageD\": {\n                \"enabled\": False, # Set to True if StageD is part of this specific notebook's pipeline\n                \"mode\": \"inference\",\n                \"device\": device,\n                \"debug_logging\": True,\n                # Placeholder for other essential StageD keys if enabled:\n                # \"ref_element_size\": 128,\n                # \"ref_atom_name_chars_size\": 256,\n                # \"profile_size\": 32,\n                # \"model_architecture\": { ... },\n                # \"diffusion\": { ... }\n            },\n        }\n    })\n    return RNAPredictor(cfg)\n\n# Usage example:\npredictor = create_predictor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:51.791335Z","iopub.execute_input":"2025-05-16T19:06:51.791699Z","iopub.status.idle":"2025-05-16T19:06:52.817529Z","shell.execute_reply.started":"2025-05-16T19:06:51.791663Z","shell.execute_reply":"2025-05-16T19:06:52.816481Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# PATCH â–¸ guarantee predict_submission returns ONE ROW per residue  âœ…\n#         â€¢ works both when Stage C gives [L, atoms, 3]  OR  [N_atoms, 3]\n#         â€¢ keeps all original columns created by coords_to_df\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport logging, torch, pandas as pd\nfrom rna_predict.interface import RNAPredictor\nfrom rna_predict.utils.submission import coords_to_df, extract_atom, reshape_coords\n\nlog = logging.getLogger(\"rna_predict.patch.flat2res\")\n\ndef _predict_submission_patched(\n    self,\n    sequence: str,\n    prediction_repeats: int | None = None,\n    residue_atom_choice: int | None = None,\n):\n    \"\"\"\n    Collapses per-atom coordinates â†’ one canonical atom per residue.\n    â€¢ Prefers phosphate (â€œPâ€); falls back to first atom per residue.\n    â€¢ Always returns exactly len(sequence) rows, preserving coords_to_df schema.\n    \"\"\"\n    # â”€â”€ original prologue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    result      = self.predict_3d_structure(sequence)\n    coords_flat = result[\"coords\"]                       # 2-D [N_atoms, 3]\n\n    # ğŸ”§ NEW: make it a plain tensor so .numpy() is allowed\n    if coords_flat.requires_grad:                        # â† the bug-fix\n        coords_flat = coords_flat.detach()\n\n    metadata        = result.get(\"atom_metadata\", {})\n    atom_names      = metadata.get(\"atom_names\", [])\n    residue_indices = metadata.get(\"residue_indices\", [])\n\n    repeats  = prediction_repeats if prediction_repeats is not None else self.default_repeats\n    atom_idx = residue_atom_choice if residue_atom_choice is not None else self.default_atom_choice\n\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # â¶  FLAT-COORDS PATH   (Stage C returned [N_atoms, 3])\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if coords_flat.dim() == 2 and coords_flat.shape[0] != len(sequence):\n        if not atom_names or not residue_indices:\n            log.error(\"[flat-coords] missing atom metadata â†’ falling back to legacy per-atom output\")\n            base = {\n                \"ID\":      range(1, len(coords_flat) + 1),\n                \"resname\": [\"X\"] * len(coords_flat),\n                \"resid\":   range(1, len(coords_flat) + 1),\n            }\n            df = pd.DataFrame(base)\n            for i in range(1, repeats + 1):\n                df[[f\"{ax}_{i}\" for ax in \"xyz\"]] = coords_flat.cpu().numpy()\n            return df\n\n        tmp = pd.DataFrame({\n            \"atom_name\": atom_names,\n            \"res0\":      residue_indices,       # 0-based residue index\n            \"x\": coords_flat[:, 0].cpu().numpy(),\n            \"y\": coords_flat[:, 1].cpu().numpy(),\n            \"z\": coords_flat[:, 2].cpu().numpy(),\n        })\n\n        # pick one atom per residue (prefer P)\n        picked = (tmp[tmp.atom_name == \"P\"]\n                  .drop_duplicates(\"res0\", keep=\"first\")\n                  .sort_values(\"res0\"))\n        if len(picked) != len(sequence):        # fallback if some Pâ€™s missing\n            log.warning(\"[flat-coords] P-selection gave %d/%d rows â€“ using first atom fallback\",\n                        len(picked), len(sequence))\n            picked = (tmp.groupby(\"res0\", as_index=False)\n                         .first()\n                         .sort_values(\"res0\"))\n\n        per_res_coords = torch.tensor(\n            picked[[\"x\", \"y\", \"z\"]].values,\n            dtype=coords_flat.dtype,\n            device=coords_flat.device,\n        )\n\n        return coords_to_df(sequence, per_res_coords, repeats)\n\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # â·  ORIGINAL â€œreshapedâ€ PATH  (Stage C returned [L, atoms, 3])\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    coords = reshape_coords(coords_flat, len(sequence))\n    if coords.dim() == 2 and coords.shape[0] != len(sequence):\n        # reshape failed â†’ treat as flat once more\n        log.warning(\"[reshape_coords] produced flat coords â€“ rerouting through flat-coords logic.\")\n        result[\"coords\"] = coords\n        return _predict_submission_patched(self, sequence, prediction_repeats, residue_atom_choice)\n\n    atom_coords = extract_atom(coords, atom_idx)\n    return coords_to_df(sequence, atom_coords, repeats)\n\n# install the patch (simple attribute assignment is enough)\nRNAPredictor.predict_submission = _predict_submission_patched\nlog.info(\"âœ“ RNAPredictor.predict_submission patched (flat-coords fix)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:52.818688Z","iopub.execute_input":"2025-05-16T19:06:52.818976Z","iopub.status.idle":"2025-05-16T19:06:52.833843Z","shell.execute_reply.started":"2025-05-16T19:06:52.818951Z","shell.execute_reply":"2025-05-16T19:06:52.832636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"toy = create_predictor().predict_submission(\"ACGUACGU\", prediction_repeats=1)\nassert len(toy) == 8            # âœ… one row per residue\nprint(toy.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:06:52.835076Z","iopub.execute_input":"2025-05-16T19:06:52.835554Z","iopub.status.idle":"2025-05-16T19:06:54.965454Z","shell.execute_reply.started":"2025-05-16T19:06:52.835511Z","shell.execute_reply":"2025-05-16T19:06:54.964350Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 6) PREDICTION UTILITIES  â— de-duplication / aggregation safeguard  âœ…\n# -----------------------------------------------------------------------\n# NOTE: This cell assumes RNAPredictor has been patched with the corrected\n#       _predict_submission_patched method in a PREVIOUS cell.\n# Fix: Ensure _collapse_to_one_row_per_residue can correctly infer num_repeats\n#      and handle the original sequence string for resname population.\n# -----------------------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nimport logging\nimport sys # For sys.stdout in logger handler\nimport os # For os.path.exists\n# Assuming create_predictor is defined in a previous cell and available in global scope\n# Assuming RNAPredictor is imported and patched in a previous cell\n\n# Configure logger for this cell if not already done globally\nlogger_cell6 = logging.getLogger(\"rna_predict.cell6_utils\")\nif not logger_cell6.handlers:\n    handler_cell6 = logging.StreamHandler(sys.stdout)\n    formatter_cell6 = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler_cell6.setFormatter(formatter_cell6)\n    logger_cell6.addHandler(handler_cell6)\nlogger_cell6.setLevel(logging.INFO)\n\n\ndef _auto_column(df: pd.DataFrame, pref: list[str]) -> str:\n    \"\"\"Return first column present in *pref* (fallback â†’ df.columns[0]).\"\"\"\n    for c in pref:\n        if c in df.columns:\n            return c\n    if df.columns.empty: # Guard against empty DataFrame columns\n        logger_cell6.warning(\"_auto_column called with DataFrame with no columns.\")\n        return \"\" # Or raise an error, depending on desired behavior\n    return df.columns[0]\n\n\ndef _collapse_to_one_row_per_residue(df_raw: pd.DataFrame, seq_id: str, original_sequence: str, num_repeats: int) -> pd.DataFrame:\n    \"\"\"\n    Ensure the DataFrame has one row per residue with clean IDs and standard columns.\n    This function now primarily serves as a schema enforcer and final sanity check.\n    \"\"\"\n    df = df_raw.copy()\n\n    if df.empty:\n        logger_cell6.warning(f\"_collapse_to_one_row_per_residue received empty DataFrame for seq_id: {seq_id}\")\n        # Define expected columns based on num_repeats for an empty DataFrame\n        cols = [\"ID\", \"resname\", \"resid\"] + [f\"{ax}_{k}\" for k in range(1, num_repeats + 1) for ax in \"xyz\"]\n        return pd.DataFrame(columns=cols)\n\n    # 1ï¸âƒ£ & 2ï¸âƒ£: First repeat/angle only & Canonical atom selection\n    # These steps are assumed to have been handled by the patched `_predict_submission_patched`\n    # The input `df_raw` should ideally be one row per residue with all 5 repeats as columns.\n\n    # 3ï¸âƒ£ Safeguard: Average duplicates if `resid` is not unique (shouldn't happen if patched predict_submission is correct)\n    if \"resid\" in df.columns and not df[\"resid\"].is_unique:\n        logger_cell6.warning(f\"Residues in DataFrame for {seq_id} are not unique. Attempting to average duplicates.\")\n        coord_cols = [c for c in df.columns if c.startswith(('x_', 'y_', 'z_'))]\n        key_cols_present = [k for k in [\"resid\", \"resname\"] if k in df.columns]\n        if key_cols_present:\n            df = (\n                df.groupby(key_cols_present, as_index=False, sort=False)[coord_cols].mean()\n                  .reset_index(drop=True)\n            )\n    \n    # 4ï¸âƒ£ Ensure 'resid' is 1-based sequential and 'ID' is correctly formatted\n    # This is important because the input `df_raw` from the patched `predict_submission`\n    # should already have `len(sequence)` rows.\n    if not df.empty:\n        if \"resid\" not in df.columns or not pd.api.types.is_numeric_dtype(df[\"resid\"]) or df[\"resid\"].isnull().any() or not df[\"resid\"].is_monotonic_increasing:\n             df[\"resid\"] = np.arange(1, len(df) + 1) # Re-create if problematic\n        \n        if \"resname\" not in df.columns: # Ensure resname column exists\n             df[\"resname\"] = list(original_sequence)[:len(df)] if len(original_sequence) >= len(df) else (list(original_sequence) + ['X']*(len(df)-len(original_sequence)))\n        \n        if \"ID\" in df.columns:\n            df = df.drop(columns=\"ID\")\n        df.insert(0, \"ID\", [f\"{seq_id}_{r}\" for r in df[\"resid\"]])\n\n\n    # 5ï¸âƒ£ Ensure all required coordinate columns (x_1..z_5 etc.) exist and have the correct names\n    # The number of repeats is now taken from the argument.\n    expected_coord_cols = [f\"{ax}_{i+1}\" for i in range(num_repeats) for ax in \"xyz\"]\n    final_cols_schema = [\"ID\", \"resname\", \"resid\"] + expected_coord_cols\n    \n    for col in final_cols_schema:\n        if col not in df.columns:\n            logger_cell6.warning(f\"Column '{col}' missing in DataFrame for {seq_id}. Adding with NaNs/defaults.\")\n            if col.startswith(('x_', 'y_', 'z_')):\n                df[col] = np.nan\n            elif col == \"resname\":\n                 df[col] = list(original_sequence)[:len(df)] if len(original_sequence) >= len(df) else (list(original_sequence) + ['X']*(len(df)-len(original_sequence)))\n            elif col == \"resid\" and \"resid\" not in df.columns : # Should have been handled\n                 df[col] = np.arange(1, len(df)+1) if not df.empty else []\n            elif col == \"ID\" and \"ID\" not in df.columns and \"resid\" in df.columns : # Should have been handled\n                 df.insert(0, \"ID\", [f\"{seq_id}_{r}\" for r in df[\"resid\"]])\n            elif not df.empty : # For any other unexpected missing column\n                 df[col] = \"\" \n    \n    # Return only the columns expected in the submission, in the correct order\n    return df[final_cols_schema]\n\n\ndef process_test_sequences(test_csv: str, sample_csv: str, out_csv: str, *, batch: int = 1):\n    \"\"\"Generate submission file after collapsing predictions.\"\"\"\n\n    df_test = pd.read_csv(test_csv)\n    logging.info(\"Loaded %d sequences for processing.\", len(df_test)) # Use global logging or logger_cell6\n\n    # create_predictor() is defined in the preceding cell (Cell 12 in your notebook structure)\n    predictor = create_predictor()\n    # Get number of repeats from the predictor's configuration\n    num_repeats = predictor.prediction_config.repeats\n\n\n    id_col  = _auto_column(df_test, [\"id\", \"ID\", \"seq_id\", \"sequence_id\"])\n    seq_col = _auto_column(df_test, [\"sequence\", \"Sequence\", \"seq\", \"SEQ\"])\n\n    frames: list[pd.DataFrame] = []\n    for start_idx in range(0, len(df_test), batch): # Renamed 'start' to 'start_idx' to avoid conflict\n        end_idx = min(start_idx + batch, len(df_test))\n        logging.info(\"Processing batch: %dâ€“%d\", start_idx + 1, end_idx) # Use global logging or logger_cell6\n        \n        for i in range(start_idx, end_idx):\n            sid = df_test.at[i, id_col]\n            seq_str = df_test.at[i, seq_col] # Store the original sequence string\n\n            # Pass current_target_id to the predictor if your patched method uses it\n            if hasattr(predictor, 'current_target_id'):\n                 predictor.current_target_id = sid\n\n            if not isinstance(seq_str, str) or not seq_str:\n                logging.warning(f\"Skipping invalid or empty sequence for ID {sid} at index {i}.\")\n                # Create an empty DataFrame with correct columns\n                temp_df_empty = pd.DataFrame(columns=[\"ID\", \"resname\", \"resid\"] + [f\"{ax}_{k}\" for k in range(1, num_repeats + 1) for ax in \"xyz\"])\n                frames.append(temp_df_empty)\n                continue\n            \n            try:\n                # The patched predict_submission is called here.\n                # It should return a DataFrame that is already one-row-per-residue.\n                raw_predictions_df  = predictor.predict_submission(seq_str) # No need to pass repeats if it uses self.config\n                \n                # Call _collapse_to_one_row_per_residue for final schema enforcement\n                tidy_predictions_df = _collapse_to_one_row_per_residue(raw_predictions_df, sid, seq_str, num_repeats)\n                frames.append(tidy_predictions_df)\n            except Exception as err:\n                logging.error(\"Sequence %s (ID: %s) failed prediction: %s\", seq_str[:30]+\"...\", sid, err, exc_info=True)\n\n\n    if not frames:\n        logging.error(\"No successful predictions to concatenate. submission.csv will be empty or not created.\")\n        sample_submission_df = pd.read_csv(sample_csv)\n        empty_submission_df = pd.DataFrame(columns=sample_submission_df.columns)\n        empty_submission_df.to_csv(out_csv, index=False)\n        return empty_submission_df\n\n    results_df = pd.concat(frames, ignore_index=True)\n    \n    # Final alignment with sample_submission.csv to ensure exact format and row order\n    try:\n        sample_submission_df = pd.read_csv(sample_csv)\n        \n        # Ensure 'ID' column in results_df matches the sample submission's ID format for merging\n        # The _collapse_to_one_row_per_residue should have formatted 'ID' correctly.\n        \n        # Use a left merge to ensure all IDs from sample_submission are present and in order\n        final_submission_df = pd.merge(sample_submission_df[['ID']], # Only take ID column for merging keys\n                                       results_df, \n                                       on='ID', \n                                       how='left') # Use left merge to keep all sample submission IDs\n        \n        # Fill NaNs for any coordinates that might be missing after the merge (e.g., if a sequence failed or had fewer residues)\n        # The columns in final_submission_df will be based on results_df after the merge.\n        # We need to ensure all columns from sample_submission_df are present.\n        for col in sample_submission_df.columns:\n            if col not in final_submission_df.columns:\n                final_submission_df[col] = np.nan # Add missing columns with NaNs\n            if col.startswith(('x_', 'y_', 'z_')): # Fill NaNs in coordinate columns with 0.0\n                final_submission_df[col] = final_submission_df[col].fillna(0.0)\n            elif col in [\"resname\", \"resid\"] and col in final_submission_df.columns: # Fill NaNs in resname/resid if they exist\n                # This part might need more sophisticated filling if a whole sequence was missing\n                # For now, simple fillna. If `resname` or `resid` are entirely NaN for a sequence,\n                # they will remain NaN unless sample_submission has values for those IDs.\n                # A more robust fill would re-derive from sample_submission for missing ID rows.\n                pass # Let merge handle this; if ID was in sample but not results, resname/resid will be NaN\n\n        # Ensure exact column order and presence as in sample_submission.csv\n        final_submission_df = final_submission_df[sample_submission_df.columns.tolist()]\n\n    except Exception as e:\n        log.error(f\"Error aligning submission with sample_submission.csv: {e}. Saving raw concatenated results.\")\n        final_submission_df = results_df # Fallback to saving whatever was concatenated\n\n    final_submission_df.to_csv(out_csv, index=False)\n    logging.info(\"Saved final submission to â†’ %s  (#rows = %s)\", out_csv, f\"{len(final_submission_df):,}\")\n    return final_submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:27.588545Z","iopub.execute_input":"2025-05-16T19:10:27.589142Z","iopub.status.idle":"2025-05-16T19:10:27.621046Z","shell.execute_reply.started":"2025-05-16T19:10:27.589084Z","shell.execute_reply":"2025-05-16T19:10:27.619148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 7) TOY SANITY-CHECK â€“ demonstrates collapse function\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"\\n=== Toy sanity-check ===\")\n# Assuming create_predictor() and _collapse_to_one_row_per_residue are defined\n# in previous cells and available in the global scope.\n# Also assuming predictor is instantiated via predictor = create_predictor()\n\ntoy_sequence = \"ACGUACGU\"\nnum_toy_repeats = 2 # Or get from predictor.prediction_config.repeats if predictor is already created\n\n# Ensure predictor is created if not already (it was in your previous cell structure)\n# If predictor is not yet created in this cell's context, uncomment the next line:\n# predictor = create_predictor() \n# num_toy_repeats = predictor.prediction_config.repeats # More robust way to get repeats\n\ntoy_raw  = predictor.predict_submission(toy_sequence, prediction_repeats=num_toy_repeats)\n\n# Call _collapse_to_one_row_per_residue with the required arguments\ntoy_comp = _collapse_to_one_row_per_residue(toy_raw, \"TOY\", toy_sequence, num_toy_repeats)\nprint(toy_comp.head())\n\n# The uniqueness check for the toy example also needs num_toy_repeats\ncoords_cols_toy = [f\"x_{i+1}\" for i in range(num_toy_repeats)] + \\\n                  [f\"y_{i+1}\" for i in range(num_toy_repeats)] + \\\n                  [f\"z_{i+1}\" for i in range(num_toy_repeats)]\nunique_structs_toy = set()\nif not toy_raw.empty: # Check if toy_raw is not empty before proceeding\n    for i in range(num_toy_repeats): \n        x_col, y_col, z_col = f\"x_{i+1}\", f\"y_{i+1}\", f\"z_{i+1}\"\n        # Ensure all necessary columns exist in toy_raw before trying to access them\n        if all(col in toy_raw.columns for col in [x_col, y_col, z_col]):\n            coords_tuple = tuple(toy_raw[[x_col, y_col, z_col]].to_numpy().flatten())\n            unique_structs_toy.add(coords_tuple)\n        else:\n            log.warning(f\"Coordinate columns for repeat {i+1} (e.g., {x_col}) not found in toy_raw DataFrame. Columns available: {toy_raw.columns.tolist()}\")\n\n\nif not toy_raw.empty:\n    log.info(f\"Toy example: Found {len(unique_structs_toy)} unique structures out of {num_toy_repeats} repeats.\")\n    if len(unique_structs_toy) < num_toy_repeats and len(unique_structs_toy) > 1 :\n        log.warning(f\"Toy example: Less than {num_toy_repeats} unique structures, but more than 1. Stochasticity is partial.\")\n    elif len(unique_structs_toy) == 1 and num_toy_repeats > 1: # Check only if more than 1 repeat was expected\n        log.error(\"Toy example: FAILURE - Only 1 unique structure found. Stochasticity is NOT working.\")\n    elif len(unique_structs_toy) == num_toy_repeats:\n        log.info(f\"Toy example: SUCCESS - {num_toy_repeats} unique structures found!\")\nelse:\n    log.warning(\"Toy example: toy_raw DataFrame is empty. Cannot check uniqueness.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:31.902867Z","iopub.execute_input":"2025-05-16T19:10:31.903280Z","iopub.status.idle":"2025-05-16T19:10:33.306387Z","shell.execute_reply.started":"2025-05-16T19:10:31.903244Z","shell.execute_reply":"2025-05-16T19:10:33.304982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCell 11: GENERATE PREDICTIONS & BUILD SUBMISSION\n------------------------------------------------\nWe'll predict (x_1, y_1, z_1) for each residue, \nthen replicate those coordinates for structures x_2..z_5.\nFinally, we'll align with sample_submission and save submission.csv.\n\"\"\"\n\n# Predict x_1, y_1, z_1\n#test_pred_x = model_x.predict(test_merged_imputed)\n#test_pred_y = model_y.predict(test_merged_imputed)\n#test_pred_z = model_z.predict(test_merged_imputed)\n\n# Build submission from test_clean_df\n#submission = test_clean_df.copy()\n\n# Add predicted coords for structure 1\n#submission['x_1'] = test_pred_x\n#submission['y_1'] = test_pred_y\n#submission['z_1'] = test_pred_z\n\n# For simplicity, replicate for structures 2..5\n#for i in [2,3,4,5]:\n#    submission[f'x_{i}'] = test_pred_x\n#    submission[f'y_{i}'] = test_pred_y\n#    submission[f'z_{i}'] = test_pred_z\n\n# Adjust ID format: ID + \"_\" + resid\n#submission['ID'] = submission['ID'] + \"_\"  + submission['resid'].astype(str)\n\n# Reorder columns to match sample_submission\n#final_cols = list(sample_submission.columns)  # ID, resname, resid, x_1..z_5\n#submission = submission[['ID','resname','resid',\n#                         'x_1','y_1','z_1',\n#                         'x_2','y_2','z_2',\n#                         'x_3','y_3','z_3',\n#                         'x_4','y_4','z_4',\n#                         'x_5','y_5','z_5']]\n\n# Merge with sample_submission to match row order\n#sample_submission['sort_order'] = range(len(sample_submission))\n#submission_merged = pd.merge(\n#    submission,\n#    sample_submission[['ID','sort_order']],\n#    on='ID',\n#    how='left'\n#).sort_values('sort_order').drop(columns='sort_order')\n\n# This is our final submission dataframe\n#submission_df = submission_merged.copy()\n\n# Save to CSV\n#submission_df.to_csv(\"submission.csv\", index=False)\n#logging.info(\"submission.csv created successfully.\")\n\n#print(\"Cell 11 complete: Submission file saved. Ready to submit!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:43.067818Z","iopub.execute_input":"2025-05-16T19:10:43.068210Z","iopub.status.idle":"2025-05-16T19:10:43.075969Z","shell.execute_reply.started":"2025-05-16T19:10:43.068176Z","shell.execute_reply":"2025-05-16T19:10:43.074668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: CONCLUSIONS & NEXT STEPS\n# ---------------------------------\n'''\nWe've done:\n- Group-based imputation\n- Preserved resname\n- Hyperparameter tuning via RandomizedSearchCV\n- Final training on full combined data\n- Test predictions with the same coordinate repeated across 5 structures\n\nSuggestions for further improvement:\n- Fine-tune hyperparameters with a broader search or Bayesian optimization\n- Explore more advanced RNA 3D features\n- Generate truly distinct 5 structures instead of repeating the same coordinates\n'''\nlogging.info(\"Notebook complete. Good luck on the leaderboard!\")\nprint(\"All done! Submit 'submission.csv' to the competition.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:45.626246Z","iopub.execute_input":"2025-05-16T19:10:45.626596Z","iopub.status.idle":"2025-05-16T19:10:45.632229Z","shell.execute_reply.started":"2025-05-16T19:10:45.626571Z","shell.execute_reply":"2025-05-16T19:10:45.630904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Cell: show whatâ€™s inside every mounted Kaggle dataset  ğŸ”\n# --------------------------------------------------------\necho -e \"\\nğŸ“‚  Listing the first two levels of /kaggle/working â€¦\\n\"\n\n# Change depth (-maxdepth) if you want more or fewer levels\nfind /kaggle/working -maxdepth 2 -mindepth 1 -print | sed 's|^|  |'\n\necho -e \"\\nâœ…  Done.\\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:48.660294Z","iopub.execute_input":"2025-05-16T19:10:48.660672Z","iopub.status.idle":"2025-05-16T19:10:48.676199Z","shell.execute_reply.started":"2025-05-16T19:10:48.660645Z","shell.execute_reply":"2025-05-16T19:10:48.675021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell : sanity-check submission.csv against test_sequences.csv  âœ…\n# ----------------------------------------------------------------\nimport pandas as pd, pathlib, textwrap, sys, itertools, numpy as np\n\nTEST_CSV = \"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\"\nSUB_CSV  = \"submission.csv\"\nTOL      = 1.0  # Ã… â€“ treat coords within Â±1 Ã… as identical\n\n# â”€â”€ 0)  helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef auto_col(df, pref):\n    for c in pref:\n        if c in df.columns:\n            return c\n    return df.columns[0]\n\ndef preview(s, n=5):\n    lst = list(s)\n    return \", \".join(lst[:n]) + (\" â€¦\" if len(lst) > n else \"\")\n\n# â”€â”€ 1)  load / basic info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfor f in (TEST_CSV, SUB_CSV):\n    if not pathlib.Path(f).is_file():\n        sys.exit(f\"[ERROR] {f} not found!\")\n\ntest_sequences = pd.read_csv(TEST_CSV)\nsubmission     = pd.read_csv(SUB_CSV)\n\nid_col_test = auto_col(test_sequences, [\"ID\", \"id\", \"seq_id\", \"sequence_id\"])\nid_col_sub  = auto_col(submission,     [\"ID\", \"id\", \"seq_id\", \"sequence_id\"])\n\n# â”€â”€ 2)  expected vs actual rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nexpected_rows = test_sequences[\"sequence\"].str.len().sum()\nprint(\"\\nâ”â” Summary â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"Expected rows        : {expected_rows:,}\")\nprint(f\"submission.csv rows  : {len(submission):,}\")\ndupes = submission[id_col_sub].duplicated().sum()\nprint(f\"Duplicate {id_col_sub!r} rows : {dupes:,}\")\n\n# â”€â”€ 3)  build the *full* ID set   \"<sequenceID>_<resIdx>\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfull_id_set = {\n    f\"{sid}_{idx}\"\n    for sid, seq in zip(test_sequences[id_col_test], test_sequences[\"sequence\"])\n    for idx in range(1, len(seq) + 1)\n}\nsub_id_set = set(submission[id_col_sub].astype(str))\n\nmissing = full_id_set - sub_id_set\nextra   = sub_id_set  - full_id_set\n\nprint(\"\\nâ”â” ID reconciliation â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"IDs missing from submission : {len(missing):,}\")\nprint(f\"Unexpected extra IDs        : {len(extra):,}\")\nif missing: print(\"  â†’ first few missing :\", preview(missing))\nif extra:   print(\"  â†’ first few extras  :\", preview(extra))\n\n# â”€â”€ 4)  per-sequence coverage (how many residues per sequence?) â”€â”€â”€â”€â”€â”€â”€â”€\nseq_len = test_sequences.set_index(id_col_test)[\"sequence\"].str.len()\n\n# **FIXED LINE BELOW** â€“ use expand=True to ensure a 1-D Series (avoids ndarray shape (n, 3))\nprefixes = (\n    submission[id_col_sub]\n    .astype(str)\n    .str.rsplit(\"_\", n=1, expand=True)[0]   # returns a Series, not a nested ndarray\n)\n\ncoverage = prefixes.value_counts().reindex(seq_len.index).fillna(0).astype(int)\nbad_cov  = coverage[coverage != seq_len]\n\nprint(\"\\nâ”â” Per-sequence coverage â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"Sequences with wrong #rows : {len(bad_cov):,}\")\nif len(bad_cov):\n    print(\"  id  | expected | got\")\n    for sid, got in itertools.islice(bad_cov.items(), 5):\n        print(f\" {sid:<6}| {seq_len[sid]:>8} | {got}\")\n\n# â”€â”€ 5)  column sanity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nREQ_COLS = [\"ID\", \"resname\", \"resid\"] + [f\"{ax}_{i}\" for i in range(1, 6) for ax in \"xyz\"]\nmissing_cols = [c for c in REQ_COLS if c not in submission.columns]\n\nprint(\"\\nâ”â” Column sanity â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"Missing required columns   : {len(missing_cols)}\")\nif missing_cols:\n    print(textwrap.fill(\", \".join(missing_cols), width=88))\n\n# â”€â”€ 6)  structure-repeat uniqueness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrip_cols = np.array([[f\"{ax}_{i}\" for ax in \"xyz\"] for i in range(1, 6)])\ncoords = submission[trip_cols.flatten()].values.reshape(len(submission), 5, 3)\n\ndef unique_triplet_count(row):\n    \"\"\"Return #unique (x,y,z) triplets in a 5Ã—3 slice.\"\"\"\n    uniq = []\n    for v in row:\n        if not any(np.allclose(v, u, atol=TOL) for u in uniq):\n            uniq.append(v)\n    return len(uniq)\n\n# ğŸ‘‰ replace apply_along_axis with a 1-liner list-comprehension  âœ…\nuniq_counts = np.array([unique_triplet_count(row) for row in coords])\n\nall_identical = (uniq_counts == 1).sum()\ntruly_unique  = (uniq_counts > 1).sum()\n\nprint(\"\\nâ”â” Structure-repeat uniqueness â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"Rows where 5 structures are identical : {all_identical:,}\")\nprint(f\"Rows with â‰¥2 distinct triplets         : {truly_unique:,}\")\n\n# Per-sequence share of unique repeats\nsub_seq_id = prefixes.to_numpy()   # 1-D array of sequence IDs\nper_seq_unique = (\n    pd.Series(uniq_counts > 1, index=sub_seq_id)\n      .groupby(level=0).mean()\n      .sort_values(ascending=False)\n)\n\nprint(\"\\nTop 5 sequences with most unique repeats:\")\nfor sid, frac in per_seq_unique.head(5).items():\n    print(f\"  {sid:<6}: {frac:6.1%} rows diversified\")\n\nprint(\"\\nâœ…  Sanity check finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:10:52.085507Z","iopub.execute_input":"2025-05-16T19:10:52.085878Z","iopub.status.idle":"2025-05-16T19:10:52.110042Z","shell.execute_reply.started":"2025-05-16T19:10:52.085848Z","shell.execute_reply":"2025-05-16T19:10:52.108388Z"}},"outputs":[],"execution_count":null}]}