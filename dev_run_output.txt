[2025-05-19 12:52:43,198] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 12:52:43.530000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/runpy.py:196: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[2025-05-19 12:52:44,717][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] Calling OmegaConf.resolve(cfg) to force interpolation...

[DEBUG][PATCH] FULL CONFIG TREE:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: cpu
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
data:
  index_csv: rna_predict/dataset/examples/kaggle_minimal_index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    enabled: true
    num_hidden: 128
    dropout: 0.3
    debug_logging: false
    freeze_params: true
    min_seq_length: 80
    batch_size: 32
    lr: 0.001
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    threshold: 0.5
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
    debug_logging: false
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
      debug_logging: false
      init_from_scratch: false
    pairformer:
      device: ${device}
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_s: 0
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      protenix_integration:
        device: ${device}
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        atoms_per_token: 4
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      use_checkpoint: true
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      block:
        n_heads: 2
        c_z: 4
        c_s: 8
        c_hidden_mul: 4
        c_hidden_pair_att: 4
        no_heads_pair: 2
        dropout: 0.25
      stack:
        n_blocks: 1
        n_heads: 2
        c_z: 4
        c_s: 8
        dropout: 0.25
        blocks_per_ckpt: null
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
        c_s_inputs: 2
        blocks_per_ckpt: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1.0
          min_bin: 1.0
          no_bins: 1.0
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
      debug_logging: false
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: false
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      c_atompair: 4
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      init_from_scratch: false
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: false
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      atom_metadata: null
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_noise_embedding: 4
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
        c_atompair: 4
        sigma_data: 1.0
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        sigma_data: 16.0
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      diffusion:
        enabled: true
        mode: inference
        device: cpu
        debug_logging: false
        ref_element_size: 4
        ref_atom_name_chars_size: 8
        profile_size: 8
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null
        init_from_scratch: false
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-19 12:52:44,746][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] After resolve: cfg.device: cpu
[2025-05-19 12:52:44,746][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] After resolve: cfg.model.stageA.device: cpu
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] After resolve: cfg.model.stageB.device: None
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] After resolve: cfg.model.stageC.device: cpu
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] - [DEBUG][PATCH] After resolve: cfg.model.stageD.device: cpu
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] - [DEBUG] Configuration keys:
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - shared
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - sequence
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - device
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - seed
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - atoms_per_residue
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - extraction_backend
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - run_stageD
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - enable_stageC
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - merge_latent
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - pipeline
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - training
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - data
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - model
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - stageD_diffusion
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] -   - test_data
[2025-05-19 12:52:44,747][rna_predict.training.train][DEBUG] - [DEBUG] Full cfg.model:
{'stageA': {'enabled': True, 'num_hidden': 128, 'dropout': 0.3, 'debug_logging': False, 'freeze_params': True, 'min_seq_length': 80, 'batch_size': 32, 'lr': 0.001, 'device': 'cpu', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'threshold': 0.5, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': 'cpu', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': False, 'init_from_scratch': False}, 'pairformer': {'device': 'cpu', 'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_s': 0, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'protenix_integration': {'device': 'cpu', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'use_checkpoint': True, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'block': {'n_heads': 2, 'c_z': 4, 'c_s': 8, 'c_hidden_mul': 4, 'c_hidden_pair_att': 4, 'no_heads_pair': 2, 'dropout': 0.25}, 'stack': {'n_blocks': 1, 'n_heads': 2, 'c_z': 4, 'c_s': 8, 'dropout': 0.25, 'blocks_per_ckpt': None}, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}, 'c_s_inputs': 2, 'blocks_per_ckpt': 1}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1.0, 'min_bin': 1.0, 'no_bins': 1.0}}, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}, 'debug_logging': False}}, 'stageD': {'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': False, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'c_atompair': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': False, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'atom_metadata': None, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'c_atompair': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'sigma_data': 16.0, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': False, 'ref_element_size': 4, 'ref_atom_name_chars_size': 8, 'profile_size': 8, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False}}, 'input_features': None}, 'protenix_integration': {'device': 'cpu', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[DEBUG] cfg.model.stageB:
{'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': 'cpu', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': False, 'init_from_scratch': False}, 'pairformer': {'device': 'cpu', 'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_s': 0, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'protenix_integration': {'device': 'cpu', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'use_checkpoint': True, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'block': {'n_heads': 2, 'c_z': 4, 'c_s': 8, 'c_hidden_mul': 4, 'c_hidden_pair_att': 4, 'no_heads_pair': 2, 'dropout': 0.25}, 'stack': {'n_blocks': 1, 'n_heads': 2, 'c_z': 4, 'c_s': 8, 'dropout': 0.25, 'blocks_per_ckpt': None}, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}, 'c_s_inputs': 2, 'blocks_per_ckpt': 1}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1.0, 'min_bin': 1.0, 'no_bins': 1.0}}, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}, 'debug_logging': False}}
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] - [DEBUG] Data configuration keys:
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - index_csv
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - root_dir
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - max_residues
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - max_atoms
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - C_element
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - C_char
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - ref_element_size
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - ref_atom_name_chars_size
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - batch_size
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - num_workers
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - load_adj
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - load_ang
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - coord_fill_value
[2025-05-19 12:52:44,748][rna_predict.training.train][DEBUG] -   - coord_dtype
[2025-05-19 12:52:44,749][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Post-mkdir checkpoint dir: /Users/tomriddle1/RNA_PREDICT/outputs/checkpoints, contents: ['last.ckpt', 'epoch=9-step=10.ckpt']
[2025-05-19 12:52:44,749][rna_predict.training.train][DEBUG] - [DEBUG] Original working directory: /Users/tomriddle1/RNA_PREDICT
[2025-05-19 12:52:44,749][rna_predict.training.train][DEBUG] - [DEBUG] Using test_data.data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
[2025-05-19 12:52:44,749][rna_predict.training.train][DEBUG] - [DEBUG] Resolved test_data.data_index path: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/kaggle_minimal_index.csv
[2025-05-19 12:52:44,749][rna_predict.training.train][DEBUG] - [DEBUG] File exists: True
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 12:52:49,274][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 12:52:49,275][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 12:52:49,317][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 12:52:49,318][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 12:52:49,318][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 12:52:49,344][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 12:52:49][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 12:52:49,345][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 12:52:49,347][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: False
[2025-05-19 12:52:49,347][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: False, None, None
[2025-05-19 12:52:49,347][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': 'cpu', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': False, 'init_from_scratch': False}
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': 'cpu', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': False, 'init_from_scratch': False}
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.device
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using direct attributes
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-19 12:52:49][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 12:52:49][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 221.97 MB
[2025-05-19 12:52:49,348][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 221.97 MB
[2025-05-19 12:52:53][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 12:52:53,082][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 12:52:53][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 12:52:53,082][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 12:52:53,084][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-19 12:52:53,084][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 427.95 MB
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 427.95 MB
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'device': 'cpu', 'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_s': 0, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'protenix_integration': {'device': 'cpu', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'use_checkpoint': True, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'block': {'n_heads': 2, 'c_z': 4, 'c_s': 8, 'c_hidden_mul': 4, 'c_hidden_pair_att': 4, 'no_heads_pair': 2, 'dropout': 0.25}, 'stack': {'n_blocks': 1, 'n_heads': 2, 'c_z': 4, 'c_s': 8, 'dropout': 0.25, 'blocks_per_ckpt': None}, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}, 'c_s_inputs': 2, 'blocks_per_ckpt': 1}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1.0, 'min_bin': 1.0, 'no_bins': 1.0}}, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}, 'debug_logging': False}
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'device': 'cpu', 'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_s': 0, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'protenix_integration': {'device': 'cpu', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'use_checkpoint': True, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'block': {'n_heads': 2, 'c_z': 4, 'c_s': 8, 'c_hidden_mul': 4, 'c_hidden_pair_att': 4, 'no_heads_pair': 2, 'dropout': 0.25}, 'stack': {'n_blocks': 1, 'n_heads': 2, 'c_z': 4, 'c_s': 8, 'dropout': 0.25, 'blocks_per_ckpt': None}, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}, 'c_s_inputs': 2, 'blocks_per_ckpt': 1}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1.0, 'min_bin': 1.0, 'no_bins': 1.0}}, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}, 'debug_logging': False}
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Using direct stageB_pairformer config
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Using direct stageB_pairformer config
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=1, c_z=2, c_s=0, dropout=0.0
[2025-05-19 12:52:53,085][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=1, c_z=2, c_s=0, dropout=0.0
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-19 12:52:53,086][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 435.52 MB
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 435.52 MB
[2025-05-19 12:52:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-19 12:52:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 435.70 MB
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 435.70 MB
[2025-05-19 12:52:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-19 12:52:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 435.70 MB
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 435.70 MB
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Initializing ProtenixDiffusionManager
[2025-05-19 12:52:53,095][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage: 435.91 MB
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg.keys(): ['enabled', 'mode', 'device', 'debug_logging', 'ref_element_size', 'ref_atom_name_chars_size', 'profile_size', 'feature_dimensions', 'test_residues_per_batch', 'model_architecture', 'transformer', 'atom_encoder', 'atom_decoder', 'noise_schedule', 'inference', 'use_memory_efficient_kernel', 'use_deepspeed_evo_attention', 'use_lma', 'inplace_safe', 'chunk_size', 'init_from_scratch']
[DEBUG][DiffusionModule.__init__] cfg.model_architecture: {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}
[DEBUG] DiffusionModule.__init__ kwargs: {}
[2025-05-19 12:52:53,097][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] self.debug_logging resolved to: False
[2025-05-19 12:52:53,097][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] config subtree used: False, None, None
[2025-05-19 12:52:53,097][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] full config: {'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': False, 'ref_element_size': 4, 'ref_atom_name_chars_size': 8, 'profile_size': 8, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False}
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DiffusionModule][__init__] Using OUTER ref_element_size: 4
[DiffusionModule][__init__] Using OUTER ref_atom_name_chars_size: 8
[2025-05-19 12:52:53,100][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[2025-05-19 12:52:53,101][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 12:52:53,102][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 12:52:53,105][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage after initialization: 439.37 MB
[DEBUG][RNADataset] Resolved device in cfg: cpu
[RNADataset] Loaded index_csv from: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/kaggle_minimal_index.csv
[RNADataset] type(self.meta): <class 'numpy.recarray'>
[RNADataset] repr(self.meta): rec.array([('1SCL_A', '1SCL_A', '/Volumes/Totallynotaharddrive/RNA_structure_PREDICT/kaggle/stanford-rna-3d-folding/train_sequences.csv', '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb')],
          dtype=[('id', 'O'), ('target_id', 'O'), ('sequence_path', 'O'), ('pdb_path', 'O')])
[RNADataset] First row fields: ('id', 'target_id', 'sequence_path', 'pdb_path')
[RNADataset] First row values: ('1SCL_A', '1SCL_A', '/Volumes/Totallynotaharddrive/RNA_structure_PREDICT/kaggle/stanford-rna-3d-folding/train_sequences.csv', '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb')
[2025-05-19 12:52:53,115][rna_predict.training.train][DEBUG] - [DEBUG][main] DataLoader num_workers=8 (device=cpu)
[2025-05-19 12:52:53,115][rna_predict.training.train][DEBUG] - [DEBUG][train.py] DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>
[2025-05-19 12:52:53,115][rna_predict.training.train][DEBUG] - [DEBUG] Dataset length: 1
[2025-05-19 12:52:53,115][rna_predict.training.train][DEBUG] - [DEBUG] DataLoader length: 1
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:53:15,598] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 12:53:16.772000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.773000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.773000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.788000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.820000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.828000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.838000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:53:16.910000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
12:53:23.95 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
12:53:23.95 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
12:53:23.95 ...... len(structure_file) = 85
12:53:23.95 ...... chain_id = 'A'
12:53:23.95 ...... backend = 'dssr'
12:53:23.95 ...... angle_set = 'canonical'
12:53:23.95 ...... kwargs = {}
12:53:23.95   15 | def extract_rna_torsions(
12:53:23.95   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
12:53:23.95   41 |     if backend == "mdanalysis":
12:53:23.95   49 |     elif backend == "dssr":
12:53:23.95   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
12:53:26.30 .............. angles = None
12:53:26.30   53 |         if angles is None:
12:53:26.30   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
12:53:26.30   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
12:53:27.65 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
12:53:27.65                                                     -1.1687942 , -2.8552787 ],
12:53:27.65                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
12:53:27.65                                                     -1.2295908 , -2.7892737 ],
12:53:27.65                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
12:53:27.65                                                     -1.2545753 , -2.729158  ],
12:53:27.65                                                    ...,
12:53:27.65                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
12:53:27.65                                                     -1.2474957 , -2.8304672 ],
12:53:27.65                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
12:53:27.65                                                     -1.2624229 , -2.6697967 ],
12:53:27.65                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
12:53:27.65                                                             nan, -2.653027  ]], dtype=float32)
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
[2025-05-19 12:53:39,342][rna_predict.training.train][DEBUG] - [DEBUG] First batch keys: ['sequence_id', 'sequence', 'coords_true', 'attention_mask', 'atom_mask', 'atom_to_token_idx', 'ref_element', 'ref_atom_name_chars', 'atom_names', 'residue_indices', 'id', 'target_id', 'sequence_path', 'pdb_path', 'adjacency', 'angles_true']
[2025-05-19 12:53:39,347][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Current working directory (os.getcwd()): /Users/tomriddle1/RNA_PREDICT/outputs/2025-05-19/12-52-44
[2025-05-19 12:53:39,354][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] cfg.training.checkpoint_dir: outputs/checkpoints
[2025-05-19 12:53:39,357][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Resolved checkpoint directory absolute path: /Users/tomriddle1/RNA_PREDICT/outputs/2025-05-19/12-52-44/outputs/checkpoints
[2025-05-19 12:53:39,357][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Directory exists: False
[2025-05-19 12:53:39,357][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Directory writable: False (parent: /Users/tomriddle1/RNA_PREDICT/outputs/2025-05-19/12-52-44/outputs)
[2025-05-19 12:53:39,398][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] About to start training with checkpoint dir: /Users/tomriddle1/RNA_PREDICT/outputs/checkpoints
GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/tomriddle1/RNA_PREDICT/outputs/checkpoints exists and is not empty.

  | Name                    | Type                       | Params | Mode 
-------------------------------------------------------------------------------
0 | stageA                  | StageARFoldPredictor       | 9.0 M  | train
1 | stageB_torsion          | StageBTorsionBertPredictor | 86.9 M | train
2 | stageB_pairformer       | PairformerWrapper          | 46.3 K | train
3 | stageC                  | StageCReconstruction       | 0      | train
4 | stageD                  | ProtenixDiffusionManager   | 7.1 K  | train
5 | latent_merger           | SimpleLatentMerger         | 29.8 K | train
6 | pipeline                | ModuleDict                 | 96.0 M | train
7 | _integration_test_dummy | Linear                     | 1.1 K  | train
-------------------------------------------------------------------------------
87.0 M    Trainable params
9.0 M     Non-trainable params
96.0 M    Total params
384.156   Total estimated model params size (MB)
233       Modules in train mode
360       Modules in eval mode
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] [2025-05-19 12:54:28,518] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,521] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,520] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,528] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,537] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,546] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,544] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:54:28,565] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 12:54:30.653000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.656000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.672000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.676000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.680000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.679000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.685000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:54:30.695000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
12:54:39.92 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
12:54:39.92 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
12:54:39.92 ...... len(structure_file) = 85
12:54:39.92 ...... chain_id = 'A'
12:54:39.92 ...... backend = 'dssr'
12:54:39.92 ...... angle_set = 'canonical'
12:54:39.92 ...... kwargs = {}
12:54:39.92   15 | def extract_rna_torsions(
12:54:39.92   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
12:54:39.92   41 |     if backend == "mdanalysis":
12:54:39.92   49 |     elif backend == "dssr":
12:54:39.92   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
12:54:45.43 .............. angles = None
12:54:45.43   53 |         if angles is None:
12:54:45.44   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
12:54:45.44   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
12:54:47.22 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
12:54:47.22                                                     -1.1687942 , -2.8552787 ],
12:54:47.22                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
12:54:47.22                                                     -1.2295908 , -2.7892737 ],
12:54:47.22                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
12:54:47.22                                                     -1.2545753 , -2.729158  ],
12:54:47.22                                                    ...,
12:54:47.22                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
12:54:47.22                                                     -1.2474957 , -2.8304672 ],
12:54:47.22                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
12:54:47.22                                                     -1.2624229 , -2.6697967 ],
12:54:47.22                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
12:54:47.22                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 12:54:53,387][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 12:54:53,387][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:54:53,400][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:54:53,400][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:54:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:54:53,571][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:54:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:54:53,573][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:54:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:54:53,578][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:54:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[2025-05-19 12:54:53,889][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[2025-05-19 12:54:53,889][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[2025-05-19 12:54:54,852][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[Debug] Creating MLP with dimensions: 16 -> 128
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 12:54:54,852][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:54:54,853][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:54:54,853][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:54:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:54:54,854][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:54:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:54:54,855][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:54:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:54:54,855][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:54:55][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 12:54:55,019][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 12:54:55,019][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x175e61840>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 0.0024556973949074745, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 0.002455709269270301, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 0.002553153783082962, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 0.0003258674405515194, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 0.0004491243453230709, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 0.00040934921707957983, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 5.250472531770356e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 0.00039672001730650663, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 1.0094877191113572e-11, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 0.0017236971762031317, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 0.0002798525965772569, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 0.0018976511200889945, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 0.0003186279209330678, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 0.00018309219740331173, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 0.00013840061728842556, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 0.0015418442199006677, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 4.931550574838184e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 0.0017857629572972655, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 0.00010987740097334608, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 0.0006217224872671068, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 0.00032482179813086987, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 0.0004388164961710572, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 2.563354792073369e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 0.00035543215926736593, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 1.8196380166535242e-11, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 0.0035883879754692316, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 0.0002824277034960687, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 0.0025743136648088694, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 0.0002209234662586823, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 0.00031609294819645584, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 0.000224509320105426, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 0.002766453893855214, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 9.751656034495682e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 0.001893583219498396, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 0.00018168959650211036, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 0.0004416039737407118, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 0.0002344928652746603, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 0.0010031124111264944, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 5.045164289185777e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 0.0024084830656647682, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 2.1534333344286338e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 0.0031355570536106825, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 0.0001531763991806656, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 0.0032326234504580498, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 0.0001750115625327453, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 0.00026749243261292577, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0001877883478300646, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 0.0017909301677718759, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 6.037565981387161e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 0.0013875666772946715, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 0.00015486909251194447, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 0.0005717656458728015, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 0.0002633111143950373, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 0.00033952679950743914, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 1.538156357128173e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 0.0005660817841999233, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 2.421371957106455e-11, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 0.0035927165299654007, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 0.0001784783962648362, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 0.004026473965495825, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 0.0002247421070933342, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 0.00036538910353556275, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0002425935963401571, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 0.002630046335980296, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 9.528607915854082e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 0.0022938810288906097, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 0.00021817124797962606, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 0.0005546372849494219, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 0.00045210670214146376, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 0.0004100718069821596, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 2.5016206564032473e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 0.0005811780574731529, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 7.101551291466279e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 0.005365312565118074, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 0.0003142878122162074, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 0.004620798863470554, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 0.0002830101002473384, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0003461952437646687, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 0.00032399428891949356, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 0.0033638400491327047, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 0.00012402847642078996, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 0.0015291847521439195, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 0.00017017890058923513, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 0.00042059479164890945, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 0.00045029091415926814, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 0.0009989194804802537, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 5.901302210986614e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 0.0009387567406520247, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 4.0167959236558914e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 0.003908214624971151, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 0.0002295066515216604, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 0.004754316061735153, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 0.0001997145009227097, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0004561098467092961, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0005077950190752745, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 0.003640368115156889, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 0.0001686212926870212, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 0.0012219793861731887, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 0.0004426124505698681, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 0.000772802100982517, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 0.0009326363797299564, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 0.0007835929282009602, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 6.160388147691265e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 0.0009899536380544305, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 4.296167588346833e-11, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.005843907129019499, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 0.0004367691290099174, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.0048322975635528564, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 0.0002520154230296612, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 0.00048769358545541763, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0005171204684302211, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.002867515431717038, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 9.680661605671048e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.0010827637743204832, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 0.000497340748552233, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 0.0007484285160899162, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 0.0008327185641974211, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 0.00025826803175732493, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 1.7544025467941537e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 0.0003800304839387536, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 4.2329282440300986e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.005910330917686224, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 0.0004079477221239358, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.005000451114028692, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 0.0001998969237320125, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0005768232513219118, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0006211791187524796, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.006849103607237339, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 0.0002954769297502935, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.006423632614314556, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 0.0003348248137626797, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.0009146247757598758, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.0009355535730719566, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 0.0002927083696704358, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 1.5110480489965994e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 0.0002686544321477413, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 1.1240661873523905e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.00938485935330391, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 0.00048425208660773933, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.0130629763007164, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 0.00031210953602567315, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0017528119497001171, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0017192813102155924, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.016329064965248108, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 0.0008283250499516726, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.019574841484427452, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 0.0009409560589119792, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.00204598275013268, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.002124888589605689, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 2.635206692502834e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 1.451538992114365e-06, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 2.966319880215451e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 9.715895449291523e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.02155160903930664, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 0.0011871075257658958, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.021209508180618286, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 0.0007186473230831325, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0030987649224698544, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0030229478143155575, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.020171863958239555, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 0.0013151501771062613, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.01685185730457306, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.001390174962580204, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.0023112657945603132, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.002167575992643833, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 3.61132492798788e-06, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 1.7714882005748223e-07, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 3.7475645058293594e-06, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 1.164761898042599e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.02845047041773796, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.001395593979395926, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.03782915323972702, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 0.001169965136796236, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.004540581721812487, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.004545888397842646, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.032905735075473785, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.0020250417292118073, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.029583094641566277, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.0023413102608174086, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.0041726077906787395, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.0040955254808068275, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 1.4166071196086705e-06, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 8.222315983630324e-08, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 1.5124003311939305e-06, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 2.2300591928647862e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.043738119304180145, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.0025382842868566513, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.06077602505683899, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.0015238330233842134, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.008611006662249565, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.009835799224674702, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.06819798797369003, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0038772516418248415, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.2046593725681305, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.007389286532998085, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.014634384773671627, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.016378333792090416, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.01238341722637415, 'stageB_torsion.model.regressor.0.bias': 0.013637391850352287, 'stageB_torsion.model.regressor.1.weight': 0.6758042573928833, 'stageB_torsion.model.regressor.1.bias': 0.024646051228046417, 'stageB_torsion.model.regressor.3.weight': 1.8616846799850464, 'stageB_torsion.model.regressor.3.bias': 0.15301468968391418, 'stageB_torsion.output_projection.weight': 1.000502347946167, 'stageB_torsion.output_projection.bias': 0.3396821916103363, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.

[2025-05-19 12:54:55,933][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 12:54:55,933][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 12:54:55,933][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 12:54:55,933][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 12:54:55,933][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 12:54:55,954][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.4498, L_coord_C=1745.4683, Total=1745.9181
Epoch 0: 100%|| 1/1 [01:16<00:00,  0.01it/s]Epoch 0: 100%|| 1/1 [01:16<00:00,  0.01it/s, v_num=27, angle_loss_step=0.450, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3]Epoch 0: 100%|| 1/1 [01:16<00:00,  0.01it/s, v_num=27, angle_loss_step=0.450, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.450, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.450, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.450, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.450, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.450, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3][RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
[2025-05-19 12:56:37,826] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:37,837] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:37,840] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:37,834] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:39,699] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:40,274] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:40,342] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:56:40,377] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 12:56:44.570000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.586000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.585000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.633000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.647000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.654000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.652000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:56:44.676000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
12:57:00.72 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
12:57:00.72 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
12:57:00.72 ...... len(structure_file) = 85
12:57:00.72 ...... chain_id = 'A'
12:57:00.72 ...... backend = 'dssr'
12:57:00.72 ...... angle_set = 'canonical'
12:57:00.72 ...... kwargs = {}
12:57:00.72   15 | def extract_rna_torsions(
12:57:00.72   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
12:57:00.72   41 |     if backend == "mdanalysis":
12:57:00.72   49 |     elif backend == "dssr":
12:57:00.72   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
12:57:10.56 .............. angles = None
12:57:10.56   53 |         if angles is None:
12:57:10.56   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
12:57:10.56   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
12:57:12.40 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
12:57:12.40                                                     -1.1687942 , -2.8552787 ],
12:57:12.40                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
12:57:12.40                                                     -1.2295908 , -2.7892737 ],
12:57:12.40                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
12:57:12.40                                                     -1.2545753 , -2.729158  ],
12:57:12.40                                                    ...,
12:57:12.40                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
12:57:12.40                                                     -1.2474957 , -2.8304672 ],
12:57:12.40                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
12:57:12.40                                                     -1.2624229 , -2.6697967 ],
12:57:12.40                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
12:57:12.40                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 12:57:23,446][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 12:57:23,446][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:57:23,459][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:57:23,459][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:57:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:57:23,670][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:57:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:57:23,674][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:57:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:57:23,676][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:57:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.25s
[2025-05-19 12:57:24,337][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.25s
[2025-05-19 12:57:24,337][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.25s
[2025-05-19 12:57:24,815][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 12:57:24,815][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:57:24,821][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:57:24,821][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:57:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:57:24,825][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:57:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:57:24,825][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:57:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:57:24,825][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:57:25][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[2025-05-19 12:57:25,106][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[2025-05-19 12:57:25,106][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16e729b40>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 0.00384509633295238, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 0.00384509633295238, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 0.0052345688454806805, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 0.0010431914124637842, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 0.0011410614242777228, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 0.0010339160216972232, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 0.0001060516297002323, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 0.0007966268458403647, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 2.0277314549677072e-11, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 0.0038457284681499004, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 0.000631322618573904, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 0.0034804604947566986, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 0.0005648772930726409, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 0.00041051022708415985, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0002805808326229453, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 0.004206583369523287, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 0.00013970679719932377, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 0.003083727788180113, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 0.00016802780737634748, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 0.0005739383632317185, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 0.0005022503319196403, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 0.0019316498655825853, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 0.00010132043098565191, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 0.0013079101918265224, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 2.3402684304341115e-11, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 0.0031116874888539314, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 0.0002790816070046276, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 0.0015240723732858896, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 0.00022267263557296246, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 0.00026106712175533175, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 0.00021055317483842373, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 0.0015558424638584256, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 5.31209479959216e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 0.0011824654648080468, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 0.00013208416930865496, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 0.0002637562865857035, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 0.00020321116608101875, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 0.0005701787886209786, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 3.115642175544053e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 0.001156028127297759, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 4.8867600938029554e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 0.0027343351393938065, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 0.00011506180453579873, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 0.0013084800448268652, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 6.226042023627087e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 0.00010925682727247477, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 7.613449270138517e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 0.0005958319525234401, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 2.2662126866634935e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 0.00033742451341822743, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 2.3537348170066252e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 0.0004551040183287114, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 9.974842396331951e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 2.168007631553337e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 1.863305897131795e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 2.4289465727633797e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 1.7127902828678598e-12, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 0.000709043291863054, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 6.102001862018369e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 0.00039779910002835095, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 6.730723544023931e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 0.00011973572691204026, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 4.062886000610888e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 0.0007243313011713326, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 2.2316362446872517e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 0.0008641639724373817, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 2.701361881918274e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 0.00018003892910201102, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 0.0002820658846758306, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 3.1212821340886876e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 1.6152719126694137e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 1.7177737390738912e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 9.104986729846498e-12, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 0.0036667040549218655, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 0.00018997797451447695, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 0.0011676282156258821, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 0.00011548289330676198, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 0.00020839467470068485, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 0.00019088992848992348, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 0.002759684808552265, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 8.452439215034246e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 0.006436123978346586, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 2.610559022286907e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 0.0033815482165664434, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 0.0033718636259436607, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 1.5538239495072048e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 9.111975884934509e-08, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 1.1379512443454587e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 1.311791647529148e-10, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 0.024582549929618835, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 0.0014414760516956449, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 0.02754979208111763, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 0.001304121338762343, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 0.002438420196995139, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 0.002637885743752122, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 0.015936844050884247, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 0.0007242591818794608, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 0.0359458290040493, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 0.002509531332179904, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 0.012559759430587292, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 0.015606365166604519, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 1.310087753836342e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 1.2679613803356915e-08, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 1.393925401771412e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 1.9251636396155902e-10, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.06962490826845169, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 0.006738592870533466, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.043215878307819366, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 0.0034766732715070248, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 0.004602531902492046, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 0.004737571347504854, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.04333975166082382, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 0.001453430624678731, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.04861415550112724, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 0.0035707815550267696, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 0.010897951200604439, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 0.010809961706399918, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 6.135821450925505e-08, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 4.068482883923252e-09, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 5.591674323568441e-08, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 5.819244530691492e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.08461466431617737, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 0.005610428750514984, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.053064845502376556, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 0.0026282737962901592, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 0.006549464073032141, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.00627059256657958, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.060752708464860916, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 0.0025638786610215902, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.05129493772983551, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 0.0024946127086877823, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.008600356988608837, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.008007516153156757, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 1.5421356636124983e-08, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 8.757278791016176e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 1.3514290131411144e-08, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 5.841048200672105e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.06621195375919342, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 0.0037599694915115833, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.05985570698976517, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 0.0020943221170455217, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.00809384323656559, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0074084741063416, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.06420629471540451, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 0.003166796173900366, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.06658380478620529, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 0.0036425315774977207, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.008127269335091114, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.010937999933958054, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 6.2849063731107435e-09, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 4.475756709698686e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 5.207681397223496e-09, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 3.5983144242024423e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.08514747023582458, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 0.00606370996683836, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.06890817731618881, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 0.003771865740418434, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.010005357675254345, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.009482878260314465, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.07646948844194412, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 0.004511402919888496, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.045446570962667465, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.004325936548411846, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.006015363615006208, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.008917233906686306, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 5.294401361766177e-09, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 3.861397590565474e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 3.81827325313111e-09, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 2.778945407122535e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.07034432888031006, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.0051302178762853146, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.05794890224933624, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 0.0035756209399551153, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.008387156762182713, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.008171689696609974, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.05936190485954285, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.003609818173572421, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.05393902212381363, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.003133851569145918, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.00790953915566206, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.008868470788002014, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 2.6391442364825934e-09, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 2.2122723097872665e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 2.259895381939714e-09, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 1.879254113434925e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.053450532257556915, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.004480505362153053, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.05533924326300621, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.0024337435606867075, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.008041166700422764, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.008602524176239967, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.061885371804237366, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0035024562384933233, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.10005012154579163, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0047911652363836765, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.009178527630865574, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.009650527499616146, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.008741706609725952, 'stageB_torsion.model.regressor.0.bias': 0.008420892059803009, 'stageB_torsion.model.regressor.1.weight': 0.3856990933418274, 'stageB_torsion.model.regressor.1.bias': 0.014063045382499695, 'stageB_torsion.model.regressor.3.weight': 0.9160360097885132, 'stageB_torsion.model.regressor.3.bias': 0.08321753889322281, 'stageB_torsion.output_projection.weight': 0.6773704886436462, 'stageB_torsion.output_projection.bias': 0.22672505676746368, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 12:57:27,806][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 12:57:27,807][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 12:57:27,807][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 12:57:27,807][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 12:57:27,807][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 12:57:27,825][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.2259, L_coord_C=1745.4683, Total=1745.6941
Epoch 1: 100%|| 1/1 [02:01<00:00,  0.01it/s, v_num=27, angle_loss_step=0.450, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.450, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 1: 100%|| 1/1 [02:01<00:00,  0.01it/s, v_num=27, angle_loss_step=0.226, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.450, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 1: 100%|| 1/1 [02:01<00:00,  0.01it/s, v_num=27, angle_loss_step=0.226, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.226, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.226, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.226, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.226, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.226, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3][RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,587] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 12:59:03,587] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 12:59:04.177000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.177000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.178000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.179000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.192000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.200000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.200000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 12:59:04.259000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
12:59:05.86 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
12:59:05.86 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
12:59:05.86 ...... len(structure_file) = 85
12:59:05.86 ...... chain_id = 'A'
12:59:05.86 ...... backend = 'dssr'
12:59:05.86 ...... angle_set = 'canonical'
12:59:05.86 ...... kwargs = {}
12:59:05.86   15 | def extract_rna_torsions(
12:59:05.86   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
12:59:05.86   41 |     if backend == "mdanalysis":
12:59:05.86   49 |     elif backend == "dssr":
12:59:05.86   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
12:59:07.33 .............. angles = None
12:59:07.33   53 |         if angles is None:
12:59:07.33   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
12:59:07.33   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
12:59:08.81 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
12:59:08.81                                                     -1.1687942 , -2.8552787 ],
12:59:08.81                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
12:59:08.81                                                     -1.2295908 , -2.7892737 ],
12:59:08.81                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
12:59:08.81                                                     -1.2545753 , -2.729158  ],
12:59:08.81                                                    ...,
12:59:08.81                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
12:59:08.81                                                     -1.2474957 , -2.8304672 ],
12:59:08.81                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
12:59:08.81                                                     -1.2624229 , -2.6697967 ],
12:59:08.81                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
12:59:08.81                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 12:59:12,952][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 12:59:12,952][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:59:12,964][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:59:12,964][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:59:13][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:59:13,156][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:59:13][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:59:13,157][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:59:13][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:59:13,163][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:59:13][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.18s
[2025-05-19 12:59:13,718][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.18s
[2025-05-19 12:59:13,718][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.18s
[2025-05-19 12:59:14,378][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 12:59:14,378][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 12:59:14,378][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:59:14,378][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 12:59:14][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 12:59:14,380][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 12:59:14][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:59:14,380][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 12:59:14][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:59:14,380][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 12:59:14][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 12:59:14,544][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 12:59:14,545][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x145eba5c0>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 4.936452387482859e-05, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 5.385797339840792e-05, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 5.1934232033090666e-05, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 1.900953611766454e-05, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 1.138115294452291e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 3.0795144994044676e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 1.2295901115066954e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 1.9431290638749488e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 5.280843579265337e-13, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 7.46831065043807e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 9.35735533857951e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 4.282633017282933e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 5.190160209167516e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 4.127000920561841e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 3.2424309210909996e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 3.6011329939356074e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 1.3746454214924597e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 3.734792335308157e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 7.397209742521227e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 1.2085418347851373e-05, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 6.141430276329629e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 1.2599594811035786e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 5.97261873735988e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 2.6186255126958713e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 7.60249014349762e-13, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 0.00010471974383108318, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 7.075225767039228e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 3.1622581445844844e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 2.8324179766059387e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 4.658356374420691e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 2.3862014586484293e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 1.5851665011723526e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 4.6259748387456057e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 4.897007966064848e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 4.866454332841386e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 2.3306476578e-05, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 3.709450538735837e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 2.781789135042345e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 2.150677005374746e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 1.745061126712244e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 2.342152079920501e-13, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 0.0002774905879050493, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 2.1588242816505954e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 5.333149601938203e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 1.8495344193070196e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 1.0111611118190922e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 8.513756256434135e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 0.00014084394206292927, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 3.1395613859785954e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 0.0002660041209310293, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 6.4450678110006265e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 0.0001759662409313023, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 0.000109062937553972, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 6.559811936313054e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 5.736734465244808e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 5.716778559872182e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 8.906756625640111e-13, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 0.0009225838584825397, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 8.071034972090274e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 0.00019934598822146654, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 8.43542002257891e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 4.555796840577386e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 4.09923231927678e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 0.0008890761528164148, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 2.053848947980441e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 0.0012491365196183324, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 1.0508974810363725e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 0.001567348255775869, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 0.0010699318954721093, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 1.589010571478866e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 7.577833116556576e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 8.895399332686793e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 6.755274811753864e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 0.016503186896443367, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 0.0007869189139455557, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 0.004038568586111069, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 0.00038805018994025886, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0005171349621377885, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0006428426713682711, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 0.009222256019711494, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 0.00025581897352822125, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 0.02113599330186844, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 8.461009565507993e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 0.012809772044420242, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 0.012553966604173183, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 3.72662526615386e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 2.1465361044192832e-08, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 5.854823257323005e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 3.9099501414341375e-10, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 0.12020333111286163, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 0.006923758890479803, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 0.10586926341056824, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 0.0048406389541924, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 0.010529843159019947, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 0.011446674354374409, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 0.11010170727968216, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 0.004995584953576326, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 0.1437828689813614, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 0.007495838683098555, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 0.05145998299121857, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 0.061589315533638, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 4.301244871385279e-08, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 4.285559906946901e-09, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 4.635458950019711e-08, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 1.1451471992884876e-09, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.24236923456192017, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 0.02414790168404579, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.1600232571363449, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 0.010898525826632977, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 0.021361026912927628, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 0.019667498767375946, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.18334563076496124, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 0.006866127718240023, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.12996813654899597, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 0.005797190126031637, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 0.01856040023267269, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 0.017895756289362907, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 4.4516752950585214e-08, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 2.5084980759260134e-09, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 3.222632827259986e-08, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 1.7860412038217532e-09, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.14515569806098938, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 0.008179398253560066, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.21517492830753326, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 0.004193523433059454, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 0.02479025162756443, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.027355510741472244, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.3422698974609375, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 0.01360572874546051, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.2445954531431198, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 0.01343513187021017, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.02442578598856926, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.02356213890016079, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 1.7862166856730255e-08, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 1.0487166690609229e-09, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 1.716440500842964e-08, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 1.0075595913150437e-09, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.1881248503923416, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 0.011045057326555252, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.18112123012542725, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 0.005929941777139902, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.02388503961265087, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.025490915402770042, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.24446208775043488, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 0.012127616442739964, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.1451258361339569, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 0.014971325173974037, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.018317578360438347, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.019051499664783478, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 2.0251732735232508e-08, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 1.161538198957146e-09, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 2.097755213981145e-08, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 1.2031444729387886e-09, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.18061727285385132, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 0.01035942044109106, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.1625351458787918, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 0.006442930083721876, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.022747568786144257, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.022273488342761993, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.14725080132484436, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 0.009035695344209671, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.10235454887151718, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.011763841845095158, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.014811698347330093, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.014483865350484848, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 9.790666055664587e-09, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 5.088479082537845e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 1.1590272741557328e-08, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 6.023799237198091e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.17724443972110748, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.009211781434714794, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.23743726313114166, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 0.00722897332161665, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.03561408445239067, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.035476621240377426, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.27245259284973145, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.016545401886105537, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.18448913097381592, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.012150014750659466, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.024899935349822044, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.023999962955713272, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 1.1735154181735652e-08, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 7.384141631483487e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 8.754385660836306e-09, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 5.507929112802401e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.160915806889534, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.010124141350388527, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.1760629117488861, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.0045414515770971775, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.026454433798789978, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.027059735730290413, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.2072581946849823, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.012275873683393002, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.20429326593875885, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.010773894377052784, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.02016390673816204, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.018890248611569405, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.017497526481747627, 'stageB_torsion.model.regressor.0.bias': 0.016012830659747124, 'stageB_torsion.model.regressor.1.weight': 0.5081132650375366, 'stageB_torsion.model.regressor.1.bias': 0.018474560230970383, 'stageB_torsion.model.regressor.3.weight': 1.0091986656188965, 'stageB_torsion.model.regressor.3.bias': 0.06883515417575836, 'stageB_torsion.output_projection.weight': 0.6867734789848328, 'stageB_torsion.output_projection.bias': 0.20725113153457642, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 12:59:15,288][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 12:59:15,288][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 12:59:15,288][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 12:59:15,288][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 12:59:15,288][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 12:59:15,291][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.1963, L_coord_C=1745.4683, Total=1745.6646
Epoch 2: 100%|| 1/1 [00:59<00:00,  0.02it/s, v_num=27, angle_loss_step=0.226, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.226, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 2: 100%|| 1/1 [00:59<00:00,  0.02it/s, v_num=27, angle_loss_step=0.196, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.226, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 2: 100%|| 1/1 [00:59<00:00,  0.02it/s, v_num=27, angle_loss_step=0.196, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.196, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.196, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.196, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.196, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.196, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3][RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-05-19 13:00:22,331] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:00:23,006] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:00:23.255000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[2025-05-19 13:00:23,373] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:00:23,452] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:00:23,479] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:00:23,492] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:00:23,533] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:00:23.621000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[2025-05-19 13:00:23,838] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:00:23.987000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:00:24.119000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:00:24.201000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:00:24.271000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:00:24.309000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:00:24.596000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:00:28.32 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:00:28.32 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:00:28.32 ...... len(structure_file) = 85
13:00:28.32 ...... chain_id = 'A'
13:00:28.32 ...... backend = 'dssr'
13:00:28.32 ...... angle_set = 'canonical'
13:00:28.32 ...... kwargs = {}
13:00:28.32   15 | def extract_rna_torsions(
13:00:28.32   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:00:28.32   41 |     if backend == "mdanalysis":
13:00:28.32   49 |     elif backend == "dssr":
13:00:28.32   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:00:30.71 .............. angles = None
13:00:30.71   53 |         if angles is None:
13:00:30.71   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:00:30.71   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:00:32.48 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:00:32.48                                                     -1.1687942 , -2.8552787 ],
13:00:32.48                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:00:32.48                                                     -1.2295908 , -2.7892737 ],
13:00:32.48                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:00:32.48                                                     -1.2545753 , -2.729158  ],
13:00:32.48                                                    ...,
13:00:32.48                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:00:32.48                                                     -1.2474957 , -2.8304672 ],
13:00:32.48                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:00:32.48                                                     -1.2624229 , -2.6697967 ],
13:00:32.48                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:00:32.48                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:00:35,459][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:00:35,459][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:00:35,464][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:00:35,464][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:00:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:00:35,497][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:00:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:00:35,498][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:00:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:00:35,500][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:00:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.07s
[2025-05-19 13:00:35,694][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.07s
[2025-05-19 13:00:35,694][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.07s
[2025-05-19 13:00:36,152][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:00:36,152][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:00:36,152][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:00:36,152][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:00:36][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:00:36,153][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:00:36][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:00:36,153][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:00:36][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:00:36,154][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:00:36][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:00:36,317][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:00:36,317][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16ebd7790>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 1.964283910638187e-06, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 1.964283910638187e-06, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 2.55272357208014e-06, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 8.355341947208217e-07, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 5.874886142009927e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 4.5207349330667057e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 5.26008108181486e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 3.5867822134605376e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 2.2964792841342375e-14, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 3.209745955246035e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 4.559049671115645e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 1.7107560097429086e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 1.9397107564600446e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 1.4860852104447986e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 1.37521823262432e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 9.808057939153514e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 3.627555855700848e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 1.369647975479893e-06, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 1.646108671593538e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 5.257271595837665e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 3.105933785718662e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 6.642421226388251e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 4.6336236181332424e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 1.200933752443234e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 2.720299842589452e-14, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 5.396318101702491e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 3.371279149178008e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 1.951328385985107e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 1.420243904703966e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 2.6706655376074195e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 1.536460274564888e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 1.1741890375560615e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 3.805485704333478e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 3.609392933867639e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 3.2993543896964184e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 1.6819390111777466e-06, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 2.5797814942052355e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 1.037566903505649e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 8.122297501245157e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 5.920643530998859e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 6.000420023051858e-14, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 1.9596021957113408e-05, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 1.5318199757530238e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 3.0372179935511667e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 1.0549574653850868e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 1.1459038660177612e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 4.867263783125964e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 8.022641850402579e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 1.8004138269134273e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 6.196025879035005e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 1.8560244541276916e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 2.361779934290098e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 2.4508726710337214e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 1.9656778249554918e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 1.704992413920081e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 2.0732528582811938e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 3.2661021239985266e-14, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 1.8456330508342944e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 1.5952466583257774e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 3.735981408681255e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 1.263078729607514e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 6.385788537954795e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 7.429424613292213e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 8.333293408213649e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 1.9825768049486214e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 1.1253335287619848e-05, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 7.811270563706785e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 5.99239183429745e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 1.0274318810843397e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 2.474715756761725e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 1.1882088202241903e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 1.2718133746147942e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 4.0310365722467933e-13, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 0.0001495253382017836, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 7.178969099186361e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 3.0448198231169954e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 2.8535419005493168e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 1.2509254702308681e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 5.002506895834813e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 3.9336937334155664e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 1.186962208521436e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 8.921189146349207e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 3.5979167023469927e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 4.977688877261244e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 4.704669117927551e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 1.7710317656849384e-09, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 1.0372982334194703e-10, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 2.849411373517796e-09, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 3.872832176482488e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 0.00042768605635501444, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 2.5049281248357147e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 0.0005443681729957461, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 2.3095219148672186e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 4.8622121539665386e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 5.395761036197655e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 0.0004442396166268736, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 2.1066123736090958e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 0.0004758719587698579, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 3.601901335059665e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 0.00013139618386048824, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 0.00018314510816708207, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 5.668902569588852e-10, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 5.286914656066344e-11, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 6.124137308383126e-10, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 4.600082901401459e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.0007901198114268482, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 7.368655496975407e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.0005935318185947835, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 3.1473486160393804e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 7.154915510909632e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 7.59955364628695e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.0007741490262560546, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 3.146771268802695e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.0009480673470534384, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 2.2898750103195198e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 0.00016936761676333845, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 0.00017667705833446234, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 6.059382440248839e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 3.258155897856163e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 3.748373555989559e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 2.0026973135411907e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.0014828738057985902, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 7.973283209139481e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.0027341777458786964, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 4.334593177190982e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 0.00034996456815861166, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0003746569564100355, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.004993917420506477, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 0.00019200834503863007, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.004485537763684988, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 0.00020336624584160745, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.000435404188465327, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.00043093354906886816, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 3.963338546242312e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 2.4533387474323654e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 3.887313804185055e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 2.4064366818676852e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.0034905667416751385, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 0.00021607140661217272, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.0038211692590266466, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 0.00011818827624665573, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0005637885187752545, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0005656543653458357, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.004702251870185137, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 0.00024236743047367781, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.0025665389839559793, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 0.00023863815295044333, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.00036075012758374214, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.0003643790550995618, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 3.298243345106755e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 1.8829013001542272e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 3.1463220917515855e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 1.796186636926489e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.0038597285747528076, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 0.00022033543791621923, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.0040586539544165134, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 0.00013861294428352267, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0006885303882881999, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0006290572928264737, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.004207601770758629, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 0.00025513037689961493, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.00273365480825305, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.00036545880720950663, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.0004098083882126957, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.00041673239320516586, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 2.953133848571099e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 1.611656294531194e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 3.180254948276229e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 1.7356071441154697e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.004692925605922937, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.0002561100118327886, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.005662628449499607, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 0.00021358414960559458, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0007948471466079354, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0007630605832673609, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.004189336206763983, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.00024199912149924785, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.004461902659386396, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.0006370593910105526, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.0007159812957979739, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.0008170096552930772, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 7.102694543625887e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 4.3421731488191284e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 6.920639061824829e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 4.2308628822596006e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.008458355441689491, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.0005170838558115065, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.014497135765850544, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.0003688980359584093, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0026347509119659662, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.002671507652848959, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.019845275208353996, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.001145820482634008, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.05960603058338165, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.001889332546852529, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.004522467963397503, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.005034798290580511, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.003919287584722042, 'stageB_torsion.model.regressor.0.bias': 0.004123896826058626, 'stageB_torsion.model.regressor.1.weight': 0.19115108251571655, 'stageB_torsion.model.regressor.1.bias': 0.0069509390741586685, 'stageB_torsion.model.regressor.3.weight': 0.5416406393051147, 'stageB_torsion.model.regressor.3.bias': 0.030866310000419617, 'stageB_torsion.output_projection.weight': 0.5875256657600403, 'stageB_torsion.output_projection.bias': 0.14341920614242554, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:00:37,038][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:00:37,039][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:00:37,039][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:00:37,039][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:00:37,039][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:00:37,062][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.1179, L_coord_C=1745.4683, Total=1745.5862
Epoch 3: 100%|| 1/1 [01:05<00:00,  0.02it/s, v_num=27, angle_loss_step=0.196, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.196, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 3: 100%|| 1/1 [01:05<00:00,  0.02it/s, v_num=27, angle_loss_step=0.118, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.196, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 3: 100%|| 1/1 [01:05<00:00,  0.02it/s, v_num=27, angle_loss_step=0.118, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.118, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.118, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.118, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.118, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.118, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3][RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
[2025-05-19 13:02:24,747] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,744] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,743] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,744] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,745] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,744] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,743] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:02:24,748] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:02:27.665000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.675000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.675000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.698000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.763000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.933000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.944000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:02:27.944000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:02:34.91 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:02:34.91 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:02:34.91 ...... len(structure_file) = 85
13:02:34.91 ...... chain_id = 'A'
13:02:34.91 ...... backend = 'dssr'
13:02:34.91 ...... angle_set = 'canonical'
13:02:34.91 ...... kwargs = {}
13:02:34.91   15 | def extract_rna_torsions(
13:02:34.91   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:02:34.91   41 |     if backend == "mdanalysis":
13:02:34.91   49 |     elif backend == "dssr":
13:02:34.91   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:02:37.47 .............. angles = None
13:02:37.47   53 |         if angles is None:
13:02:37.47   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:02:37.47   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:02:38.98 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:02:38.98                                                     -1.1687942 , -2.8552787 ],
13:02:38.98                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:02:38.98                                                     -1.2295908 , -2.7892737 ],
13:02:38.98                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:02:38.98                                                     -1.2545753 , -2.729158  ],
13:02:38.98                                                    ...,
13:02:38.98                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:02:38.98                                                     -1.2474957 , -2.8304672 ],
13:02:38.98                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:02:38.98                                                     -1.2624229 , -2.6697967 ],
13:02:38.98                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:02:38.98                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:02:49,662][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:02:49,662][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:02:49,707][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:02:49,707][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:02:50][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:02:50,119][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:02:50][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:02:50,121][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:02:50][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:02:50,144][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:02:51][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.35s
[2025-05-19 13:02:51,001][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.35s
[2025-05-19 13:02:51,001][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.35s
[2025-05-19 13:02:54,253][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:02:54,253][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:02:54,269][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:02:54,269][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:02:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:02:54,301][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:02:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:02:54,302][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:02:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:02:54,302][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:02:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[2025-05-19 13:02:54,590][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[2025-05-19 13:02:54,590][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.08s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16eaa3b20>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 3.7896700888495616e-08, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 3.789669733578194e-08, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 2.597854908970021e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 1.0099296510190925e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 5.215698983818129e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 1.1286060086490579e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 1.1563897617250518e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 9.675541257081477e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 5.140306790685407e-16, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 4.3929418325205916e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 4.2013468259938236e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 2.9594630746032635e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 2.78078871041032e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 4.129565578381289e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 2.2292210299923454e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 2.8692927145357316e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 9.901457431737981e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 2.9214103136609992e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 5.386644463811763e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 1.07051238984468e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 6.706734279049442e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 1.8360143627660364e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 9.098449771371975e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 2.4284480204528336e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 1.0223236628314992e-15, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 8.94542822038602e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 5.240068379208651e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 5.4692748108209344e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 3.6478255971417184e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 7.362833009949554e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 4.406162101844302e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 6.907756500140749e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 2.2770194618715323e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 2.3425081963068806e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 2.3204425048106714e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 7.534240609174958e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 1.6501692812198598e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 7.041172089827796e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 5.524523616351473e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 3.660547864825503e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 2.0591559412582805e-15, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 1.1647316568996757e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 9.121689714675085e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 1.6493576993070747e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 5.388331913991351e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 1.664442628168672e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 2.4870887571637468e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 4.289377670829708e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 9.796798039474197e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 2.9022174885540153e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 9.738178263773989e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 2.7237277322456066e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 1.3324091696631513e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 7.311414140787065e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 6.284838760528544e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 1.0444844988910518e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 3.474690297655355e-15, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 1.0695082437450765e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 9.145850299319136e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 3.300303319520026e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 8.434647469357515e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 6.383616835137218e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 5.0720814215310384e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 7.325052138185129e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 1.861246801126981e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 1.1084656534876558e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 6.537688612695547e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 7.323774866563326e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 9.546971568852314e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 1.062364152204509e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 5.130238456274583e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 5.499722899315884e-09, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 5.70804390082405e-14, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 1.3125645637046546e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 6.338338494060736e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 3.390625352039933e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 3.0258715355557797e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 1.1889908364537405e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 5.355361167858064e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 5.8662112678575795e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 1.9297654318961577e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 1.5955469280015677e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 6.615420033995179e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 9.083944860321935e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 8.68178358359728e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 2.1263912852731437e-10, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 1.2645552140144733e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 3.3580746516825855e-10, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 1.0179631660181543e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 8.323747897520661e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 4.950019501848146e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 0.00010764524631667882, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 4.158977390034124e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 1.160627834906336e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 1.2228450032125693e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 0.00010374555859016255, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 5.097676421428332e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 8.59597057569772e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 8.777723451203201e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 2.473611311870627e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 3.0630402761744335e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 1.0085335344633961e-10, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 8.635315552896206e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 9.09813960281447e-11, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 9.721737304069222e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.00017701672913972288, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 1.5156178960751276e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.00015849145711399615, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 6.524932814500062e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 2.041672451014165e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 2.0193536329315975e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.00017379288328811526, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 7.462282155756839e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.00023117985983844846, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 4.9056679927161895e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 4.463060031412169e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 4.7713456297060475e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 2.5414462201389654e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 1.3648032357038709e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 1.9373820603352243e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 1.0399171974928567e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.00042041941196657717, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 2.2576956325792708e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.0008505703299306333, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 1.3078110896458384e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 0.00010755106632132083, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.00011651769455056638, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.0014253074768930674, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 5.454804340843111e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.001430403208360076, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 5.836751006427221e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.0001657024258747697, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.0001537197967991233, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 1.5564108335475169e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 9.548612768528475e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 1.4473726384078844e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 8.87956028150505e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.001199902268126607, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 7.36140864319168e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.001273643341846764, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 3.818909317487851e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.00025243934942409396, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.00021246193500701338, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.001529382192529738, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 7.948505663080141e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.0010025189258158207, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 7.298174023162574e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.0001362030307063833, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.00013196926738601178, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 1.8521477407329456e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 1.0381978263196423e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 1.7647355798899866e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 9.892013423662416e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.0013240918051451445, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 7.422098860843107e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.0015438053524121642, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 5.1530918426578864e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.00021486399054992944, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0002188083017244935, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.0014485475840047002, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 8.384780085179955e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.0008153210510499775, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.00015736740897409618, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.00016355754632968456, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.00016425571811851114, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 1.9062668110692016e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 1.0923951847263691e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 2.0369803066522252e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 1.1673035801851306e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.0018670385470613837, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.0001069911741069518, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.002146879443898797, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 8.424419502262026e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0003196480975020677, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0003180447092745453, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.002470934297889471, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.00014050636673346162, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.002019614679738879, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.0002083698782371357, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.00032146109151653945, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.00032812677090987563, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 5.033138350540867e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 3.099466863720757e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 4.400030062523541e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 2.7095553231659686e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.003287333296611905, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.0002024273999268189, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.006296531762927771, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.00014253055269364268, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0012309369631111622, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.001269723055884242, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.009105904027819633, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0005234990385361016, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.028903543949127197, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0009328688611276448, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0024428206961601973, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0026253501418977976, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.0021345794666558504, 'stageB_torsion.model.regressor.0.bias': 0.0021732929162681103, 'stageB_torsion.model.regressor.1.weight': 0.10368267446756363, 'stageB_torsion.model.regressor.1.bias': 0.0037664351984858513, 'stageB_torsion.model.regressor.3.weight': 0.35151296854019165, 'stageB_torsion.model.regressor.3.bias': 0.01638009585440159, 'stageB_torsion.output_projection.weight': 0.5330458879470825, 'stageB_torsion.output_projection.bias': 0.11884765326976776, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:02:57,627][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:02:57,627][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:02:57,627][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:02:57,627][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:02:57,628][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:02:57,638][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0954, L_coord_C=1745.4683, Total=1745.5636
Epoch 4: 100%|| 1/1 [01:42<00:00,  0.01it/s, v_num=27, angle_loss_step=0.118, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.118, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 4: 100%|| 1/1 [01:42<00:00,  0.01it/s, v_num=27, angle_loss_step=0.0954, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.118, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 4: 100%|| 1/1 [01:42<00:00,  0.01it/s, v_num=27, angle_loss_step=0.0954, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0954, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0954, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0954, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0954, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0954, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]python3(82691) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82692) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82695) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82696) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82697) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
To disable this warning, you can either:
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTo disable this warning, you can either:
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable =(true | false)
TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
=(true | false)
python3(82699) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
python3(82746) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82747) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82748) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82750) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82751) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82752) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82753) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82755) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82757) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82759) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82754) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82756) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82758) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82761) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,207] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:04:05,207] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:04:06.050000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.050000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.050000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.050000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.054000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.056000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.068000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:04:06.090000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
python3(82762) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
13:04:10.86 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:04:10.86 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:04:10.86 ...... len(structure_file) = 85
13:04:10.86 ...... chain_id = 'A'
13:04:10.86 ...... backend = 'dssr'
13:04:10.86 ...... angle_set = 'canonical'
13:04:10.86 ...... kwargs = {}
13:04:10.86   15 | def extract_rna_torsions(
13:04:10.87   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:04:10.87   41 |     if backend == "mdanalysis":
13:04:10.87   49 |     elif backend == "dssr":
13:04:10.87   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
python3(82763) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
13:04:17.56 .............. angles = None
13:04:17.56   53 |         if angles is None:
13:04:17.57   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:04:17.57   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:04:19.50 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:04:19.50                                                     -1.1687942 , -2.8552787 ],
13:04:19.50                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:04:19.50                                                     -1.2295908 , -2.7892737 ],
13:04:19.50                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:04:19.50                                                     -1.2545753 , -2.729158  ],
13:04:19.50                                                    ...,
13:04:19.50                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:04:19.50                                                     -1.2474957 , -2.8304672 ],
13:04:19.50                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:04:19.50                                                     -1.2624229 , -2.6697967 ],
13:04:19.50                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:04:19.50                                                             nan, -2.653027  ]], dtype=float32)
python3(82774) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[2025-05-19 13:04:22,622][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:04:22,622][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:04:22,630][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:04:22,630][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:04:22][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:04:22,728][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:04:22][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:04:22,729][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:04:22][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:04:22,733][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:04:22][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.11s
[2025-05-19 13:04:22,974][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.11s
[2025-05-19 13:04:22,974][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.11s
[2025-05-19 13:04:23,690][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:04:23,690][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:04:23,692][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:04:23,692][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:04:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:04:23,699][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:04:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:04:23,699][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:04:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:04:23,699][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:04:23][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:04:23,915][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:04:23,915][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16ea02410>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 2.9697972081521584e-08, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 2.9697970305164745e-08, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 4.476310877521428e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 1.2378580827032692e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 1.0357195101562411e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 8.040387911023572e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 1.0417234852511115e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 6.110665751180022e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 2.5896467998325773e-16, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 4.24654835740057e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 6.738622548851936e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 2.7063963514706302e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 2.8557303188847527e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 3.339654552902971e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 2.3213670985455792e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 1.6425913074158416e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 5.629685051466993e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 2.1599721122811388e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 2.725487058263809e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 9.18689124773664e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 5.625911736473199e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 1.225949475269772e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 7.690908465640689e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 2.1057843468952342e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 1.525197382737484e-15, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 8.746999924369447e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 5.313558038011479e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 3.278817928276112e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 2.1933477256652623e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 4.627731087225584e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 2.7167166294361778e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 1.6659830848198e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 5.800051550153285e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 3.6958397231501294e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 3.8119690737303813e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 6.251899264952954e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 2.847086655322073e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 1.7039896160753187e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 1.337924215860653e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 1.0858550725245664e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 3.4245563233315044e-16, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 2.545043855661788e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 1.994298415297635e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 4.5817294846983714e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 1.4084761801314016e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 3.0923157368079046e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 6.260899798604669e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 6.240978223104321e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 1.4539685011527581e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 4.8988638923219696e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 1.7115197037398389e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 6.871677271647059e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 2.5112491641721135e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 1.6785506318228727e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 1.4254758484710806e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 3.752335775430993e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 4.0358406784258237e-16, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 2.148363336118564e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 1.818534300923602e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 9.17313585091506e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 1.8425419412437805e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 2.618492267458805e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 1.1232789809412225e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 1.7366062365908874e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 4.789463936560878e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 2.755733987669373e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 1.4761596389689657e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 2.1206682276897482e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 2.254100621712496e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 2.7953131476188275e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 1.3558554452786087e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 1.7737297741682312e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 2.2447774094733718e-14, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 3.1025019779917784e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 1.504898392568066e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 9.589157343725674e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 8.149438457394353e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 2.967366299344576e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 1.5101257133665058e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 1.351854393760732e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 4.772739714553609e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 3.1107340419112006e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 1.3330542003586743e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 2.0987833977414994e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 2.015003246924607e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 3.050860952757972e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 1.838503958448423e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 1.800335922008678e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 2.4633620880967766e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 1.8544351405580528e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 1.1174665814905893e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 2.1666248358087614e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 7.689392873544421e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 2.9718783025600715e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 2.9993454973009648e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 2.3792512365616858e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 1.1937042927456787e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 1.3163912626623642e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 1.5839688103369554e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 3.7861973396502435e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 4.631765932572307e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 9.694344285660073e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 7.849519103285407e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 6.2038009278342354e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 2.499240235338124e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 2.6877976779360324e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 2.1763182758149924e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 2.742466313065961e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 9.538346148474375e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 3.2580330753262388e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 3.321925078125787e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 2.7127947760163806e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 1.196853190776892e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 3.822005965048447e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 8.456751743324276e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 7.709290912316646e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 7.825738066458143e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 3.8457594747631774e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 2.0665095069177797e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 2.279919308900702e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 1.2250474072661444e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 7.069820276228711e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 3.798917759922915e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.0001626766170375049, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 2.4571188532718224e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 1.972331665456295e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 2.1984344130032696e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.00029199119308032095, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 1.1155622814840171e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.00030647459789179265, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 1.1739988622139208e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 3.4705924917943776e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 3.3888900361489505e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 6.452503115150776e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 3.919580805755718e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 6.33808908134803e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 3.850075640243755e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.00027590771787799895, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 1.67598500411259e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.00031606454285793006, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 9.383423275721725e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 4.783217082149349e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 4.850086406804621e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.0003832050715573132, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 1.9960860299761407e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.0003082211478613317, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 2.0647659766837023e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 3.808012479566969e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 3.9338676288025454e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 8.250439220702788e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 4.645298468802794e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 6.700783677926481e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 3.7727984798785474e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.0004084714164491743, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 2.2997777705313638e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.0005208732909522951, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 1.735256773827132e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 7.330605876632035e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 7.379058661172166e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.0004366221546661109, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 2.5099636332015507e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.00031478036544285715, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 6.36053882772103e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 6.548662349814549e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 6.652992306044325e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 6.460369045280245e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 3.744496900048855e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 6.788896528275856e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 3.934911858155532e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.0007370032835751772, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 4.271748548489995e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.000907845504116267, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 3.492244650260545e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.00014127341273706406, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.00014190956426318735, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.001074175233952701, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 6.090514943934977e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.000987452920526266, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.0001011443164316006, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.00016522008809261024, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.0001651453203521669, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 3.988378516339708e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 2.4414992597088236e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 4.038710199605333e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 2.4722758562578662e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.0016823455225676298, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.00010298263805452734, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.0033039404079318047, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 7.178454688983038e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0006490108789876103, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0006957111763767898, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.005425226874649525, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0003142101631965488, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.016710614785552025, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0005051257321611047, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0013440577313303947, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0014871892053633928, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.001208305126056075, 'stageB_torsion.model.regressor.0.bias': 0.0012506955536082387, 'stageB_torsion.model.regressor.1.weight': 0.05470336228609085, 'stageB_torsion.model.regressor.1.bias': 0.0019853401463478804, 'stageB_torsion.model.regressor.3.weight': 0.21562153100967407, 'stageB_torsion.model.regressor.3.bias': 0.008687579073011875, 'stageB_torsion.output_projection.weight': 0.4769898056983948, 'stageB_torsion.output_projection.bias': 0.10264994204044342, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:04:24,980][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:04:24,982][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:04:24,982][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:04:24,982][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:04:24,982][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:04:24,987][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0828, L_coord_C=1745.4683, Total=1745.5511
Epoch 5: 100%|| 1/1 [01:06<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0954, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0954, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 5: 100%|| 1/1 [01:06<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0828, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0954, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 5: 100%|| 1/1 [01:06<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0828, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0828, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0828, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0828, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0828, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0828, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]python3(82793) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82794) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82795) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82796) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82797) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82798) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82799) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(82800) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTo disable this warning, you can either:
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
[2025-05-19 13:05:49,362] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,362] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,362] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:05:49,364] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:05:51.112000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.124000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.153000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.184000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.185000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.227000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.229000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:05:51.347000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:06:10.70 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:06:10.70 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:06:10.70 ...... len(structure_file) = 85
13:06:10.70 ...... chain_id = 'A'
13:06:10.70 ...... backend = 'dssr'
13:06:10.70 ...... angle_set = 'canonical'
13:06:10.70 ...... kwargs = {}
13:06:10.70   15 | def extract_rna_torsions(
13:06:10.70   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:06:10.70   41 |     if backend == "mdanalysis":
13:06:10.70   49 |     elif backend == "dssr":
13:06:10.70   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:06:15.54 .............. angles = None
13:06:15.54   53 |         if angles is None:
13:06:15.55   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:06:15.55   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:06:18.32 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:06:18.32                                                     -1.1687942 , -2.8552787 ],
13:06:18.32                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:06:18.32                                                     -1.2295908 , -2.7892737 ],
13:06:18.32                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:06:18.32                                                     -1.2545753 , -2.729158  ],
13:06:18.32                                                    ...,
13:06:18.32                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:06:18.32                                                     -1.2474957 , -2.8304672 ],
13:06:18.32                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:06:18.32                                                     -1.2624229 , -2.6697967 ],
13:06:18.32                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:06:18.32                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:06:31,499][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:06:31,499][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:06:31,544][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:06:31,544][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:06:31][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:06:31,646][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:06:31][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:06:31,647][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:06:31][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:06:31,652][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:06:32][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.24s
[2025-05-19 13:06:32,323][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.24s
[2025-05-19 13:06:32,323][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.24s
[2025-05-19 13:06:35,104][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:06:35,104][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:06:35,111][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:06:35,111][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:06:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:06:35,141][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:06:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:06:35,142][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:06:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:06:35,144][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:06:35][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[2025-05-19 13:06:35,421][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[2025-05-19 13:06:35,421][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16e7eea70>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 1.4988154362072237e-07, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 1.4988155783157708e-07, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 2.2155302303872304e-07, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 5.809787850807879e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 5.156760707336616e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 4.4072649529880437e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 5.573097539013361e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 3.428355199730504e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 1.5038479173019393e-15, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 1.9416258112414653e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 3.1891648433202135e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 1.2708899532754003e-07, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 1.3047649183306476e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 1.5424019395027244e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 1.1350141271293523e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 6.816222963834662e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 2.3590311926113827e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 8.854801336610763e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 1.0934523286820763e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 3.721176611293231e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 2.438664736814644e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 4.8529496865512556e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 3.09228442851861e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 8.663451467327832e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 2.833140555383133e-15, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 3.8301550375763327e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 2.2960968237839552e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 1.5249656826199498e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 9.688812419028636e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 2.1290961882414194e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 1.2932254378483776e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 8.688078878549277e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 3.0810525242230824e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 2.0140379319855128e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 2.1527781779440147e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 2.403226062597241e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 1.4772747647384676e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 4.5768362433307175e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 3.596760111967967e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 3.164361439544905e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 1.3737919374816722e-15, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 1.2985776720597642e-06, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 1.0187066834532743e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 2.2824882250915834e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 6.613840497493584e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 1.107511096165581e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 3.2115153203449154e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 2.8273652219468204e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 6.749284242602016e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 2.025123819748842e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 6.713925859713754e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 2.165102017670506e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 1.0717907628077228e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 3.5880554083433935e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 3.011712823575152e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 1.015410244775694e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 1.4075013076835516e-15, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 8.916004503589647e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 7.480652186586667e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 4.148209313825646e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 7.114487488024679e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 9.271725076587245e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 5.047135331892605e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 6.090331794439408e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 1.799324245155276e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 1.028492647492385e-06, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 5.284679360784139e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 7.98037319782452e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 7.922876648081001e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 4.329515079781743e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 2.109358000768058e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 2.068829413337525e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 4.208966346026105e-14, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 1.0337717867514584e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 5.036662855673057e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 3.2786417705210624e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 2.681146895611164e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 9.922756589730852e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 5.030831857766316e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 4.904326942778425e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 1.8262464607232687e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 1.364703530271072e-05, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 6.048725254004239e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 8.438496479357127e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 8.122178769554012e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 7.984851668751958e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 4.867006103692617e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 4.1067708261843805e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 7.783104672906549e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 7.578607619507238e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 4.619243100023596e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 9.818982653087005e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 3.2463792649650713e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 1.239325683854986e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 1.307568254560465e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 9.937170398188755e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 5.0467433538869955e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 7.351215754169971e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 9.201767170452513e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 1.8637481844052672e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 2.248918463010341e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 2.210461674811981e-11, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 1.707427168401443e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 2.0360019226117743e-11, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 1.5469377081384073e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 0.00014258426381275058, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 1.1013278708560392e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 0.00014153154916130006, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 4.4219536903256085e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 1.841509401856456e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 1.768834044924006e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 0.00014865759294480085, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 6.636989837716101e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 0.00016709283227100968, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 4.191337666270556e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 3.583981015253812e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 3.531025868142024e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 1.9618887070471658e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 1.0564689748027156e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 1.4642737022896313e-10, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 7.88497739323546e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 0.0003315154172014445, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 1.7851807569968514e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 0.0007139803492464125, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 1.0707666660891846e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 9.59873286774382e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 0.00010321466834284365, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.0013221625704318285, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 5.0460497732274234e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.0013668196042999625, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 5.053154382039793e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 0.00016530591528862715, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 0.0001601009425939992, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 2.2410669153760665e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 1.3489752717643633e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 2.2889985740182084e-10, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 1.3778314426893257e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.0013095218455418944, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 7.882287900429219e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.001407878939062357, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 4.163294215686619e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0002518598339520395, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0002295114245498553, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.0017756620654836297, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 9.265281551051885e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.001316557638347149, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 8.503802382620052e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 0.00016613111074548215, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 0.00016829224477987736, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 1.8981874405632482e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 1.0761157590022385e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 1.7098723825714757e-10, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 9.69354371077591e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.00170259946025908, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 9.652281005401164e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.0018580140313133597, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 6.201162614161149e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0002678650780580938, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0002680244797375053, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.001653308980166912, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 9.501281601842493e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.0010452506830915809, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 0.0001983677502721548, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 0.00022033866844139993, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 0.0002132528752554208, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 1.4485293520216658e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 8.394703335246056e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 1.661552007092837e-10, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 9.629234909436235e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.0025400270242244005, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 0.00014720049512106925, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.0027849453035742044, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 0.00010310184006812051, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.00043376022949814796, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.00045687402598559856, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.003979625180363655, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 0.00022601638920605183, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.002732774941250682, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 0.00028073429712094367, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.0004371106915641576, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.00046783973812125623, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 5.21662102404008e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 3.1876654094098456e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 5.626606403019707e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 3.4381719693499235e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.004303246270865202, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.0002629505761433393, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.007591016590595245, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 0.0001620323455426842, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0016985054826363921, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.001651531900279224, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.012526125647127628, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0007294378592632711, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.03712299466133118, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0010807050857692957, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0031262326519936323, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0032362500205636024, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.0025749956257641315, 'stageB_torsion.model.regressor.0.bias': 0.0026399039197713137, 'stageB_torsion.model.regressor.1.weight': 0.11573979258537292, 'stageB_torsion.model.regressor.1.bias': 0.004197097849100828, 'stageB_torsion.model.regressor.3.weight': 0.3832763731479645, 'stageB_torsion.model.regressor.3.bias': 0.013735692948102951, 'stageB_torsion.output_projection.weight': 0.4607315957546234, 'stageB_torsion.output_projection.bias': 0.0984090194106102, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:06:39,633][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:06:39,634][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:06:39,634][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:06:39,634][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:06:39,634][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:06:39,646][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0799, L_coord_C=1745.4683, Total=1745.5481
Epoch 6: 100%|| 1/1 [01:49<00:00,  0.01it/s, v_num=27, angle_loss_step=0.0828, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0828, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 6: 100%|| 1/1 [01:49<00:00,  0.01it/s, v_num=27, angle_loss_step=0.0799, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0828, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 6: 100%|| 1/1 [01:49<00:00,  0.01it/s, v_num=27, angle_loss_step=0.0799, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0799, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0799, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0799, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0799, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0799, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]python3(83173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83174) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83175) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83176) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83177) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83178) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83179) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
=(true | false)
[2025-05-19 13:07:33,825] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,825] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,825] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,825] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,831] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,891] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,891] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:07:33,978] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:07:34.289000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.289000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.295000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.315000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.331000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.366000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.379000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:07:34.492000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:07:36.02 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:07:36.02 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:07:36.02 ...... len(structure_file) = 85
13:07:36.02 ...... chain_id = 'A'
13:07:36.02 ...... backend = 'dssr'
13:07:36.02 ...... angle_set = 'canonical'
13:07:36.02 ...... kwargs = {}
13:07:36.02   15 | def extract_rna_torsions(
13:07:36.02   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:07:36.02   41 |     if backend == "mdanalysis":
13:07:36.02   49 |     elif backend == "dssr":
13:07:36.02   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:07:38.01 .............. angles = None
13:07:38.01   53 |         if angles is None:
13:07:38.01   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:07:38.01   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:07:39.47 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:07:39.47                                                     -1.1687942 , -2.8552787 ],
13:07:39.47                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:07:39.47                                                     -1.2295908 , -2.7892737 ],
13:07:39.47                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:07:39.47                                                     -1.2545753 , -2.729158  ],
13:07:39.47                                                    ...,
13:07:39.47                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:07:39.47                                                     -1.2474957 , -2.8304672 ],
13:07:39.47                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:07:39.47                                                     -1.2624229 , -2.6697967 ],
13:07:39.47                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:07:39.47                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:07:44,987][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:07:44,987][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:07:44,994][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:07:44,994][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:07:45][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:07:45,041][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:07:45][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:07:45,043][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:07:45][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:07:45,048][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:07:45][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.12s
[2025-05-19 13:07:45,419][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.12s
[2025-05-19 13:07:45,419][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.12s
[2025-05-19 13:07:46,312][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:07:46,312][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:07:46,313][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:07:46,313][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:07:46][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:07:46,314][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:07:46][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:07:46,314][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:07:46][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:07:46,314][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:07:46][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:07:46,479][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:07:46,479][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16fda0280>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 3.0786917903924405e-08, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 3.0786921456638083e-08, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 4.2873175232216454e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 1.0995509391875657e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 1.0034582942353154e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 9.834203673619868e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 1.2197098886446156e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 7.884347397180136e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 5.458051083815361e-16, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 3.461983055785822e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 5.769701161284502e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 2.308708957343697e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 2.3223993839138757e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 2.796521902936888e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 2.0730992478235066e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 1.1258388887824822e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 3.909253198930429e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 1.321110065788389e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 1.629346241260876e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 5.550802484322048e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 3.7179619383209683e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 7.485098763027054e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 4.440007805861512e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 1.3686901390030926e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 6.535695633102341e-16, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 5.603354935601601e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 3.3091334117330007e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 2.2471656535572038e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 1.3626220152218593e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 3.314502672324693e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 1.9615189472688144e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 1.2488098555252236e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 4.4957698674963353e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 2.4762025319091663e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 2.7040505945485904e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 4.144449761156466e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 1.913072900094903e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 5.016718707118173e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 3.93999381731458e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 3.4286884442735754e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 3.1550039697355843e-16, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 1.7017997322454903e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 1.3368750551023822e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 3.250763924711464e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 8.96019169971396e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 1.643129010631128e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 4.1243306547755765e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 3.3911113916929025e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 8.293451481122815e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 2.2014921441382285e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 6.543264263747517e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 4.82322271011526e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 1.2578351693548484e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 3.2647431980059594e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 2.7154617096569744e-11, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 9.479619311747456e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 4.973145367940363e-16, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 1.0326463240062367e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 8.60171933680931e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 5.338526420928247e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 8.261912043394659e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 1.5500933869816436e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 5.8618594422910064e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 7.683695457672002e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 2.3900807999410745e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 1.5482503101793554e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 7.818635183731715e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 1.9452512844964076e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 1.1702596225404704e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 1.0515059656324155e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 5.145201972811542e-12, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 4.986029644715728e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 5.8595299836164405e-15, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 1.5009115941211348e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 7.344324615132791e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 5.928010295974673e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 4.698892297483326e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 1.913980725021247e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 9.171289150344819e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 9.796494850888848e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 3.800941428266924e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 2.2439526219386607e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 1.0263793370768326e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 1.3969537349112215e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 1.4587916439268156e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 7.957476691466958e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 4.898182554002872e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 4.203607080477401e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 7.471375406374603e-14, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 1.2814187357435003e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 7.887272204243345e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 1.765738670655992e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 5.509535867531667e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 2.6345956030127127e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 2.6327400064474205e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 2.755012064881157e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 1.4096398217589012e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 1.2668915587710217e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 1.5419948340422707e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 2.9952855129522504e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 3.6010444546263898e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 3.685014533100217e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 2.809757092790688e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 2.484708185729323e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 1.8868066831156938e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 2.144886639143806e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 1.6354354102077195e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 2.5135037503787316e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 7.453376724697591e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 2.8005051717627794e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 2.7766845960286446e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 2.2014568457962014e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 9.875705018203007e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 2.436960130580701e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 7.086903224262642e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 5.078037247585598e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 4.931325293000555e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 1.673120322842614e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 9.044255145154723e-13, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 1.304437027144223e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 7.05126007496254e-13, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 4.134838673053309e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 2.235059127997374e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 8.737645111978054e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 1.312173935730243e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 1.1754577826650348e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 1.304686247749487e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.0001577039947733283, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 6.0141114772704896e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.00023864698596298695, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 8.508083737979177e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 2.7769199732574634e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 2.7950620278716087e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 6.87478615701842e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 4.097464988833677e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 6.52528656108764e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 3.889162863285334e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.00023385236272588372, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 1.3937909898231737e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.00024540378944948316, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 7.257818197103916e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 4.069996066391468e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 3.9224942156579345e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.00032080369419418275, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 1.680278182902839e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.00027917209081351757, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 1.7098836906370707e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 3.763557833735831e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 3.5788907553069293e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 4.232487971211896e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 2.384142145858692e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 3.4058204173570417e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 1.918498779979183e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.00038644304731860757, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 2.176800262532197e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.00047674839152023196, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 1.572110522829462e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 7.027009269222617e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 6.86902494635433e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.0004190201871097088, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 2.4020426280912943e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.00029014065512456, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 5.38634012627881e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 5.5664619139861315e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 5.774181772721931e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 5.412079465139108e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 3.1184907087200564e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 6.434231619723008e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 3.7074922125401866e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.0006653229938820004, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 3.8336314901243895e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.000793848535977304, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 2.838301588781178e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 0.00014327680401038378, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0001378610759275034, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.001056517125107348, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 5.996867912472226e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.001080268993973732, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 9.582196071278304e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.000153658096678555, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.0001749948860378936, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 3.360473010971532e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 2.0184869400918792e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 3.393756942138282e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 2.0384751178714744e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.001689823460765183, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 0.00010149841546081007, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.0035017230547964573, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 7.080488285282627e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.00075954117346555, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0007818119483999908, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.006551799364387989, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.0003814385854639113, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.01903892494738102, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0005426066927611828, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0015484971227124333, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0016701740678399801, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.0014108156319707632, 'stageB_torsion.model.regressor.0.bias': 0.0014577907277271152, 'stageB_torsion.model.regressor.1.weight': 0.05760783702135086, 'stageB_torsion.model.regressor.1.bias': 0.002087564440444112, 'stageB_torsion.model.regressor.3.weight': 0.2620304226875305, 'stageB_torsion.model.regressor.3.bias': 0.008584696799516678, 'stageB_torsion.output_projection.weight': 0.45057469606399536, 'stageB_torsion.output_projection.bias': 0.09171156585216522, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:07:47,281][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:07:47,281][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:07:47,281][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:07:47,281][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:07:47,281][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:07:47,285][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0754, L_coord_C=1745.4683, Total=1745.5437
Epoch 7: 100%|| 1/1 [00:45<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0799, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0799, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 7: 100%|| 1/1 [00:45<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0754, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0799, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 7: 100%|| 1/1 [00:46<00:00,  0.02it/s, v_num=27, angle_loss_step=0.0754, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0754, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0754, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0754, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0754, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0754, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]python3(83348) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83349) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83350) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83351) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83352) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83353) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83354) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83355) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable =(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
[2025-05-19 13:08:46,572] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,575] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,590] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,596] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,748] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,750] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:08:46,757] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:08:46.963000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:46.964000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:46.965000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:46.980000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:46.984000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:47.216000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:47.262000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:08:47.287000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:08:48.95 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:08:48.95 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:08:48.95 ...... len(structure_file) = 85
13:08:48.95 ...... chain_id = 'A'
13:08:48.95 ...... backend = 'dssr'
13:08:48.95 ...... angle_set = 'canonical'
13:08:48.95 ...... kwargs = {}
13:08:48.95   15 | def extract_rna_torsions(
13:08:48.95   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:08:48.95   41 |     if backend == "mdanalysis":
13:08:48.95   49 |     elif backend == "dssr":
13:08:48.95   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:08:49.84 .............. angles = None
13:08:49.84   53 |         if angles is None:
13:08:49.84   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:08:49.84   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:08:51.25 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:08:51.25                                                     -1.1687942 , -2.8552787 ],
13:08:51.25                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:08:51.25                                                     -1.2295908 , -2.7892737 ],
13:08:51.25                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:08:51.25                                                     -1.2545753 , -2.729158  ],
13:08:51.25                                                    ...,
13:08:51.25                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:08:51.25                                                     -1.2474957 , -2.8304672 ],
13:08:51.25                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:08:51.25                                                     -1.2624229 , -2.6697967 ],
13:08:51.25                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:08:51.25                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:08:52,408][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:08:52,408][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:08:52,411][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:08:52,411][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:08:52][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:08:52,435][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:08:52][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:08:52,435][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:08:52][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:08:52,436][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:08:52][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[2025-05-19 13:08:52,655][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[2025-05-19 13:08:52,655][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.09s
[2025-05-19 13:08:53,006][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:08:53,006][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:08:53,006][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:08:53,006][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:08:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:08:53,008][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:08:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:08:53,008][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:08:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:08:53,008][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:08:53][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:08:53,151][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:08:53,151][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16e49b9a0>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 5.950822412614798e-08, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 5.950822767886166e-08, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 8.295231879174025e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 2.0822881197091192e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 1.9404245321652525e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 1.902236412831826e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 2.3482173983069288e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 1.5256222951620657e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 7.240052233138759e-16, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 6.78105891438463e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 1.130050986120068e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 4.7129415037261424e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 4.4991530501192756e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 5.3544799705207424e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 4.3141437089389e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 2.1489647394901112e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 7.522696909845195e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 2.6026180321991887e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 2.9982799576444563e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 1.0885223389323073e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 7.494771026017588e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 1.4624342625779718e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 8.607948798200482e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 2.7568480831519082e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 1.9205093532869604e-15, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 1.1602693206214099e-07, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 6.728431145575087e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 4.954813803692559e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 2.863238091066478e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 7.115365630028236e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 4.436091938231357e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 3.1049449233933046e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 1.1290015589082714e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 6.627573867490355e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 7.464165396875444e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 7.550789860033547e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 4.7923304435926184e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 5.436887051679662e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 4.277031853683333e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 3.878933285683672e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 6.138850638797452e-16, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 4.2190114868390083e-07, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 3.318917407568733e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 7.700634085949787e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 2.031043244699049e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 3.042244145490258e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 1.0517307025281752e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 7.483466646363013e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 1.8708750104678984e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 6.300366806044622e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 1.804575600061753e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 8.260858663788895e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 3.45579955762787e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 4.934775366116639e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 4.080925528060497e-11, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 1.2202298060870476e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 4.726749836331192e-16, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 2.9896906994508754e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 2.4752958793783364e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 1.6156026561020553e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 2.33163657270552e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 3.379034296813188e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 1.94137346198886e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 1.947586696360304e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 6.291265730595796e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 3.3970323443099915e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 1.705440233479294e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 4.0365441122958146e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 2.518301869258721e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 1.0866746391613447e-10, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 5.339334009646368e-12, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 5.258267432584063e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 1.8456148271456772e-14, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 3.2060179364634678e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 1.575255339503201e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 1.1664276371448068e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 9.006033252489942e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 3.616763422087388e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 1.7786315709145128e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 1.5089356111275265e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 6.050729695061818e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 3.6704163903777953e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 1.7276963859558236e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 2.2557221655006288e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 2.2492290554509964e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 1.09195066183565e-11, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 6.778239536493758e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 7.438494264988549e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 3.056472718150627e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 2.1545032723224722e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 1.3373238516578567e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 2.983944614243228e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 8.880636528374453e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 4.0619329411129e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 4.180768428341253e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 3.667281271191314e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 1.885503934317967e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 1.726409391267225e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 2.0021668660774594e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 4.055486442666734e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 4.4571961552719586e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 1.0146236064864644e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 7.760045590051584e-14, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 8.364607292400683e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 6.295131245708657e-14, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 2.2400214220397174e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 1.7131646927737165e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 2.8862927138106897e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 8.335839538631262e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 2.9708282909268746e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 3.045566018045065e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 2.706124541873578e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 1.2149764643254457e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 2.6900965167442337e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 9.021174491863349e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 5.213998974795686e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 5.25642008142313e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 2.3725703693355804e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 1.2852706095592414e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 2.2615662814695625e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 1.2251736083990217e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 4.4942884414922446e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 2.434572252241196e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 9.163057256955653e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 1.3814544672641205e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 1.17525014502462e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 1.3497969121090136e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 0.00015856789832469076, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 6.0435668274294585e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.00019925196829717606, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 6.913406195963034e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 2.272175879625138e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 2.3677801436861046e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 2.8809217164638135e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 1.7015355053759396e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 2.8050744019236795e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 1.6567394157951387e-12, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.00019164914556313306, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 1.1319249097141437e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.00020130009215790778, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 5.960370344837429e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 3.239445140934549e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 3.191866562701762e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.00024892931105569005, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 1.3087438674119767e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.000218316592508927, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 1.2880988833785523e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 2.7906808099942282e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 2.7609637982095592e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 5.485339266142475e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 3.0721352111545697e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 4.551303778299598e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 2.5490009408768444e-12, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.0002827561111189425, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 1.5835699741728604e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.00034886939101852477, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 1.1389446626708377e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 4.6506374928867444e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 4.8014120693551376e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.0003295415954198688, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 1.8885502868215553e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.00019256790983490646, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 3.3382111723767594e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 3.4133612643927336e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 3.5651231883093715e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 3.166557641098855e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 1.8031216710706621e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 3.5107215745622256e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 1.999092948470893e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.00036567822098731995, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 2.0822661099373363e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.0004711973015218973, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 1.6420324755017646e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 7.69304097048007e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 8.030544267967343e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.0007692402577959001, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 4.3682266550604254e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.0007592757465317845, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 6.10322822467424e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 0.00011453762999735773, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 0.00011887050641234964, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 2.356565775851749e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 1.4100648600134935e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 2.431802537117278e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 1.4550857047046506e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.0012296095956116915, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 7.357282447628677e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.002592803444713354, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 5.103967123432085e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0005549256457015872, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0005846862914040685, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.004286195617169142, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.00024925507022999227, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.013701263815164566, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0003867215127684176, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0011285293148830533, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0012095723068341613, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.0009383090655319393, 'stageB_torsion.model.regressor.0.bias': 0.0010092887096107006, 'stageB_torsion.model.regressor.1.weight': 0.04620489105582237, 'stageB_torsion.model.regressor.1.bias': 0.0016732836375012994, 'stageB_torsion.model.regressor.3.weight': 0.20795175433158875, 'stageB_torsion.model.regressor.3.bias': 0.006284111645072699, 'stageB_torsion.output_projection.weight': 0.44574975967407227, 'stageB_torsion.output_projection.bias': 0.08827190101146698, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:08:53,991][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:08:53,991][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:08:53,991][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:08:53,991][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:08:53,991][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:08:53,994][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0732, L_coord_C=1745.4683, Total=1745.5415
Epoch 8: 100%|| 1/1 [00:35<00:00,  0.03it/s, v_num=27, angle_loss_step=0.0754, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0754, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 8: 100%|| 1/1 [00:35<00:00,  0.03it/s, v_num=27, angle_loss_step=0.0732, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0754, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 8: 100%|| 1/1 [00:35<00:00,  0.03it/s, v_num=27, angle_loss_step=0.0732, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0732, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0732, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0732, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=27, angle_loss_step=0.0732, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0732, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]python3(83396) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83397) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83398) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83399) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83400) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
python3(83401) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISMTo disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable =(true | false)
To disable this warning, you can either:
TOKENIZERS_PARALLELISM	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
python3(83402) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
python3(83403) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32
[2025-05-19 13:09:16,182] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,391] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,392] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,457] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,463] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,525] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:09:16.546000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[2025-05-19 13:09:16,597] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:09:16,622] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
W0519 13:09:16.682000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.712000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.772000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.772000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.887000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.911000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
W0519 13:09:16.932000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
13:09:18.05 >>> Call to extract_rna_torsions in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/preprocessing/angles.py", line 15
13:09:18.05 ...... structure_file = '/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb'
13:09:18.05 ...... len(structure_file) = 85
13:09:18.05 ...... chain_id = 'A'
13:09:18.05 ...... backend = 'dssr'
13:09:18.05 ...... angle_set = 'canonical'
13:09:18.05 ...... kwargs = {}
13:09:18.05   15 | def extract_rna_torsions(
13:09:18.05   40 |     print(f"[DEBUG] extract_rna_torsions called with: structure_file={structure_file}, chain_id={chain_id}, backend={backend}, angle_set={angle_set}")
13:09:18.05   41 |     if backend == "mdanalysis":
13:09:18.05   49 |     elif backend == "dssr":
13:09:18.05   51 |         angles = _extract_rna_torsions_dssr(structure_file, chain_id, angle_set)
13:09:18.76 .............. angles = None
13:09:18.76   53 |         if angles is None:
13:09:18.76   54 |             print(f"[DEBUG] DSSR returned None; falling back to MDAnalysis for {structure_file}, angle_set={angle_set}")
13:09:18.76   55 |             return _extract_rna_torsions_mdanalysis(structure_file, chain_id, angle_set)
13:09:20.05 <<< Return value from extract_rna_torsions: array([[        nan, -3.0098917 ,  0.8642612 , ..., -2.6056218 ,
13:09:20.05                                                     -1.1687942 , -2.8552787 ],
13:09:20.05                                                    [-1.1830193 ,  3.0696287 ,  0.855325  , ..., -2.6342597 ,
13:09:20.05                                                     -1.2295908 , -2.7892737 ],
13:09:20.05                                                    [-1.1557424 ,  2.996131  ,  0.97343844, ..., -2.6753435 ,
13:09:20.05                                                     -1.2545753 , -2.729158  ],
13:09:20.05                                                    ...,
13:09:20.05                                                    [-1.0929137 ,  3.0127153 ,  0.9671528 , ..., -2.6051028 ,
13:09:20.05                                                     -1.2474957 , -2.8304672 ],
13:09:20.05                                                    [-1.1152396 ,  3.0490658 ,  0.8933744 , ..., -2.6351929 ,
13:09:20.05                                                     -1.2624229 , -2.6697967 ],
13:09:20.05                                                    [-1.1356908 ,  3.0447676 ,  0.9264612 , ...,         nan,
13:09:20.05                                                             nan, -2.653027  ]], dtype=float32)
[2025-05-19 13:09:21,168][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[NOISE-PRINT] Entered training_step
[2025-05-19 13:09:21,168][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:09:21,174][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:09:21,174][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:09:21,204][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:09:21,205][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:09:21,207][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.06s
[2025-05-19 13:09:21,404][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.06s
[2025-05-19 13:09:21,404][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.06s
[2025-05-19 13:09:21,802][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[2025-05-19 13:09:21,802][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Predicting for sequence of length 29
[2025-05-19 13:09:21,802][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:09:21,802][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Using provided adjacency matrix with shape torch.Size([1, 512, 512])
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[DEBUG][PAIRFORMER] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][PAIRFORMER] s_emb.shape: torch.Size([29, 0])
[DEBUG][FORWARD] s_emb shape after predict: torch.Size([29, 0])
[DEBUG][FORWARD] sequence length: 29
[DEBUG][FORWARD] s_emb shape before unsqueeze: torch.Size([29, 0])
[2025-05-19 13:09:21,804][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:09:21,804][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': False}
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:09:21,804][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:09:21][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:09:21,957][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[2025-05-19 13:09:21,957][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.04s
[DEBUG][FORWARD] output['s_embeddings'] shape: torch.Size([29, 0])
[TRAIN DEBUG] predicted_angles_sincos device: cpu
[TRAIN DEBUG] true_angles_sincos device: cpu
[DEBUG][LOSS] loss_angle.requires_grad: True
[DEBUG][LOSS] loss_angle.grad_fn: <DivBackward0 object at 0x16e730dc0>
[DEBUG][LOSS] Grad norms after backward: {'stageA.model.encoder.enc.0.conv.0.weight': None, 'stageA.model.encoder.enc.0.conv.0.bias': None, 'stageA.model.encoder.enc.0.conv.1.weight': None, 'stageA.model.encoder.enc.0.conv.1.bias': None, 'stageA.model.encoder.enc.0.conv.3.weight': None, 'stageA.model.encoder.enc.0.conv.3.bias': None, 'stageA.model.encoder.enc.0.conv.4.weight': None, 'stageA.model.encoder.enc.0.conv.4.bias': None, 'stageA.model.encoder.enc.1.1.conv.0.weight': None, 'stageA.model.encoder.enc.1.1.conv.0.bias': None, 'stageA.model.encoder.enc.1.1.conv.1.weight': None, 'stageA.model.encoder.enc.1.1.conv.1.bias': None, 'stageA.model.encoder.enc.1.1.conv.3.weight': None, 'stageA.model.encoder.enc.1.1.conv.3.bias': None, 'stageA.model.encoder.enc.1.1.conv.4.weight': None, 'stageA.model.encoder.enc.1.1.conv.4.bias': None, 'stageA.model.encoder.enc.2.1.conv.0.weight': None, 'stageA.model.encoder.enc.2.1.conv.0.bias': None, 'stageA.model.encoder.enc.2.1.conv.1.weight': None, 'stageA.model.encoder.enc.2.1.conv.1.bias': None, 'stageA.model.encoder.enc.2.1.conv.3.weight': None, 'stageA.model.encoder.enc.2.1.conv.3.bias': None, 'stageA.model.encoder.enc.2.1.conv.4.weight': None, 'stageA.model.encoder.enc.2.1.conv.4.bias': None, 'stageA.model.encoder.enc.3.1.conv.0.weight': None, 'stageA.model.encoder.enc.3.1.conv.0.bias': None, 'stageA.model.encoder.enc.3.1.conv.1.weight': None, 'stageA.model.encoder.enc.3.1.conv.1.bias': None, 'stageA.model.encoder.enc.3.1.conv.3.weight': None, 'stageA.model.encoder.enc.3.1.conv.3.bias': None, 'stageA.model.encoder.enc.3.1.conv.4.weight': None, 'stageA.model.encoder.enc.3.1.conv.4.bias': None, 'stageA.model.encoder.enc.4.1.conv.0.weight': None, 'stageA.model.encoder.enc.4.1.conv.0.bias': None, 'stageA.model.encoder.enc.4.1.conv.1.weight': None, 'stageA.model.encoder.enc.4.1.conv.1.bias': None, 'stageA.model.encoder.enc.4.1.conv.3.weight': None, 'stageA.model.encoder.enc.4.1.conv.3.bias': None, 'stageA.model.encoder.enc.4.1.conv.4.weight': None, 'stageA.model.encoder.enc.4.1.conv.4.bias': None, 'stageA.model.decoder.dec.0.0.up.1.weight': None, 'stageA.model.decoder.dec.0.0.up.1.bias': None, 'stageA.model.decoder.dec.0.0.up.2.weight': None, 'stageA.model.decoder.dec.0.0.up.2.bias': None, 'stageA.model.decoder.dec.0.1.conv.0.weight': None, 'stageA.model.decoder.dec.0.1.conv.0.bias': None, 'stageA.model.decoder.dec.0.1.conv.1.weight': None, 'stageA.model.decoder.dec.0.1.conv.1.bias': None, 'stageA.model.decoder.dec.0.1.conv.3.weight': None, 'stageA.model.decoder.dec.0.1.conv.3.bias': None, 'stageA.model.decoder.dec.0.1.conv.4.weight': None, 'stageA.model.decoder.dec.0.1.conv.4.bias': None, 'stageA.model.decoder.dec.1.0.up.1.weight': None, 'stageA.model.decoder.dec.1.0.up.1.bias': None, 'stageA.model.decoder.dec.1.0.up.2.weight': None, 'stageA.model.decoder.dec.1.0.up.2.bias': None, 'stageA.model.decoder.dec.1.1.conv.0.weight': None, 'stageA.model.decoder.dec.1.1.conv.0.bias': None, 'stageA.model.decoder.dec.1.1.conv.1.weight': None, 'stageA.model.decoder.dec.1.1.conv.1.bias': None, 'stageA.model.decoder.dec.1.1.conv.3.weight': None, 'stageA.model.decoder.dec.1.1.conv.3.bias': None, 'stageA.model.decoder.dec.1.1.conv.4.weight': None, 'stageA.model.decoder.dec.1.1.conv.4.bias': None, 'stageA.model.decoder.dec.2.0.up.1.weight': None, 'stageA.model.decoder.dec.2.0.up.1.bias': None, 'stageA.model.decoder.dec.2.0.up.2.weight': None, 'stageA.model.decoder.dec.2.0.up.2.bias': None, 'stageA.model.decoder.dec.2.1.conv.0.weight': None, 'stageA.model.decoder.dec.2.1.conv.0.bias': None, 'stageA.model.decoder.dec.2.1.conv.1.weight': None, 'stageA.model.decoder.dec.2.1.conv.1.bias': None, 'stageA.model.decoder.dec.2.1.conv.3.weight': None, 'stageA.model.decoder.dec.2.1.conv.3.bias': None, 'stageA.model.decoder.dec.2.1.conv.4.weight': None, 'stageA.model.decoder.dec.2.1.conv.4.bias': None, 'stageA.model.decoder.dec.3.0.up.1.weight': None, 'stageA.model.decoder.dec.3.0.up.1.bias': None, 'stageA.model.decoder.dec.3.0.up.2.weight': None, 'stageA.model.decoder.dec.3.0.up.2.bias': None, 'stageA.model.decoder.dec.3.1.conv.0.weight': None, 'stageA.model.decoder.dec.3.1.conv.0.bias': None, 'stageA.model.decoder.dec.3.1.conv.1.weight': None, 'stageA.model.decoder.dec.3.1.conv.1.bias': None, 'stageA.model.decoder.dec.3.1.conv.3.weight': None, 'stageA.model.decoder.dec.3.1.conv.3.bias': None, 'stageA.model.decoder.dec.3.1.conv.4.weight': None, 'stageA.model.decoder.dec.3.1.conv.4.bias': None, 'stageA.model.readout.weight': None, 'stageA.model.readout.bias': None, 'stageA.model.seq2map.tok_embedding.weight': None, 'stageA.model.seq2map.pos_embedding.weight': None, 'stageA.model.seq2map.layer.norm.weight': None, 'stageA.model.seq2map.layer.norm.bias': None, 'stageA.model.seq2map.layer.to_qk.0.weight': None, 'stageA.model.seq2map.layer.to_qk.0.bias': None, 'stageA.model.seq2map.layer.offsetscale.gamma': None, 'stageA.model.seq2map.layer.offsetscale.beta': None, 'stageB_torsion.model.dnabert.embeddings.word_embeddings.weight': 1.1561985147068299e-08, 'stageB_torsion.model.dnabert.embeddings.position_embeddings.weight': 1.1561984258889879e-08, 'stageB_torsion.model.dnabert.embeddings.token_type_embeddings.weight': 1.606470512172109e-08, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.weight': 3.9668499596245965e-09, 'stageB_torsion.model.dnabert.embeddings.LayerNorm.bias': 3.757578248553273e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.weight': 3.7081901993474276e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.query.bias': 4.562870081770143e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.weight': 2.983434388426076e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.key.bias': 1.9071282973631514e-16, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.weight': 1.3220259553747837e-08, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.self.value.bias': 2.2006008126851384e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.weight': 9.572689307901783e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.dense.bias': 8.738829659904468e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 1.039753838583124e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 8.863250688939672e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.weight': 4.183136503854712e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.intermediate.dense.bias': 1.4787014668282694e-10, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.weight': 5.153439897043199e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.dense.bias': 5.699425653649115e-11, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.weight': 2.1614954270887665e-09, 'stageB_torsion.model.dnabert.encoder.layer.0.output.LayerNorm.bias': 1.52729018321196e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.weight': 2.7444608807769555e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.query.bias': 1.5753254256622995e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.weight': 5.177811956968981e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.key.bias': 2.8040766315013516e-16, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.weight': 2.3435593021758905e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.self.value.bias': 1.3365711870605423e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.weight': 1.015594186526414e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.dense.bias': 5.641230815811582e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 1.4880811027850882e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 9.269622291974144e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.weight': 6.614859771048032e-09, 'stageB_torsion.model.dnabert.encoder.layer.1.intermediate.dense.bias': 2.4214147353873727e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.weight': 1.2380266589673283e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.dense.bias': 1.4310323759314514e-10, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.weight': 1.7834288712492707e-08, 'stageB_torsion.model.dnabert.encoder.layer.1.output.LayerNorm.bias': 9.2234397897073e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.weight': 6.66533564452898e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.query.bias': 5.246701077071814e-12, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.weight': 4.8126141960480595e-11, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.key.bias': 8.870886721088132e-17, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.weight': 8.228635550722174e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.self.value.bias': 6.480942005282486e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.weight': 1.6272904801439836e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.dense.bias': 4.130930708612368e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 6.5055063558361326e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 2.0620869456422497e-09, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.weight': 1.3523642650170586e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.intermediate.dense.bias': 3.4512251390061977e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.weight': 1.2107591373933246e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.dense.bias': 3.310177687509963e-10, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.weight': 2.4394163133933944e-08, 'stageB_torsion.model.dnabert.encoder.layer.2.output.LayerNorm.bias': 6.4864469351277876e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.weight': 6.741588537417798e-11, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.query.bias': 5.5586169347976444e-12, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.weight': 1.6486063902920733e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.key.bias': 2.9033195114641977e-16, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.weight': 5.6079915822238036e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.self.value.bias': 4.624135296893428e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.weight': 3.312401375410445e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.dense.bias': 4.5553805172460216e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 8.698271436458072e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 3.4189209241475282e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.weight': 4.5502179801815146e-08, 'stageB_torsion.model.dnabert.encoder.layer.3.intermediate.dense.bias': 1.511145542032466e-09, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.weight': 1.0226819568970313e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.dense.bias': 5.131407521119513e-10, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.weight': 1.482369782479509e-07, 'stageB_torsion.model.dnabert.encoder.layer.3.output.LayerNorm.bias': 7.597370910161771e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.weight': 2.592250719635203e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.query.bias': 1.2785762032452297e-12, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.weight': 1.3190994303802217e-11, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.key.bias': 1.2730452034605353e-14, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.weight': 1.1106898227808415e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.self.value.bias': 5.478484865761857e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.weight': 4.542567353382765e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.dense.bias': 3.42875807746168e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 1.7782144823286217e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 6.248865958013994e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.weight': 6.196773938427214e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.intermediate.dense.bias': 2.5531885938789856e-08, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.weight': 1.5508875321756932e-06, 'stageB_torsion.model.dnabert.encoder.layer.4.output.dense.bias': 7.491957276783978e-09, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.weight': 9.58324108069064e-07, 'stageB_torsion.model.dnabert.encoder.layer.4.output.LayerNorm.bias': 1.038375103235012e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.weight': 1.6456440160225738e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.query.bias': 1.0288712698654223e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.weight': 1.813696653800434e-12, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.key.bias': 1.0708046184774653e-13, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.weight': 8.063222594500985e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.self.value.bias': 5.041221697865694e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.weight': 1.2904197319585364e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.dense.bias': 3.6941420944458514e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 1.7435894505979377e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 1.862051362877537e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.weight': 1.8360808098805137e-05, 'stageB_torsion.model.dnabert.encoder.layer.5.intermediate.dense.bias': 9.470754775975365e-07, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.weight': 9.905997103487607e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.dense.bias': 1.0925606375167263e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.weight': 2.127519792338717e-06, 'stageB_torsion.model.dnabert.encoder.layer.5.output.LayerNorm.bias': 2.4459573069179896e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.weight': 1.320461643673776e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.query.bias': 1.0165113650990876e-13, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.weight': 1.0682124672659055e-12, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.key.bias': 8.196844895544397e-14, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.weight': 1.4021071365277749e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.self.value.bias': 1.0793474984893692e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.weight': 1.7862554159364663e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.dense.bias': 5.081263338979625e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 1.8895030962085002e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 1.8487245370124583e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.weight': 1.6391401004511863e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.intermediate.dense.bias': 7.351264912358602e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.weight': 1.4725964319950435e-05, 'stageB_torsion.model.dnabert.encoder.layer.6.output.dense.bias': 5.614562610389839e-07, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.weight': 2.8734477837133454e-06, 'stageB_torsion.model.dnabert.encoder.layer.6.output.LayerNorm.bias': 2.8070098778698593e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.weight': 2.110092309215439e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.query.bias': 1.1451000737905392e-12, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.weight': 1.677158238677645e-11, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.key.bias': 9.101744965350766e-13, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.weight': 2.4652159481775016e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.self.value.bias': 1.3377955383475637e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.weight': 4.806906872545369e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.dense.bias': 7.285382253030548e-07, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 6.656197911070194e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 7.2348243520536926e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.weight': 8.363451343029737e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.intermediate.dense.bias': 3.188023356415215e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.weight': 0.0001145035566878505, 'stageB_torsion.model.dnabert.encoder.layer.7.output.dense.bias': 3.907638983946526e-06, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.weight': 1.3773875252809376e-05, 'stageB_torsion.model.dnabert.encoder.layer.7.output.LayerNorm.bias': 1.368307857774198e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.weight': 1.2814881968081782e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.query.bias': 7.512945886072042e-13, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.weight': 1.2519566112978442e-11, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.key.bias': 7.339808014844573e-13, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.weight': 0.0001135094280471094, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.self.value.bias': 6.654676781181479e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.weight': 0.00011301416088826954, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.dense.bias': 3.3522842386446428e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 1.764328953868244e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 1.8092119717039168e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.weight': 0.00015172270650509745, 'stageB_torsion.model.dnabert.encoder.layer.8.intermediate.dense.bias': 7.995668966032099e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.weight': 0.00012622648500837386, 'stageB_torsion.model.dnabert.encoder.layer.8.output.dense.bias': 7.286125310201896e-06, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.weight': 1.6920781490625814e-05, 'stageB_torsion.model.dnabert.encoder.layer.8.output.LayerNorm.bias': 1.627749952604063e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.weight': 1.770108365439782e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.query.bias': 9.88631810965035e-13, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.weight': 1.3438687662681303e-11, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.key.bias': 7.505681731516389e-13, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.weight': 0.00015905193868093193, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.self.value.bias': 8.883187547326088e-06, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.weight': 0.0001918420020956546, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.dense.bias': 6.234166903595906e-06, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 2.7664733352139592e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 2.7425241569289938e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.weight': 0.0001918372727232054, 'stageB_torsion.model.dnabert.encoder.layer.9.intermediate.dense.bias': 1.0990307600877713e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.weight': 0.00011886665015481412, 'stageB_torsion.model.dnabert.encoder.layer.9.output.dense.bias': 1.929943209688645e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.weight': 1.9244644136051647e-05, 'stageB_torsion.model.dnabert.encoder.layer.9.output.LayerNorm.bias': 2.0864270481979474e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.weight': 1.870017682370495e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.query.bias': 1.053903275413659e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.weight': 2.3373837587659096e-11, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.key.bias': 1.3173101932190123e-12, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.weight': 0.00023981774575076997, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.self.value.bias': 1.3515372302208561e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.weight': 0.00031236588256433606, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.dense.bias': 1.0795291927934159e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 5.732913632527925e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 5.552991933654994e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.weight': 0.0004763891629409045, 'stageB_torsion.model.dnabert.encoder.layer.10.intermediate.dense.bias': 2.7043985028285533e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.weight': 0.0005134776583872736, 'stageB_torsion.model.dnabert.encoder.layer.10.output.dense.bias': 3.885415935656056e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.weight': 8.069703471846879e-05, 'stageB_torsion.model.dnabert.encoder.layer.10.output.LayerNorm.bias': 7.974002073751763e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.weight': 2.8253865913541176e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.query.bias': 1.6882720915711857e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.weight': 2.961963729841699e-10, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.key.bias': 1.7698740042981775e-11, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.weight': 0.0008491647895425558, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.self.value.bias': 5.0740000006044284e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.weight': 0.0017429274739697576, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.dense.bias': 3.369997284607962e-05, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 0.00038908058195374906, 'stageB_torsion.model.dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0004001570923719555, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.weight': 0.003399509470909834, 'stageB_torsion.model.dnabert.encoder.layer.11.intermediate.dense.bias': 0.00019743091252166778, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.weight': 0.009555509313941002, 'stageB_torsion.model.dnabert.encoder.layer.11.output.dense.bias': 0.0002678875462152064, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.weight': 0.0008338315528817475, 'stageB_torsion.model.dnabert.encoder.layer.11.output.LayerNorm.bias': 0.0008526864694431424, 'stageB_torsion.model.dnabert.pooler.dense.weight': None, 'stageB_torsion.model.dnabert.pooler.dense.bias': None, 'stageB_torsion.model.regressor.0.weight': 0.0006855510291643441, 'stageB_torsion.model.regressor.0.bias': 0.0007133929175324738, 'stageB_torsion.model.regressor.1.weight': 0.03152432292699814, 'stageB_torsion.model.regressor.1.bias': 0.001140975276939571, 'stageB_torsion.model.regressor.3.weight': 0.169089674949646, 'stageB_torsion.model.regressor.3.bias': 0.0047612665221095085, 'stageB_torsion.output_projection.weight': 0.4363195598125458, 'stageB_torsion.output_projection.bias': 0.08574228733778, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_out.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_z.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_in.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.layer_norm_out.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_a_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_p.bias': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_mul_in.linear_b_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_start.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.layer_norm.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.linear.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_q.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_k.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_v.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_o.bias': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.weight': None, 'stageB_pairformer.stack.blocks.0.tri_att_end.mha.linear_g.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.layernorm1.bias': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_a.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias_b.weight': None, 'stageB_pairformer.stack.blocks.0.pair_transition.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.w': None, 'stageD.diffusion_module.diffusion_conditioning.fourier_embedding.b': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.layernorm_n.bias': None, 'stageD.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight': None, 'stageD.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.3.weight': None, 'stageD.diffusion_module.atom_attention_encoder.small_mlp.5.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight': None, 'stageD.diffusion_module.layernorm_s.weight': None, 'stageD.diffusion_module.layernorm_s.bias': None, 'stageD.diffusion_module.linear_no_bias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias': None, 'stageD.diffusion_module.layernorm_a.weight': None, 'stageD.diffusion_module.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_p.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.layernorm_q.bias': None, 'stageD.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_q.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_k.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_v.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.to_out.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.gating_linear.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_a.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight': None, 'stageD.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias': None, 'latent_merger.mlp.0.weight': None, 'latent_merger.mlp.0.bias': None, 'latent_merger.mlp.2.weight': None, 'latent_merger.mlp.2.bias': None, '_integration_test_dummy.weight': None, '_integration_test_dummy.bias': None}
[2025-05-19 13:09:22,795][rna_predict.training.rna_lightning_module][INFO] - Streamline mode active: Training TorsionBERT + Stage C with L_angle and L_coord_C.
[2025-05-19 13:09:22,795][rna_predict.utils.tensor_utils.types][INFO] - Deriving residue-atom map from sequence and standard atom counts.
[2025-05-19 13:09:22,795][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] sequence_list: ['G', 'G', 'G', 'U', 'G', 'C', 'U', 'C', 'A', 'G', 'U', 'A', 'C', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'C', 'C', 'G', 'C', 'A', 'C', 'C', 'C']
[2025-05-19 13:09:22,795][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] counts_map: {'A': 22, 'U': 20, 'G': 23, 'C': 20}
[2025-05-19 13:09:22,795][rna_predict.utils.tensor_utils.types][INFO] - [DEBUG][StageD] expected_atom_counts: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20], sum=624
[DEBUG][BRIDGE] stage_c_output.dim=2, shape=(624, 3)
[DEBUG][BRIDGE] New stage_c_output.dim=3, shape=(1, 624, 3)
[DEBUG][BRIDGE] first residues maps lengths: [23, 23, 23, 20, 23]
[2025-05-19 13:09:22,798][rna_predict.training.rna_lightning_module][INFO] - Streamlined loss: L_angle=0.0717, L_coord_C=1745.4683, Total=1745.5399
Epoch 9: 100%|| 1/1 [00:24<00:00,  0.04it/s, v_num=27, angle_loss_step=0.0732, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0732, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 9: 100%|| 1/1 [00:24<00:00,  0.04it/s, v_num=27, angle_loss_step=0.0717, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0732, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]Epoch 9: 100%|| 1/1 [00:24<00:00,  0.04it/s, v_num=27, angle_loss_step=0.0717, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0717, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3]`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: 100%|| 1/1 [00:39<00:00,  0.03it/s, v_num=27, angle_loss_step=0.0717, train/coord_loss_C_streamlined_step=1.75e+3, train/total_loss_streamlined_step=1.75e+3, angle_loss_epoch=0.0717, train/coord_loss_C_streamlined_epoch=1.75e+3, train/total_loss_streamlined_epoch=1.75e+3][RNADataset][_load_atom_features] Using OUTER ref_element_size: 128
[RNADataset][_load_atom_features] Using OUTER ref_atom_name_chars_size: 256
[DEBUG][_load_atom_features] coords[0] device after: cpu
[DEBUG][_load_atom_features] coords[1] device after: cpu
[RNADataset][_load_atom_features] coords tensor summary: device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][DEBUG-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][DEBUG-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][DEBUG-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[DEBUG][RNADataset._load_angles] Using backend: dssr
[DEBUG][RNADataset._load_angles] No chain_id in row or config; defaulting to 'A'
[DEBUG][RNADataset._load_angles] Available chain IDs in /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb: ['B']
[DEBUG] extract_rna_torsions called with: structure_file=/Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, backend=dssr, angle_set=canonical
[DEBUG] DSSR JSON keys: ['num_pairs', 'pairs', 'num_multiplets', 'multiplets', 'num_helices', 'helices', 'num_stems', 'stems', 'num_isoCanonPairs', 'isoCanonPairs', 'num_coaxStacks', 'coaxStacks', 'num_stacks', 'stacks', 'nonStack', 'num_atom2bases', 'atom2bases', 'num_hairpins', 'hairpins', 'num_bulges', 'bulges', 'num_iloops', 'iloops', 'num_junctions', 'junctions', 'num_ssSegments', 'ssSegments', 'num_Aminors', 'Aminors', 'num_splayUnits', 'splayUnits', 'dbn', 'chains', 'num_nts', 'nts', 'num_hbonds', 'hbonds', 'refCoords', 'metadata']
[DEBUG] DSSR returned None; falling back to MDAnalysis for /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, angle_set=canonical
[DEBUG] _extract_rna_torsions_mdanalysis called with: /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb, chain_id=A, angle_set=canonical
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=A
[DEBUG] _select_chain: No atoms found for chain_id=A
[INFO] Requested chain_id=A not found, but only one chain (B) present. Using it.
[DEBUG] _select_chain: available chainIDs={'B'}, segids={'B'}, requested=B
[DEBUG] _select_chain: Found 5129 atoms for chain_id=B
[DEBUG] Number of residues in chain: 240
[DEBUG] torsion_data shape: (240, 7) (should be [L,7] or [L,14])
[DEBUG][RNADataset._load_angles] ang device after torch.tensor(..., device=self.device): cpu, type: <class 'torch.Tensor'>
[DEBUG][RNADataset._load_angles] ang device after .contiguous(): cpu, type: <class 'torch.Tensor'>
[WARNING][RNADataset._load_angles] NaNs detected in angle tensor. Replacing NaNs with zeros.
[DEBUG][RNADataset._load_angles] ang device before cat: cpu, pad device before cat: cpu
[DEBUG][RNADataset._load_angles] ang device after direct cat: cpu
[RNADataset][DEBUG] angles_true shape: torch.Size([512, 7]), device: cpu
[RNADataset][FINAL-DEVICE] key 'coords_true': device=cpu, shape=torch.Size([4096, 3]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'attention_mask': device=cpu, shape=torch.Size([512]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_mask': device=cpu, shape=torch.Size([4096]), dtype=torch.bool
[RNADataset][FINAL-DEVICE] key 'atom_to_token_idx': device=cpu, shape=torch.Size([4096]), dtype=torch.int64
[RNADataset][FINAL-DEVICE] key 'ref_element': device=cpu, shape=torch.Size([4096, 128]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'ref_atom_name_chars': device=cpu, shape=torch.Size([4096, 256]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'adjacency': device=cpu, shape=torch.Size([512, 512]), dtype=torch.float32
[RNADataset][FINAL-DEVICE] key 'angles_true': device=cpu, shape=torch.Size([512, 7]), dtype=torch.float32

[2025-05-19 13:09:45,085][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] After training contents of /Users/tomriddle1/RNA_PREDICT/outputs/checkpoints: ['last.ckpt', 'epoch=9-step=10.ckpt', 'epoch=9-step=10-v1.ckpt', 'last-v1.ckpt']
[2025-05-19 13:09:45,085][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] Training completed. Checkpoints should be saved to: /Users/tomriddle1/RNA_PREDICT/outputs/checkpoints
[2025-05-19 13:09:45,085][rna_predict.training.train][INFO] - [CHECKPOINT-DEBUG] End of main training block reached.
