{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RNA Predict Documentation","text":"<p>This directory contains comprehensive documentation for the RNA Predict project.</p>"},{"location":"#directory-structure","title":"Directory Structure","text":""},{"location":"#components","title":"<code>/components</code>","text":"<p>Documentation for individual components of the pipeline: - <code>/stageA</code> - 2D Adjacency prediction - <code>/stageB</code> - Torsion and pairwise embeddings - <code>/stageC</code> - 3D Reconstruction - <code>/stageD</code> - Diffusion refinement - <code>/unified_latent</code> - Latent merger components</p>"},{"location":"#integration","title":"<code>/integration</code>","text":"<p>Documentation for integrating components: - <code>/hydra</code> - Hydra configuration and integration - <code>/pipeline</code> - Full pipeline orchestration - <code>/testing</code> - Integration testing</p>"},{"location":"#reference","title":"<code>/reference</code>","text":"<p>Reference documentation: - <code>/api</code> - API documentation - <code>/architecture</code> - Architecture diagrams and explanations - <code>/methods</code> - Detailed explanations of methods used</p>"},{"location":"#guides","title":"<code>/guides</code>","text":"<p>User and developer guides: - <code>/getting_started</code> - Getting started guides - <code>/best_practices</code> - Best practices for development - <code>/debugging</code> - Debugging guides</p>"},{"location":"#advanced","title":"<code>/advanced</code>","text":"<p>Advanced topics: - <code>/perceiver_io</code> - Perceiver IO implementation details - <code>/energy_minimization</code> - Energy minimization and MD - <code>/optimization</code> - Performance optimization</p>"},{"location":"#documentation-standards","title":"Documentation Standards","text":"<p>All documentation should follow these standards: 1. Use Markdown format 2. Include a clear title and purpose 3. Provide code examples where appropriate 4. Include references to related documentation 5. Keep files focused on a single topic 6. Use consistent terminology across documents</p>"},{"location":"#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>When adding new documentation: 1. Place it in the appropriate subdirectory 2. Update this README if adding new directories 3. Link to it from related documentation 4. Follow the documentation standards </p>"},{"location":"streamline_plan/","title":"Streamline plan","text":"<p>Okay, that's a significant confidence boost for the \"3DXSSR\" component! Knowing it's a quick integration (&lt;= 1 day) makes the Streamlined Plan even more attractive and frees up potential time.</p> <p>Let's re-evaluate with this new information.</p> <p>Current Situation:</p> <ul> <li>Today: May 12th.</li> <li>Kaggle Deadline: May 29th.</li> <li>Available working days (approx.): 12-13 days.</li> </ul> <p>Revised \"Streamlined Plan\" Timeline (incorporating 1-day 3DXSSR):</p> <ol> <li> <p>Verify and Stabilize TorsionBERT + Stage C Training with <code>L_angle</code>:</p> <ul> <li>Implement direct angle loss (<code>L_angle</code>) for TorsionBERT.</li> <li>Ensure stable training and sensible angle predictions from TorsionBERT.</li> <li>Ensure Stage C correctly reconstructs from these angles.</li> <li>Estimated Time: 2-3 working days. (This is crucial and was part of original Phase 1).</li> </ul> </li> <li> <p>Implement/Integrate \"3DXSSR\" Refinement:</p> <ul> <li>Per your input.</li> <li>Estimated Time: 1 working day.</li> </ul> </li> <li> <p>End-to-End Pipeline for Submission &amp; Initial Leaderboard Check:</p> <ul> <li>Stitch: TorsionBERT (<code>L_angle</code> trained) -&gt; Stage C -&gt; \"3DXSSR\".</li> <li>Implement logic for 5 distinct predictions (e.g., TorsionBERT seeds).</li> <li>Format output for Kaggle.</li> <li>Make an initial submission to get a TM-score baseline.</li> <li>Estimated Time: 2-3 working days.</li> </ul> </li> </ol> <p>Total for this Core Streamlined Plan: 2 (Torsion) + 1 (3DXSSR) + 2 (Pipeline) = 5 working days (optimistic) to 3 + 1 + 3 = 7 working days (pessimistic).</p> <p>This leaves 5-8 working days (12/13 available - 7/5 used) before the May 29th deadline.</p> <p>What Can We Do With the Remaining 5-8 Working Days?</p> <p>This is a good amount of buffer! We can now consider re-introducing the most impactful and feasible elements from the original Phase 2 and 3 to boost performance beyond the basic TorsionBERT + Stage C + 3DXSSR.</p> <p>Prioritizing for Performance Boost within 5-8 Days:</p> <p>Based on your \"Phase 2 &amp; 3 Components: Mission-Critical vs. Optional\" analysis:</p> <ol> <li> <p>Pairformer Global Context (Mission-Critical for High Performance):</p> <ul> <li>Goal: Get Pairformer trained and providing global context. As your analysis stated, the current training loop wasn't training it.</li> <li>Challenge: How to train Pairformer if we are not implementing the full Stage D diffusion loss, which is how AlphaFold trains its trunk?</li> <li>\"Pairformer Lite\" Approach:<ul> <li>Run Pairformer in the forward pass to get <code>s_embeddings</code> and <code>z_embeddings</code>.</li> <li>Use the <code>UnifiedLatentMerger</code> to combine these (and potentially TorsionBERT's direct angle outputs if beneficial).</li> <li>Crucial Change: This <code>unified_latent</code> (or <code>s_embeddings</code> / <code>z_embeddings</code> directly if merger is too complex for now) needs to feed into a part of the pipeline that contributes to a trainable loss.<ul> <li>Option A (Simpler): Modify TorsionBERT to accept these Pairformer-derived embeddings as additional conditioning inputs to its transformer layers. The existing <code>L_angle</code> (and indirect coordinate loss via Stage C) would then train Pairformer. This requires changes to TorsionBERT's architecture.</li> <li>Option B (Potentially Cleaner for Loss): Add a small, trainable refinement head (e.g., a few MLP layers or a very shallow transformer) after Stage C. This head would take Stage C's coordinates AND the Pairformer-derived embeddings (or <code>unified_latent</code>) as input and predict coordinate corrections or refined coordinates. The final MSE loss would be on these refined coordinates, thus training Pairformer.</li> </ul> </li> </ul> </li> <li>Complexity: Medium to Large. Involves architectural changes and ensuring gradient flow.</li> <li>Estimated Time for \"Pairformer Lite\" (Option A or B): 3-5 working days.</li> </ul> </li> <li> <p>Multi-Seed Ensemble Predictions (Strategic for Competitiveness - from Phase 3):</p> <ul> <li>Goal: Generate 5 distinct predictions.</li> <li>Implementation: Relatively straightforward scripting around your main prediction pipeline (run TorsionBERT with different random seeds, or if using a diffusion-like sampler in \"3DXSSR\", different noise seeds).</li> <li>Complexity: Small.</li> <li>Estimated Time: 1 working day (can be parallelized or done towards the end).</li> </ul> </li> </ol> <p>Proposed \"Streamlined Plus\" Plan &amp; Timeline:</p> <p>(Total Available: ~12-13 working days)</p> <ul> <li> <p>Part 1: Core Submittable Pipeline (5-7 working days)</p> <ol> <li>TorsionBERT + <code>L_angle</code> + Stage C Stabilization: (2-3 days)<ul> <li>Focus: Stable training, sensible angle predictions.</li> </ul> </li> <li>\"3DXSSR\" Integration: (1 day)<ul> <li>Focus: Functional refinement step.</li> </ul> </li> <li>Basic Submission Pipeline &amp; Initial Leaderboard Test: (2-3 days)<ul> <li>Focus: Get a score on the board, ensure 5 predictions are generated (even if identical initially or trivially varied).</li> </ul> </li> </ol> <p>Completion of Part 1 by: May 17th - May 21st. Remaining Time: 5-8 working days.</p> </li> <li> <p>Part 2: Performance Boost - \"Pairformer Lite\" (3-5 working days, if Part 1 successful)</p> <ol> <li>Integrate Pairformer Training:<ul> <li>Choose Option A or B for making Pairformer outputs influence a trainable loss.</li> <li>Implement changes, ensure forward pass works.</li> <li>Verify gradients flow to Pairformer LoRA adapters.</li> <li>Retrain with TorsionBERT + Pairformer active.</li> </ul> </li> <li>Update Submission Pipeline with Trained Pairformer.</li> <li>New Leaderboard Submission.</li> </ol> <p>If Part 2 pursued, completion by: May 22nd - May 27th. Remaining Time: 0-3 working days.</p> </li> <li> <p>Part 3: Final Polish &amp; Submissions (Use any remaining time from buffer, or the 0-3 days after Part 2)</p> <ol> <li>Refine Multi-Seed Ensembling: Ensure genuinely diverse predictions.</li> <li>Hyperparameter Tuning (via Hydra): Focus on learning rates, LoRA ranks, loss weights for <code>L_angle</code> and the coordinate loss (which now trains Pairformer too). This is where Hydra's \"free\" tuning comes in \u2013 easy to launch, but runs still take time.</li> <li>Iterative Leaderboard Submissions: Analyze results, make final tweaks.</li> <li>(Optional, if very quick): Simple external relaxation (e.g., very short OpenMM minimization if PDBs have bad geometry from \"3DXSSR\").</li> </ol> </li> </ul> <p>Can we make a submission and have time to hyperparameter tune?</p> <ul> <li>With the \"Streamlined Plus\" Plan: Yes.<ul> <li>You'd have a baseline submission after Part 1 (5-7 working days).</li> <li>You'd have an improved submission after Part 2 (another 3-5 working days).</li> <li>This leaves 0-3 full working days plus any time saved from optimistic estimates dedicated to hyperparameter tuning runs launched via Hydra and iterating on Kaggle submissions. This is tight but feasible for some tuning.</li> </ul> </li> </ul> <p>Timeline for \"Streamlined Plus\" Plan:</p> <ul> <li>Total estimated effort: 5-7 (Part 1) + 3-5 (Part 2) = 8-12 working days.</li> <li>This fits within the 12-13 available working days, leaving a small buffer or dedicated time for tuning/iteration at the very end.</li> </ul> <p>Key Success Factors:</p> <ul> <li>Rapid execution of the 1-day \"3DXSSR\".</li> <li>Decisive choice and clean implementation for \"Pairformer Lite\" (Option A or B).</li> <li>Efficient debugging of gradient flow for Pairformer.</li> <li>Parallelizing Hydra runs for hyperparameter tuning during the final days.</li> </ul> <p>This \"Streamlined Plus\" plan is aggressive but balances the need for a timely submission with the desire to incorporate more performance-critical components (Pairformer) than the absolute bare-bones approach. It leverages your confidence in 3DXSSR to buy time for these other improvements.</p>"},{"location":"examples/compare_precomputed_torsions/","title":"!/usr/bin/env python","text":"<p>\"\"\" compare_precomputed_torsions.py</p> <p>Example script to:   1) Read a CSV/TSV containing columns: alpha, beta, gamma, delta, epsilon, zeta, chi   2) Parse an mmCIF structure with MDAnalysis   3) Recompute those torsions from the .cif file (using the same definitions      as custom_torsion_example.py, for instance)   4) Compare the two sets of angles and print the differences</p> <p>Usage:   python compare_precomputed_torsions.py my_torsions.csv my_structure.cif [chainID]</p> <p>Requires: pandas (for CSV) and MDAnalysis \"\"\"</p> <p>import sys</p> <p>import numpy as np import pandas as pd from mdanalysis_torsion_example import (     calculate_rna_torsions_mdanalysis, )  # or custom_torsion_example</p> <p>def compare_torsions(csv_file, cif_file, chain_id=\"A\"):     \"\"\"     Compare precomputed torsion angles in csv_file with newly computed angles from cif_file.</p> <pre><code>CSV columns expected: 'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'chi'\n(Plus possibly other columns.)\n\nThe CIF is read by MDAnalysis, then we call calculate_rna_torsions_mdanalysis\nto get angles as a dictionary of lists in residue order.\n\nWe'll print out differences for each angle name.\n\"\"\"\n# 1) Read the data\n# The user's file \u201c1a9n_1_Q\u201d is basically a CSV with a header line containing column names\n# \"index_chain,old_nt_resnum,nt_position,nt_name,nt_code,nt_align_code,...,alpha,beta,gamma,delta,epsilon,zeta,chi...\"\n# We can specify them or let pandas read it directly.\n#\n# We'll skip any leading lines that don't start with a digit or \"index_chain\".\n# We'll read them as CSV with comma as separator.\n\n# Option 1: Attempt to read automatically if the file has the header line.\n# If there's any weird lines, let's skip them:\nimport re\n\n# We'll gather lines that have a consistent count of commas\n# and do a quick guess to skip lines with '====' or directory references.\nlines = []\nwith open(csv_file, \"r\") as f_in:\n    for line in f_in:\n        # skip blank lines or lines that have '===='\n        if not line.strip() or \"====\" in line:\n            continue\n        # if the line starts with \"index_chain\" or a digit, assume it's data\n        if line.startswith(\"index_chain\") or re.match(r\"^\\d\", line.strip()):\n            lines.append(line)\n\n# Now parse them with pandas\nfrom io import StringIO\n\ncontent_str = \"\".join(lines)\ndf = pd.read_csv(StringIO(content_str), sep=\",\")\n# We now have a DataFrame with a bunch of columns, including alpha,beta,gamma...\n# Convert them to numpy arrays for easy subtraction\nalpha_csv = df[\"alpha\"].to_numpy(dtype=float)\nbeta_csv = df[\"beta\"].to_numpy(dtype=float)\ngamma_csv = df[\"gamma\"].to_numpy(dtype=float)\ndelta_csv = df[\"delta\"].to_numpy(dtype=float)\nepsilon_csv = df[\"epsilon\"].to_numpy(dtype=float)\nzeta_csv = df[\"zeta\"].to_numpy(dtype=float)\nchi_csv = df[\"chi\"].to_numpy(dtype=float)\n\n# 2) Compute angles from the CIF\n# We'll rely on the fallback approach to handle chain selection\nangles_mdanalysis = calculate_rna_torsions_mdanalysis(\n    cif_file, chain_id, fallback=True, csv_file=csv_file\n)\n# angles_mdanalysis is a dict of lists: angles_mdanalysis[\"alpha\"] -&gt; list of floats\nalpha_new = np.array(angles_mdanalysis[\"alpha\"], dtype=float)\nbeta_new = np.array(angles_mdanalysis[\"beta\"], dtype=float)\ngamma_new = np.array(angles_mdanalysis[\"gamma\"], dtype=float)\ndelta_new = np.array(angles_mdanalysis[\"delta\"], dtype=float)\nepsilon_new = np.array(angles_mdanalysis[\"epsilon\"], dtype=float)\nzeta_new = np.array(angles_mdanalysis[\"zeta\"], dtype=float)\nchi_new = np.array(angles_mdanalysis[\"chi\"], dtype=float)\n\n# For a valid comparison, the residue counts must match.\n# If your CSV has a different residue indexing approach,\n# you may need to align them by residue name or ID. We'll do naive same-length compare:\nn_csv = len(alpha_csv)\nn_new = len(alpha_new)\nn_min = min(n_csv, n_new)\nif n_csv != n_new:\n    print(\n        f\"Warning: CSV has {n_csv} residues, CIF gave {n_new} angles. Truncating to {n_min} for comparison.\"\n    )\n\n# 3) Differences\ndef print_stats(name, arr_csv, arr_new):\n    # Subset to n_min\n    arr_diff = arr_new[:n_min] - arr_csv[:n_min]\n    mean_diff = np.nanmean(arr_diff)\n    std_diff = np.nanstd(arr_diff)\n    print(\n        f\"{name}: mean diff={mean_diff:.2f} deg, std={std_diff:.2f}, range=({np.nanmin(arr_diff):.2f},{np.nanmax(arr_diff):.2f})\"\n    )\n\nprint_stats(\"alpha\", alpha_csv, alpha_new)\nprint_stats(\"beta\", beta_csv, beta_new)\nprint_stats(\"gamma\", gamma_csv, gamma_new)\nprint_stats(\"delta\", delta_csv, delta_new)\nprint_stats(\"epsilon\", epsilon_csv, epsilon_new)\nprint_stats(\"zeta\", zeta_csv, zeta_new)\nprint_stats(\"chi\", chi_csv, chi_new)\n</code></pre> <p>def main():     if len(sys.argv) &lt; 3:         print(             \"Usage: python compare_precomputed_torsions.py   [chainID]\"         )         sys.exit(1) <pre><code>csv_file = sys.argv[1]\ncif_file = sys.argv[2]\nchain_id = sys.argv[3] if len(sys.argv) &gt;= 4 else \"A\"\n\ncompare_torsions(csv_file, cif_file, chain_id=chain_id)\n</code></pre> <p>if name == \"main\":     main()</p>"},{"location":"examples/custom_torsion_example/","title":"!/usr/bin/env python","text":"<p>\"\"\" mdanalysis_torsion_example.py</p> <p>Demonstrates how to calculate torsion angles (e.g. \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7) in an RNA structure using MDAnalysis. \"\"\"</p> <p>import sys</p> <p>import MDAnalysis as mda import numpy as np from MDAnalysis.lib.distances import calc_dihedrals</p> <p>def compute_dihedral(atom_selection):     \"\"\"     Helper function to compute a single dihedral using MDAnalysis's calc_dihedrals.     Expects a list of 4 MDAnalysis atom objects.     Returns the angle in degrees or None if any atom is missing.     \"\"\"     # Explicitly check each item for None     if len(atom_selection) &lt; 4:         return None     for atom in atom_selection:         if atom is None:             return None</p> <pre><code># Extract each atom's position as shape (1,3)\np1 = atom_selection[0].position.reshape(1, 3)\np2 = atom_selection[1].position.reshape(1, 3)\np3 = atom_selection[2].position.reshape(1, 3)\np4 = atom_selection[3].position.reshape(1, 3)\n\n# Pass four separate arrays to calc_dihedrals\nradians = calc_dihedrals(p1, p2, p3, p4)[0]\nreturn np.degrees(radians)\n</code></pre> <p>def calculate_rna_torsions_mdanalysis(pdb_file, chain_id=\"A\", fallback=False):     \"\"\"     Calculates backbone (alpha, beta, gamma, delta, epsilon, zeta)     and glycosidic (chi) torsion angles for the specified RNA chain.     Args:         pdb_file (str): Path to the PDB/mmCIF file containing the RNA structure.         chain_id (str): Which chain to analyze, default 'A'.         fallback (bool): Whether to fallback to selecting all nucleic if no chain is found.     Returns:         dict: { 'alpha': [...], 'beta': [...], 'gamma': [...],                 'delta': [...], 'epsilon': [...], 'zeta': [...], 'chi': [...] }               Each entry is a list of angles (or np.nan) in residue order.     \"\"\"     u = mda.Universe(pdb_file)     print(\"Segments:\", u.segments)     print(\"Residues:\", u.residues)     # Unify chain selection     rna_chain = u.select_atoms(f\"(segid {chain_id}) or (chainID {chain_id})\")     if len(rna_chain) == 0:         if fallback:             print(                 f\"No atoms found with segid/chainID={chain_id}, falling back to all nucleic.\"             )             rna_chain = u.select_atoms(\"nucleic\")         else:             raise ValueError(                 f\"No atoms found for chain/segment='{chain_id}'. Check PDB labeling.\"             )</p> <pre><code>torsion_data = {\n    \"alpha\": [],\n    \"beta\": [],\n    \"gamma\": [],\n    \"delta\": [],\n    \"epsilon\": [],\n    \"zeta\": [],\n    \"chi\": [],\n}\n\nresidues = rna_chain.residues\nn_res = len(residues)\nfor i, res in enumerate(residues):\n    prev_res = residues[i - 1] if i &gt; 0 else None\n    next_res = residues[i + 1] if i &lt; n_res - 1 else None\n\n    # \u03b1: O3'(i-1) - P(i) - O5'(i) - C5'(i)\n    if prev_res:\n        alpha_val = compute_dihedral(\n            [\n                prev_res.atoms.select_atoms(\"name O3'\")[0]\n                if prev_res.atoms.select_atoms(\"name O3'\")\n                else None,\n                res.atoms.select_atoms(\"name P\")[0]\n                if res.atoms.select_atoms(\"name P\")\n                else None,\n                res.atoms.select_atoms(\"name O5'\")[0]\n                if res.atoms.select_atoms(\"name O5'\")\n                else None,\n                res.atoms.select_atoms(\"name C5'\")[0]\n                if res.atoms.select_atoms(\"name C5'\")\n                else None,\n            ]\n        )\n    else:\n        alpha_val = None\n    if alpha_val is None:\n        alpha_val = np.nan\n    else:\n        alpha_val = np.float64(alpha_val)\n    torsion_data[\"alpha\"].append(alpha_val)\n\n    # \u03b2: P(i) - O5'(i) - C5'(i) - C4'(i)\n    beta_val = compute_dihedral(\n        [\n            res.atoms.select_atoms(\"name P\")[0]\n            if res.atoms.select_atoms(\"name P\")\n            else None,\n            res.atoms.select_atoms(\"name O5'\")[0]\n            if res.atoms.select_atoms(\"name O5'\")\n            else None,\n            res.atoms.select_atoms(\"name C5'\")[0]\n            if res.atoms.select_atoms(\"name C5'\")\n            else None,\n            res.atoms.select_atoms(\"name C4'\")[0]\n            if res.atoms.select_atoms(\"name C4'\")\n            else None,\n        ]\n    )\n    if beta_val is None:\n        beta_val = np.nan\n    else:\n        beta_val = np.float64(beta_val)\n    torsion_data[\"beta\"].append(beta_val)\n\n    # \u03b3: O5'(i) - C5'(i) - C4'(i) - C3'(i)\n    gamma_val = compute_dihedral(\n        [\n            res.atoms.select_atoms(\"name O5'\")[0]\n            if res.atoms.select_atoms(\"name O5'\")\n            else None,\n            res.atoms.select_atoms(\"name C5'\")[0]\n            if res.atoms.select_atoms(\"name C5'\")\n            else None,\n            res.atoms.select_atoms(\"name C4'\")[0]\n            if res.atoms.select_atoms(\"name C4'\")\n            else None,\n            res.atoms.select_atoms(\"name C3'\")[0]\n            if res.atoms.select_atoms(\"name C3'\")\n            else None,\n        ]\n    )\n    if gamma_val is None:\n        gamma_val = np.nan\n    else:\n        gamma_val = np.float64(gamma_val)\n    torsion_data[\"gamma\"].append(gamma_val)\n\n    # \u03b4: C5'(i) - C4'(i) - C3'(i) - O3'(i)\n    delta_val = compute_dihedral(\n        [\n            res.atoms.select_atoms(\"name C5'\")[0]\n            if res.atoms.select_atoms(\"name C5'\")\n            else None,\n            res.atoms.select_atoms(\"name C4'\")[0]\n            if res.atoms.select_atoms(\"name C4'\")\n            else None,\n            res.atoms.select_atoms(\"name C3'\")[0]\n            if res.atoms.select_atoms(\"name C3'\")\n            else None,\n            res.atoms.select_atoms(\"name O3'\")[0]\n            if res.atoms.select_atoms(\"name O3'\")\n            else None,\n        ]\n    )\n    if delta_val is None:\n        delta_val = np.nan\n    else:\n        delta_val = np.float64(delta_val)\n    torsion_data[\"delta\"].append(delta_val)\n\n    # \u03b5: C4'(i) - C3'(i) - O3'(i) - P(i+1)\n    if next_res:\n        epsilon_val = compute_dihedral(\n            [\n                res.atoms.select_atoms(\"name C4'\")[0]\n                if res.atoms.select_atoms(\"name C4'\")\n                else None,\n                res.atoms.select_atoms(\"name C3'\")[0]\n                if res.atoms.select_atoms(\"name C3'\")\n                else None,\n                res.atoms.select_atoms(\"name O3'\")[0]\n                if res.atoms.select_atoms(\"name O3'\")\n                else None,\n                next_res.atoms.select_atoms(\"name P\")[0]\n                if next_res.atoms.select_atoms(\"name P\")\n                else None,\n            ]\n        )\n    else:\n        epsilon_val = None\n    if epsilon_val is None:\n        epsilon_val = np.nan\n    else:\n        epsilon_val = np.float64(epsilon_val)\n    torsion_data[\"epsilon\"].append(epsilon_val)\n\n    # \u03b6: C3'(i) - O3'(i) - P(i+1) - O5'(i+1)\n    if next_res:\n        zeta_val = compute_dihedral(\n            [\n                res.atoms.select_atoms(\"name C3'\")[0]\n                if res.atoms.select_atoms(\"name C3'\")\n                else None,\n                res.atoms.select_atoms(\"name O3'\")[0]\n                if res.atoms.select_atoms(\"name O3'\")\n                else None,\n                next_res.atoms.select_atoms(\"name P\")[0]\n                if next_res.atoms.select_atoms(\"name P\")\n                else None,\n                next_res.atoms.select_atoms(\"name O5'\")[0]\n                if next_res.atoms.select_atoms(\"name O5'\")\n                else None,\n            ]\n        )\n    else:\n        zeta_val = None\n    if zeta_val is None:\n        zeta_val = np.nan\n    else:\n        zeta_val = np.float64(zeta_val)\n    torsion_data[\"zeta\"].append(zeta_val)\n\n    # \u03c7: glycosidic angle\n    # Purines (A,G): O4' - C1' - N9 - C4\n    # Pyrimidines (U,C): O4' - C1' - N1 - C2\n    resname = res.resname.strip().upper()\n    O4 = (\n        res.atoms.select_atoms(\"name O4'\")[0]\n        if res.atoms.select_atoms(\"name O4'\")\n        else None\n    )\n    C1 = (\n        res.atoms.select_atoms(\"name C1'\")[0]\n        if res.atoms.select_atoms(\"name C1'\")\n        else None\n    )\n    if resname.startswith(\"A\") or resname.startswith(\"G\"):\n        N_base = (\n            res.atoms.select_atoms(\"name N9\")[0]\n            if res.atoms.select_atoms(\"name N9\")\n            else None\n        )\n        C_base = (\n            res.atoms.select_atoms(\"name C4\")[0]\n            if res.atoms.select_atoms(\"name C4\")\n            else None\n        )\n    else:\n        N_base = (\n            res.atoms.select_atoms(\"name N1\")[0]\n            if res.atoms.select_atoms(\"name N1\")\n            else None\n        )\n        C_base = (\n            res.atoms.select_atoms(\"name C2\")[0]\n            if res.atoms.select_atoms(\"name C2\")\n            else None\n        )\n\n    chi_val = None\n    if O4 and C1 and N_base and C_base:\n        chi_val = compute_dihedral([O4, C1, N_base, C_base])\n    if chi_val is None:\n        chi_val = np.nan\n    else:\n        chi_val = np.float64(chi_val)\n    torsion_data[\"chi\"].append(chi_val)\n\nreturn torsion_data\n</code></pre> <p>def main():     if len(sys.argv) &lt; 2:         print(             \"Usage: python mdanalysis_torsion_example.py  [chainID]\"         )         sys.exit(1) <pre><code>pdb_path = sys.argv[1]\nchain_id = sys.argv[2] if len(sys.argv) &gt;= 3 else \"A\"\n\nangles = calculate_rna_torsions_mdanalysis(\n    pdb_path, chain_id=chain_id, fallback=True\n)\nfor angle_name, values in angles.items():\n    print(f\"{angle_name}: {values}\")\n</code></pre> <p>if name == \"main\":     main()</p> <pre><code>\"\"\"(\n</code></pre> <p>uv run /Users/tomriddle1/RNA_PREDICT/rna_predict/scripts/custom_torsion_example.py \\          /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb B</p> <p>uv run /Users/tomriddle1/RNA_PREDICT/rna_predict/scripts/mdanalysis_torsion_example.py \\          /Users/tomriddle1/RNA_PREDICT/rna_predict/dataset/examples/synthetic_cppc_0000001.pdb B ) &gt; torsion_results.txt 2&gt;&amp;1     \"\"\"</p>"},{"location":"examples/mdanalysis_torsion_example/","title":"!/usr/bin/env python","text":"<p>\"\"\" mdanalysis_torsion_example.py</p> <p>Implements a method for computing RNA torsion angles using MDAnalysis. Handles both PDB and mmCIF:  - If .pdb, parse directly with MDAnalysis.  - If .cif, convert to a temporary PDB via BioPython, then parse that with MDAnalysis. \"\"\"</p> <p>import os import re import sys import tempfile from io import StringIO</p> <p>import numpy as np</p>"},{"location":"examples/mdanalysis_torsion_example/#well-implement-a-real-angle-difference-scoring-function-to-compare-with-csv-angles","title":"We'll implement a real angle-difference scoring function to compare with CSV angles:","text":"<p>import pandas as pd from Bio.PDB.MMCIFParser import MMCIFParser from Bio.PDB.PDBIO import PDBIO</p>"},{"location":"examples/mdanalysis_torsion_example/#from-mdanalysis_torsion_example-import-calculate_rna_torsions_mdanalysis-as-local_calc","title":"from mdanalysis_torsion_example import calculate_rna_torsions_mdanalysis as local_calc","text":"<p>def safe_select_atom(res, name):     \"\"\"     Return the first position array if the named atom is present     in the residue, else return None.     \"\"\"     if res is None:         return None     sel = res.atoms.select_atoms(f\"name {name}\")     if sel and len(sel.positions) &gt; 0:         return sel.positions[0]     return None</p> <p>def gather_atoms_for_alpha(prev_res, cur_res):     \"\"\"     Gather atoms for alpha torsion:     O3'(i-1) - P(i) - O5'(i) - C5'(i).     \"\"\"     return (         safe_select_atom(prev_res, \"O3'\"),         safe_select_atom(cur_res, \"P\"),         safe_select_atom(cur_res, \"O5'\"),         safe_select_atom(cur_res, \"C5'\"),     )</p> <p>def gather_atoms_for_beta(cur_res):     \"\"\"     Gather atoms for beta torsion:     P(i) - O5'(i) - C5'(i) - C4'(i).     \"\"\"     return (         safe_select_atom(cur_res, \"P\"),         safe_select_atom(cur_res, \"O5'\"),         safe_select_atom(cur_res, \"C5'\"),         safe_select_atom(cur_res, \"C4'\"),     )</p> <p>def gather_atoms_for_gamma(cur_res):     \"\"\"     Gather atoms for gamma torsion:     O5'(i) - C5'(i) - C4'(i) - C3'(i).     \"\"\"     return (         safe_select_atom(cur_res, \"O5'\"),         safe_select_atom(cur_res, \"C5'\"),         safe_select_atom(cur_res, \"C4'\"),         safe_select_atom(cur_res, \"C3'\"),     )</p> <p>def gather_atoms_for_delta(cur_res):     \"\"\"     Gather atoms for delta torsion:     C5'(i) - C4'(i) - C3'(i) - O3'(i).     \"\"\"     return (         safe_select_atom(cur_res, \"C5'\"),         safe_select_atom(cur_res, \"C4'\"),         safe_select_atom(cur_res, \"C3'\"),         safe_select_atom(cur_res, \"O3'\"),     )</p> <p>def gather_atoms_for_epsilon(cur_res, next_res):     \"\"\"     Gather atoms for epsilon torsion:     C4'(i) - C3'(i) - O3'(i) - P(i+1).     \"\"\"     return (         safe_select_atom(cur_res, \"C4'\"),         safe_select_atom(cur_res, \"C3'\"),         safe_select_atom(cur_res, \"O3'\"),         safe_select_atom(next_res, \"P\"),     )</p> <p>def gather_atoms_for_zeta(cur_res, next_res):     \"\"\"     Gather atoms for zeta torsion:     C3'(i) - O3'(i) - P(i+1) - O5'(i+1).     \"\"\"     return (         safe_select_atom(cur_res, \"C3'\"),         safe_select_atom(cur_res, \"O3'\"),         safe_select_atom(next_res, \"P\"),         safe_select_atom(next_res, \"O5'\"),     )</p> <p>def gather_atoms_for_chi(cur_res):     \"\"\"     Gather atoms for the glycosidic angle (chi).     For purines: O4' - C1' - N9 - C4     For pyrimidines: O4' - C1' - N1 - C2     \"\"\"     O4 = safe_select_atom(cur_res, \"O4'\")     C1 = safe_select_atom(cur_res, \"C1'\")     resname = cur_res.resname.strip().upper()     if resname.startswith(\"A\") or resname.startswith(\"G\"):         N_base = safe_select_atom(cur_res, \"N9\")         C_base = safe_select_atom(cur_res, \"C4\")     else:         N_base = safe_select_atom(cur_res, \"N1\")         C_base = safe_select_atom(cur_res, \"C2\")     return (O4, C1, N_base, C_base)</p> <p>def compute_dihedral_or_nan(atom_tuple):     \"\"\"     If any of the four atoms is None, return NaN.     Otherwise, compute the dihedral angle using calc_dihedral.     \"\"\"     if any(a is None for a in atom_tuple):         return np.nan     p1, p2, p3, p4 = atom_tuple     angle_deg = calc_dihedral(p1, p2, p3, p4)     if angle_deg is None:         return np.nan     return np.float64(angle_deg)</p> <p>def calc_dihedral(p1, p2, p3, p4):     \"\"\"     Calculate the dihedral angle (in degrees) for four 3D points     p1, p2, p3, p4 (each a NumPy array [x,y,z]).     If cross products are too small, return None.     \"\"\"     b1 = p2 - p1     b2 = p3 - p2     b3 = p4 - p3     n1 = np.cross(b1, b2)     n2 = np.cross(b2, b3)</p> <pre><code>norm_n1 = np.linalg.norm(n1)\nnorm_n2 = np.linalg.norm(n2)\nif norm_n1 &lt; 1e-12 or norm_n2 &lt; 1e-12:\n    return None\n\ncos_angle = np.dot(n1, n2) / (norm_n1 * norm_n2)\ncos_angle = max(-1.0, min(1.0, cos_angle))  # clamp\nsign = np.dot(b2, np.cross(n1, n2))\nphi = np.arccos(cos_angle)\nif sign &lt; 0.0:\n    phi = -phi\nreturn np.degrees(phi)\n</code></pre> <p>def convert_cif_to_pdb(cif_file):     \"\"\"     Convert an mmCIF file to a temporary PDB file using BioPython.     Returns the path to the temporary PDB file.     \"\"\"     parser = MMCIFParser(QUIET=True)     structure = parser.get_structure(\"mmcif_structure\", cif_file)</p> <pre><code># Write to a temp PDB\ntmp_handle = tempfile.NamedTemporaryFile(suffix=\".pdb\", delete=False)\ntmp_handle.close()  # We'll pass its name to PDBIO\npdb_path = tmp_handle.name\n\nio = PDBIO()\nio.set_structure(structure)\nio.save(pdb_path)\nreturn pdb_path\n</code></pre> <p>def calculate_rna_torsions_mdanalysis(     pdb_file, chain_id=\"A\", fallback=False, csv_file=None ):     \"\"\"     Calculate backbone and glycosidic torsion angles (alpha, beta, gamma,     delta, epsilon, zeta, chi) for an RNA chain using MDAnalysis.     For .cif files, we convert them to PDB with BioPython first.</p> <pre><code>This implementation uses a safe \"gather atoms, then compute dihedral\" approach,\nensuring that each torsion angle is computed independently, so we don't rely\non partially assigned local variables.\n\"\"\"\nfrom MDAnalysis import Universe\n\n# 1) If it's a .cif file, convert to .pdb\n_, ext = os.path.splitext(pdb_file)\next = ext.lower()\n\nusing_temp = False\ntemp_pdb_path = None\nif ext == \".cif\":\n    using_temp = True\n    temp_pdb_path = convert_cif_to_pdb(pdb_file)\n    mdanalysis_file = temp_pdb_path\nelse:\n    mdanalysis_file = pdb_file\n\n# 2) Create the Universe\ntry:\n    u = Universe(mdanalysis_file)\nfinally:\n    pass\n\n# Debug: Print all segments and their chain IDs\nprint(\"=== DEBUG: Segments in this Universe ===\")\nfor seg in u.segments:\n    print(f\"  Segment segid={seg.segid}, n_res={len(seg.residues)}\")\n\n# Debug: Print any chain info from 'nucleic' selection, in case there's a label mismatch\nall_nucleic = u.select_atoms(\"nucleic\")\nunique_segids = set(a.segment.segid for a in all_nucleic.atoms)\nprint(f\"=== DEBUG: Found segids in 'nucleic': {unique_segids}\")\n\n# 3) Attempt chain selection\nprint(\"Segments:\", u.segments)\nprint(\"Residues:\", u.residues)\n\nchain = u.select_atoms(f\"(segid {chain_id}) or (chainID {chain_id})\")\nprint(f\"Selecting chain with chain_id='{chain_id}'... Found {len(chain)} atoms.\")\n\nif len(chain) == 0:\n    # Attempt to autodetect among all segids found in nucleic\n    candidate_segids = set(a.segment.segid for a in u.select_atoms(\"nucleic\").atoms)\n    print(\n        f\"No atoms found for chainID={chain_id}. Trying all candidate segids: {candidate_segids}\"\n    )\n\n    best_seg = None\n    best_chain = None\n    best_score = float(\"inf\")\n\n    def compute_angle_score(test_chain, csv_file):\n        # 1) Load CSV angles\n        lines_csv = []\n        with open(csv_file, \"r\") as f_in:\n            for line in f_in:\n                if not line.strip() or \"====\" in line:\n                    continue\n                if line.startswith(\"index_chain\") or re.match(\n                    r\"^\\\\d\", line.strip()\n                ):\n                    lines_csv.append(line)\n        df_str = \"\".join(lines_csv)\n        df = pd.read_csv(StringIO(df_str), sep=\",\")\n        alpha_csv = df[\"alpha\"].to_numpy(dtype=float)\n        beta_csv = df[\"beta\"].to_numpy(dtype=float)\n        gamma_csv = df[\"gamma\"].to_numpy(dtype=float)\n        delta_csv = df[\"delta\"].to_numpy(dtype=float)\n        epsilon_csv = df[\"epsilon\"].to_numpy(dtype=float)\n        zeta_csv = df[\"zeta\"].to_numpy(dtype=float)\n        chi_csv = df[\"chi\"].to_numpy(dtype=float)\n\n        # 2) Compute new angles from test_chain\n        # We'll create a Universe with only the test_chain: or just pass to local_calc\n        # But local_calc expects a path, so we do an inline approach:\n        # Or we can call local_calc on the entire Universe, but restricting to test_chain's residues.\n        # For simplicity, let's do a partial approach:\n        # We'll rely on your existing logic to gather angles from 'test_chain'.\n        # We'll copy/paste the gather logic from within the same file or from custom script.\n\n        # For demonstration, let's call local_calc on a temporary file approach:\n        # This might be tricky if we only have an in-memory selection. Alternatively, we can adapt the logic directly.\n        # Here we show a pseudo approach:\n        new_angles = {\n            \"alpha\": [],\n            \"beta\": [],\n            \"gamma\": [],\n            \"delta\": [],\n            \"epsilon\": [],\n            \"zeta\": [],\n            \"chi\": [],\n        }\n        # We'll gather real angles from the 'test_chain' the same way the main code does:\n        all_res = test_chain.residues\n        for i, res in enumerate(all_res):\n            prev_res = all_res[i - 1] if i &gt; 0 else None\n            next_res = all_res[i + 1] if i &lt; len(all_res) - 1 else None\n            # we can call gather_atoms, compute_dihedral, etc.\n            # For brevity, let's do alpha only. In reality, replicate for all angles:\n            # Already in the same file, no need for relative import. We'll directly reference local methods.\n            # Use gather_atoms_for_alpha, gather_atoms_for_beta, gather_atoms_for_gamma, gather_atoms_for_delta,\n            # gather_atoms_for_epsilon, gather_atoms_for_zeta, gather_atoms_for_chi, compute_dihedral_or_nan\n            # from the same file.\n            if prev_res:\n                alpha_tuple = gather_atoms_for_alpha(prev_res, res)\n                alpha_val = compute_dihedral_or_nan(alpha_tuple)\n            else:\n                alpha_val = np.nan\n            new_angles[\"alpha\"].append(alpha_val)\n\n            beta_tuple = gather_atoms_for_beta(res)\n            beta_val = compute_dihedral_or_nan(beta_tuple)\n            new_angles[\"beta\"].append(beta_val)\n\n            gamma_tuple = gather_atoms_for_gamma(res)\n            gamma_val = compute_dihedral_or_nan(gamma_tuple)\n            new_angles[\"gamma\"].append(gamma_val)\n\n            delta_tuple = gather_atoms_for_delta(res)\n            delta_val = compute_dihedral_or_nan(delta_tuple)\n            new_angles[\"delta\"].append(delta_val)\n\n            if next_res:\n                eps_tuple = gather_atoms_for_epsilon(res, next_res)\n                eps_val = compute_dihedral_or_nan(eps_tuple)\n            else:\n                eps_val = np.nan\n            new_angles[\"epsilon\"].append(eps_val)\n\n            if next_res:\n                zeta_tuple = gather_atoms_for_zeta(res, next_res)\n                zeta_val = compute_dihedral_or_nan(zeta_tuple)\n            else:\n                zeta_val = np.nan\n            new_angles[\"zeta\"].append(zeta_val)\n\n            chi_tuple = gather_atoms_for_chi(res)\n            chi_val = compute_dihedral_or_nan(chi_tuple)\n            new_angles[\"chi\"].append(chi_val)\n\n        alpha_new = np.array(new_angles[\"alpha\"], dtype=float)\n        beta_new = np.array(new_angles[\"beta\"], dtype=float)\n        gamma_new = np.array(new_angles[\"gamma\"], dtype=float)\n        delta_new = np.array(new_angles[\"delta\"], dtype=float)\n        epsilon_new = np.array(new_angles[\"epsilon\"], dtype=float)\n        zeta_new = np.array(new_angles[\"zeta\"], dtype=float)\n        chi_new = np.array(new_angles[\"chi\"], dtype=float)\n\n        # 3) Align/truncate\n        n_csv = len(alpha_csv)\n        n_new = len(alpha_new)\n        n_min = min(n_csv, n_new)\n        # 4) compute average absolute difference\n        diff_alpha = np.nanmean(np.abs(alpha_new[:n_min] - alpha_csv[:n_min]))\n        diff_beta = np.nanmean(np.abs(beta_new[:n_min] - beta_csv[:n_min]))\n        diff_gamma = np.nanmean(np.abs(gamma_new[:n_min] - gamma_csv[:n_min]))\n        diff_delta = np.nanmean(np.abs(delta_new[:n_min] - delta_csv[:n_min]))\n        diff_eps = np.nanmean(np.abs(epsilon_new[:n_min] - epsilon_csv[:n_min]))\n        diff_zeta = np.nanmean(np.abs(zeta_new[:n_min] - zeta_csv[:n_min]))\n        diff_chi = np.nanmean(np.abs(chi_new[:n_min] - chi_csv[:n_min]))\n\n        # Combine them:\n        overall_score = np.mean(\n            [\n                diff_alpha,\n                diff_beta,\n                diff_gamma,\n                diff_delta,\n                diff_eps,\n                diff_zeta,\n                diff_chi,\n            ]\n        )\n        return overall_score\n\n    for seg in candidate_segids:\n        test_chain = u.select_atoms(f\"(segid {seg}) and nucleic\")\n        if len(test_chain.residues) &lt; 1:\n            continue\n        # Now that csv_file is passed in, we can safely call compute_angle_score:\n        score = compute_angle_score(test_chain, csv_file)\n        if score &lt; best_score:\n            best_score = score\n            best_seg = seg\n            best_chain = test_chain\n\n    if best_chain is not None:\n        print(f\"Auto-selected chain segid='{best_seg}' with score={best_score}\")\n        chain = best_chain\n    else:\n        if fallback:\n            print(\"All segids tested but none chosen. Falling back to all nucleic.\")\n            chain = u.select_atoms(\"nucleic\")\n        else:\n            raise ValueError(\n                f\"No valid chain found for chainID='{chain_id}' and autodetect failed. Check your PDB/cif labeling.\"\n            )\n\n# Extra debug: show residue numbering in the chain\nprint(\"=== DEBUG: Residue numbering in selected chain ===\")\nfor r in chain.residues:\n    print(f\"   Residue {r.resname}, resid={r.resid}, segid={r.segid}\")\n\ntorsion_data = {\n    \"alpha\": [],\n    \"beta\": [],\n    \"gamma\": [],\n    \"delta\": [],\n    \"epsilon\": [],\n    \"zeta\": [],\n    \"chi\": [],\n}\n\nresidues = chain.residues\nfor i, res in enumerate(residues):\n    prev_res = residues[i - 1] if i &gt; 0 else None\n    next_res = residues[i + 1] if i &lt; len(residues) - 1 else None\n\n    # alpha: gather from prev &amp; cur\n    if prev_res:\n        alpha_tuple = gather_atoms_for_alpha(prev_res, res)\n        alpha_val = compute_dihedral_or_nan(alpha_tuple)\n    else:\n        alpha_val = np.nan\n    torsion_data[\"alpha\"].append(alpha_val)\n\n    # beta\n    beta_tuple = gather_atoms_for_beta(res)\n    beta_val = compute_dihedral_or_nan(beta_tuple)\n    torsion_data[\"beta\"].append(beta_val)\n\n    # gamma\n    gamma_tuple = gather_atoms_for_gamma(res)\n    gamma_val = compute_dihedral_or_nan(gamma_tuple)\n    torsion_data[\"gamma\"].append(gamma_val)\n\n    # delta\n    delta_tuple = gather_atoms_for_delta(res)\n    delta_val = compute_dihedral_or_nan(delta_tuple)\n    torsion_data[\"delta\"].append(delta_val)\n\n    # epsilon\n    if next_res:\n        epsilon_tuple = gather_atoms_for_epsilon(res, next_res)\n        epsilon_val = compute_dihedral_or_nan(epsilon_tuple)\n    else:\n        epsilon_val = np.nan\n    torsion_data[\"epsilon\"].append(epsilon_val)\n\n    # zeta\n    if next_res:\n        zeta_tuple = gather_atoms_for_zeta(res, next_res)\n        zeta_val = compute_dihedral_or_nan(zeta_tuple)\n    else:\n        zeta_val = np.nan\n    torsion_data[\"zeta\"].append(zeta_val)\n\n    # chi: gather from current residue alone\n    chi_tuple = gather_atoms_for_chi(res)\n    chi_val = compute_dihedral_or_nan(chi_tuple)\n    torsion_data[\"chi\"].append(chi_val)\n\n# Clean up temporary file if used\nif using_temp and temp_pdb_path is not None and os.path.exists(temp_pdb_path):\n    os.remove(temp_pdb_path)\n\nreturn torsion_data\n</code></pre> <p>def main():     if len(sys.argv) &lt; 2:         print(             \"Usage: python mdanalysis_torsion_example.py  [chainID]\"         )         sys.exit(1) <pre><code>pdb_path = sys.argv[1]\nchain_id = sys.argv[2] if len(sys.argv) &gt;= 3 else \"A\"\n\ntorsions = calculate_rna_torsions_mdanalysis(\n    pdb_path, chain_id=chain_id, fallback=True\n)\nfor angle_name, values in torsions.items():\n    print(f\"{angle_name}: {values}\")\n</code></pre> <p>if name == \"main\":     main()</p>"},{"location":"examples/prompt_template/","title":"Prompt template","text":"<p>You are given one or more automatically generated Python test files that test various classes and functions. These tests may have issues such as poor naming conventions, inconsistent usage of self, lack of setUp methods, minimal docstrings, redundant or duplicate tests, and limited assertion coverage. They may also fail to leverage hypothesis and unittest.mock effectively, and might not be logically grouped.</p> <p>Your task is to produce a single, consolidated, high-quality test file from the given input files. The refactored test file should incorporate the following improvements:     1.  Consolidation and Organization     \u2022   Combine all tests from the provided files into one coherent Python test file.     \u2022   Group tests into classes that correspond logically to the functionality they are testing (e.g., separate test classes by the class or function under test).     \u2022   Within each class, order test methods logically (e.g., basic functionality first, edge cases, error handling, round-trip tests afterward).     2.  Clean, Readable Code     \u2022   Use descriptive, PEP 8-compliant class and method names.     \u2022   Add docstrings to each test class and test method, explaining their purpose and what they verify.     \u2022   Remove redundant, duplicate, or meaningless tests. Combine or refactor tests that cover the same functionality into a single, comprehensive test method when appropriate.     3.  Proper Test Fixtures     \u2022   Utilize setUp methods to instantiate commonly used objects before each test method, reducing redundancy.     \u2022   Ensure that instance methods of classes under test are called on properly instantiated objects rather than passing self incorrectly as an argument.     4.  Robust Assertions and Coverage     \u2022   Include multiple assertions in each test to thoroughly verify behavior and correctness.     \u2022   Use unittest\u2019s assertRaises for expected exceptions to validate error handling.     \u2022   Implement at least one round-trip test (e.g., encode then decode a data structure, or transform an object multiple times to ensure idempotency).     5.  Effective Use of Hypothesis     \u2022   Employ hypothesis to generate a wide range of input data, ensuring better coverage and exposing edge cases.     \u2022   Use strategies like st.builds to create complex objects (e.g., custom dataclasses) with varied attribute values.     \u2022   Enforce constraints (e.g., allow_nan=False) to avoid nonsensical test inputs.     6.  Mocking External Dependencies     \u2022   Use unittest.mock where appropriate to simulate external dependencies or environments, ensuring tests are reliable and isolated from external conditions.</p> <p>\u2e3b</p> <p>Additional Context: Getting Started with Hypothesis</p> <p>Below is a practical guide that outlines common use cases and best practices for leveraging hypothesis:     1.  Basic Usage     \u2022   Decorate test functions with @given and specify a strategy (e.g., @given(st.text())).     \u2022   Let hypothesis generate diverse test cases automatically.     2.  Common Strategies     \u2022   Use built-in strategies like st.integers(), st.floats(), st.text(), etc.     \u2022   Combine strategies with st.lists, st.builds, or st.composite to generate complex objects.     3.  Composing Tests     \u2022   Employ assume() to filter out unwanted test cases.     \u2022   Compose or build custom objects to test domain-specific logic.     4.  Advanced Features     \u2022   Fine-tune test runs with @settings (e.g., max_examples=1000).     \u2022   Create reusable strategies via @composite.     5.  Best Practices     \u2022   Keep tests focused on one property at a time.     \u2022   Use explicit examples with @example() for edge cases.     \u2022   Manage performance by choosing realistic strategy bounds.     6.  Debugging Failed Tests     \u2022   Hypothesis shows minimal failing examples and seeds to help reproduce and fix issues.</p> <p>\u2e3b</p> <p>Input Format</p> <p>TEST CODE: {TEST_CODE} FULL SRC CODE: {FULL_SRC_CODE}</p> <p>Where:     \u2022   {TEST_CODE} is the content of your automatically generated Python test files (potentially multiple files\u2019 content combined or listed).     \u2022   {FULL_SRC_CODE} is the content of the source code under test (if needed for context).</p> <p>Output Format</p> <p>Provide a single Python code block containing the fully refactored, consolidated test file. The output should be ready-to-run with:</p> <p>python -m unittest</p> <p>It must exhibit all of the improvements listed above, including:     \u2022   Logical grouping of tests,     \u2022   Clear and correct usage of setUp,     \u2022   Docstrings for test classes and methods,     \u2022   Consolidated and refactored tests (no duplicates),     \u2022   Robust assertions and coverage,     \u2022   Use of hypothesis with one or more examples,     \u2022   Use of mock where appropriate.</p>"},{"location":"examples/prompt_template/#_1","title":"\u2e3b","text":"<p>EXTRA USEFUL CONTEXT TO AID YOU IN YOUR TASK: Hypothesis: A Comprehensive Best-Practice and Reference Guide</p> <p>Hypothesis is a powerful property-based testing library for Python, designed to help you find subtle bugs by generating large numbers of test inputs and minimizing failing examples. This document combines the strengths and core ideas of three earlier guides. It serves as a broad, in-depth resource: covering Hypothesis usage from the basics to advanced methods, including background on its internal mechanisms (Conjecture) and integration with complex workflows.</p> <p>\u2e3b</p> <p>Table of Contents     1.  Introduction to Property-Based Testing 1.1 What Is Property-Based Testing? 1.2 Why Use Property-Based Testing? 1.3 Installing Hypothesis     2.  First Steps with Hypothesis 2.1 A Simple Example 2.2 Basic Workflows and Key Concepts 2.3 Troubleshooting the First Failures     3.  Core Hypothesis Concepts 3.1 The @given Decorator 3.2 Strategies: Building and Composing Data Generators 3.3 Shrinking and Minimizing Failing Examples 3.4 Example Database and Replay     4.  Advanced Data Generation 4.1 Understanding Strategies vs. Types 4.2 Composing Strategies (map, filter, flatmap) 4.3 Working with Complex or Recursive Data 4.4 Using @composite Functions 4.5 Integration and Edge Cases     5.  Practical Usage Patterns 5.1 Testing Numeric Code (Floating-Point, Bounds) 5.2 Text and String Generation (Character Sets, Regex) 5.3 Dates, Times, and Time Zones 5.4 Combining Hypothesis with Fixtures and Other Test Tools     6.  Stateful/Model-Based Testing 6.1 The RuleBasedStateMachine and @rule Decorators 6.2 Designing Operations and Invariants 6.3 Managing Complex State and Multiple Bundles 6.4 Example: Testing a CRUD System or Other Stateful API     7.  Performance and Health Checks 7.1 Diagnosing Slow Tests with Deadlines 7.2 Common Health Check Warnings and Their Meanings 7.3 Filtering Pitfalls (assume / Over-Filters) 7.4 Tuning Hypothesis Settings (max_examples, phases, etc.) 7.5 Speed vs. Thoroughness     8.  Multiple Failures and Multi-Bug Discovery 8.1 How Hypothesis Detects and Distinguishes Bugs 8.2 Typical Bug Slippage and the \u201cThreshold Problem\u201d 8.3 Strategies for Handling Multiple Distinct Failures     9.  Internals: The Conjecture Engine 9.1 Overview of Bytestream-Based Generation 9.2 Integrated Shrinking vs. Type-Based Shrinking 9.3 How Conjecture Tracks and Minimizes Examples 9.4 The Example Database in Depth     10. Hypothesis in Real-World Scenarios 10.1 Using Hypothesis in CI/CD 10.2 Collaborative Testing in Teams 10.3 Integrating with Other Tools (pytest, coverage, etc.) 10.4 Best Practices for Large Projects     11. Extensibility and Advanced Topics 11.1 Third-Party Extensions (e.g., Hypothesis-Bio, Hypothesis-NetworkX) 11.2 Targeted Property-Based Testing (Scoring) 11.3 Hybrid Approaches (Combining Examples with Generation) 11.4 Glass-Box Testing and Potential Future Work     12. Troubleshooting and FAQs 12.1 Common Error Messages 12.2 Reproduce Failures with @reproduce_failure and Seeds 12.3 Overcoming Flaky or Non-Deterministic Tests 12.4 Interpreting Statistics     13. Summary and Further Reading 13.1 Key Takeaways and Next Steps 13.2 Recommended Resources and Papers 13.3 Contributing to Hypothesis</p> <p>\u2e3b</p> <ol> <li>Introduction to Property-Based Testing</li> </ol> <p>1.1 What Is Property-Based Testing?</p> <p>Property-based testing (PBT) shifts your focus from manually enumerating test inputs to describing the properties your code should fulfill for all valid inputs. Instead of hardcoding specific examples (like assert f(2) == 4), you define requirements: e.g., \u201cSorting a list is idempotent.\u201d Then the library (Hypothesis) generates test inputs to find edge cases or scenarios violating those properties.</p> <p>Example</p> <p>from hypothesis import given, strategies as st</p> <p>@given(st.lists(st.integers())) def test_sort_idempotent(xs):     once = sorted(xs)     twice = sorted(once)     assert once == twice</p> <p>Hypothesis tries diverse lists (including empty lists, duplicates, large sizes, negative or positive numbers). If something fails, it shrinks the input to a minimal failing example.</p> <p>1.2 Why Use Property-Based Testing?     \u2022   Coverage of Edge Cases: Automatically covers many corner cases\u2014empty inputs, large values, special floats, etc.     \u2022   Reduced Manual Labor: You specify broad properties, and the tool handles enumerations.     \u2022   Debugging Aid: Found a failing input? Hypothesis shrinks it to a simpler version, making debug cycles shorter.     \u2022   Less Test Boilerplate: Fewer individual test cases to write while achieving higher coverage.</p> <p>1.3 Installing Hypothesis</p> <p>You can install the base library with pip install hypothesis. For specialized extras (e.g., date/time, Django), consult Hypothesis extras docs.</p> <p>\u2e3b</p> <ol> <li>First Steps with Hypothesis</li> </ol> <p>2.1 A Simple Example</p> <p>from hypothesis import given from hypothesis.strategies import integers</p> <p>@given(integers()) def test_square_is_nonnegative(x):     assert x*x &gt;= 0</p> <p>Run with pytest, unittest, or another runner. Hypothesis calls test_square_is_nonnegative multiple times with varied integers (positive, negative, zero).</p> <p>2.2 Basic Workflows and Key Concepts     1.  Test Functions: Decorate with @given().     2.  Generation and Execution: Hypothesis runs tests many times with random values, tries to find failures.     3.  Shrinking: If a failure occurs, Hypothesis narrows down (shrinks) the input to a minimal failing example. <p>2.3 Troubleshooting the First Failures     \u2022   Assertion Errors: If you see Falsifying example: ..., Hypothesis found a failing scenario. Use that scenario to fix your code or refine your property.     \u2022   Health Check Warnings: If you see warnings like \u201cfilter_too_much\u201d or \u201ctoo_slow,\u201d see the Health Checks section.</p> <p>\u2e3b</p> <ol> <li>Core Hypothesis Concepts</li> </ol> <p>3.1 The @given Decorator</p> <p>@given ties strategies to a test function\u2019s parameters:</p> <p>from hypothesis import given from hypothesis.strategies import text, emails</p> <p>@given(email=emails(), note=text()) def test_process_email(email, note):     ...</p> <p>Hypothesis calls test_process_email() repeatedly with random emails and text. If everything passes, the test is green. Otherwise, you get a shrunk failing example.</p> <p>3.2 Strategies: Building and Composing Data Generators</p> <p>Hypothesis\u2019s data generation revolves around \u201cstrategies.\u201d Basic ones:     \u2022   integers(), floats(), text(), booleans(), etc.     \u2022   Containers: lists(elements, ...), dictionaries(keys=..., values=...)     \u2022   Map/Filter: Transform or constrain existing strategies.     \u2022   Composite: Build custom strategies for domain objects.</p> <p>3.3 Shrinking and Minimizing Failing Examples</p> <p>If a test fails on a complicated input, Hypothesis tries simpler versions: removing elements from lists, changing large ints to smaller ints, etc. The final reported failing input is minimal by lex ordering.</p> <p>Falsifying example: test_sort_idempotent(xs=[2, 1, 1])</p> <p>Hypothesis might have started with [random, complicated list] but ended with [2,1,1].</p> <p>3.4 Example Database and Replay</p> <p>Failures are saved in a local .hypothesis/ directory. On subsequent runs, Hypothesis replays known failing inputs before generating fresh ones. This ensures consistent reporting once a failing case is discovered.</p> <p>\u2e3b</p> <ol> <li>Advanced Data Generation</li> </ol> <p>4.1 Understanding Strategies vs. Types</p> <p>Hypothesis does not rely solely on type information. You can define custom constraints to ensure the data you generate matches your domain. E.g., generating only non-empty lists or restricting floats to finite values:</p> <p>import math</p> <p>@given(st.lists(st.floats(allow_infinity=False, allow_nan=False), min_size=1)) def test_mean_in_bounds(xs):     avg = sum(xs)/len(xs)     assert min(xs) &lt;= avg &lt;= max(xs)</p> <p>4.2 Composing Strategies (map, filter, flatmap)     \u2022   map(f) transforms data after generation:</p> <p>even_integers = st.integers().map(lambda x: x * 2)</p> <pre><code>\u2022   filter(pred) discards values that fail pred; be mindful of over-filtering performance.\n\u2022   flatmap(...) draws a value, then uses it to define a new strategy:\n</code></pre>"},{"location":"examples/prompt_template/#draw-an-int-n-then-a-list-of-length-n","title":"Draw an int n, then a list of length n","text":"<p>st.integers(min_value=0, max_value=10).flatmap(lambda n: st.lists(st.text(), min_size=n, max_size=n))</p> <p>4.3 Working with Complex or Recursive Data</p> <p>For tree-like or nested data, use st.recursive(base_strategy, extend_strategy, max_leaves=...) to limit growth. Also consider the @composite decorator to build logic step by step.</p> <p>from hypothesis import strategies as st, composite</p> <p>@composite def user_records(draw):     name = draw(st.text(min_size=1))     age = draw(st.integers(min_value=0))     return \"name\": name, \"age\": age</p> <p>4.4 Using @composite Functions</p> <p>@composite is a more explicit style than map/flatmap. It helps define multi-step draws within one function. It\u2019s usually simpler for highly interdependent data.</p> <p>4.5 Integration and Edge Cases     \u2022   Ensuring Valid Domain Data: Use composites or partial filtering. Overuse of filter(...) can cause slow tests and health-check failures.     \u2022   Large/Complex Structures: Limit sizes or use constraints (max_size, bounding integers, etc.) to avoid timeouts.</p> <p>\u2e3b</p> <ol> <li>Practical Usage Patterns</li> </ol> <p>5.1 Testing Numeric Code (Floating-Point, Bounds)</p> <p>Floating point nuances:</p> <p>@given(st.floats(allow_nan=False, allow_infinity=False)) def test_floats(x):     ...</p> <p>Constrain or skip NaNs/infinities if your domain doesn\u2019t handle them. Keep an eye on overflows if sums get large.</p> <p>5.2 Text and String Generation (Character Sets, Regex)</p> <p>Hypothesis can generate ASCII, Unicode, or custom sets:</p> <p>from hypothesis.strategies import text</p> <p>@given(text(alphabet=\"ABCDE\", min_size=1)) def test_some_text(s):     assert s[0] in \"ABCDE\"</p> <p>Or use from_regex(r\"MyPattern\") for more specialized scenarios.</p> <p>5.3 Dates, Times, and Time Zones</p> <p>Install hypothesis[datetime] for strategies like dates(), datetimes(), timezones(). These handle cross-timezone issues or restricted intervals.</p> <p>5.4 Combining Hypothesis with Fixtures and Other Test Tools</p> <p>With pytest, you can pass both fixture arguments and Hypothesis strategy arguments:</p> <p>import pytest</p> <p>@pytest.fixture def db():     return init_db()</p> <p>@given(x=st.integers()) def test_db_invariant(db, x):     assert my_query(db, x) == ...</p> <p>Function-scoped fixtures are invoked once per test function, not per example, so plan accordingly or do manual setup for each iteration.</p> <p>\u2e3b</p> <ol> <li>Stateful/Model-Based Testing</li> </ol> <p>6.1 The RuleBasedStateMachine and @rule Decorators</p> <p>For testing stateful systems, Hypothesis uses a rule-based approach:</p> <p>from hypothesis.stateful import RuleBasedStateMachine, rule</p> <p>class SimpleCounter(RuleBasedStateMachine):     def init(self):         super().init()         self.counter = 0</p> <pre><code>@rule(increment=st.integers(min_value=1, max_value=100))\ndef inc(self, increment):\n    self.counter += increment\n    assert self.counter &gt;= 0\n</code></pre> <p>TestCounter = SimpleCounter.TestCase</p> <p>Hypothesis runs random sequences of operations, checking for invariant violations.</p> <p>6.2 Designing Operations and Invariants     \u2022   Each @rule modifies the system under test.     \u2022   Use @precondition to ensure certain rules only fire in valid states.     \u2022   Use @invariant to check conditions after each rule.</p> <p>6.3 Managing Complex State and Multiple Bundles     \u2022   Bundle(...) helps track created objects and pass them between rules.     \u2022   Perfect for simulating CRUD or multi-object interactions.</p> <p>6.4 Example: Testing a CRUD System or Other Stateful API</p> <p>class CRUDSystem(RuleBasedStateMachine):     Records = Bundle('records')</p> <pre><code>@rule(target=Records, data=st.text())\ndef create(self, data):\n    record_id = my_create_fn(data)\n    return record_id\n\n@rule(record=Records)\ndef delete(self, record):\n    my_delete_fn(record)\n</code></pre> <p>Hypothesis will produce sequences of create/delete calls. If a bug arises, it provides a minimal sequence reproducing it.</p> <p>\u2e3b</p> <ol> <li>Performance and Health Checks</li> </ol> <p>7.1 Diagnosing Slow Tests with Deadlines</p> <p>Hypothesis can treat slow examples as errors:</p> <p>from hypothesis import settings, HealthCheck</p> <p>@settings(deadline=100)  # 100ms deadline @given(st.lists(st.integers())) def test_something(xs):     ...</p> <p>If a single test run exceeds 100 ms, it raises DeadlineExceeded. This helps identify performance bottlenecks quickly.</p> <p>7.2 Common Health Check Warnings and Their Meanings     \u2022   filter_too_much: A large proportion of generated data is being thrown away. Fix by refining your strategy or combining strategies (instead of heavy use of filter).     \u2022   too_slow: The test or generation logic is slow. Lower max_examples or investigate your code\u2019s performance.     \u2022   data_too_large: Possibly generating very large structures. Restrict sizes.</p> <p>7.3 Filtering Pitfalls (assume / Over-Filters)</p> <p>Using assume(condition) forcibly discards any example that doesn\u2019t meet condition. Overdoing it can degrade performance drastically. Instead, refine your data strategies:</p>"},{"location":"examples/prompt_template/#instead-of","title":"Instead of:","text":"<p>@given(st.lists(st.integers()).filter(lambda xs: sum(xs) &lt; 100))</p>"},{"location":"examples/prompt_template/#use-a-better-approach","title":"Use a better approach:","text":"<p>@given(st.lists(st.integers(max_value=100), max_size=10))</p> <p>7.4 Tuning Hypothesis Settings (max_examples, phases, etc.)     \u2022   max_examples: Controls how many examples are generated per test (default ~200).     \u2022   phases: Choose which parts of the test lifecycle (e.g. \u201cshrink\u201d, \u201creuse\u201d) run.     \u2022   suppress_health_check: Silence known but acceptable warnings.</p> <p>7.5 Speed vs. Thoroughness</p> <p>Balance thorough coverage with test suite runtime. Trim unhelpful extra complexity in data generation. Use deadline or lower max_examples for large test suites.</p> <p>\u2e3b</p> <ol> <li>Multiple Failures and Multi-Bug Discovery</li> </ol> <p>8.1 How Hypothesis Detects and Distinguishes Bugs</p> <p>Hypothesis typically shrinks until it finds the smallest failing example. But if a test can fail in multiple ways, Hypothesis 3.29+ tries to keep track of each distinct bug (by exception type and line number).</p> <p>8.2 Typical Bug Slippage and the \u201cThreshold Problem\u201d     \u2022   Bug Slippage: Starting with one bug scenario but shrinking to a different scenario. Hypothesis tries to keep track and track distinct failures.     \u2022   Threshold Problem: When tests fail due to crossing a numeric threshold, shrunk examples tend to be just barely beyond that threshold, potentially obscuring the severity of the issue. Techniques to mitigate this can involve \u201ctargeting\u201d or custom test logic.</p> <p>8.3 Strategies for Handling Multiple Distinct Failures</p> <p>Hypothesis\u2019s multi-failure mode ensures it shrinks each failing scenario independently. You may see multiple minimal failures reported. This can be turned on automatically if distinct bug states are detected.</p> <p>\u2e3b</p> <ol> <li>Internals: The Conjecture Engine</li> </ol> <p>9.1 Overview of Bytestream-Based Generation</p> <p>Conjecture is the underlying fuzzing engine. It treats every generated example as a lazily consumed byte stream. Strategies interpret segments of bytes as integers, floats, text, etc. This uniform approach:     \u2022   Simplifies storing known failures to replay them.     \u2022   Allows integrated shrinking by reducing or rewriting parts of the byte stream.</p> <p>9.2 Integrated Shrinking vs. Type-Based Shrinking</p> <p>Old or simpler property-based systems often rely on \u201ctype-based\u201d shrinking. Conjecture\u2019s approach integrates shrinking with data generation. This ensures that if you build data by composition (e.g. mapping or flattening strategies), Hypothesis can still shrink effectively.</p> <p>9.3 How Conjecture Tracks and Minimizes Examples     \u2022   Each test run has a \u201cbuffer\u201d of bytes.     \u2022   On failure, Conjecture tries different transformations (removing or reducing bytes).     \u2022   The result is simpler failing input but consistent with the constraints of your strategy.</p> <p>9.4 The Example Database in Depth</p> <p>All interesting examples get stored in .hypothesis/examples by default. On re-run, Hypothesis tries these before generating new data. This yields repeatable failures for regression tests\u2014especially helpful in CI setups.</p> <p>\u2e3b</p> <ol> <li>Hypothesis in Real-World Scenarios</li> </ol> <p>10.1 Using Hypothesis in CI/CD     \u2022   Run Hypothesis-based tests as part of your continuous integration.     \u2022   The example database can be committed to share known failures across devs.     \u2022   Set a deadline or use smaller max_examples to keep test times predictable.</p> <p>10.2 Collaborative Testing in Teams     \u2022   Consistent Strategy Definitions: Keep your custom strategies in a shared \u201cstrategies.py.\u201d     \u2022   Version Control: The .hypothesis directory can be versioned to share known failing examples, though watch out for merge conflicts.</p> <p>10.3 Integrating with Other Tools (pytest, coverage, etc.)     \u2022   Pytest integration is seamless\u2014just write @given tests, run pytest.     \u2022   Coverage tools measure tested code as usual, but remember Hypothesis can deeply cover corner cases.</p> <p>10.4 Best Practices for Large Projects     \u2022   Modular Strategies: Break them down for maintainability.     \u2022   Tackle Invariants Early: Short-circuit with assume() or well-structured strategies.     \u2022   Monitor Performance: Use health checks, deadlines, and max_examples config to scale.</p> <p>\u2e3b</p> <ol> <li>Extensibility and Advanced Topics</li> </ol> <p>11.1 Third-Party Extensions     \u2022   hypothesis-bio: Specialized for bioinformatics data formats.     \u2022   hypothesis-networkx: Generate networkx graphs, test graph algorithms.     \u2022   Many more unofficial or domain-specific libraries exist. Creating your own extension is easy.</p> <p>11.2 Targeted Property-Based Testing (Scoring)</p> <p>You can \u201cguide\u201d test generation by calling target(score) in your code. Hypothesis tries to evolve test cases with higher scores, focusing on \u201cinteresting\u201d or extreme behaviors (like maximizing error metrics).</p> <p>from hypothesis import given, target from hypothesis.strategies import floats</p> <p>@given(x=floats(-1e6, 1e6)) def test_numerical_stability(x):     err = some_error_metric(x)     target(err)     assert err &lt; 9999</p> <p>11.3 Hybrid Approaches (Combining Examples with Generation)</p> <p>You can add \u201cexample-based tests\u201d to complement property-based ones. Also, you can incorporate real-world test data as seeds or partial strategies.</p> <p>11.4 Glass-Box Testing and Potential Future Work</p> <p>Hypothesis largely treats tests as a black box but can be extended with coverage data or other instrumentation for more advanced test generation. This is an open area of R&amp;D.</p> <p>\u2e3b</p> <ol> <li>Troubleshooting and FAQs</li> </ol> <p>12.1 Common Error Messages     \u2022   Unsatisfiable: Hypothesis can\u2019t find enough valid examples. Possibly an over-filter or an unrealistic requirement.     \u2022   DeadlineExceeded: Your test or code is too slow for the set deadline ms.     \u2022   FailedHealthCheck: Usually means you\u2019re doing too much filtering or the example is too large.</p> <p>12.2 Reproduce Failures with @reproduce_failure and Seeds</p> <p>If Hypothesis can\u2019t express your failing data via a standard repr, it shows a snippet like:</p> <p>@reproduce_failure('3.62.0', b'...') def test_something():     ...</p> <p>Adding that snippet ensures the bug is replayed exactly. Alternatively, you can do:</p> <p>from hypothesis import seed</p> <p>@seed(12345) @given(st.integers()) def test_x(x):     ...</p> <p>But seeds alone are insufficient if your .hypothesis database is relevant or if your test uses inline data.</p> <p>12.3 Overcoming Flaky or Non-Deterministic Tests</p> <p>If code is time-sensitive or concurrency-based, you may see spurious failures. Try limiting concurrency, raising deadlines, or disabling shrinking for certain tests. Alternatively, fix the non-determinism in the tested code.</p> <p>12.4 Interpreting Statistics</p> <p>Running pytest --hypothesis-show-statistics yields info on distribution of generated examples, data-generation time vs. test time, etc. This helps find bottlenecks, excessive filtering, or unexpectedly large inputs.</p> <p>\u2e3b</p> <ol> <li>Summary and Further Reading</li> </ol> <p>13.1 Key Takeaways and Next Steps     \u2022   Write Clear Properties: A crisp property is simpler for Hypothesis to exploit.     \u2022   Refine Strategies: Good strategy design yields fewer discards and faster tests.     \u2022   Use Health Checks: They highlight anti-patterns early.     \u2022   Explore Stateful Testing: Perfect for integration tests or persistent-state bugs.</p> <p>13.2 Recommended Resources and Papers     \u2022   Official Hypothesis Documentation     \u2022   QuickCheck papers: Claessen and Hughes, 2000     \u2022   Testing\u2013reduction synergy: Regehr et al. \u201cTest-case Reduction via Delta Debugging\u201d (PLDI 2012)     \u2022   \u201cHypothesis: A New Approach to Property-Based Testing\u201d (HypothesisWorks website)</p> <p>13.3 Contributing to Hypothesis</p> <p>Hypothesis is open source. If you have ideas or find issues:     \u2022   Check our GitHub repo     \u2022   Read the Contributing Guide     \u2022   Every improvement is welcomed\u2014documentation, bug reports, or code!</p> <p>\u2e3b</p> <p>Final Thoughts</p> <p>We hope this unified, comprehensive guide helps you unlock the power of Hypothesis. From quick introductions to advanced stateful testing, from performance pitfalls to internal design details, you now have a toolkit for robust property-based testing in Python.</p> <p>Happy testing! If you run into any questions, re-check the relevant sections here or visit the community resources. Once you incorporate Hypothesis into your testing workflow, you might find hidden bugs you never anticipated\u2014and that\u2019s the point!</p>"},{"location":"examples/test_wrapped_hypot_test_gen/","title":"Test wrapped hypot test gen","text":"<p>You are given one or more automatically generated Python test files that test various classes and functions. These tests may have issues such as poor naming conventions, inconsistent usage of self, lack of setUp methods, minimal docstrings, redundant or duplicate tests, and limited assertion coverage. They may also fail to leverage hypothesis and unittest.mock effectively, and might not be logically grouped.</p> <p>Your task is to produce a single, consolidated, high-quality test file from the given input files. The refactored test file should incorporate the following improvements:     1.  Consolidation and Organization     \u2022   Combine all tests from the provided files into one coherent Python test file.     \u2022   Group tests into classes that correspond logically to the functionality they are testing (e.g., separate test classes by the class or function under test).     \u2022   Within each class, order test methods logically (e.g., basic functionality first, edge cases, error handling, round-trip tests afterward).     2.  Clean, Readable Code     \u2022   Use descriptive, PEP 8-compliant class and method names.     \u2022   Add docstrings to each test class and test method, explaining their purpose and what they verify.     \u2022   Remove redundant, duplicate, or meaningless tests. Combine or refactor tests that cover the same functionality into a single, comprehensive test method when appropriate.     3.  Proper Test Fixtures     \u2022   Utilize setUp methods to instantiate commonly used objects before each test method, reducing redundancy.     \u2022   Ensure that instance methods of classes under test are called on properly instantiated objects rather than passing self incorrectly as an argument.     4.  Robust Assertions and Coverage     \u2022   Include multiple assertions in each test to thoroughly verify behavior and correctness.     \u2022   Use unittest\u2019s assertRaises for expected exceptions to validate error handling.     \u2022   Implement at least one round-trip test (e.g., encode then decode a data structure, or transform an object multiple times to ensure idempotency).     5.  Effective Use of Hypothesis     \u2022   Employ hypothesis to generate a wide range of input data, ensuring better coverage and exposing edge cases.     \u2022   Use strategies like st.builds to create complex objects (e.g., custom dataclasses) with varied attribute values.     \u2022   Enforce constraints (e.g., allow_nan=False) to avoid nonsensical test inputs.     6.  Mocking External Dependencies     \u2022   Use unittest.mock where appropriate to simulate external dependencies or environments, ensuring tests are reliable and isolated from external conditions.</p> <p>\u2e3b</p> <p>Additional Context: Getting Started with Hypothesis</p> <p>Below is a practical guide that outlines common use cases and best practices for leveraging hypothesis:     1.  Basic Usage     \u2022   Decorate test functions with @given and specify a strategy (e.g., @given(st.text())).     \u2022   Let hypothesis generate diverse test cases automatically.     2.  Common Strategies     \u2022   Use built-in strategies like st.integers(), st.floats(), st.text(), etc.     \u2022   Combine strategies with st.lists, st.builds, or st.composite to generate complex objects.     3.  Composing Tests     \u2022   Employ assume() to filter out unwanted test cases.     \u2022   Compose or build custom objects to test domain-specific logic.     4.  Advanced Features     \u2022   Fine-tune test runs with @settings (e.g., max_examples=1000).     \u2022   Create reusable strategies via @composite.     5.  Best Practices     \u2022   Keep tests focused on one property at a time.     \u2022   Use explicit examples with @example() for edge cases.     \u2022   Manage performance by choosing realistic strategy bounds.     6.  Debugging Failed Tests     \u2022   Hypothesis shows minimal failing examples and seeds to help reproduce and fix issues.</p> <p>\u2e3b</p> <p>Input Format</p> <p>TEST CODE: </p>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_test_variants_basicpy-","title":"----- test_TestGenerator_generate_test_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Test_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_test_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_test_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_errorspy-","title":"----- test_TestGenerator_write_and_verify_output_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_idempotentpy-","title":"----- test_TestGenerator_process_entities_idempotent.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestIdempotentTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_idempotent_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    result = hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n    repeat = hypot_test_gen.TestGenerator.process_entities(self=result, entities=entities, total_variants=total_variants, module_path=module_path)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_generated_output_errorspy-","title":"----- test_TestGenerator_handle_generated_output_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Generated_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), output=st.text())\ndef test_fuzz_TestGenerator_handle_generated_output(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], output: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.handle_generated_output(self=self, entity=entity, variant=variant, output=output)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_idempotentpy-","title":"----- test_TestGenerator_process_hypothesis_result_idempotent.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, strategies as st from subprocess import CompletedProcess</p> <p>class TestIdempotentTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_idempotent_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    result = hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n    repeat = hypot_test_gen.TestGenerator.process_hypothesis_result(self=result, result=result)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_extract_imports_errorspy-","title":"----- test_TestGenerator_extract_imports_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorextract_Imports(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_extract_imports(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.extract_imports(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_errorspy-","title":"----- test_ModuleParser_process_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_test_variants_errorspy-","title":"----- test_TestGenerator_generate_test_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Test_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_test_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_test_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testfixer_visit_functiondef_errorspy-","title":"----- test_TestFixer_visit_FunctionDef_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestfixervisit_Functiondef(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_TestFixer_visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.TestFixer.visit_FunctionDef(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_validationpy-","title":"----- test_TestGenerator_verify_output_dir_validation.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_get_base_name_basicpy-","title":"----- test_ModuleParser_get_base_name_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import AST from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserget_Base_Name(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), base=st.builds(AST))\ndef test_fuzz_ModuleParser_get_base_name(self, base: ast.AST) -&gt; None:\n    hypot_test_gen.ModuleParser.get_base_name(self=self, base=base)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_pythonpath_basicpy-","title":"----- test_fix_pythonpath_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzFix_Pythonpath(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(pathlib.Path))\ndef test_fuzz_fix_pythonpath(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.fix_pythonpath(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_basicpy-","title":"----- test_TestGenerator_verify_output_dir_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_environment_errorspy-","title":"----- test_TestGenerator_log_environment_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_log_environment(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.log_environment(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_prepare_environment_errorspy-","title":"----- test_TestGenerator_prepare_environment_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorprepare_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_prepare_environment(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.prepare_environment(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_remove_logger_lines_basicpy-","title":"----- test_remove_logger_lines_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzRemove_Logger_Lines(unittest.TestCase):</p> <pre><code>@given(text=st.text())\ndef test_fuzz_remove_logger_lines(self, text: str) -&gt; None:\n    hypot_test_gen.remove_logger_lines(text=text)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_construct_src_path_basicpy-","title":"----- test_construct_src_path_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzConstruct_Src_Path(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(pathlib.Path))\ndef test_fuzz_construct_src_path(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.construct_src_path(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_debug_command_output_basicpy-","title":"----- test_debug_command_output_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzDebug_Command_Output(unittest.TestCase):</p> <pre><code>@given(cmd=st.text(), stdout=st.text(), stderr=st.text(), returncode=st.integers())\ndef test_fuzz_debug_command_output(self, cmd: str, stdout: str, stderr: str, returncode: int) -&gt; None:\n    hypot_test_gen.debug_command_output(cmd=cmd, stdout=stdout, stderr=stderr, returncode=returncode)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_parse_args_basicpy-","title":"----- test_parse_args_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzParse_Args(unittest.TestCase):</p> <pre><code>@given(args=st.one_of(st.none(), st.builds(list)))\ndef test_fuzz_parse_args(self, args: typing.Optional[list]) -&gt; None:\n    hypot_test_gen.parse_args(args=args)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_get_module_contents_basicpy-","title":"----- test_TestGenerator_get_module_contents_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorget_Module_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_get_module_contents(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.get_module_contents(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_basicpy-","title":"----- test_ModuleParser_process_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_basicpy-","title":"----- test_TestGenerator_combine_and_cleanup_tests_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorcombine_And_Cleanup_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_combine_and_cleanup_tests(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_all_tests_basicpy-","title":"----- test_TestGenerator_generate_all_tests_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorgenerate_All_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_generate_all_tests(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_all_tests(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_try_generate_test_basicpy-","title":"----- test_TestGenerator_try_generate_test_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratortry_Generate_Test(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), max_retries=st.integers())\ndef test_fuzz_TestGenerator_try_generate_test(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], max_retries: int) -&gt; None:\n    hypot_test_gen.TestGenerator.try_generate_test(self=self, entity=entity, variant=variant, max_retries=max_retries)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_basicpy-","title":"----- test_ModuleParser_add_function_entity_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparseradd_Function_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.add_function_entity(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_populate_entities_basicpy-","title":"----- test_TestGenerator_populate_entities_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import ModuleParser from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpopulate_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), parser=st.builds(ModuleParser), module_path=st.text())\ndef test_fuzz_TestGenerator_populate_entities(self, parser: hypot_test_gen.ModuleParser, module_path: str) -&gt; None:\n    hypot_test_gen.TestGenerator.populate_entities(self=self, parser=parser, module_path=module_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_leading_zeros_basicpy-","title":"----- test_fix_leading_zeros_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzFix_Leading_Zeros(unittest.TestCase):</p> <pre><code>@given(test_code=st.text())\ndef test_fuzz_fix_leading_zeros(self, test_code: str) -&gt; None:\n    hypot_test_gen.fix_leading_zeros(test_code=test_code)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_construct_module_path_errorspy-","title":"----- test_TestGenerator_construct_module_path_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorconstruct_Module_Path(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_construct_module_path(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.construct_module_path(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_get_module_contents_errorspy-","title":"----- test_TestGenerator_get_module_contents_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorget_Module_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_get_module_contents(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.get_module_contents(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_create_variant_basicpy-","title":"----- test_TestGenerator_create_variant_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorcreate_Variant(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), variant_type=st.text(), cmd=st.text())\ndef test_fuzz_TestGenerator_create_variant(self, variant_type: str, cmd: str) -&gt; None:\n    hypot_test_gen.TestGenerator.create_variant(self=self, variant_type=variant_type, cmd=cmd)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_run_test_generation_basicpy-","title":"----- test_run_test_generation_basic.py -----","text":"<p>import hypot_test_gen import pathlib import typing import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzRun_Test_Generation(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(typing.Union[str, pathlib.Path]))\ndef test_fuzz_run_test_generation(self, file_path: typing.Union[str, pathlib.Path]) -&gt; None:\n    hypot_test_gen.run_test_generation(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_construct_module_path_basicpy-","title":"----- test_TestGenerator_construct_module_path_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorconstruct_Module_Path(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_construct_module_path(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.construct_module_path(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_basicpy-","title":"----- test_ModuleParser_add_class_entity_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparseradd_Class_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.add_class_entity(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_extract_imports_basicpy-","title":"----- test_TestGenerator_extract_imports_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorextract_Imports(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_extract_imports(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.extract_imports(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testfixer_visit_functiondef_basicpy-","title":"----- test_TestFixer_visit_FunctionDef_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzTestfixervisit_Functiondef(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_TestFixer_visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.TestFixer.visit_FunctionDef(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_validationpy-","title":"----- test_TestGenerator_write_and_verify_output_validation.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_wrap_with_prompt_errorspy-","title":"----- test_TestGenerator_wrap_with_prompt_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorwrap_With_Prompt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), combined_test_code=st.text(), original_source_code=st.text())\ndef test_fuzz_TestGenerator_wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.wrap_with_prompt(self=self, combined_test_code=combined_test_code, original_source_code=original_source_code)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_errorspy-","title":"----- test_TestGenerator_combine_and_cleanup_tests_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorcombine_And_Cleanup_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_combine_and_cleanup_tests(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_get_base_name_errorspy-","title":"----- test_ModuleParser_get_base_name_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import AST from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserget_Base_Name(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), base=st.builds(AST))\ndef test_fuzz_ModuleParser_get_base_name(self, base: ast.AST) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.get_base_name(self=self, base=base)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_main_basicpy-","title":"----- test_main_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzMain(unittest.TestCase):</p> <pre><code>@given(args=st.one_of(st.none(), st.builds(list)))\ndef test_fuzz_main(self, args: typing.Optional[list]) -&gt; None:\n    hypot_test_gen.main(args=args)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_display_module_info_errorspy-","title":"----- test_TestGenerator_display_module_info_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratordisplay_Module_Info(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), module_path=st.text(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_display_module_info(self, module_path: str, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.display_module_info(self=self, module_path=module_path, entities=entities)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_prepare_environment_basicpy-","title":"----- test_TestGenerator_prepare_environment_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorprepare_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_prepare_environment(self) -&gt; None:\n    hypot_test_gen.TestGenerator.prepare_environment(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_try_generate_test_errorspy-","title":"----- test_TestGenerator_try_generate_test_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratortry_Generate_Test(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), max_retries=st.integers())\ndef test_fuzz_TestGenerator_try_generate_test(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], max_retries: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.try_generate_test(self=self, entity=entity, variant=variant, max_retries=max_retries)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_function_variants_errorspy-","title":"----- test_TestGenerator_generate_function_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Function_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_function_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_function_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_function_variants_basicpy-","title":"----- test_TestGenerator_generate_function_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Function_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_function_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_function_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_errorspy-","title":"----- test_ModuleParser_add_function_entity_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparseradd_Function_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.add_function_entity(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_errorspy-","title":"----- test_ModuleParser_add_class_entity_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparseradd_Class_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.add_class_entity(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_basicpy-","title":"----- test_TestGenerator_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgenerator(unittest.TestCase):</p> <pre><code>@given(output_dir=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator(self, output_dir: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator(output_dir=output_dir)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_basicpy-","title":"----- test_TestGenerator_process_entities_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_fuzz_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_determine_instance_method_errorspy-","title":"----- test_ModuleParser_determine_instance_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserdetermine_Instance_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_determine_instance_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.determine_instance_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_should_skip_method_basicpy-","title":"----- test_ModuleParser_should_skip_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparsershould_Skip_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_should_skip_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.should_skip_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_errorspy-","title":"----- test_TestGenerator_process_hypothesis_result_errors.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, reject, strategies as st from subprocess import CompletedProcess</p> <p>class TestFuzzTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_fuzz_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_binary-oppy-","title":"----- test_TestGenerator_combine_and_cleanup_tests_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationcombine_and_cleanup_tests(unittest.TestCase):     combine_and_cleanup_tests_operands = st.from_type(pathlib.Path)</p> <pre><code>@given(a=combine_and_cleanup_tests_operands, b=combine_and_cleanup_tests_operands, c=combine_and_cleanup_tests_operands)\ndef test_associative_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=b, file_path=c))\n    right = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=b), file_path=c)\n    self.assertEqual(left, right)\n\n@given(a=combine_and_cleanup_tests_operands, b=combine_and_cleanup_tests_operands)\ndef test_commutative_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a, b) -&gt; None:\n    left = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=b)\n    right = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=b, file_path=a)\n    self.assertEqual(left, right)\n\n@given(a=combine_and_cleanup_tests_operands)\ndef test_identity_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a) -&gt; None:\n    identity = PosixPath('.')\n    self.assertEqual(a, hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=identity))\n    self.assertEqual(a, hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=identity, file_path=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_entities_summary_errorspy-","title":"----- test_TestGenerator_log_entities_summary_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Entities_Summary(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_log_entities_summary(self, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.log_entities_summary(self=self, entities=entities)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_should_skip_method_errorspy-","title":"----- test_ModuleParser_should_skip_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparsershould_Skip_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_should_skip_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.should_skip_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_idempotentpy-","title":"----- test_ModuleParser_process_class_contents_idempotent.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestIdempotentModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_idempotent_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    result = hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n    repeat = hypot_test_gen.ModuleParser.process_class_contents(self=result, node=node)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_failed_attempt_basicpy-","title":"----- test_TestGenerator_handle_failed_attempt_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Failed_Attempt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_handle_failed_attempt(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    hypot_test_gen.TestGenerator.handle_failed_attempt(self=self, entity=entity, variant=variant, attempt=attempt)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_attempt_test_generation_basicpy-","title":"----- test_TestGenerator_attempt_test_generation_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorattempt_Test_Generation(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_attempt_test_generation(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    hypot_test_gen.TestGenerator.attempt_test_generation(self=self, entity=entity, variant=variant, attempt=attempt)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_is_known_error_basicpy-","title":"----- test_TestGenerator_is_known_error_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratoris_Known_Error(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), stderr=st.text())\ndef test_fuzz_TestGenerator_is_known_error(self, stderr: str) -&gt; None:\n    hypot_test_gen.TestGenerator.is_known_error(self=self, stderr=stderr)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_basicpy-","title":"----- test_TestGenerator_post_process_test_content_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_binary-oppy-","title":"----- test_ModuleParser_add_function_entity_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_function_entity(unittest.TestCase):     add_function_entity_operands = st.builds(FunctionDef)</p> <pre><code>@given(a=add_function_entity_operands, b=add_function_entity_operands, c=add_function_entity_operands)\ndef test_associative_binary_operation_ModuleParser_add_function_entity(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_function_entity(self=a, node=hypot_test_gen.ModuleParser.add_function_entity(self=b, node=c))\n    right = hypot_test_gen.ModuleParser.add_function_entity(self=hypot_test_gen.ModuleParser.add_function_entity(self=a, node=b), node=c)\n    self.assertEqual(left, right)\n\n@given(a=add_function_entity_operands, b=add_function_entity_operands)\ndef test_commutative_binary_operation_ModuleParser_add_function_entity(self, a, b) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_function_entity(self=a, node=b)\n    right = hypot_test_gen.ModuleParser.add_function_entity(self=b, node=a)\n    self.assertEqual(left, right)\n\n@given(a=add_function_entity_operands)\ndef test_identity_binary_operation_ModuleParser_add_function_entity(self, a) -&gt; None:\n    identity = '&lt;ast.FunctionDef object at 0x125566c20&gt;'\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_function_entity(self=a, node=identity))\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_function_entity(self=identity, node=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_create_variant_errorspy-","title":"----- test_TestGenerator_create_variant_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorcreate_Variant(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), variant_type=st.text(), cmd=st.text())\ndef test_fuzz_TestGenerator_create_variant(self, variant_type: str, cmd: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.create_variant(self=self, variant_type=variant_type, cmd=cmd)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_run_hypothesis_write_basicpy-","title":"----- test_TestGenerator_run_hypothesis_write_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorrun_Hypothesis_Write(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), command=st.text())\ndef test_fuzz_TestGenerator_run_hypothesis_write(self, command: str) -&gt; None:\n    hypot_test_gen.TestGenerator.run_hypothesis_write(self=self, command=command)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_determine_instance_method_basicpy-","title":"----- test_ModuleParser_determine_instance_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserdetermine_Instance_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_determine_instance_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.determine_instance_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_display_module_info_basicpy-","title":"----- test_TestGenerator_display_module_info_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratordisplay_Module_Info(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), module_path=st.text(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_display_module_info(self, module_path: str, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    hypot_test_gen.TestGenerator.display_module_info(self=self, module_path=module_path, entities=entities)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_basicpy-","title":"----- test_ModuleParser_process_class_contents_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_parse_ast_basicpy-","title":"----- test_TestGenerator_parse_ast_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorparse_Ast(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_parse_ast(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.parse_ast(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_errorspy-","title":"----- test_TestGenerator_process_entities_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_fuzz_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_store_class_bases_errorspy-","title":"----- test_ModuleParser_store_class_bases_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserstore_Class_Bases(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.store_class_bases(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_run_hypothesis_write_errorspy-","title":"----- test_TestGenerator_run_hypothesis_write_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorrun_Hypothesis_Write(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), command=st.text())\ndef test_fuzz_TestGenerator_run_hypothesis_write(self, command: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.run_hypothesis_write(self=self, command=command)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_wrap_with_prompt_basicpy-","title":"----- test_TestGenerator_wrap_with_prompt_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorwrap_With_Prompt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), combined_test_code=st.text(), original_source_code=st.text())\ndef test_fuzz_TestGenerator_wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; None:\n    hypot_test_gen.TestGenerator.wrap_with_prompt(self=self, combined_test_code=combined_test_code, original_source_code=original_source_code)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_pre_run_cleanup_errorspy-","title":"----- test_TestGenerator_pre_run_cleanup_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpre_Run_Cleanup(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_pre_run_cleanup(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.pre_run_cleanup(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testableentity_basicpy-","title":"----- test_TestableEntity_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestableentity(unittest.TestCase):</p> <pre><code>@given(name=st.text(), module_path=st.text(), entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), parent_class=st.one_of(st.none(), st.text()))\ndef test_fuzz_TestableEntity(self, name: str, module_path: str, entity_type, parent_class: typing.Optional[str]) -&gt; None:\n    hypot_test_gen.TestableEntity(name=name, module_path=module_path, entity_type=entity_type, parent_class=parent_class)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_basicpy-","title":"----- test_TestGenerator_write_and_verify_output_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_duplicate_self_basicpy-","title":"----- test_fix_duplicate_self_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzFix_Duplicate_Self(unittest.TestCase):</p> <pre><code>@given(test_content=st.text())\ndef test_fuzz_fix_duplicate_self(self, test_content: str) -&gt; None:\n    hypot_test_gen.fix_duplicate_self(test_content=test_content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_is_known_error_errorspy-","title":"----- test_TestGenerator_is_known_error_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratoris_Known_Error(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), stderr=st.text())\ndef test_fuzz_TestGenerator_is_known_error(self, stderr: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.is_known_error(self=self, stderr=stderr)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_errorspy-","title":"----- test_ModuleParser_process_class_contents_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_all_tests_errorspy-","title":"----- test_TestGenerator_generate_all_tests_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorgenerate_All_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_generate_all_tests(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_all_tests(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_errorspy-","title":"----- test_TestGenerator_post_process_test_content_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_binary-oppy-","title":"----- test_ModuleParser_add_class_entity_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_class_entity(unittest.TestCase):     add_class_entity_operands = st.builds(ClassDef)</p> <pre><code>@given(a=add_class_entity_operands, b=add_class_entity_operands, c=add_class_entity_operands)\ndef test_associative_binary_operation_ModuleParser_add_class_entity(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_class_entity(self=a, node=hypot_test_gen.ModuleParser.add_class_entity(self=b, node=c))\n    right = hypot_test_gen.ModuleParser.add_class_entity(self=hypot_test_gen.ModuleParser.add_class_entity(self=a, node=b), node=c)\n    self.assertEqual(left, right)\n\n@given(a=add_class_entity_operands, b=add_class_entity_operands)\ndef test_commutative_binary_operation_ModuleParser_add_class_entity(self, a, b) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_class_entity(self=a, node=b)\n    right = hypot_test_gen.ModuleParser.add_class_entity(self=b, node=a)\n    self.assertEqual(left, right)\n\n@given(a=add_class_entity_operands)\ndef test_identity_binary_operation_ModuleParser_add_class_entity(self, a) -&gt; None:\n    identity = '&lt;ast.ClassDef object at 0x125deec50&gt;'\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_class_entity(self=a, node=identity))\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_class_entity(self=identity, node=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_errorspy-","title":"----- test_TestGenerator_verify_output_dir_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_generated_output_basicpy-","title":"----- test_TestGenerator_handle_generated_output_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Generated_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), output=st.text())\ndef test_fuzz_TestGenerator_handle_generated_output(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], output: str) -&gt; None:\n    hypot_test_gen.TestGenerator.handle_generated_output(self=self, entity=entity, variant=variant, output=output)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_idempotentpy-","title":"----- test_ModuleParser_process_method_idempotent.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestIdempotentModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_idempotent_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    result = hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n    repeat = hypot_test_gen.ModuleParser.process_method(self=result, node=node)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_basicpy-","title":"----- test_TestGenerator_process_hypothesis_result_basic.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, strategies as st from subprocess import CompletedProcess</p> <p>class TestFuzzTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_fuzz_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_method_variants_basicpy-","title":"----- test_TestGenerator_generate_method_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Method_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_method_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_method_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_idempotentpy-","title":"----- test_TestGenerator_post_process_test_content_idempotent.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestIdempotentTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_idempotent_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    result = hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n    repeat = hypot_test_gen.TestGenerator.post_process_test_content(self=result, content=content)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_add_to_sys_path_basicpy-","title":"----- test_add_to_sys_path_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzAdd_To_Sys_Path(unittest.TestCase):</p> <pre><code>@given(path=st.text(), description=st.text())\ndef test_fuzz_add_to_sys_path(self, path: str, description: str) -&gt; None:\n    hypot_test_gen.add_to_sys_path(path=path, description=description)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_method_variants_errorspy-","title":"----- test_TestGenerator_generate_method_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Method_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_method_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_method_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_parse_ast_errorspy-","title":"----- test_TestGenerator_parse_ast_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorparse_Ast(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_parse_ast(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.parse_ast(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_store_class_bases_basicpy-","title":"----- test_ModuleParser_store_class_bases_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserstore_Class_Bases(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.store_class_bases(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_entities_summary_basicpy-","title":"----- test_TestGenerator_log_entities_summary_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Entities_Summary(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_log_entities_summary(self, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    hypot_test_gen.TestGenerator.log_entities_summary(self=self, entities=entities)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_failed_attempt_errorspy-","title":"----- test_TestGenerator_handle_failed_attempt_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Failed_Attempt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_handle_failed_attempt(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.handle_failed_attempt(self=self, entity=entity, variant=variant, attempt=attempt)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_add_to_sys_path_binary-oppy-","title":"----- test_add_to_sys_path_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_to_sys_path(unittest.TestCase):     add_to_sys_path_operands = st.text()</p> <pre><code>@given(a=add_to_sys_path_operands, b=add_to_sys_path_operands, c=add_to_sys_path_operands)\ndef test_associative_binary_operation_add_to_sys_path(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.add_to_sys_path(path=a, description=hypot_test_gen.add_to_sys_path(path=b, description=c))\n    right = hypot_test_gen.add_to_sys_path(path=hypot_test_gen.add_to_sys_path(path=a, description=b), description=c)\n    self.assertEqual(left, right)\n\n@given(a=add_to_sys_path_operands, b=add_to_sys_path_operands)\ndef test_commutative_binary_operation_add_to_sys_path(self, a, b) -&gt; None:\n    left = hypot_test_gen.add_to_sys_path(path=a, description=b)\n    right = hypot_test_gen.add_to_sys_path(path=b, description=a)\n    self.assertEqual(left, right)\n\n@given(a=add_to_sys_path_operands)\ndef test_identity_binary_operation_add_to_sys_path(self, a) -&gt; None:\n    identity = ''\n    self.assertEqual(a, hypot_test_gen.add_to_sys_path(path=a, description=identity))\n    self.assertEqual(a, hypot_test_gen.add_to_sys_path(path=identity, description=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_pre_run_cleanup_basicpy-","title":"----- test_TestGenerator_pre_run_cleanup_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpre_Run_Cleanup(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_pre_run_cleanup(self) -&gt; None:\n    hypot_test_gen.TestGenerator.pre_run_cleanup(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_populate_entities_errorspy-","title":"----- test_TestGenerator_populate_entities_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import ModuleParser from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpopulate_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), parser=st.builds(ModuleParser), module_path=st.text())\ndef test_fuzz_TestGenerator_populate_entities(self, parser: hypot_test_gen.ModuleParser, module_path: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.populate_entities(self=self, parser=parser, module_path=module_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_environment_basicpy-","title":"----- test_TestGenerator_log_environment_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_log_environment(self) -&gt; None:\n    hypot_test_gen.TestGenerator.log_environment(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_attempt_test_generation_errorspy-","title":"----- test_TestGenerator_attempt_test_generation_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorattempt_Test_Generation(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_attempt_test_generation(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.attempt_test_generation(self=self, entity=entity, variant=variant, attempt=attempt)\n    except (TypeError, ValueError):\n        reject()\n</code></pre> <p>FULL SRC CODE: import ast import logging import os import subprocess import sys import time from dataclasses import dataclass from pathlib import Path from typing import Dict, List, Optional, Tuple, Union, Literal, Any</p> <p>import snoop  # type: ignore</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#removed-unused-from-hypothesis-import-strategies-as-st","title":"Removed unused: from hypothesis import strategies as st","text":"<p>import importlib.util  # For dynamic imports</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#set-up-logging-with-file-and-console-output","title":"Set up logging with file and console output","text":"<p>log_file = \"test_generator_debug.log\" logging.basicConfig(     level=logging.DEBUG,     format=\"%(asctime)s - %(levelname)s - %(message)s\",     handlers=[logging.FileHandler(log_file), logging.StreamHandler()], ) logger = logging.getLogger(name)</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#well-assume-the-prompt_templatemd-is-in-the-same-directory-as-this-script","title":"We'll assume the prompt_template.md is in the same directory as this script","text":"<p>PROMPT_TEMPLATE_FILE = Path(file).parent / \"prompt_template.md\"</p> <p>def load_text_prompt_template() -&gt; str:     \"\"\"     Load the text prompt template from the prompt_template.md file.     \"\"\"     try:         return PROMPT_TEMPLATE_FILE.read_text(encoding=\"utf-8\")     except FileNotFoundError:         logger.error(\"prompt_template.md not found. Please ensure it is in the same directory.\")         return \"\"</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#configure-snoop-to-write-to-a-separate-debug-log","title":"Configure snoop to write to a separate debug log","text":"<p>snoop.install(out=Path(\"snoop_debug.log\"))</p> <p>def fix_leading_zeros(test_code: str) -&gt; str:     \"\"\"     Replace decimal integers with leading zeros (except a standalone \"0\") with their corrected form.     For example, \"007\" becomes \"7\" and \"-0123\" becomes \"-123\".     \"\"\"     import re     # Use a regex with negative lookbehind and lookahead to match numbers that start with one or more zeros.     # The pattern (?&lt;!\\d)(-?)0+(\\d+)(?!\\d) ensures that a minus sign is captured if present,     # and that only isolated numbers are matched.     fixed_code = re.sub(r'(?&lt;!\\d)(-?)0+(\\d+)(?!\\d)', lambda m: m.group(1) + str(int(m.group(2))), test_code)     return fixed_code</p> <p>def remove_logger_lines(text: str) -&gt; str:     \"\"\"     Remove extraneous logging lines from the generated test content.     This function filters out:       - Lines starting with a bracketed or non-bracketed timestamp (e.g. \"[2025-3-27 14:55:48,330] ...\" or \"2025-03-27 14:55:48,330 - ...\").       - Lines containing known noisy substrings such as 'real_accelerator.py:' or 'Setting ds_accelerator to'.     \"\"\"     import re     lines = text.splitlines()     filtered = []     timestamp_pattern = re.compile(r'^[?\\d{4}-\\d{1,2}-\\d{1,2}')     for line in lines:         # Skip lines matching a leading timestamp         if timestamp_pattern.match(line):             continue         # Skip lines containing known noisy substrings         if 'real_accelerator.py:' in line or 'Setting ds_accelerator to' in line:             continue         filtered.append(line)     return \"\\n\".join(filtered).strip()</p> <p>@dataclass class TestableEntity:     \"\"\"Represents a class, method, or function that can be tested\"\"\"     name: str     module_path: str     entity_type: Literal['class', 'method', 'function', 'instance_method']  # More restrictive type     parent_class: Optional[str] = None</p> <p>def fix_pythonpath(file_path: Path) -&gt; None:     \"\"\"Ensure the module being tested is in Python's path\"\"\"     parent_dir = str(file_path.parent.absolute())     add_to_sys_path(parent_dir, \"parent directory\")</p> <pre><code>if \"src\" in file_path.parts:\n    src_path = construct_src_path(file_path)\n    add_to_sys_path(src_path, \"src directory\")\n</code></pre> <p>def add_to_sys_path(path: str, description: str) -&gt; None:     \"\"\"Helper function to add a path to sys.path if not already present\"\"\"     if path not in sys.path:         sys.path.insert(0, path)         logger.debug(f\"Added {description} to sys.path: {path}\")</p> <p>def construct_src_path(file_path: Path) -&gt; str:     \"\"\"Construct the src path from the file path\"\"\"     src_index = file_path.parts.index(\"src\")     src_path = str(Path(*file_path.parts[: src_index + 1]).absolute())     return src_path</p> <p>class ModuleParser(ast.NodeVisitor):     \"\"\"AST-based parser for Python modules\"\"\"</p> <pre><code>def __init__(self):\n    self.entities: List[TestableEntity] = []\n    self.current_class: Optional[str] = None\n    self.class_bases: Dict[str, List[str]] = {}\n\ndef visit_ClassDef(self, node: ast.ClassDef) -&gt; None:\n    if node.name.startswith(\"_\"):\n        return\n    self.store_class_bases(node)\n    self.add_class_entity(node)\n    self.process_class_contents(node)\n\ndef store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Store base classes for inheritance checking\"\"\"\n    bases = []\n    for base in node.bases:\n        base_name = self.get_base_name(base)\n        if base_name:\n            bases.append(base_name)\n    self.class_bases[node.name] = bases\n    logger.debug(f\"Stored bases for class {node.name}: {bases}\")\n\ndef get_base_name(self, base: ast.AST) -&gt; Optional[str]:\n    \"\"\"Retrieve the base class name from the AST node\"\"\"\n    if isinstance(base, ast.Name):\n        return base.id\n    elif isinstance(base, ast.Attribute):\n        if isinstance(base.value, ast.Name):\n            return f\"{base.value.id}.{base.attr}\"\n    return None\n\ndef add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Add the class itself to entities\"\"\"\n    self.entities.append(TestableEntity(node.name, \"\", \"class\"))\n    logger.debug(f\"Added class entity: {node.name}\")\n\ndef process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Process the contents of the class\"\"\"\n    old_class = self.current_class\n    self.current_class = node.name\n    self.generic_visit(node)\n    self.current_class = old_class\n    logger.debug(f\"Processed contents of class {node.name}\")\n\ndef visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    if node.name.startswith(\"_\"):\n        return\n    if self.current_class:\n        self.process_method(node)\n    else:\n        self.add_function_entity(node)\n\ndef process_method(self, node: ast.FunctionDef) -&gt; None:\n    \"\"\"Process a method within a class\"\"\"\n    if self.should_skip_method(node):\n        return\n\n    is_instance_method = self.determine_instance_method(node)\n    entity_type = \"instance_method\" if is_instance_method else \"method\"\n\n    # The method path should include the class\n    method_name = f\"{self.current_class}.{node.name}\" if self.current_class else node.name\n\n    self.entities.append(\n        TestableEntity(\n            name=node.name,\n            module_path=\"\",\n            entity_type=entity_type,\n            parent_class=self.current_class,\n        )\n    )\n    logger.debug(\n        f\"Added {'instance_method' if is_instance_method else 'method'} entity: {method_name}\"\n    )\n\ndef determine_instance_method(self, node: ast.FunctionDef) -&gt; bool:\n    \"\"\"Determine if the method is an instance method\"\"\"\n    for decorator in node.decorator_list:\n        if isinstance(decorator, ast.Name) and decorator.id in {\"classmethod\", \"staticmethod\"}:\n            return False\n    return True\n\ndef should_skip_method(self, node: ast.FunctionDef) -&gt; bool:\n    \"\"\"Determine if the method should be skipped based on inheritance or naming\"\"\"\n    current_bases = self.class_bases.get(self.current_class, [])\n    if any(base in {\"NodeVisitor\", \"ast.NodeVisitor\"} for base in current_bases):\n        if node.name.startswith(\"visit_\"):\n            logger.debug(f\"Skipping inherited visit method: {node.name}\")\n            return True\n    if node.name in {\"__init__\", \"__str__\", \"__repr__\", \"property\"}:\n        logger.debug(f\"Skipping magic or property method: {node.name}\")\n        return True\n    return False\n\ndef add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    \"\"\"Add a standalone function to entities\"\"\"\n    self.entities.append(TestableEntity(node.name, \"\", \"function\"))\n    logger.debug(f\"Added function entity: {node.name}\")\n</code></pre> <p>def debug_command_output(cmd: str, stdout: str, stderr: str, returncode: int) -&gt; None:     \"\"\"Helper function to debug command execution\"\"\"     logger.debug(\"Command execution details:\")     logger.debug(f\"Command: {cmd}\")     logger.debug(f\"Return code: {returncode}\")     logger.debug(f\"stdout length: {len(stdout)}\")     logger.debug(f\"stderr length: {len(stderr)}\")     logger.debug(\"First 1000 chars of stdout:\")     logger.debug(stdout[:1000])     logger.debug(\"First 1000 chars of stderr:\")     logger.debug(stderr[:1000])</p> <p>class TestFixer(ast.NodeTransformer):     \"\"\"AST transformer to fix duplicate self parameters\"\"\"</p> <pre><code>def visit_FunctionDef(self, node: ast.FunctionDef) -&gt; ast.FunctionDef:\n    seen_self = False\n    new_args = []\n\n    for arg in node.args.args:\n        if arg.arg == 'self':\n            if not seen_self:\n                seen_self = True\n                new_args.append(arg)\n        else:\n            new_args.append(arg)\n\n    node.args.args = new_args\n    return node\n</code></pre> <p>def fix_duplicate_self(test_content: str) -&gt; Optional[str]:     \"\"\"     Fix duplicate self parameters in test content.</p> <pre><code>Args:\n    test_content: String containing the test code\n\nReturns:\n    Fixed test code string, or None if parsing fails\n\"\"\"\ntry:\n    tree = ast.parse(test_content)\n\n    fixer = TestFixer()\n    fixed_tree = fixer.visit(tree)\n\n    try:\n        return ast.unparse(fixed_tree)\n    except AttributeError:\n        import astunparse\n        return astunparse.unparse(fixed_tree)\n\nexcept Exception as e:\n    print(f\"Error fixing test content: {e}\")\n    return None\n</code></pre> <p>class TestGenerator:     \"\"\"Manages generation of Hypothesis tests for Python modules\"\"\"     def wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; str:         \"\"\"         Wrap the combined test code and original source code in the custom text prompt         read from 'prompt_template.md'.         \"\"\"         prompt_template = load_text_prompt_template()         return prompt_template.format(             TEST_CODE=combined_test_code,             FULL_SRC_CODE=original_source_code         )</p> <pre><code>def pre_run_cleanup(self) -&gt; None:\n    \"\"\"\n    Remove any leftover combined test files (matching 'test_hyp_*.py') from previous runs.\n    This ensures we don't mix old combined files with new runs.\n    \"\"\"\n    leftover_files = list(self.output_dir.glob(\"test_hyp_*.py\"))\n    for leftover in leftover_files:\n        try:\n            leftover.unlink()\n            logger.debug(f\"Removed leftover combined file: {leftover.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete leftover file {leftover.name}: {e}\")\n\ndef __init__(self, output_dir: Path = Path(\"generated_tests\")):\n    self.output_dir = output_dir\n    self.output_dir.mkdir(exist_ok=True)\n    self.verify_output_dir()\n\ndef verify_output_dir(self) -&gt; None:\n    \"\"\"Verify that the output directory exists and is writable\"\"\"\n    logger.debug(f\"Test generator initialized with output dir: {self.output_dir}\")\n    logger.debug(f\"Output dir exists: {self.output_dir.exists()}\")\n    logger.debug(f\"Output dir is writable: {os.access(self.output_dir, os.W_OK)}\")\n\ndef run_hypothesis_write(self, command: str) -&gt; Optional[str]:\n    \"\"\"Execute hypothesis write command and return output if successful\"\"\"\n    full_cmd = f\"hypothesis write {command}\"\n    logger.debug(f\"Executing hypothesis command: {full_cmd}\")\n\n    try:\n        self.log_environment()\n        env = self.prepare_environment()\n\n        result = subprocess.run(\n            full_cmd, shell=True, capture_output=True, text=True, env=env\n        )\n\n        debug_command_output(\n            full_cmd, result.stdout, result.stderr, result.returncode\n        )\n\n        return self.process_hypothesis_result(result)\n\n    except Exception as e:\n        logger.error(f\"Error running hypothesis: {e}\", exc_info=True)\n        return None\n\ndef log_environment(self) -&gt; None:\n    \"\"\"Log the current environment settings\"\"\"\n    logger.debug(f\"PYTHONPATH before modification: {os.getenv('PYTHONPATH')}\")\n    logger.debug(f\"sys.path: {sys.path}\")\n    logger.debug(f\"Current working directory: {os.getcwd()}\")\n\ndef prepare_environment(self) -&gt; Dict[str, str]:\n    \"\"\"Prepare the environment variables for subprocess\"\"\"\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = \":\".join(sys.path)\n    env.setdefault(\"PYTHONIOENCODING\", \"utf-8\")\n    return env\n\ndef process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; Optional[str]:\n    \"\"\"Process the result of the hypothesis command\"\"\"\n    if result.returncode == 0 and result.stdout:\n        content = result.stdout.strip()\n\n        # Remove extraneous logging lines first\n        content = remove_logger_lines(content)\n\n        if not content or len(content) &lt; 50:\n            logger.warning(\"Hypothesis generated insufficient content\")\n            return None\n\n        # Process and fix the test content using post_process_test_content\n        fixed_content = self.post_process_test_content(content)\n        if fixed_content is None:\n            logger.warning(\"Failed to process test content\")\n            return None\n\n        logger.info(\"Successfully generated and processed test content\")\n        return fixed_content\n\n    if result.stderr and not self.is_known_error(result.stderr):\n        logger.warning(f\"Command failed: {result.stderr}\")\n    return None\n\ndef post_process_test_content(self, content: str) -&gt; Optional[str]:\n    \"\"\"Post-process generated test content\"\"\"\n    try:\n        # Also remove extraneous logger lines (defensive)\n        content = remove_logger_lines(content)\n\n        # First, fix any leading zeros in integer literals\n        content = fix_leading_zeros(content)\n        # Then, fix duplicate self parameters\n        fixed_content = fix_duplicate_self(content)\n        if fixed_content is None:\n            logger.warning(\"Failed to fix duplicate self parameters.\")\n            return content\n        return fixed_content\n    except Exception as e:\n        logger.error(f\"Error processing test content: {e}\", exc_info=True)\n        return None\n\ndef is_known_error(self, stderr: str) -&gt; bool:\n    \"\"\"Check if the stderr contains known non-critical errors\"\"\"\n    known_errors = [\n        \"InvalidArgument: Got non-callable\",\n        \"Could not resolve\",\n        \"but it doesn't have a\",\n    ]\n    return any(msg in stderr for msg in known_errors)\n\ndef try_generate_test(\n    self, entity: TestableEntity, variant: Dict[str, str], max_retries: int = 3\n) -&gt; bool:\n    \"\"\"Attempt to generate a specific test variant with retries\"\"\"\n    for attempt in range(1, max_retries + 1):\n        logger.debug(\n            f\"Attempt {attempt} for {variant['type']} test on {entity.name}\"\n        )\n        output = self.attempt_test_generation(entity, variant, attempt)\n        if output:\n            return True\n    return False\n\ndef attempt_test_generation(\n    self, entity: TestableEntity, variant: Dict[str, str], attempt: int\n) -&gt; Optional[bool]:\n    \"\"\"Attempt a single test generation\"\"\"\n    output = self.run_hypothesis_write(variant[\"cmd\"])\n    if output:\n        return self.handle_generated_output(entity, variant, output)\n    else:\n        return self.handle_failed_attempt(entity, variant, attempt)\n\ndef handle_generated_output(\n    self, entity: TestableEntity, variant: Dict[str, str], output: str\n) -&gt; bool:\n    \"\"\"Handle the output from a successful hypothesis generation\"\"\"\n    name_prefix = (\n        f\"{entity.parent_class}_{entity.name}\"\n        if entity.parent_class\n        else entity.name\n    )\n    output_file = self.output_dir / f\"test_{name_prefix}_{variant['type']}.py\"\n\n    try:\n        self.write_and_verify_output(output_file, output)\n        logger.info(f\"Successfully generated test at {output_file}\")\n        print(f\"Generated {variant['type']} test: {output_file}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error writing test file: {e}\", exc_info=True)\n        return False\n\ndef write_and_verify_output(self, output_file: Path, content: str) -&gt; None:\n    \"\"\"Write the test content to a file and verify its integrity\"\"\"\n    logger.debug(\"Test content details:\")\n    logger.debug(f\"Content length: {len(content)}\")\n    logger.debug(f\"Content preview:\\n{content[:1000]}\")\n    logger.debug(f\"Writing to file: {output_file}\")\n\n    output_file.write_text(content)\n\n    written_content = output_file.read_text()\n    if not written_content:\n        logger.error(f\"File {output_file} is empty after writing!\")\n        raise ValueError(f\"Empty file: {output_file}\")\n\n    if written_content != content:\n        logger.error(\"Written content doesn't match original content!\")\n        logger.debug(f\"Original length: {len(content)}\")\n        logger.debug(f\"Written length: {len(written_content)}\")\n        raise ValueError(\"Content mismatch after writing\")\n\n    logger.debug(f\"Final file size: {output_file.stat().st_size} bytes\")\n\ndef handle_failed_attempt(\n    self, entity: TestableEntity, variant: Dict[str, str], attempt: int\n) -&gt; Optional[bool]:\n    \"\"\"Handle a failed test generation attempt\"\"\"\n    if attempt &lt; 3:\n        logger.warning(f\"Attempt {attempt} failed, retrying...\")\n        time.sleep(1)\n    else:\n        logger.error(f\"All attempts failed for {entity.name}\")\n    return None\n\ndef get_module_contents(self, file_path: Path) -&gt; Tuple[str, List[TestableEntity]]:\n    \"\"\"Extract module path and testable entities using AST parsing\"\"\"\n    logger.debug(f\"Reading file: {file_path}\")\n    try:\n        module_path = self.construct_module_path(file_path)\n        content = file_path.read_text()\n        parser = self.parse_ast(content)\n        imports = self.extract_imports(content)\n\n        entities = self.populate_entities(parser, module_path)\n        self.log_entities_summary(entities)\n        return module_path, entities\n\n    except Exception as e:\n        logger.error(f\"Error parsing module contents: {e}\", exc_info=True)\n        raise\n\ndef construct_module_path(self, file_path: Path) -&gt; str:\n    \"\"\"Construct the module path from the file path\"\"\"\n    parts = file_path.parts\n    if \"src\" in parts:\n        src_index = parts.index(\"src\")\n        module_parts = list(parts[src_index + 1 :])\n    else:\n        module_parts = [file_path.stem]\n    module_path = \".\".join([p.replace(\".py\", \"\") for p in module_parts])\n    logger.debug(f\"Constructed module path: {module_path}\")\n    return module_path\n\ndef parse_ast(self, content: str) -&gt; ModuleParser:\n    \"\"\"Parse the AST of the given content\"\"\"\n    tree = ast.parse(content)\n    parser = ModuleParser()\n    parser.visit(tree)\n    return parser\n\ndef extract_imports(self, content: str) -&gt; set:\n    \"\"\"Extract import statements from the content\"\"\"\n    tree = ast.parse(content)\n    imports = set()\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Import):\n            for name in node.names:\n                imports.add(name.name)\n        elif isinstance(node, ast.ImportFrom):\n            if node.module:\n                imports.add(node.module)\n    logger.debug(f\"Found imports: {imports}\")\n    return imports\n\ndef populate_entities(self, parser: ModuleParser, module_path: str) -&gt; List[TestableEntity]:\n    \"\"\"Populate entities with correct module paths\"\"\"\n    entities = []\n    for entity in parser.entities:\n        entity.module_path = module_path\n        entities.append(entity)\n    return entities\n\ndef log_entities_summary(self, entities: List[TestableEntity]) -&gt; None:\n    \"\"\"Log a summary of found entities\"\"\"\n    classes = sum(1 for e in entities if e.entity_type == \"class\")\n    methods = sum(\n        1 for e in entities if e.entity_type in {\"method\", \"instance_method\"}\n    )\n    functions = sum(1 for e in entities if e.entity_type == \"function\")\n    logger.info(\n        f\"Found {classes} classes, {methods} methods, and {functions} functions\"\n    )\n\ndef generate_all_tests(self, file_path: Path) -&gt; None:\n    \"\"\"Generate all possible test variants for a Python file\"\"\"\n    logger.info(f\"Generating tests for file: {file_path}\")\n    try:\n        fix_pythonpath(file_path)\n        module_path, entities = self.get_module_contents(file_path)\n        self.display_module_info(module_path, entities)\n        total_variants = sum(len(self.generate_test_variants(e)) for e in entities)\n        self.process_entities(entities, total_variants, module_path)\n        print()\n        self.combine_and_cleanup_tests(file_path)\n    except Exception:\n        logger.error(\"Test generation failed\", exc_info=True)\n        raise\n\ndef display_module_info(self, module_path: str, entities: List[TestableEntity]) -&gt; None:\n    \"\"\"Display information about the module and its entities\"\"\"\n    print(f\"\\nProcessing module: {module_path}\")\n    print(\n        f\"Found {len([e for e in entities if e.entity_type == 'class'])} classes, \"\n        f\"{len([e for e in entities if e.entity_type in {'method', 'instance_method'}])} methods, and \"\n        f\"{len([e for e in entities if e.entity_type == 'function'])} functions\"\n    )\n\ndef process_entities(self, entities: List[TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    \"\"\"Process each entity and generate tests\"\"\"\n    current = 0\n    for entity in entities:\n        print(f\"\\nGenerating tests for: {module_path}.{entity.name}\")\n        variants = self.generate_test_variants(entity)\n        for variant in variants:\n            current += 1\n            print(f\"\\rGenerating tests: [{current}/{total_variants}]\", end=\"\")\n            self.try_generate_test(entity, variant)\n    print()\n\ndef _get_object(self, path: str) -&gt; Optional[Any]:\n    \"\"\"Get the actual object from its module path\"\"\"\n    try:\n        module_parts = path.split('.')\n        module_path = '.'.join(module_parts[:-1])\n        obj_name = module_parts[-1]\n\n        spec = importlib.util.find_spec(module_path)\n        if spec and spec.loader:\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            return getattr(module, obj_name, None)\n    except Exception:\n        return None\n\ndef generate_method_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate test variants for methods and instance methods\"\"\"\n    if entity.entity_type in {\"method\", \"instance_method\"}:\n        method_path = f\"{entity.module_path}.{entity.parent_class}.{entity.name}\"\n    else:\n        method_path = f\"{entity.module_path}.{entity.name}\"\n\n    # Start with basic test with type inference\n    variants = [\n        self.create_variant(\n            \"basic\",\n            f\"--style=unittest --annotate {method_path}\"\n        )\n    ]\n\n    # Add error variant\n    variants.append(\n        self.create_variant(\n            \"errors\",\n            f\"--style=unittest --annotate --except ValueError --except TypeError {method_path}\"\n        )\n    )\n\n    # Add special variants based on method name\n    name = entity.name.lower()\n    variants.extend(self._generate_special_variants(name, method_path))\n\n    return variants\n\ndef _generate_special_variants(self, name: str, method_path: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate special variants based on method name\"\"\"\n    special_variants = []\n\n    if any(x in name for x in [\"transform\", \"convert\", \"process\", \"format\"]):\n        special_variants.append(\n            self.create_variant(\n                \"idempotent\",\n                f\"--style=unittest --annotate --idempotent {method_path}\"\n            )\n        )\n\n    if any(x in name for x in [\"validate\", \"verify\", \"check\", \"assert\"]):\n        special_variants.append(\n            self.create_variant(\n                \"validation\",\n                f\"--style=unittest --annotate --errors-equivalent {method_path}\"\n            )\n        )\n\n    if \"encode\" in name or \"decode\" in name:\n        special_variants.append(\n            self.create_variant(\n                \"roundtrip\",\n                f\"--style=unittest --annotate --roundtrip {method_path}\"\n            )\n        )\n\n    if any(x in name for x in [\"add\", \"multiply\", \"subtract\", \"combine\", \"merge\"]):\n        special_variants.append(\n            self.create_variant(\n                \"binary-op\",\n                f\"--style=unittest --annotate --binary-op {method_path}\"\n            )\n        )\n\n    return special_variants\n\ndef generate_function_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate test variants for standalone functions\"\"\"\n    base_cmd = f\"--style=unittest --annotate {entity.module_path}.{entity.name}\"\n    variants = [self.create_variant(\"basic\", base_cmd)]\n\n    # Add special variants for functions if needed\n    name = entity.name.lower()\n    if \"encode\" in name or \"decode\" in name or \"serialize\" in name or \"deserialize\" in name:\n        variants.append(self.create_variant(\"roundtrip\", f\"{base_cmd} --roundtrip\"))\n    elif any(x in name for x in [\"add\", \"sub\", \"mul\", \"combine\", \"merge\"]):\n        variants.append(self.create_variant(\"binary-op\", f\"{base_cmd} --binary-op\"))\n\n    return variants\n\ndef generate_test_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate all applicable test variants for an entity\"\"\"\n    variants = []\n    if entity.entity_type == \"class\":\n        # For classes, just a basic annotated variant\n        variants.append(self.create_variant(\"basic\", f\"--style=unittest --annotate {entity.module_path}.{entity.name}\"))\n    elif entity.entity_type in {\"method\", \"instance_method\"}:\n        variants.extend(self.generate_method_variants(entity))\n    else:\n        variants.extend(self.generate_function_variants(entity))\n    logger.debug(f\"Generated variants for {entity.name}: {[v['type'] for v in variants]}\")\n    return variants\n\ndef create_variant(self, variant_type: str, cmd: str) -&gt; Dict[str, str]:\n    \"\"\"Create a test variant dictionary with properly formatted command\"\"\"\n    return {\n        \"type\": variant_type,\n        \"cmd\": cmd.strip()  # Ensure no extra whitespace in command\n    }\n\ndef combine_and_cleanup_tests(self, file_path: Path) -&gt; None:\n    \"\"\"\n    Combines individual test files into a single file and deletes the originals,\n    then removes the combined .py file so only the final markdown remains.\n\n    Args:\n        file_path (Path): The original Python file used for test generation.\n    \"\"\"\n    # Step 1: Derive the combined file name from the original file\n    original_stem = file_path.stem  # e.g., \"my_module\"\n    combined_filename = f\"test_hyp_{original_stem}.py\"\n    combined_filepath = self.output_dir / combined_filename\n\n    # Step 2: Collect all generated test files in the output directory\n    # Using \"test_*.py\" so that it naturally ignores any leftover .md files\n    test_files = list(self.output_dir.glob(\"test_*.py\"))\n\n    # Step 3: Combine contents of each test file into a single string\n    combined_content = \"\"\n    for test_file in test_files:\n        content = test_file.read_text()\n        separator = f\"\\n# ----- {test_file.name} -----\\n\"\n        combined_content += separator + content + \"\\n\"\n\n    # Step 4: Write the combined content to the new file\n    combined_filepath.write_text(combined_content)\n\n    # Step 5: Wrap the combined test code with the prompt to produce the final Markdown\n    original_source_code = file_path.read_text()\n    final_wrapped_content = self.wrap_with_prompt(combined_content, original_source_code)\n    final_wrapped_file = self.output_dir / f\"test_wrapped_{original_stem}.md\"\n    final_wrapped_file.write_text(final_wrapped_content)\n    logger.info(f\"Final wrapped test file created at {final_wrapped_file}\")\n\n    # Optional: verify the combined file\n    if not combined_filepath.exists() or len(combined_filepath.read_text()) &lt; 50:\n        logger.error(f\"Combined test file {combined_filepath} appears to be incomplete.\")\n        return\n\n    # Step 6: Cleanup - delete individual test files\n    for test_file in test_files:\n        try:\n            test_file.unlink()\n            logger.debug(f\"Deleted individual test file: {test_file.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete {test_file.name}: {e}\")\n\n    # Step 7: Logging and feedback\n    logger.info(f\"Combined {len(test_files)} test files into {combined_filename} and removed originals.\")\n\n    # Step 8: Apply Ruff cleaning commands to the combined file\n    cmds = [\n        f\"ruff check {combined_filepath}\",\n        f\"ruff check --fix {combined_filepath}\",\n        f\"ruff format {combined_filepath}\",\n        f\"ruff check --select I --fix {combined_filepath}\",\n        f\"ruff format {combined_filepath}\"\n    ]\n    for cmd in cmds:\n        try:\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n            if result.returncode != 0:\n                logger.error(f\"Ruff command '{cmd}' failed: {result.stderr}\")\n            else:\n                logger.info(f\"Ruff command '{cmd}' succeeded: {result.stdout}\")\n        except Exception as e:\n            logger.error(f\"Failed to run ruff command '{cmd}': {e}\")\n\n    # Finally, remove the combined .py file so only the markdown remains\n    if combined_filepath.exists():\n        combined_filepath.unlink()\n        logger.info(f\"Deleted the combined file {combined_filepath} so that only the Markdown file remains.\")\n</code></pre> <p>def parse_args(args: Optional[list] = None) -&gt; Path:     \"\"\"     Parse command line arguments and validate file path.</p> <pre><code>Args:\n    args: Optional list of command line arguments. If None, uses sys.argv[1:]\n\nReturns:\n    Path object for the input file\n\nRaises:\n    ValueError: If arguments are invalid or file doesn't exist\n\"\"\"\nif args is None:\n    args = sys.argv[1:]\n\nif len(args) != 1:\n    raise ValueError(\"Exactly one argument (path to Python file) required\")\n\nfile_path = Path(args[0])\nif not file_path.exists() or not file_path.is_file():\n    raise ValueError(f\"File does not exist or is not a file: {file_path}\")\n\nreturn file_path\n</code></pre> <p>def run_test_generation(file_path: Union[str, Path]) -&gt; bool:     \"\"\"     Run the test generation process for a given file.     Now also calls pre_run_cleanup before generate_all_tests.</p> <pre><code>Args:\n    file_path: Path to the Python file to generate tests for\n\nReturns:\n    bool: True if test generation was successful, False otherwise\n\nRaises:\n    Exception: If test generation fails\n\"\"\"\ntry:\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    logger.info(f\"Starting test generator for {file_path}\")\n    generator = TestGenerator()\n\n    # Clean up any leftover combined files from prior runs\n    generator.pre_run_cleanup()\n\n    # Proceed with the standard generation workflow\n    generator.generate_all_tests(file_path)\n    return True\n\nexcept Exception as e:\n    logger.error(f\"Test generation failed: {e}\", exc_info=True)\n    return False\n</code></pre> <p>def main(args: Optional[list] = None) -&gt; int:     \"\"\"     Main entry point for the test generator script.</p> <pre><code>Args:\n    args: Optional list of command line arguments. If None, uses sys.argv[1:]\n\nReturns:\n    int: Exit code (0 for success, 1 for failure)\n\"\"\"\ntry:\n    file_path = parse_args(args)\n    success = run_test_generation(file_path)\n    return 0 if success else 1\n\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    logger.error(f\"Invalid arguments: {e}\")\n    print(\"Usage: python test_generator.py &lt;path_to_python_file&gt;\")\n    return 1\n\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n    logger.error(\"Unexpected error during execution\", exc_info=True)\n    return 1\n</code></pre> <p>if name == \"main\":     sys.exit(main())</p> <p>Where:     \u2022   </p>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_test_variants_basicpy-_1","title":"----- test_TestGenerator_generate_test_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Test_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_test_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_test_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_errorspy-_1","title":"----- test_TestGenerator_write_and_verify_output_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_idempotentpy-_1","title":"----- test_TestGenerator_process_entities_idempotent.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestIdempotentTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_idempotent_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    result = hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n    repeat = hypot_test_gen.TestGenerator.process_entities(self=result, entities=entities, total_variants=total_variants, module_path=module_path)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_generated_output_errorspy-_1","title":"----- test_TestGenerator_handle_generated_output_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Generated_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), output=st.text())\ndef test_fuzz_TestGenerator_handle_generated_output(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], output: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.handle_generated_output(self=self, entity=entity, variant=variant, output=output)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_idempotentpy-_1","title":"----- test_TestGenerator_process_hypothesis_result_idempotent.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, strategies as st from subprocess import CompletedProcess</p> <p>class TestIdempotentTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_idempotent_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    result = hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n    repeat = hypot_test_gen.TestGenerator.process_hypothesis_result(self=result, result=result)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_extract_imports_errorspy-_1","title":"----- test_TestGenerator_extract_imports_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorextract_Imports(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_extract_imports(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.extract_imports(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_errorspy-_1","title":"----- test_ModuleParser_process_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_test_variants_errorspy-_1","title":"----- test_TestGenerator_generate_test_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Test_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_test_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_test_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testfixer_visit_functiondef_errorspy-_1","title":"----- test_TestFixer_visit_FunctionDef_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestfixervisit_Functiondef(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_TestFixer_visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.TestFixer.visit_FunctionDef(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_validationpy-_1","title":"----- test_TestGenerator_verify_output_dir_validation.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_get_base_name_basicpy-_1","title":"----- test_ModuleParser_get_base_name_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import AST from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserget_Base_Name(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), base=st.builds(AST))\ndef test_fuzz_ModuleParser_get_base_name(self, base: ast.AST) -&gt; None:\n    hypot_test_gen.ModuleParser.get_base_name(self=self, base=base)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_pythonpath_basicpy-_1","title":"----- test_fix_pythonpath_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzFix_Pythonpath(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(pathlib.Path))\ndef test_fuzz_fix_pythonpath(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.fix_pythonpath(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_basicpy-_1","title":"----- test_TestGenerator_verify_output_dir_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_environment_errorspy-_1","title":"----- test_TestGenerator_log_environment_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_log_environment(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.log_environment(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_prepare_environment_errorspy-_1","title":"----- test_TestGenerator_prepare_environment_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorprepare_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_prepare_environment(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.prepare_environment(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_remove_logger_lines_basicpy-_1","title":"----- test_remove_logger_lines_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzRemove_Logger_Lines(unittest.TestCase):</p> <pre><code>@given(text=st.text())\ndef test_fuzz_remove_logger_lines(self, text: str) -&gt; None:\n    hypot_test_gen.remove_logger_lines(text=text)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_construct_src_path_basicpy-_1","title":"----- test_construct_src_path_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzConstruct_Src_Path(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(pathlib.Path))\ndef test_fuzz_construct_src_path(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.construct_src_path(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_debug_command_output_basicpy-_1","title":"----- test_debug_command_output_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzDebug_Command_Output(unittest.TestCase):</p> <pre><code>@given(cmd=st.text(), stdout=st.text(), stderr=st.text(), returncode=st.integers())\ndef test_fuzz_debug_command_output(self, cmd: str, stdout: str, stderr: str, returncode: int) -&gt; None:\n    hypot_test_gen.debug_command_output(cmd=cmd, stdout=stdout, stderr=stderr, returncode=returncode)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_parse_args_basicpy-_1","title":"----- test_parse_args_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzParse_Args(unittest.TestCase):</p> <pre><code>@given(args=st.one_of(st.none(), st.builds(list)))\ndef test_fuzz_parse_args(self, args: typing.Optional[list]) -&gt; None:\n    hypot_test_gen.parse_args(args=args)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_get_module_contents_basicpy-_1","title":"----- test_TestGenerator_get_module_contents_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorget_Module_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_get_module_contents(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.get_module_contents(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_basicpy-_1","title":"----- test_ModuleParser_process_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_basicpy-_1","title":"----- test_TestGenerator_combine_and_cleanup_tests_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorcombine_And_Cleanup_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_combine_and_cleanup_tests(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_all_tests_basicpy-_1","title":"----- test_TestGenerator_generate_all_tests_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorgenerate_All_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_generate_all_tests(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_all_tests(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_try_generate_test_basicpy-_1","title":"----- test_TestGenerator_try_generate_test_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratortry_Generate_Test(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), max_retries=st.integers())\ndef test_fuzz_TestGenerator_try_generate_test(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], max_retries: int) -&gt; None:\n    hypot_test_gen.TestGenerator.try_generate_test(self=self, entity=entity, variant=variant, max_retries=max_retries)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_basicpy-_1","title":"----- test_ModuleParser_add_function_entity_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparseradd_Function_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.add_function_entity(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_populate_entities_basicpy-_1","title":"----- test_TestGenerator_populate_entities_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import ModuleParser from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpopulate_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), parser=st.builds(ModuleParser), module_path=st.text())\ndef test_fuzz_TestGenerator_populate_entities(self, parser: hypot_test_gen.ModuleParser, module_path: str) -&gt; None:\n    hypot_test_gen.TestGenerator.populate_entities(self=self, parser=parser, module_path=module_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_leading_zeros_basicpy-_1","title":"----- test_fix_leading_zeros_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzFix_Leading_Zeros(unittest.TestCase):</p> <pre><code>@given(test_code=st.text())\ndef test_fuzz_fix_leading_zeros(self, test_code: str) -&gt; None:\n    hypot_test_gen.fix_leading_zeros(test_code=test_code)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_construct_module_path_errorspy-_1","title":"----- test_TestGenerator_construct_module_path_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorconstruct_Module_Path(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_construct_module_path(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.construct_module_path(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_get_module_contents_errorspy-_1","title":"----- test_TestGenerator_get_module_contents_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorget_Module_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_get_module_contents(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.get_module_contents(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_create_variant_basicpy-_1","title":"----- test_TestGenerator_create_variant_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorcreate_Variant(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), variant_type=st.text(), cmd=st.text())\ndef test_fuzz_TestGenerator_create_variant(self, variant_type: str, cmd: str) -&gt; None:\n    hypot_test_gen.TestGenerator.create_variant(self=self, variant_type=variant_type, cmd=cmd)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_run_test_generation_basicpy-_1","title":"----- test_run_test_generation_basic.py -----","text":"<p>import hypot_test_gen import pathlib import typing import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzRun_Test_Generation(unittest.TestCase):</p> <pre><code>@given(file_path=st.from_type(typing.Union[str, pathlib.Path]))\ndef test_fuzz_run_test_generation(self, file_path: typing.Union[str, pathlib.Path]) -&gt; None:\n    hypot_test_gen.run_test_generation(file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_construct_module_path_basicpy-_1","title":"----- test_TestGenerator_construct_module_path_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorconstruct_Module_Path(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_construct_module_path(self, file_path: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator.construct_module_path(self=self, file_path=file_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_basicpy-_1","title":"----- test_ModuleParser_add_class_entity_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparseradd_Class_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.add_class_entity(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_extract_imports_basicpy-_1","title":"----- test_TestGenerator_extract_imports_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorextract_Imports(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_extract_imports(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.extract_imports(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testfixer_visit_functiondef_basicpy-_1","title":"----- test_TestFixer_visit_FunctionDef_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzTestfixervisit_Functiondef(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_TestFixer_visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.TestFixer.visit_FunctionDef(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_validationpy-_1","title":"----- test_TestGenerator_write_and_verify_output_validation.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_wrap_with_prompt_errorspy-_1","title":"----- test_TestGenerator_wrap_with_prompt_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorwrap_With_Prompt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), combined_test_code=st.text(), original_source_code=st.text())\ndef test_fuzz_TestGenerator_wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.wrap_with_prompt(self=self, combined_test_code=combined_test_code, original_source_code=original_source_code)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_errorspy-_1","title":"----- test_TestGenerator_combine_and_cleanup_tests_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorcombine_And_Cleanup_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_combine_and_cleanup_tests(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_get_base_name_errorspy-_1","title":"----- test_ModuleParser_get_base_name_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import AST from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserget_Base_Name(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), base=st.builds(AST))\ndef test_fuzz_ModuleParser_get_base_name(self, base: ast.AST) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.get_base_name(self=self, base=base)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_main_basicpy-_1","title":"----- test_main_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzMain(unittest.TestCase):</p> <pre><code>@given(args=st.one_of(st.none(), st.builds(list)))\ndef test_fuzz_main(self, args: typing.Optional[list]) -&gt; None:\n    hypot_test_gen.main(args=args)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_display_module_info_errorspy-_1","title":"----- test_TestGenerator_display_module_info_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratordisplay_Module_Info(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), module_path=st.text(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_display_module_info(self, module_path: str, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.display_module_info(self=self, module_path=module_path, entities=entities)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_prepare_environment_basicpy-_1","title":"----- test_TestGenerator_prepare_environment_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorprepare_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_prepare_environment(self) -&gt; None:\n    hypot_test_gen.TestGenerator.prepare_environment(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_try_generate_test_errorspy-_1","title":"----- test_TestGenerator_try_generate_test_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratortry_Generate_Test(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), max_retries=st.integers())\ndef test_fuzz_TestGenerator_try_generate_test(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], max_retries: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.try_generate_test(self=self, entity=entity, variant=variant, max_retries=max_retries)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_function_variants_errorspy-_1","title":"----- test_TestGenerator_generate_function_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Function_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_function_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_function_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_function_variants_basicpy-_1","title":"----- test_TestGenerator_generate_function_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Function_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_function_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_function_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_errorspy-_1","title":"----- test_ModuleParser_add_function_entity_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparseradd_Function_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.add_function_entity(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_errorspy-_1","title":"----- test_ModuleParser_add_class_entity_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparseradd_Class_Entity(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.add_class_entity(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_basicpy-_1","title":"----- test_TestGenerator_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgenerator(unittest.TestCase):</p> <pre><code>@given(output_dir=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator(self, output_dir: pathlib.Path) -&gt; None:\n    hypot_test_gen.TestGenerator(output_dir=output_dir)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_basicpy-_1","title":"----- test_TestGenerator_process_entities_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_fuzz_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_determine_instance_method_errorspy-_1","title":"----- test_ModuleParser_determine_instance_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserdetermine_Instance_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_determine_instance_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.determine_instance_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_should_skip_method_basicpy-_1","title":"----- test_ModuleParser_should_skip_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparsershould_Skip_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_should_skip_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.should_skip_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_errorspy-_1","title":"----- test_TestGenerator_process_hypothesis_result_errors.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, reject, strategies as st from subprocess import CompletedProcess</p> <p>class TestFuzzTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_fuzz_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_combine_and_cleanup_tests_binary-oppy-_1","title":"----- test_TestGenerator_combine_and_cleanup_tests_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationcombine_and_cleanup_tests(unittest.TestCase):     combine_and_cleanup_tests_operands = st.from_type(pathlib.Path)</p> <pre><code>@given(a=combine_and_cleanup_tests_operands, b=combine_and_cleanup_tests_operands, c=combine_and_cleanup_tests_operands)\ndef test_associative_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=b, file_path=c))\n    right = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=b), file_path=c)\n    self.assertEqual(left, right)\n\n@given(a=combine_and_cleanup_tests_operands, b=combine_and_cleanup_tests_operands)\ndef test_commutative_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a, b) -&gt; None:\n    left = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=b)\n    right = hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=b, file_path=a)\n    self.assertEqual(left, right)\n\n@given(a=combine_and_cleanup_tests_operands)\ndef test_identity_binary_operation_TestGenerator_combine_and_cleanup_tests(self, a) -&gt; None:\n    identity = PosixPath('.')\n    self.assertEqual(a, hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=a, file_path=identity))\n    self.assertEqual(a, hypot_test_gen.TestGenerator.combine_and_cleanup_tests(self=identity, file_path=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_entities_summary_errorspy-_1","title":"----- test_TestGenerator_log_entities_summary_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Entities_Summary(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_log_entities_summary(self, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.log_entities_summary(self=self, entities=entities)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_should_skip_method_errorspy-_1","title":"----- test_ModuleParser_should_skip_method_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparsershould_Skip_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_should_skip_method(self, node: ast.FunctionDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.should_skip_method(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_idempotentpy-_1","title":"----- test_ModuleParser_process_class_contents_idempotent.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestIdempotentModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_idempotent_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    result = hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n    repeat = hypot_test_gen.ModuleParser.process_class_contents(self=result, node=node)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_failed_attempt_basicpy-_1","title":"----- test_TestGenerator_handle_failed_attempt_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Failed_Attempt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_handle_failed_attempt(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    hypot_test_gen.TestGenerator.handle_failed_attempt(self=self, entity=entity, variant=variant, attempt=attempt)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_attempt_test_generation_basicpy-_1","title":"----- test_TestGenerator_attempt_test_generation_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorattempt_Test_Generation(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_attempt_test_generation(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    hypot_test_gen.TestGenerator.attempt_test_generation(self=self, entity=entity, variant=variant, attempt=attempt)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_is_known_error_basicpy-_1","title":"----- test_TestGenerator_is_known_error_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratoris_Known_Error(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), stderr=st.text())\ndef test_fuzz_TestGenerator_is_known_error(self, stderr: str) -&gt; None:\n    hypot_test_gen.TestGenerator.is_known_error(self=self, stderr=stderr)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_basicpy-_1","title":"----- test_TestGenerator_post_process_test_content_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_function_entity_binary-oppy-_1","title":"----- test_ModuleParser_add_function_entity_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_function_entity(unittest.TestCase):     add_function_entity_operands = st.builds(FunctionDef)</p> <pre><code>@given(a=add_function_entity_operands, b=add_function_entity_operands, c=add_function_entity_operands)\ndef test_associative_binary_operation_ModuleParser_add_function_entity(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_function_entity(self=a, node=hypot_test_gen.ModuleParser.add_function_entity(self=b, node=c))\n    right = hypot_test_gen.ModuleParser.add_function_entity(self=hypot_test_gen.ModuleParser.add_function_entity(self=a, node=b), node=c)\n    self.assertEqual(left, right)\n\n@given(a=add_function_entity_operands, b=add_function_entity_operands)\ndef test_commutative_binary_operation_ModuleParser_add_function_entity(self, a, b) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_function_entity(self=a, node=b)\n    right = hypot_test_gen.ModuleParser.add_function_entity(self=b, node=a)\n    self.assertEqual(left, right)\n\n@given(a=add_function_entity_operands)\ndef test_identity_binary_operation_ModuleParser_add_function_entity(self, a) -&gt; None:\n    identity = '&lt;ast.FunctionDef object at 0x125566c20&gt;'\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_function_entity(self=a, node=identity))\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_function_entity(self=identity, node=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_create_variant_errorspy-_1","title":"----- test_TestGenerator_create_variant_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorcreate_Variant(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), variant_type=st.text(), cmd=st.text())\ndef test_fuzz_TestGenerator_create_variant(self, variant_type: str, cmd: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.create_variant(self=self, variant_type=variant_type, cmd=cmd)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_run_hypothesis_write_basicpy-_1","title":"----- test_TestGenerator_run_hypothesis_write_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorrun_Hypothesis_Write(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), command=st.text())\ndef test_fuzz_TestGenerator_run_hypothesis_write(self, command: str) -&gt; None:\n    hypot_test_gen.TestGenerator.run_hypothesis_write(self=self, command=command)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_determine_instance_method_basicpy-_1","title":"----- test_ModuleParser_determine_instance_method_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserdetermine_Instance_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_fuzz_ModuleParser_determine_instance_method(self, node: ast.FunctionDef) -&gt; None:\n    hypot_test_gen.ModuleParser.determine_instance_method(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_display_module_info_basicpy-_1","title":"----- test_TestGenerator_display_module_info_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratordisplay_Module_Info(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), module_path=st.text(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_display_module_info(self, module_path: str, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    hypot_test_gen.TestGenerator.display_module_info(self=self, module_path=module_path, entities=entities)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_basicpy-_1","title":"----- test_ModuleParser_process_class_contents_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_parse_ast_basicpy-_1","title":"----- test_TestGenerator_parse_ast_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorparse_Ast(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_parse_ast(self, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.parse_ast(self=self, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_entities_errorspy-_1","title":"----- test_TestGenerator_process_entities_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorprocess_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))), total_variants=st.integers(), module_path=st.text())\ndef test_fuzz_TestGenerator_process_entities(self, entities: typing.List[hypot_test_gen.TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.process_entities(self=self, entities=entities, total_variants=total_variants, module_path=module_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_store_class_bases_errorspy-_1","title":"----- test_ModuleParser_store_class_bases_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserstore_Class_Bases(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.store_class_bases(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_run_hypothesis_write_errorspy-_1","title":"----- test_TestGenerator_run_hypothesis_write_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorrun_Hypothesis_Write(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), command=st.text())\ndef test_fuzz_TestGenerator_run_hypothesis_write(self, command: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.run_hypothesis_write(self=self, command=command)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_wrap_with_prompt_basicpy-_1","title":"----- test_TestGenerator_wrap_with_prompt_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorwrap_With_Prompt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), combined_test_code=st.text(), original_source_code=st.text())\ndef test_fuzz_TestGenerator_wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; None:\n    hypot_test_gen.TestGenerator.wrap_with_prompt(self=self, combined_test_code=combined_test_code, original_source_code=original_source_code)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_pre_run_cleanup_errorspy-_1","title":"----- test_TestGenerator_pre_run_cleanup_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpre_Run_Cleanup(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_pre_run_cleanup(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.pre_run_cleanup(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testableentity_basicpy-_1","title":"----- test_TestableEntity_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestableentity(unittest.TestCase):</p> <pre><code>@given(name=st.text(), module_path=st.text(), entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), parent_class=st.one_of(st.none(), st.text()))\ndef test_fuzz_TestableEntity(self, name: str, module_path: str, entity_type, parent_class: typing.Optional[str]) -&gt; None:\n    hypot_test_gen.TestableEntity(name=name, module_path=module_path, entity_type=entity_type, parent_class=parent_class)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_write_and_verify_output_basicpy-_1","title":"----- test_TestGenerator_write_and_verify_output_basic.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorwrite_And_Verify_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), output_file=st.from_type(pathlib.Path), content=st.text())\ndef test_fuzz_TestGenerator_write_and_verify_output(self, output_file: pathlib.Path, content: str) -&gt; None:\n    hypot_test_gen.TestGenerator.write_and_verify_output(self=self, output_file=output_file, content=content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_fix_duplicate_self_basicpy-_1","title":"----- test_fix_duplicate_self_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzFix_Duplicate_Self(unittest.TestCase):</p> <pre><code>@given(test_content=st.text())\ndef test_fuzz_fix_duplicate_self(self, test_content: str) -&gt; None:\n    hypot_test_gen.fix_duplicate_self(test_content=test_content)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_is_known_error_errorspy-_1","title":"----- test_TestGenerator_is_known_error_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratoris_Known_Error(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), stderr=st.text())\ndef test_fuzz_TestGenerator_is_known_error(self, stderr: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.is_known_error(self=self, stderr=stderr)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_class_contents_errorspy-_1","title":"----- test_ModuleParser_process_class_contents_errors.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzModuleparserprocess_Class_Contents(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    try:\n        hypot_test_gen.ModuleParser.process_class_contents(self=self, node=node)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_all_tests_errorspy-_1","title":"----- test_TestGenerator_generate_all_tests_errors.py -----","text":"<p>import hypot_test_gen import pathlib import unittest from hypothesis import given, reject, strategies as st from pathlib import Path</p> <p>class TestFuzzTestgeneratorgenerate_All_Tests(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), file_path=st.from_type(pathlib.Path))\ndef test_fuzz_TestGenerator_generate_all_tests(self, file_path: pathlib.Path) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_all_tests(self=self, file_path=file_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_errorspy-_1","title":"----- test_TestGenerator_post_process_test_content_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_add_class_entity_binary-oppy-_1","title":"----- test_ModuleParser_add_class_entity_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_class_entity(unittest.TestCase):     add_class_entity_operands = st.builds(ClassDef)</p> <pre><code>@given(a=add_class_entity_operands, b=add_class_entity_operands, c=add_class_entity_operands)\ndef test_associative_binary_operation_ModuleParser_add_class_entity(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_class_entity(self=a, node=hypot_test_gen.ModuleParser.add_class_entity(self=b, node=c))\n    right = hypot_test_gen.ModuleParser.add_class_entity(self=hypot_test_gen.ModuleParser.add_class_entity(self=a, node=b), node=c)\n    self.assertEqual(left, right)\n\n@given(a=add_class_entity_operands, b=add_class_entity_operands)\ndef test_commutative_binary_operation_ModuleParser_add_class_entity(self, a, b) -&gt; None:\n    left = hypot_test_gen.ModuleParser.add_class_entity(self=a, node=b)\n    right = hypot_test_gen.ModuleParser.add_class_entity(self=b, node=a)\n    self.assertEqual(left, right)\n\n@given(a=add_class_entity_operands)\ndef test_identity_binary_operation_ModuleParser_add_class_entity(self, a) -&gt; None:\n    identity = '&lt;ast.ClassDef object at 0x125deec50&gt;'\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_class_entity(self=a, node=identity))\n    self.assertEqual(a, hypot_test_gen.ModuleParser.add_class_entity(self=identity, node=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_verify_output_dir_errorspy-_1","title":"----- test_TestGenerator_verify_output_dir_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorverify_Output_Dir(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_verify_output_dir(self) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.verify_output_dir(self=self)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_generated_output_basicpy-_1","title":"----- test_TestGenerator_handle_generated_output_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Generated_Output(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), output=st.text())\ndef test_fuzz_TestGenerator_handle_generated_output(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], output: str) -&gt; None:\n    hypot_test_gen.TestGenerator.handle_generated_output(self=self, entity=entity, variant=variant, output=output)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_process_method_idempotentpy-_1","title":"----- test_ModuleParser_process_method_idempotent.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import FunctionDef from hypothesis import given, strategies as st</p> <p>class TestIdempotentModuleparserprocess_Method(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(FunctionDef))\ndef test_idempotent_ModuleParser_process_method(self, node: ast.FunctionDef) -&gt; None:\n    result = hypot_test_gen.ModuleParser.process_method(self=self, node=node)\n    repeat = hypot_test_gen.ModuleParser.process_method(self=result, node=node)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_process_hypothesis_result_basicpy-_1","title":"----- test_TestGenerator_process_hypothesis_result_basic.py -----","text":"<p>import hypot_test_gen import subprocess import unittest from hypothesis import given, strategies as st from subprocess import CompletedProcess</p> <p>class TestFuzzTestgeneratorprocess_Hypothesis_Result(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), result=st.from_type(subprocess.CompletedProcess))\ndef test_fuzz_TestGenerator_process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; None:\n    hypot_test_gen.TestGenerator.process_hypothesis_result(self=self, result=result)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_method_variants_basicpy-_1","title":"----- test_TestGenerator_generate_method_variants_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Method_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_method_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    hypot_test_gen.TestGenerator.generate_method_variants(self=self, entity=entity)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_post_process_test_content_idempotentpy-_1","title":"----- test_TestGenerator_post_process_test_content_idempotent.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestIdempotentTestgeneratorpost_Process_Test_Content(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_idempotent_TestGenerator_post_process_test_content(self, content: str) -&gt; None:\n    result = hypot_test_gen.TestGenerator.post_process_test_content(self=self, content=content)\n    repeat = hypot_test_gen.TestGenerator.post_process_test_content(self=result, content=content)\n    self.assertEqual(result, repeat)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_add_to_sys_path_basicpy-_1","title":"----- test_add_to_sys_path_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzAdd_To_Sys_Path(unittest.TestCase):</p> <pre><code>@given(path=st.text(), description=st.text())\ndef test_fuzz_add_to_sys_path(self, path: str, description: str) -&gt; None:\n    hypot_test_gen.add_to_sys_path(path=path, description=description)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_generate_method_variants_errorspy-_1","title":"----- test_TestGenerator_generate_method_variants_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorgenerate_Method_Variants(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())))\ndef test_fuzz_TestGenerator_generate_method_variants(self, entity: hypot_test_gen.TestableEntity) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.generate_method_variants(self=self, entity=entity)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_parse_ast_errorspy-_1","title":"----- test_TestGenerator_parse_ast_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorparse_Ast(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), content=st.text())\ndef test_fuzz_TestGenerator_parse_ast(self, content: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.parse_ast(self=self, content=content)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_moduleparser_store_class_bases_basicpy-_1","title":"----- test_ModuleParser_store_class_bases_basic.py -----","text":"<p>import ast import hypot_test_gen import unittest from ast import ClassDef from hypothesis import given, strategies as st</p> <p>class TestFuzzModuleparserstore_Class_Bases(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), node=st.builds(ClassDef))\ndef test_fuzz_ModuleParser_store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    hypot_test_gen.ModuleParser.store_class_bases(self=self, node=node)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_entities_summary_basicpy-_1","title":"----- test_TestGenerator_log_entities_summary_basic.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Entities_Summary(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entities=st.lists(st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text()))))\ndef test_fuzz_TestGenerator_log_entities_summary(self, entities: typing.List[hypot_test_gen.TestableEntity]) -&gt; None:\n    hypot_test_gen.TestGenerator.log_entities_summary(self=self, entities=entities)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_handle_failed_attempt_errorspy-_1","title":"----- test_TestGenerator_handle_failed_attempt_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorhandle_Failed_Attempt(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_handle_failed_attempt(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.handle_failed_attempt(self=self, entity=entity, variant=variant, attempt=attempt)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_add_to_sys_path_binary-oppy-_1","title":"----- test_add_to_sys_path_binary-op.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestBinaryOperationadd_to_sys_path(unittest.TestCase):     add_to_sys_path_operands = st.text()</p> <pre><code>@given(a=add_to_sys_path_operands, b=add_to_sys_path_operands, c=add_to_sys_path_operands)\ndef test_associative_binary_operation_add_to_sys_path(self, a, b, c) -&gt; None:\n    left = hypot_test_gen.add_to_sys_path(path=a, description=hypot_test_gen.add_to_sys_path(path=b, description=c))\n    right = hypot_test_gen.add_to_sys_path(path=hypot_test_gen.add_to_sys_path(path=a, description=b), description=c)\n    self.assertEqual(left, right)\n\n@given(a=add_to_sys_path_operands, b=add_to_sys_path_operands)\ndef test_commutative_binary_operation_add_to_sys_path(self, a, b) -&gt; None:\n    left = hypot_test_gen.add_to_sys_path(path=a, description=b)\n    right = hypot_test_gen.add_to_sys_path(path=b, description=a)\n    self.assertEqual(left, right)\n\n@given(a=add_to_sys_path_operands)\ndef test_identity_binary_operation_add_to_sys_path(self, a) -&gt; None:\n    identity = ''\n    self.assertEqual(a, hypot_test_gen.add_to_sys_path(path=a, description=identity))\n    self.assertEqual(a, hypot_test_gen.add_to_sys_path(path=identity, description=a))\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_pre_run_cleanup_basicpy-_1","title":"----- test_TestGenerator_pre_run_cleanup_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorpre_Run_Cleanup(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_pre_run_cleanup(self) -&gt; None:\n    hypot_test_gen.TestGenerator.pre_run_cleanup(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_populate_entities_errorspy-_1","title":"----- test_TestGenerator_populate_entities_errors.py -----","text":"<p>import hypot_test_gen import unittest from hypot_test_gen import ModuleParser from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorpopulate_Entities(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), parser=st.builds(ModuleParser), module_path=st.text())\ndef test_fuzz_TestGenerator_populate_entities(self, parser: hypot_test_gen.ModuleParser, module_path: str) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.populate_entities(self=self, parser=parser, module_path=module_path)\n    except (TypeError, ValueError):\n        reject()\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_log_environment_basicpy-_1","title":"----- test_TestGenerator_log_environment_basic.py -----","text":"<p>import hypot_test_gen import unittest from hypothesis import given, strategies as st</p> <p>class TestFuzzTestgeneratorlog_Environment(unittest.TestCase):</p> <pre><code>@given(self=st.nothing())\ndef test_fuzz_TestGenerator_log_environment(self) -&gt; None:\n    hypot_test_gen.TestGenerator.log_environment(self=self)\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#-test_testgenerator_attempt_test_generation_errorspy-_1","title":"----- test_TestGenerator_attempt_test_generation_errors.py -----","text":"<p>import hypot_test_gen import typing import unittest from hypot_test_gen import TestableEntity from hypothesis import given, reject, strategies as st</p> <p>class TestFuzzTestgeneratorattempt_Test_Generation(unittest.TestCase):</p> <pre><code>@given(self=st.nothing(), entity=st.builds(TestableEntity, entity_type=st.sampled_from(['instance_method', 'function', 'method', 'class']), module_path=st.text(), name=st.text(), parent_class=st.one_of(st.none(), st.none(), st.text())), variant=st.dictionaries(keys=st.text(), values=st.text()), attempt=st.integers())\ndef test_fuzz_TestGenerator_attempt_test_generation(self, entity: hypot_test_gen.TestableEntity, variant: typing.Dict[str, str], attempt: int) -&gt; None:\n    try:\n        hypot_test_gen.TestGenerator.attempt_test_generation(self=self, entity=entity, variant=variant, attempt=attempt)\n    except (TypeError, ValueError):\n        reject()\n</code></pre> <p>is the content of your automatically generated Python test files (potentially multiple files\u2019 content combined or listed).     \u2022   import ast import logging import os import subprocess import sys import time from dataclasses import dataclass from pathlib import Path from typing import Dict, List, Optional, Tuple, Union, Literal, Any</p> <p>import snoop  # type: ignore</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#removed-unused-from-hypothesis-import-strategies-as-st_1","title":"Removed unused: from hypothesis import strategies as st","text":"<p>import importlib.util  # For dynamic imports</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#set-up-logging-with-file-and-console-output_1","title":"Set up logging with file and console output","text":"<p>log_file = \"test_generator_debug.log\" logging.basicConfig(     level=logging.DEBUG,     format=\"%(asctime)s - %(levelname)s - %(message)s\",     handlers=[logging.FileHandler(log_file), logging.StreamHandler()], ) logger = logging.getLogger(name)</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#well-assume-the-prompt_templatemd-is-in-the-same-directory-as-this-script_1","title":"We'll assume the prompt_template.md is in the same directory as this script","text":"<p>PROMPT_TEMPLATE_FILE = Path(file).parent / \"prompt_template.md\"</p> <p>def load_text_prompt_template() -&gt; str:     \"\"\"     Load the text prompt template from the prompt_template.md file.     \"\"\"     try:         return PROMPT_TEMPLATE_FILE.read_text(encoding=\"utf-8\")     except FileNotFoundError:         logger.error(\"prompt_template.md not found. Please ensure it is in the same directory.\")         return \"\"</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#configure-snoop-to-write-to-a-separate-debug-log_1","title":"Configure snoop to write to a separate debug log","text":"<p>snoop.install(out=Path(\"snoop_debug.log\"))</p> <p>def fix_leading_zeros(test_code: str) -&gt; str:     \"\"\"     Replace decimal integers with leading zeros (except a standalone \"0\") with their corrected form.     For example, \"007\" becomes \"7\" and \"-0123\" becomes \"-123\".     \"\"\"     import re     # Use a regex with negative lookbehind and lookahead to match numbers that start with one or more zeros.     # The pattern (?&lt;!\\d)(-?)0+(\\d+)(?!\\d) ensures that a minus sign is captured if present,     # and that only isolated numbers are matched.     fixed_code = re.sub(r'(?&lt;!\\d)(-?)0+(\\d+)(?!\\d)', lambda m: m.group(1) + str(int(m.group(2))), test_code)     return fixed_code</p> <p>def remove_logger_lines(text: str) -&gt; str:     \"\"\"     Remove extraneous logging lines from the generated test content.     This function filters out:       - Lines starting with a bracketed or non-bracketed timestamp (e.g. \"[2025-3-27 14:55:48,330] ...\" or \"2025-03-27 14:55:48,330 - ...\").       - Lines containing known noisy substrings such as 'real_accelerator.py:' or 'Setting ds_accelerator to'.     \"\"\"     import re     lines = text.splitlines()     filtered = []     timestamp_pattern = re.compile(r'^[?\\d{4}-\\d{1,2}-\\d{1,2}')     for line in lines:         # Skip lines matching a leading timestamp         if timestamp_pattern.match(line):             continue         # Skip lines containing known noisy substrings         if 'real_accelerator.py:' in line or 'Setting ds_accelerator to' in line:             continue         filtered.append(line)     return \"\\n\".join(filtered).strip()</p> <p>@dataclass class TestableEntity:     \"\"\"Represents a class, method, or function that can be tested\"\"\"     name: str     module_path: str     entity_type: Literal['class', 'method', 'function', 'instance_method']  # More restrictive type     parent_class: Optional[str] = None</p> <p>def fix_pythonpath(file_path: Path) -&gt; None:     \"\"\"Ensure the module being tested is in Python's path\"\"\"     parent_dir = str(file_path.parent.absolute())     add_to_sys_path(parent_dir, \"parent directory\")</p> <pre><code>if \"src\" in file_path.parts:\n    src_path = construct_src_path(file_path)\n    add_to_sys_path(src_path, \"src directory\")\n</code></pre> <p>def add_to_sys_path(path: str, description: str) -&gt; None:     \"\"\"Helper function to add a path to sys.path if not already present\"\"\"     if path not in sys.path:         sys.path.insert(0, path)         logger.debug(f\"Added {description} to sys.path: {path}\")</p> <p>def construct_src_path(file_path: Path) -&gt; str:     \"\"\"Construct the src path from the file path\"\"\"     src_index = file_path.parts.index(\"src\")     src_path = str(Path(*file_path.parts[: src_index + 1]).absolute())     return src_path</p> <p>class ModuleParser(ast.NodeVisitor):     \"\"\"AST-based parser for Python modules\"\"\"</p> <pre><code>def __init__(self):\n    self.entities: List[TestableEntity] = []\n    self.current_class: Optional[str] = None\n    self.class_bases: Dict[str, List[str]] = {}\n\ndef visit_ClassDef(self, node: ast.ClassDef) -&gt; None:\n    if node.name.startswith(\"_\"):\n        return\n    self.store_class_bases(node)\n    self.add_class_entity(node)\n    self.process_class_contents(node)\n\ndef store_class_bases(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Store base classes for inheritance checking\"\"\"\n    bases = []\n    for base in node.bases:\n        base_name = self.get_base_name(base)\n        if base_name:\n            bases.append(base_name)\n    self.class_bases[node.name] = bases\n    logger.debug(f\"Stored bases for class {node.name}: {bases}\")\n\ndef get_base_name(self, base: ast.AST) -&gt; Optional[str]:\n    \"\"\"Retrieve the base class name from the AST node\"\"\"\n    if isinstance(base, ast.Name):\n        return base.id\n    elif isinstance(base, ast.Attribute):\n        if isinstance(base.value, ast.Name):\n            return f\"{base.value.id}.{base.attr}\"\n    return None\n\ndef add_class_entity(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Add the class itself to entities\"\"\"\n    self.entities.append(TestableEntity(node.name, \"\", \"class\"))\n    logger.debug(f\"Added class entity: {node.name}\")\n\ndef process_class_contents(self, node: ast.ClassDef) -&gt; None:\n    \"\"\"Process the contents of the class\"\"\"\n    old_class = self.current_class\n    self.current_class = node.name\n    self.generic_visit(node)\n    self.current_class = old_class\n    logger.debug(f\"Processed contents of class {node.name}\")\n\ndef visit_FunctionDef(self, node: ast.FunctionDef) -&gt; None:\n    if node.name.startswith(\"_\"):\n        return\n    if self.current_class:\n        self.process_method(node)\n    else:\n        self.add_function_entity(node)\n\ndef process_method(self, node: ast.FunctionDef) -&gt; None:\n    \"\"\"Process a method within a class\"\"\"\n    if self.should_skip_method(node):\n        return\n\n    is_instance_method = self.determine_instance_method(node)\n    entity_type = \"instance_method\" if is_instance_method else \"method\"\n\n    # The method path should include the class\n    method_name = f\"{self.current_class}.{node.name}\" if self.current_class else node.name\n\n    self.entities.append(\n        TestableEntity(\n            name=node.name,\n            module_path=\"\",\n            entity_type=entity_type,\n            parent_class=self.current_class,\n        )\n    )\n    logger.debug(\n        f\"Added {'instance_method' if is_instance_method else 'method'} entity: {method_name}\"\n    )\n\ndef determine_instance_method(self, node: ast.FunctionDef) -&gt; bool:\n    \"\"\"Determine if the method is an instance method\"\"\"\n    for decorator in node.decorator_list:\n        if isinstance(decorator, ast.Name) and decorator.id in {\"classmethod\", \"staticmethod\"}:\n            return False\n    return True\n\ndef should_skip_method(self, node: ast.FunctionDef) -&gt; bool:\n    \"\"\"Determine if the method should be skipped based on inheritance or naming\"\"\"\n    current_bases = self.class_bases.get(self.current_class, [])\n    if any(base in {\"NodeVisitor\", \"ast.NodeVisitor\"} for base in current_bases):\n        if node.name.startswith(\"visit_\"):\n            logger.debug(f\"Skipping inherited visit method: {node.name}\")\n            return True\n    if node.name in {\"__init__\", \"__str__\", \"__repr__\", \"property\"}:\n        logger.debug(f\"Skipping magic or property method: {node.name}\")\n        return True\n    return False\n\ndef add_function_entity(self, node: ast.FunctionDef) -&gt; None:\n    \"\"\"Add a standalone function to entities\"\"\"\n    self.entities.append(TestableEntity(node.name, \"\", \"function\"))\n    logger.debug(f\"Added function entity: {node.name}\")\n</code></pre> <p>def debug_command_output(cmd: str, stdout: str, stderr: str, returncode: int) -&gt; None:     \"\"\"Helper function to debug command execution\"\"\"     logger.debug(\"Command execution details:\")     logger.debug(f\"Command: {cmd}\")     logger.debug(f\"Return code: {returncode}\")     logger.debug(f\"stdout length: {len(stdout)}\")     logger.debug(f\"stderr length: {len(stderr)}\")     logger.debug(\"First 1000 chars of stdout:\")     logger.debug(stdout[:1000])     logger.debug(\"First 1000 chars of stderr:\")     logger.debug(stderr[:1000])</p> <p>class TestFixer(ast.NodeTransformer):     \"\"\"AST transformer to fix duplicate self parameters\"\"\"</p> <pre><code>def visit_FunctionDef(self, node: ast.FunctionDef) -&gt; ast.FunctionDef:\n    seen_self = False\n    new_args = []\n\n    for arg in node.args.args:\n        if arg.arg == 'self':\n            if not seen_self:\n                seen_self = True\n                new_args.append(arg)\n        else:\n            new_args.append(arg)\n\n    node.args.args = new_args\n    return node\n</code></pre> <p>def fix_duplicate_self(test_content: str) -&gt; Optional[str]:     \"\"\"     Fix duplicate self parameters in test content.</p> <pre><code>Args:\n    test_content: String containing the test code\n\nReturns:\n    Fixed test code string, or None if parsing fails\n\"\"\"\ntry:\n    tree = ast.parse(test_content)\n\n    fixer = TestFixer()\n    fixed_tree = fixer.visit(tree)\n\n    try:\n        return ast.unparse(fixed_tree)\n    except AttributeError:\n        import astunparse\n        return astunparse.unparse(fixed_tree)\n\nexcept Exception as e:\n    print(f\"Error fixing test content: {e}\")\n    return None\n</code></pre> <p>class TestGenerator:     \"\"\"Manages generation of Hypothesis tests for Python modules\"\"\"     def wrap_with_prompt(self, combined_test_code: str, original_source_code: str) -&gt; str:         \"\"\"         Wrap the combined test code and original source code in the custom text prompt         read from 'prompt_template.md'.         \"\"\"         prompt_template = load_text_prompt_template()         return prompt_template.format(             TEST_CODE=combined_test_code,             FULL_SRC_CODE=original_source_code         )</p> <pre><code>def pre_run_cleanup(self) -&gt; None:\n    \"\"\"\n    Remove any leftover combined test files (matching 'test_hyp_*.py') from previous runs.\n    This ensures we don't mix old combined files with new runs.\n    \"\"\"\n    leftover_files = list(self.output_dir.glob(\"test_hyp_*.py\"))\n    for leftover in leftover_files:\n        try:\n            leftover.unlink()\n            logger.debug(f\"Removed leftover combined file: {leftover.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete leftover file {leftover.name}: {e}\")\n\ndef __init__(self, output_dir: Path = Path(\"generated_tests\")):\n    self.output_dir = output_dir\n    self.output_dir.mkdir(exist_ok=True)\n    self.verify_output_dir()\n\ndef verify_output_dir(self) -&gt; None:\n    \"\"\"Verify that the output directory exists and is writable\"\"\"\n    logger.debug(f\"Test generator initialized with output dir: {self.output_dir}\")\n    logger.debug(f\"Output dir exists: {self.output_dir.exists()}\")\n    logger.debug(f\"Output dir is writable: {os.access(self.output_dir, os.W_OK)}\")\n\ndef run_hypothesis_write(self, command: str) -&gt; Optional[str]:\n    \"\"\"Execute hypothesis write command and return output if successful\"\"\"\n    full_cmd = f\"hypothesis write {command}\"\n    logger.debug(f\"Executing hypothesis command: {full_cmd}\")\n\n    try:\n        self.log_environment()\n        env = self.prepare_environment()\n\n        result = subprocess.run(\n            full_cmd, shell=True, capture_output=True, text=True, env=env\n        )\n\n        debug_command_output(\n            full_cmd, result.stdout, result.stderr, result.returncode\n        )\n\n        return self.process_hypothesis_result(result)\n\n    except Exception as e:\n        logger.error(f\"Error running hypothesis: {e}\", exc_info=True)\n        return None\n\ndef log_environment(self) -&gt; None:\n    \"\"\"Log the current environment settings\"\"\"\n    logger.debug(f\"PYTHONPATH before modification: {os.getenv('PYTHONPATH')}\")\n    logger.debug(f\"sys.path: {sys.path}\")\n    logger.debug(f\"Current working directory: {os.getcwd()}\")\n\ndef prepare_environment(self) -&gt; Dict[str, str]:\n    \"\"\"Prepare the environment variables for subprocess\"\"\"\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = \":\".join(sys.path)\n    env.setdefault(\"PYTHONIOENCODING\", \"utf-8\")\n    return env\n\ndef process_hypothesis_result(self, result: subprocess.CompletedProcess) -&gt; Optional[str]:\n    \"\"\"Process the result of the hypothesis command\"\"\"\n    if result.returncode == 0 and result.stdout:\n        content = result.stdout.strip()\n\n        # Remove extraneous logging lines first\n        content = remove_logger_lines(content)\n\n        if not content or len(content) &lt; 50:\n            logger.warning(\"Hypothesis generated insufficient content\")\n            return None\n\n        # Process and fix the test content using post_process_test_content\n        fixed_content = self.post_process_test_content(content)\n        if fixed_content is None:\n            logger.warning(\"Failed to process test content\")\n            return None\n\n        logger.info(\"Successfully generated and processed test content\")\n        return fixed_content\n\n    if result.stderr and not self.is_known_error(result.stderr):\n        logger.warning(f\"Command failed: {result.stderr}\")\n    return None\n\ndef post_process_test_content(self, content: str) -&gt; Optional[str]:\n    \"\"\"Post-process generated test content\"\"\"\n    try:\n        # Also remove extraneous logger lines (defensive)\n        content = remove_logger_lines(content)\n\n        # First, fix any leading zeros in integer literals\n        content = fix_leading_zeros(content)\n        # Then, fix duplicate self parameters\n        fixed_content = fix_duplicate_self(content)\n        if fixed_content is None:\n            logger.warning(\"Failed to fix duplicate self parameters.\")\n            return content\n        return fixed_content\n    except Exception as e:\n        logger.error(f\"Error processing test content: {e}\", exc_info=True)\n        return None\n\ndef is_known_error(self, stderr: str) -&gt; bool:\n    \"\"\"Check if the stderr contains known non-critical errors\"\"\"\n    known_errors = [\n        \"InvalidArgument: Got non-callable\",\n        \"Could not resolve\",\n        \"but it doesn't have a\",\n    ]\n    return any(msg in stderr for msg in known_errors)\n\ndef try_generate_test(\n    self, entity: TestableEntity, variant: Dict[str, str], max_retries: int = 3\n) -&gt; bool:\n    \"\"\"Attempt to generate a specific test variant with retries\"\"\"\n    for attempt in range(1, max_retries + 1):\n        logger.debug(\n            f\"Attempt {attempt} for {variant['type']} test on {entity.name}\"\n        )\n        output = self.attempt_test_generation(entity, variant, attempt)\n        if output:\n            return True\n    return False\n\ndef attempt_test_generation(\n    self, entity: TestableEntity, variant: Dict[str, str], attempt: int\n) -&gt; Optional[bool]:\n    \"\"\"Attempt a single test generation\"\"\"\n    output = self.run_hypothesis_write(variant[\"cmd\"])\n    if output:\n        return self.handle_generated_output(entity, variant, output)\n    else:\n        return self.handle_failed_attempt(entity, variant, attempt)\n\ndef handle_generated_output(\n    self, entity: TestableEntity, variant: Dict[str, str], output: str\n) -&gt; bool:\n    \"\"\"Handle the output from a successful hypothesis generation\"\"\"\n    name_prefix = (\n        f\"{entity.parent_class}_{entity.name}\"\n        if entity.parent_class\n        else entity.name\n    )\n    output_file = self.output_dir / f\"test_{name_prefix}_{variant['type']}.py\"\n\n    try:\n        self.write_and_verify_output(output_file, output)\n        logger.info(f\"Successfully generated test at {output_file}\")\n        print(f\"Generated {variant['type']} test: {output_file}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error writing test file: {e}\", exc_info=True)\n        return False\n\ndef write_and_verify_output(self, output_file: Path, content: str) -&gt; None:\n    \"\"\"Write the test content to a file and verify its integrity\"\"\"\n    logger.debug(\"Test content details:\")\n    logger.debug(f\"Content length: {len(content)}\")\n    logger.debug(f\"Content preview:\\n{content[:1000]}\")\n    logger.debug(f\"Writing to file: {output_file}\")\n\n    output_file.write_text(content)\n\n    written_content = output_file.read_text()\n    if not written_content:\n        logger.error(f\"File {output_file} is empty after writing!\")\n        raise ValueError(f\"Empty file: {output_file}\")\n\n    if written_content != content:\n        logger.error(\"Written content doesn't match original content!\")\n        logger.debug(f\"Original length: {len(content)}\")\n        logger.debug(f\"Written length: {len(written_content)}\")\n        raise ValueError(\"Content mismatch after writing\")\n\n    logger.debug(f\"Final file size: {output_file.stat().st_size} bytes\")\n\ndef handle_failed_attempt(\n    self, entity: TestableEntity, variant: Dict[str, str], attempt: int\n) -&gt; Optional[bool]:\n    \"\"\"Handle a failed test generation attempt\"\"\"\n    if attempt &lt; 3:\n        logger.warning(f\"Attempt {attempt} failed, retrying...\")\n        time.sleep(1)\n    else:\n        logger.error(f\"All attempts failed for {entity.name}\")\n    return None\n\ndef get_module_contents(self, file_path: Path) -&gt; Tuple[str, List[TestableEntity]]:\n    \"\"\"Extract module path and testable entities using AST parsing\"\"\"\n    logger.debug(f\"Reading file: {file_path}\")\n    try:\n        module_path = self.construct_module_path(file_path)\n        content = file_path.read_text()\n        parser = self.parse_ast(content)\n        imports = self.extract_imports(content)\n\n        entities = self.populate_entities(parser, module_path)\n        self.log_entities_summary(entities)\n        return module_path, entities\n\n    except Exception as e:\n        logger.error(f\"Error parsing module contents: {e}\", exc_info=True)\n        raise\n\ndef construct_module_path(self, file_path: Path) -&gt; str:\n    \"\"\"Construct the module path from the file path\"\"\"\n    parts = file_path.parts\n    if \"src\" in parts:\n        src_index = parts.index(\"src\")\n        module_parts = list(parts[src_index + 1 :])\n    else:\n        module_parts = [file_path.stem]\n    module_path = \".\".join([p.replace(\".py\", \"\") for p in module_parts])\n    logger.debug(f\"Constructed module path: {module_path}\")\n    return module_path\n\ndef parse_ast(self, content: str) -&gt; ModuleParser:\n    \"\"\"Parse the AST of the given content\"\"\"\n    tree = ast.parse(content)\n    parser = ModuleParser()\n    parser.visit(tree)\n    return parser\n\ndef extract_imports(self, content: str) -&gt; set:\n    \"\"\"Extract import statements from the content\"\"\"\n    tree = ast.parse(content)\n    imports = set()\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Import):\n            for name in node.names:\n                imports.add(name.name)\n        elif isinstance(node, ast.ImportFrom):\n            if node.module:\n                imports.add(node.module)\n    logger.debug(f\"Found imports: {imports}\")\n    return imports\n\ndef populate_entities(self, parser: ModuleParser, module_path: str) -&gt; List[TestableEntity]:\n    \"\"\"Populate entities with correct module paths\"\"\"\n    entities = []\n    for entity in parser.entities:\n        entity.module_path = module_path\n        entities.append(entity)\n    return entities\n\ndef log_entities_summary(self, entities: List[TestableEntity]) -&gt; None:\n    \"\"\"Log a summary of found entities\"\"\"\n    classes = sum(1 for e in entities if e.entity_type == \"class\")\n    methods = sum(\n        1 for e in entities if e.entity_type in {\"method\", \"instance_method\"}\n    )\n    functions = sum(1 for e in entities if e.entity_type == \"function\")\n    logger.info(\n        f\"Found {classes} classes, {methods} methods, and {functions} functions\"\n    )\n\ndef generate_all_tests(self, file_path: Path) -&gt; None:\n    \"\"\"Generate all possible test variants for a Python file\"\"\"\n    logger.info(f\"Generating tests for file: {file_path}\")\n    try:\n        fix_pythonpath(file_path)\n        module_path, entities = self.get_module_contents(file_path)\n        self.display_module_info(module_path, entities)\n        total_variants = sum(len(self.generate_test_variants(e)) for e in entities)\n        self.process_entities(entities, total_variants, module_path)\n        print()\n        self.combine_and_cleanup_tests(file_path)\n    except Exception:\n        logger.error(\"Test generation failed\", exc_info=True)\n        raise\n\ndef display_module_info(self, module_path: str, entities: List[TestableEntity]) -&gt; None:\n    \"\"\"Display information about the module and its entities\"\"\"\n    print(f\"\\nProcessing module: {module_path}\")\n    print(\n        f\"Found {len([e for e in entities if e.entity_type == 'class'])} classes, \"\n        f\"{len([e for e in entities if e.entity_type in {'method', 'instance_method'}])} methods, and \"\n        f\"{len([e for e in entities if e.entity_type == 'function'])} functions\"\n    )\n\ndef process_entities(self, entities: List[TestableEntity], total_variants: int, module_path: str) -&gt; None:\n    \"\"\"Process each entity and generate tests\"\"\"\n    current = 0\n    for entity in entities:\n        print(f\"\\nGenerating tests for: {module_path}.{entity.name}\")\n        variants = self.generate_test_variants(entity)\n        for variant in variants:\n            current += 1\n            print(f\"\\rGenerating tests: [{current}/{total_variants}]\", end=\"\")\n            self.try_generate_test(entity, variant)\n    print()\n\ndef _get_object(self, path: str) -&gt; Optional[Any]:\n    \"\"\"Get the actual object from its module path\"\"\"\n    try:\n        module_parts = path.split('.')\n        module_path = '.'.join(module_parts[:-1])\n        obj_name = module_parts[-1]\n\n        spec = importlib.util.find_spec(module_path)\n        if spec and spec.loader:\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            return getattr(module, obj_name, None)\n    except Exception:\n        return None\n\ndef generate_method_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate test variants for methods and instance methods\"\"\"\n    if entity.entity_type in {\"method\", \"instance_method\"}:\n        method_path = f\"{entity.module_path}.{entity.parent_class}.{entity.name}\"\n    else:\n        method_path = f\"{entity.module_path}.{entity.name}\"\n\n    # Start with basic test with type inference\n    variants = [\n        self.create_variant(\n            \"basic\",\n            f\"--style=unittest --annotate {method_path}\"\n        )\n    ]\n\n    # Add error variant\n    variants.append(\n        self.create_variant(\n            \"errors\",\n            f\"--style=unittest --annotate --except ValueError --except TypeError {method_path}\"\n        )\n    )\n\n    # Add special variants based on method name\n    name = entity.name.lower()\n    variants.extend(self._generate_special_variants(name, method_path))\n\n    return variants\n\ndef _generate_special_variants(self, name: str, method_path: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate special variants based on method name\"\"\"\n    special_variants = []\n\n    if any(x in name for x in [\"transform\", \"convert\", \"process\", \"format\"]):\n        special_variants.append(\n            self.create_variant(\n                \"idempotent\",\n                f\"--style=unittest --annotate --idempotent {method_path}\"\n            )\n        )\n\n    if any(x in name for x in [\"validate\", \"verify\", \"check\", \"assert\"]):\n        special_variants.append(\n            self.create_variant(\n                \"validation\",\n                f\"--style=unittest --annotate --errors-equivalent {method_path}\"\n            )\n        )\n\n    if \"encode\" in name or \"decode\" in name:\n        special_variants.append(\n            self.create_variant(\n                \"roundtrip\",\n                f\"--style=unittest --annotate --roundtrip {method_path}\"\n            )\n        )\n\n    if any(x in name for x in [\"add\", \"multiply\", \"subtract\", \"combine\", \"merge\"]):\n        special_variants.append(\n            self.create_variant(\n                \"binary-op\",\n                f\"--style=unittest --annotate --binary-op {method_path}\"\n            )\n        )\n\n    return special_variants\n\ndef generate_function_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate test variants for standalone functions\"\"\"\n    base_cmd = f\"--style=unittest --annotate {entity.module_path}.{entity.name}\"\n    variants = [self.create_variant(\"basic\", base_cmd)]\n\n    # Add special variants for functions if needed\n    name = entity.name.lower()\n    if \"encode\" in name or \"decode\" in name or \"serialize\" in name or \"deserialize\" in name:\n        variants.append(self.create_variant(\"roundtrip\", f\"{base_cmd} --roundtrip\"))\n    elif any(x in name for x in [\"add\", \"sub\", \"mul\", \"combine\", \"merge\"]):\n        variants.append(self.create_variant(\"binary-op\", f\"{base_cmd} --binary-op\"))\n\n    return variants\n\ndef generate_test_variants(self, entity: TestableEntity) -&gt; List[Dict[str, str]]:\n    \"\"\"Generate all applicable test variants for an entity\"\"\"\n    variants = []\n    if entity.entity_type == \"class\":\n        # For classes, just a basic annotated variant\n        variants.append(self.create_variant(\"basic\", f\"--style=unittest --annotate {entity.module_path}.{entity.name}\"))\n    elif entity.entity_type in {\"method\", \"instance_method\"}:\n        variants.extend(self.generate_method_variants(entity))\n    else:\n        variants.extend(self.generate_function_variants(entity))\n    logger.debug(f\"Generated variants for {entity.name}: {[v['type'] for v in variants]}\")\n    return variants\n\ndef create_variant(self, variant_type: str, cmd: str) -&gt; Dict[str, str]:\n    \"\"\"Create a test variant dictionary with properly formatted command\"\"\"\n    return {\n        \"type\": variant_type,\n        \"cmd\": cmd.strip()  # Ensure no extra whitespace in command\n    }\n\ndef combine_and_cleanup_tests(self, file_path: Path) -&gt; None:\n    \"\"\"\n    Combines individual test files into a single file and deletes the originals,\n    then removes the combined .py file so only the final markdown remains.\n\n    Args:\n        file_path (Path): The original Python file used for test generation.\n    \"\"\"\n    # Step 1: Derive the combined file name from the original file\n    original_stem = file_path.stem  # e.g., \"my_module\"\n    combined_filename = f\"test_hyp_{original_stem}.py\"\n    combined_filepath = self.output_dir / combined_filename\n\n    # Step 2: Collect all generated test files in the output directory\n    # Using \"test_*.py\" so that it naturally ignores any leftover .md files\n    test_files = list(self.output_dir.glob(\"test_*.py\"))\n\n    # Step 3: Combine contents of each test file into a single string\n    combined_content = \"\"\n    for test_file in test_files:\n        content = test_file.read_text()\n        separator = f\"\\n# ----- {test_file.name} -----\\n\"\n        combined_content += separator + content + \"\\n\"\n\n    # Step 4: Write the combined content to the new file\n    combined_filepath.write_text(combined_content)\n\n    # Step 5: Wrap the combined test code with the prompt to produce the final Markdown\n    original_source_code = file_path.read_text()\n    final_wrapped_content = self.wrap_with_prompt(combined_content, original_source_code)\n    final_wrapped_file = self.output_dir / f\"test_wrapped_{original_stem}.md\"\n    final_wrapped_file.write_text(final_wrapped_content)\n    logger.info(f\"Final wrapped test file created at {final_wrapped_file}\")\n\n    # Optional: verify the combined file\n    if not combined_filepath.exists() or len(combined_filepath.read_text()) &lt; 50:\n        logger.error(f\"Combined test file {combined_filepath} appears to be incomplete.\")\n        return\n\n    # Step 6: Cleanup - delete individual test files\n    for test_file in test_files:\n        try:\n            test_file.unlink()\n            logger.debug(f\"Deleted individual test file: {test_file.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete {test_file.name}: {e}\")\n\n    # Step 7: Logging and feedback\n    logger.info(f\"Combined {len(test_files)} test files into {combined_filename} and removed originals.\")\n\n    # Step 8: Apply Ruff cleaning commands to the combined file\n    cmds = [\n        f\"ruff check {combined_filepath}\",\n        f\"ruff check --fix {combined_filepath}\",\n        f\"ruff format {combined_filepath}\",\n        f\"ruff check --select I --fix {combined_filepath}\",\n        f\"ruff format {combined_filepath}\"\n    ]\n    for cmd in cmds:\n        try:\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n            if result.returncode != 0:\n                logger.error(f\"Ruff command '{cmd}' failed: {result.stderr}\")\n            else:\n                logger.info(f\"Ruff command '{cmd}' succeeded: {result.stdout}\")\n        except Exception as e:\n            logger.error(f\"Failed to run ruff command '{cmd}': {e}\")\n\n    # Finally, remove the combined .py file so only the markdown remains\n    if combined_filepath.exists():\n        combined_filepath.unlink()\n        logger.info(f\"Deleted the combined file {combined_filepath} so that only the Markdown file remains.\")\n</code></pre> <p>def parse_args(args: Optional[list] = None) -&gt; Path:     \"\"\"     Parse command line arguments and validate file path.</p> <pre><code>Args:\n    args: Optional list of command line arguments. If None, uses sys.argv[1:]\n\nReturns:\n    Path object for the input file\n\nRaises:\n    ValueError: If arguments are invalid or file doesn't exist\n\"\"\"\nif args is None:\n    args = sys.argv[1:]\n\nif len(args) != 1:\n    raise ValueError(\"Exactly one argument (path to Python file) required\")\n\nfile_path = Path(args[0])\nif not file_path.exists() or not file_path.is_file():\n    raise ValueError(f\"File does not exist or is not a file: {file_path}\")\n\nreturn file_path\n</code></pre> <p>def run_test_generation(file_path: Union[str, Path]) -&gt; bool:     \"\"\"     Run the test generation process for a given file.     Now also calls pre_run_cleanup before generate_all_tests.</p> <pre><code>Args:\n    file_path: Path to the Python file to generate tests for\n\nReturns:\n    bool: True if test generation was successful, False otherwise\n\nRaises:\n    Exception: If test generation fails\n\"\"\"\ntry:\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    logger.info(f\"Starting test generator for {file_path}\")\n    generator = TestGenerator()\n\n    # Clean up any leftover combined files from prior runs\n    generator.pre_run_cleanup()\n\n    # Proceed with the standard generation workflow\n    generator.generate_all_tests(file_path)\n    return True\n\nexcept Exception as e:\n    logger.error(f\"Test generation failed: {e}\", exc_info=True)\n    return False\n</code></pre> <p>def main(args: Optional[list] = None) -&gt; int:     \"\"\"     Main entry point for the test generator script.</p> <pre><code>Args:\n    args: Optional list of command line arguments. If None, uses sys.argv[1:]\n\nReturns:\n    int: Exit code (0 for success, 1 for failure)\n\"\"\"\ntry:\n    file_path = parse_args(args)\n    success = run_test_generation(file_path)\n    return 0 if success else 1\n\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    logger.error(f\"Invalid arguments: {e}\")\n    print(\"Usage: python test_generator.py &lt;path_to_python_file&gt;\")\n    return 1\n\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n    logger.error(\"Unexpected error during execution\", exc_info=True)\n    return 1\n</code></pre> <p>if name == \"main\":     sys.exit(main()) is the content of the source code under test (if needed for context).</p> <p>Output Format</p> <p>Provide a single Python code block containing the fully refactored, consolidated test file. The output should be ready-to-run with:</p> <p>python -m unittest</p> <p>It must exhibit all of the improvements listed above, including:     \u2022   Logical grouping of tests,     \u2022   Clear and correct usage of setUp,     \u2022   Docstrings for test classes and methods,     \u2022   Consolidated and refactored tests (no duplicates),     \u2022   Robust assertions and coverage,     \u2022   Use of hypothesis with one or more examples,     \u2022   Use of mock where appropriate.</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#_1","title":"\u2e3b","text":"<p>EXTRA USEFUL CONTEXT TO AID YOU IN YOUR TASK: Hypothesis: A Comprehensive Best-Practice and Reference Guide</p> <p>Hypothesis is a powerful property-based testing library for Python, designed to help you find subtle bugs by generating large numbers of test inputs and minimizing failing examples. This document combines the strengths and core ideas of three earlier guides. It serves as a broad, in-depth resource: covering Hypothesis usage from the basics to advanced methods, including background on its internal mechanisms (Conjecture) and integration with complex workflows.</p> <p>\u2e3b</p> <p>Table of Contents     1.  Introduction to Property-Based Testing 1.1 What Is Property-Based Testing? 1.2 Why Use Property-Based Testing? 1.3 Installing Hypothesis     2.  First Steps with Hypothesis 2.1 A Simple Example 2.2 Basic Workflows and Key Concepts 2.3 Troubleshooting the First Failures     3.  Core Hypothesis Concepts 3.1 The @given Decorator 3.2 Strategies: Building and Composing Data Generators 3.3 Shrinking and Minimizing Failing Examples 3.4 Example Database and Replay     4.  Advanced Data Generation 4.1 Understanding Strategies vs. Types 4.2 Composing Strategies (map, filter, flatmap) 4.3 Working with Complex or Recursive Data 4.4 Using @composite Functions 4.5 Integration and Edge Cases     5.  Practical Usage Patterns 5.1 Testing Numeric Code (Floating-Point, Bounds) 5.2 Text and String Generation (Character Sets, Regex) 5.3 Dates, Times, and Time Zones 5.4 Combining Hypothesis with Fixtures and Other Test Tools     6.  Stateful/Model-Based Testing 6.1 The RuleBasedStateMachine and @rule Decorators 6.2 Designing Operations and Invariants 6.3 Managing Complex State and Multiple Bundles 6.4 Example: Testing a CRUD System or Other Stateful API     7.  Performance and Health Checks 7.1 Diagnosing Slow Tests with Deadlines 7.2 Common Health Check Warnings and Their Meanings 7.3 Filtering Pitfalls (assume / Over-Filters) 7.4 Tuning Hypothesis Settings (max_examples, phases, etc.) 7.5 Speed vs. Thoroughness     8.  Multiple Failures and Multi-Bug Discovery 8.1 How Hypothesis Detects and Distinguishes Bugs 8.2 Typical Bug Slippage and the \u201cThreshold Problem\u201d 8.3 Strategies for Handling Multiple Distinct Failures     9.  Internals: The Conjecture Engine 9.1 Overview of Bytestream-Based Generation 9.2 Integrated Shrinking vs. Type-Based Shrinking 9.3 How Conjecture Tracks and Minimizes Examples 9.4 The Example Database in Depth     10. Hypothesis in Real-World Scenarios 10.1 Using Hypothesis in CI/CD 10.2 Collaborative Testing in Teams 10.3 Integrating with Other Tools (pytest, coverage, etc.) 10.4 Best Practices for Large Projects     11. Extensibility and Advanced Topics 11.1 Third-Party Extensions (e.g., Hypothesis-Bio, Hypothesis-NetworkX) 11.2 Targeted Property-Based Testing (Scoring) 11.3 Hybrid Approaches (Combining Examples with Generation) 11.4 Glass-Box Testing and Potential Future Work     12. Troubleshooting and FAQs 12.1 Common Error Messages 12.2 Reproduce Failures with @reproduce_failure and Seeds 12.3 Overcoming Flaky or Non-Deterministic Tests 12.4 Interpreting Statistics     13. Summary and Further Reading 13.1 Key Takeaways and Next Steps 13.2 Recommended Resources and Papers 13.3 Contributing to Hypothesis</p> <p>\u2e3b</p> <ol> <li>Introduction to Property-Based Testing</li> </ol> <p>1.1 What Is Property-Based Testing?</p> <p>Property-based testing (PBT) shifts your focus from manually enumerating test inputs to describing the properties your code should fulfill for all valid inputs. Instead of hardcoding specific examples (like assert f(2) == 4), you define requirements: e.g., \u201cSorting a list is idempotent.\u201d Then the library (Hypothesis) generates test inputs to find edge cases or scenarios violating those properties.</p> <p>Example</p> <p>from hypothesis import given, strategies as st</p> <p>@given(st.lists(st.integers())) def test_sort_idempotent(xs):     once = sorted(xs)     twice = sorted(once)     assert once == twice</p> <p>Hypothesis tries diverse lists (including empty lists, duplicates, large sizes, negative or positive numbers). If something fails, it shrinks the input to a minimal failing example.</p> <p>1.2 Why Use Property-Based Testing?     \u2022   Coverage of Edge Cases: Automatically covers many corner cases\u2014empty inputs, large values, special floats, etc.     \u2022   Reduced Manual Labor: You specify broad properties, and the tool handles enumerations.     \u2022   Debugging Aid: Found a failing input? Hypothesis shrinks it to a simpler version, making debug cycles shorter.     \u2022   Less Test Boilerplate: Fewer individual test cases to write while achieving higher coverage.</p> <p>1.3 Installing Hypothesis</p> <p>You can install the base library with pip install hypothesis. For specialized extras (e.g., date/time, Django), consult Hypothesis extras docs.</p> <p>\u2e3b</p> <ol> <li>First Steps with Hypothesis</li> </ol> <p>2.1 A Simple Example</p> <p>from hypothesis import given from hypothesis.strategies import integers</p> <p>@given(integers()) def test_square_is_nonnegative(x):     assert x*x &gt;= 0</p> <p>Run with pytest, unittest, or another runner. Hypothesis calls test_square_is_nonnegative multiple times with varied integers (positive, negative, zero).</p> <p>2.2 Basic Workflows and Key Concepts     1.  Test Functions: Decorate with @given().     2.  Generation and Execution: Hypothesis runs tests many times with random values, tries to find failures.     3.  Shrinking: If a failure occurs, Hypothesis narrows down (shrinks) the input to a minimal failing example. <p>2.3 Troubleshooting the First Failures     \u2022   Assertion Errors: If you see Falsifying example: ..., Hypothesis found a failing scenario. Use that scenario to fix your code or refine your property.     \u2022   Health Check Warnings: If you see warnings like \u201cfilter_too_much\u201d or \u201ctoo_slow,\u201d see the Health Checks section.</p> <p>\u2e3b</p> <ol> <li>Core Hypothesis Concepts</li> </ol> <p>3.1 The @given Decorator</p> <p>@given ties strategies to a test function\u2019s parameters:</p> <p>from hypothesis import given from hypothesis.strategies import text, emails</p> <p>@given(email=emails(), note=text()) def test_process_email(email, note):     ...</p> <p>Hypothesis calls test_process_email() repeatedly with random emails and text. If everything passes, the test is green. Otherwise, you get a shrunk failing example.</p> <p>3.2 Strategies: Building and Composing Data Generators</p> <p>Hypothesis\u2019s data generation revolves around \u201cstrategies.\u201d Basic ones:     \u2022   integers(), floats(), text(), booleans(), etc.     \u2022   Containers: lists(elements, ...), dictionaries(keys=..., values=...)     \u2022   Map/Filter: Transform or constrain existing strategies.     \u2022   Composite: Build custom strategies for domain objects.</p> <p>3.3 Shrinking and Minimizing Failing Examples</p> <p>If a test fails on a complicated input, Hypothesis tries simpler versions: removing elements from lists, changing large ints to smaller ints, etc. The final reported failing input is minimal by lex ordering.</p> <p>Falsifying example: test_sort_idempotent(xs=[2, 1, 1])</p> <p>Hypothesis might have started with [random, complicated list] but ended with [2,1,1].</p> <p>3.4 Example Database and Replay</p> <p>Failures are saved in a local .hypothesis/ directory. On subsequent runs, Hypothesis replays known failing inputs before generating fresh ones. This ensures consistent reporting once a failing case is discovered.</p> <p>\u2e3b</p> <ol> <li>Advanced Data Generation</li> </ol> <p>4.1 Understanding Strategies vs. Types</p> <p>Hypothesis does not rely solely on type information. You can define custom constraints to ensure the data you generate matches your domain. E.g., generating only non-empty lists or restricting floats to finite values:</p> <p>import math</p> <p>@given(st.lists(st.floats(allow_infinity=False, allow_nan=False), min_size=1)) def test_mean_in_bounds(xs):     avg = sum(xs)/len(xs)     assert min(xs) &lt;= avg &lt;= max(xs)</p> <p>4.2 Composing Strategies (map, filter, flatmap)     \u2022   map(f) transforms data after generation:</p> <p>even_integers = st.integers().map(lambda x: x * 2)</p> <pre><code>\u2022   filter(pred) discards values that fail pred; be mindful of over-filtering performance.\n\u2022   flatmap(...) draws a value, then uses it to define a new strategy:\n</code></pre>"},{"location":"examples/test_wrapped_hypot_test_gen/#draw-an-int-n-then-a-list-of-length-n","title":"Draw an int n, then a list of length n","text":"<p>st.integers(min_value=0, max_value=10).flatmap(lambda n: st.lists(st.text(), min_size=n, max_size=n))</p> <p>4.3 Working with Complex or Recursive Data</p> <p>For tree-like or nested data, use st.recursive(base_strategy, extend_strategy, max_leaves=...) to limit growth. Also consider the @composite decorator to build logic step by step.</p> <p>from hypothesis import strategies as st, composite</p> <p>@composite def user_records(draw):     name = draw(st.text(min_size=1))     age = draw(st.integers(min_value=0))     return \"name\": name, \"age\": age</p> <p>4.4 Using @composite Functions</p> <p>@composite is a more explicit style than map/flatmap. It helps define multi-step draws within one function. It\u2019s usually simpler for highly interdependent data.</p> <p>4.5 Integration and Edge Cases     \u2022   Ensuring Valid Domain Data: Use composites or partial filtering. Overuse of filter(...) can cause slow tests and health-check failures.     \u2022   Large/Complex Structures: Limit sizes or use constraints (max_size, bounding integers, etc.) to avoid timeouts.</p> <p>\u2e3b</p> <ol> <li>Practical Usage Patterns</li> </ol> <p>5.1 Testing Numeric Code (Floating-Point, Bounds)</p> <p>Floating point nuances:</p> <p>@given(st.floats(allow_nan=False, allow_infinity=False)) def test_floats(x):     ...</p> <p>Constrain or skip NaNs/infinities if your domain doesn\u2019t handle them. Keep an eye on overflows if sums get large.</p> <p>5.2 Text and String Generation (Character Sets, Regex)</p> <p>Hypothesis can generate ASCII, Unicode, or custom sets:</p> <p>from hypothesis.strategies import text</p> <p>@given(text(alphabet=\"ABCDE\", min_size=1)) def test_some_text(s):     assert s[0] in \"ABCDE\"</p> <p>Or use from_regex(r\"MyPattern\") for more specialized scenarios.</p> <p>5.3 Dates, Times, and Time Zones</p> <p>Install hypothesis[datetime] for strategies like dates(), datetimes(), timezones(). These handle cross-timezone issues or restricted intervals.</p> <p>5.4 Combining Hypothesis with Fixtures and Other Test Tools</p> <p>With pytest, you can pass both fixture arguments and Hypothesis strategy arguments:</p> <p>import pytest</p> <p>@pytest.fixture def db():     return init_db()</p> <p>@given(x=st.integers()) def test_db_invariant(db, x):     assert my_query(db, x) == ...</p> <p>Function-scoped fixtures are invoked once per test function, not per example, so plan accordingly or do manual setup for each iteration.</p> <p>\u2e3b</p> <ol> <li>Stateful/Model-Based Testing</li> </ol> <p>6.1 The RuleBasedStateMachine and @rule Decorators</p> <p>For testing stateful systems, Hypothesis uses a rule-based approach:</p> <p>from hypothesis.stateful import RuleBasedStateMachine, rule</p> <p>class SimpleCounter(RuleBasedStateMachine):     def init(self):         super().init()         self.counter = 0</p> <pre><code>@rule(increment=st.integers(min_value=1, max_value=100))\ndef inc(self, increment):\n    self.counter += increment\n    assert self.counter &gt;= 0\n</code></pre> <p>TestCounter = SimpleCounter.TestCase</p> <p>Hypothesis runs random sequences of operations, checking for invariant violations.</p> <p>6.2 Designing Operations and Invariants     \u2022   Each @rule modifies the system under test.     \u2022   Use @precondition to ensure certain rules only fire in valid states.     \u2022   Use @invariant to check conditions after each rule.</p> <p>6.3 Managing Complex State and Multiple Bundles     \u2022   Bundle(...) helps track created objects and pass them between rules.     \u2022   Perfect for simulating CRUD or multi-object interactions.</p> <p>6.4 Example: Testing a CRUD System or Other Stateful API</p> <p>class CRUDSystem(RuleBasedStateMachine):     Records = Bundle('records')</p> <pre><code>@rule(target=Records, data=st.text())\ndef create(self, data):\n    record_id = my_create_fn(data)\n    return record_id\n\n@rule(record=Records)\ndef delete(self, record):\n    my_delete_fn(record)\n</code></pre> <p>Hypothesis will produce sequences of create/delete calls. If a bug arises, it provides a minimal sequence reproducing it.</p> <p>\u2e3b</p> <ol> <li>Performance and Health Checks</li> </ol> <p>7.1 Diagnosing Slow Tests with Deadlines</p> <p>Hypothesis can treat slow examples as errors:</p> <p>from hypothesis import settings, HealthCheck</p> <p>@settings(deadline=100)  # 100ms deadline @given(st.lists(st.integers())) def test_something(xs):     ...</p> <p>If a single test run exceeds 100 ms, it raises DeadlineExceeded. This helps identify performance bottlenecks quickly.</p> <p>7.2 Common Health Check Warnings and Their Meanings     \u2022   filter_too_much: A large proportion of generated data is being thrown away. Fix by refining your strategy or combining strategies (instead of heavy use of filter).     \u2022   too_slow: The test or generation logic is slow. Lower max_examples or investigate your code\u2019s performance.     \u2022   data_too_large: Possibly generating very large structures. Restrict sizes.</p> <p>7.3 Filtering Pitfalls (assume / Over-Filters)</p> <p>Using assume(condition) forcibly discards any example that doesn\u2019t meet condition. Overdoing it can degrade performance drastically. Instead, refine your data strategies:</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#instead-of","title":"Instead of:","text":"<p>@given(st.lists(st.integers()).filter(lambda xs: sum(xs) &lt; 100))</p>"},{"location":"examples/test_wrapped_hypot_test_gen/#use-a-better-approach","title":"Use a better approach:","text":"<p>@given(st.lists(st.integers(max_value=100), max_size=10))</p> <p>7.4 Tuning Hypothesis Settings (max_examples, phases, etc.)     \u2022   max_examples: Controls how many examples are generated per test (default ~200).     \u2022   phases: Choose which parts of the test lifecycle (e.g. \u201cshrink\u201d, \u201creuse\u201d) run.     \u2022   suppress_health_check: Silence known but acceptable warnings.</p> <p>7.5 Speed vs. Thoroughness</p> <p>Balance thorough coverage with test suite runtime. Trim unhelpful extra complexity in data generation. Use deadline or lower max_examples for large test suites.</p> <p>\u2e3b</p> <ol> <li>Multiple Failures and Multi-Bug Discovery</li> </ol> <p>8.1 How Hypothesis Detects and Distinguishes Bugs</p> <p>Hypothesis typically shrinks until it finds the smallest failing example. But if a test can fail in multiple ways, Hypothesis 3.29+ tries to keep track of each distinct bug (by exception type and line number).</p> <p>8.2 Typical Bug Slippage and the \u201cThreshold Problem\u201d     \u2022   Bug Slippage: Starting with one bug scenario but shrinking to a different scenario. Hypothesis tries to keep track and track distinct failures.     \u2022   Threshold Problem: When tests fail due to crossing a numeric threshold, shrunk examples tend to be just barely beyond that threshold, potentially obscuring the severity of the issue. Techniques to mitigate this can involve \u201ctargeting\u201d or custom test logic.</p> <p>8.3 Strategies for Handling Multiple Distinct Failures</p> <p>Hypothesis\u2019s multi-failure mode ensures it shrinks each failing scenario independently. You may see multiple minimal failures reported. This can be turned on automatically if distinct bug states are detected.</p> <p>\u2e3b</p> <ol> <li>Internals: The Conjecture Engine</li> </ol> <p>9.1 Overview of Bytestream-Based Generation</p> <p>Conjecture is the underlying fuzzing engine. It treats every generated example as a lazily consumed byte stream. Strategies interpret segments of bytes as integers, floats, text, etc. This uniform approach:     \u2022   Simplifies storing known failures to replay them.     \u2022   Allows integrated shrinking by reducing or rewriting parts of the byte stream.</p> <p>9.2 Integrated Shrinking vs. Type-Based Shrinking</p> <p>Old or simpler property-based systems often rely on \u201ctype-based\u201d shrinking. Conjecture\u2019s approach integrates shrinking with data generation. This ensures that if you build data by composition (e.g. mapping or flattening strategies), Hypothesis can still shrink effectively.</p> <p>9.3 How Conjecture Tracks and Minimizes Examples     \u2022   Each test run has a \u201cbuffer\u201d of bytes.     \u2022   On failure, Conjecture tries different transformations (removing or reducing bytes).     \u2022   The result is simpler failing input but consistent with the constraints of your strategy.</p> <p>9.4 The Example Database in Depth</p> <p>All interesting examples get stored in .hypothesis/examples by default. On re-run, Hypothesis tries these before generating new data. This yields repeatable failures for regression tests\u2014especially helpful in CI setups.</p> <p>\u2e3b</p> <ol> <li>Hypothesis in Real-World Scenarios</li> </ol> <p>10.1 Using Hypothesis in CI/CD     \u2022   Run Hypothesis-based tests as part of your continuous integration.     \u2022   The example database can be committed to share known failures across devs.     \u2022   Set a deadline or use smaller max_examples to keep test times predictable.</p> <p>10.2 Collaborative Testing in Teams     \u2022   Consistent Strategy Definitions: Keep your custom strategies in a shared \u201cstrategies.py.\u201d     \u2022   Version Control: The .hypothesis directory can be versioned to share known failing examples, though watch out for merge conflicts.</p> <p>10.3 Integrating with Other Tools (pytest, coverage, etc.)     \u2022   Pytest integration is seamless\u2014just write @given tests, run pytest.     \u2022   Coverage tools measure tested code as usual, but remember Hypothesis can deeply cover corner cases.</p> <p>10.4 Best Practices for Large Projects     \u2022   Modular Strategies: Break them down for maintainability.     \u2022   Tackle Invariants Early: Short-circuit with assume() or well-structured strategies.     \u2022   Monitor Performance: Use health checks, deadlines, and max_examples config to scale.</p> <p>\u2e3b</p> <ol> <li>Extensibility and Advanced Topics</li> </ol> <p>11.1 Third-Party Extensions     \u2022   hypothesis-bio: Specialized for bioinformatics data formats.     \u2022   hypothesis-networkx: Generate networkx graphs, test graph algorithms.     \u2022   Many more unofficial or domain-specific libraries exist. Creating your own extension is easy.</p> <p>11.2 Targeted Property-Based Testing (Scoring)</p> <p>You can \u201cguide\u201d test generation by calling target(score) in your code. Hypothesis tries to evolve test cases with higher scores, focusing on \u201cinteresting\u201d or extreme behaviors (like maximizing error metrics).</p> <p>from hypothesis import given, target from hypothesis.strategies import floats</p> <p>@given(x=floats(-1e6, 1e6)) def test_numerical_stability(x):     err = some_error_metric(x)     target(err)     assert err &lt; 9999</p> <p>11.3 Hybrid Approaches (Combining Examples with Generation)</p> <p>You can add \u201cexample-based tests\u201d to complement property-based ones. Also, you can incorporate real-world test data as seeds or partial strategies.</p> <p>11.4 Glass-Box Testing and Potential Future Work</p> <p>Hypothesis largely treats tests as a black box but can be extended with coverage data or other instrumentation for more advanced test generation. This is an open area of R&amp;D.</p> <p>\u2e3b</p> <ol> <li>Troubleshooting and FAQs</li> </ol> <p>12.1 Common Error Messages     \u2022   Unsatisfiable: Hypothesis can\u2019t find enough valid examples. Possibly an over-filter or an unrealistic requirement.     \u2022   DeadlineExceeded: Your test or code is too slow for the set deadline ms.     \u2022   FailedHealthCheck: Usually means you\u2019re doing too much filtering or the example is too large.</p> <p>12.2 Reproduce Failures with @reproduce_failure and Seeds</p> <p>If Hypothesis can\u2019t express your failing data via a standard repr, it shows a snippet like:</p> <p>@reproduce_failure('3.62.0', b'...') def test_something():     ...</p> <p>Adding that snippet ensures the bug is replayed exactly. Alternatively, you can do:</p> <p>from hypothesis import seed</p> <p>@seed(12345) @given(st.integers()) def test_x(x):     ...</p> <p>But seeds alone are insufficient if your .hypothesis database is relevant or if your test uses inline data.</p> <p>12.3 Overcoming Flaky or Non-Deterministic Tests</p> <p>If code is time-sensitive or concurrency-based, you may see spurious failures. Try limiting concurrency, raising deadlines, or disabling shrinking for certain tests. Alternatively, fix the non-determinism in the tested code.</p> <p>12.4 Interpreting Statistics</p> <p>Running pytest --hypothesis-show-statistics yields info on distribution of generated examples, data-generation time vs. test time, etc. This helps find bottlenecks, excessive filtering, or unexpectedly large inputs.</p> <p>\u2e3b</p> <ol> <li>Summary and Further Reading</li> </ol> <p>13.1 Key Takeaways and Next Steps     \u2022   Write Clear Properties: A crisp property is simpler for Hypothesis to exploit.     \u2022   Refine Strategies: Good strategy design yields fewer discards and faster tests.     \u2022   Use Health Checks: They highlight anti-patterns early.     \u2022   Explore Stateful Testing: Perfect for integration tests or persistent-state bugs.</p> <p>13.2 Recommended Resources and Papers     \u2022   Official Hypothesis Documentation     \u2022   QuickCheck papers: Claessen and Hughes, 2000     \u2022   Testing\u2013reduction synergy: Regehr et al. \u201cTest-case Reduction via Delta Debugging\u201d (PLDI 2012)     \u2022   \u201cHypothesis: A New Approach to Property-Based Testing\u201d (HypothesisWorks website)</p> <p>13.3 Contributing to Hypothesis</p> <p>Hypothesis is open source. If you have ideas or find issues:     \u2022   Check our GitHub repo     \u2022   Read the Contributing Guide     \u2022   Every improvement is welcomed\u2014documentation, bug reports, or code!</p> <p>\u2e3b</p> <p>Final Thoughts</p> <p>We hope this unified, comprehensive guide helps you unlock the power of Hypothesis. From quick introductions to advanced stateful testing, from performance pitfalls to internal design details, you now have a toolkit for robust property-based testing in Python.</p> <p>Happy testing! If you run into any questions, re-check the relevant sections here or visit the community resources. Once you incorporate Hypothesis into your testing workflow, you might find hidden bugs you never anticipated\u2014and that\u2019s the point!</p>"},{"location":"guides/training_loss_hypothesis_analysis/","title":"RNA_PREDICT Training &amp; Loss Function: Hypothesis-Driven Analysis","text":"<p>Date: 2025-05-03 Branch: <code>fix/training-loss-function-implementation</code></p>"},{"location":"guides/training_loss_hypothesis_analysis/#purpose","title":"Purpose","text":"<p>This document systematically analyzes the current state of the training and loss function implementation in the RNA_PREDICT pipeline. The analysis follows a hypothesis-driven approach to confirm or reject key concerns about the pipeline's training signal and gradient flow, based on code inspection and evidence.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#key-hypotheses-evidence","title":"Key Hypotheses &amp; Evidence","text":""},{"location":"guides/training_loss_hypothesis_analysis/#h1-the-current-training-step-only-computes-loss-on-stage-c-outputs-not-on-stage-d-outputs","title":"H1: The current training step only computes loss on Stage C outputs, not on Stage D outputs.","text":"<p>Evidence: - The <code>training_step</code> method in <code>rna_lightning_module.py</code> computes loss using <code>predicted_coords = output[\"coords\"]</code> (output from Stage C). - No evidence of Stage D output being used for loss. - The loss is MSE between these coordinates and ground truth. Conclusion: - Accepted. Loss is only on Stage C outputs.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#h2-stage-d-diffusion-is-not-called-at-all-during-training","title":"H2: Stage D (Diffusion) is not called at all during training.","text":"<p>Evidence: - In both <code>forward</code> and <code>training_step</code>, there is no call to <code>self.stageD</code>. - Stage D is instantiated but not invoked in the forward or training path. Conclusion: - Accepted. Stage D is not called during training.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#h3-pairformer-stage-b-is-not-trained-as-its-outputs-do-not-contribute-to-the-loss","title":"H3: Pairformer (Stage B) is not trained, as its outputs do not contribute to the loss.","text":"<p>Evidence: - Pairformer outputs are calculated, but only Stage C output is used for loss. - No direct or indirect use of Pairformer outputs in the loss computation. - Gradients would not flow to Pairformer parameters. Conclusion: - Accepted. Pairformer is not being trained.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#h4-torsionbert-is-trained-but-only-indirectly-via-stage-cs-coordinate-reconstruction","title":"H4: TorsionBERT is trained, but only indirectly via Stage C\u2019s coordinate reconstruction.","text":"<p>Evidence: - TorsionBERT outputs torsion angles, which are used by Stage C to generate coordinates. - The loss is on coordinates, so gradients flow back through Stage C to TorsionBERT. - There is no direct angle supervision. Conclusion: - Accepted. TorsionBERT is trained, but only indirectly.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#h5-there-is-no-direct-angle-supervision-loss-l_angle-in-the-current-training-step","title":"H5: There is no direct angle supervision loss (<code>L_angle</code>) in the current training step.","text":"<p>Evidence: - No angle-based loss found in <code>training_step</code>. - Only coordinate-based MSE loss is present. Conclusion: - Accepted. No direct angle supervision loss is implemented.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#h6-the-loss-function-used-for-stage-d-is-not-the-correct-denoisingdiffusion-objective","title":"H6: The loss function used for Stage D is not the correct denoising/diffusion objective.","text":"<p>Evidence: - No diffusion/denoising loss is implemented. - No noise sampling, no call to Stage D, no diffusion loss logic. Conclusion: - Accepted. Diffusion loss is not implemented.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#summary-table","title":"Summary Table","text":"Hypothesis Status Evidence/Notes H1 Accepted Loss is only on Stage C output coords H2 Accepted Stage D not called in forward/training_step H3 Accepted Pairformer outputs not used in loss H4 Accepted TorsionBERT trained indirectly via coords H5 Accepted No direct angle loss present H6 Accepted No diffusion loss or Stage D integration"},{"location":"guides/training_loss_hypothesis_analysis/#conclusions-recommendations","title":"Conclusions &amp; Recommendations","text":"<ul> <li>The current training implementation is misaligned with the intended multi-stage pipeline design.</li> <li>Only TorsionBERT receives a training signal, and only indirectly.</li> <li>Pairformer and Stage D receive no training signal at all.</li> <li>No direct angle supervision or diffusion loss is present.</li> </ul> <p>Actionable Steps: 1. Implement direct angle supervision (<code>L_angle</code>). 2. Integrate Stage D into the forward and training path. 3. Implement the correct diffusion loss objective. 4. Ensure gradients flow to all intended components (TorsionBERT, Pairformer, Stage D). 5. Test with debug output and gradient checks after each incremental change.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#this-analysis-provides-a-clear-evidence-based-foundation-for-the-required-refactoring-and-implementation-work","title":"This analysis provides a clear, evidence-based foundation for the required refactoring and implementation work.","text":"<p>Okay, let's systematically break down the training and loss function situation in your <code>RNA_PREDICT</code> pipeline. Your analysis that the current setup might be incorrect is spot-on, particularly given the staged nature and the different model types involved.</p> <p>Refined Hypothesis:</p> <p>The current training implementation in <code>RNALightningModule</code> is fundamentally misaligned with the intended pipeline design described in the documentation (internal design docs, TorsionBERT paper, AF3 paper). Specifically:</p> <ol> <li>The loss is calculated based on the 3D coordinate output of Stage C (MP-NeRF), not the final output of Stage D (Diffusion).</li> <li>This results in Stage D (Diffusion model) and Stage B (Pairformer) receiving no training signal (zero gradients).</li> <li>Stage B (TorsionBERT) is being trained, but indirectly and suboptimally, via gradients flowing back through the Stage C geometric reconstruction, rather than through direct angle supervision or via gradients from the final Stage D output.</li> <li>The current MSE loss on coordinates is inappropriate for training the Stage D diffusion model, which requires a specific denoising objective.</li> </ol> <p>This discrepancy means the core components responsible for global context (Pairformer) and final structure refinement (Diffusion) are not learning, and TorsionBERT is learning from a potentially noisy, indirect signal.</p> <p>Detailed Analysis:</p> <ol> <li> <p>Analysis of <code>RNALightningModule</code> Training Code:</p> <ul> <li><code>forward</code> method: Executes Stages A, B (Torsion &amp; Pairformer), and C sequentially. It produces <code>coords</code> from Stage C. Crucially, it does not execute <code>self.stageD</code> (ProtenixDiffusionManager).</li> <li><code>training_step</code> method:<ul> <li>Calls <code>self.forward(batch)</code>.</li> <li>Extracts <code>predicted_coords = output[\"coords\"]</code> (output of Stage C).</li> <li>Calculates <code>loss = MSE(predicted_coords, real_target_coords)</code>.</li> <li>The complex masking logic attempts to align atoms for this Stage C vs Ground Truth comparison.</li> <li>It checks <code>predicted_coords.requires_grad</code>, confirming the intent to train upstream models.</li> </ul> </li> </ul> </li> <li> <p>Analysis of Gradient Flow:</p> <ul> <li>Loss originates from Stage C's output MSE.</li> <li>Gradients flow backward: <code>Loss -&gt; predicted_coords</code>.</li> <li>Stage C (MP-NeRF): Since it's differentiable geometric operations based on input angles (<code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_fold</code>), gradients flow through it to its input (<code>torsion_angles</code>). As stated, Stage C has no trainable parameters, so it's just a conduit.</li> <li>Stage B (TorsionBERT): Receives gradients because <code>torsion_angles</code> (its output) is the input to the differentiable Stage C. It IS being trained, but the signal is \"produce angles that, after MP-NeRF reconstruction, match the ground truth 3D coords\". This is indirect.</li> <li>Stage B (Pairformer): Its outputs (<code>s_embeddings</code>, <code>z_embeddings</code>) are calculated but do not contribute to the <code>predicted_coords</code> used in the loss. They only feed the <code>latent_merger</code>, whose output is also ignored by the loss. Pairformer is NOT being trained.</li> <li>Stage D (Diffusion): The <code>self.stageD</code> module is never called in the training forward path. Stage D is NOT being trained.</li> <li>Stage A (RFold): No trainable parameters, not involved in loss. Not trained.</li> </ul> </li> <li> <p>Cross-Referencing with Documentation:</p> <ul> <li>TorsionBERT Paper (<code>torsionBert_full_paper.md</code>):<ul> <li>Focus: Predicting angles accurately. Metrics: MCQ, MAE on angles.</li> <li>Training: Direct supervision comparing predicted angles (or sin/cos) to ground truth angles.</li> <li>Discrepancy: Current loss trains indirectly via 3D coords, lacking direct angle supervision.</li> </ul> </li> <li>AlphaFold 3 Paper (<code>AF3_paper.md</code> Supplement):<ul> <li>Pairformer (Trunk): Produces <code>s_i</code>, <code>z_ij</code> to condition the Diffusion module. Trained implicitly via gradients from the final diffusion loss (AF3 Eq 15).</li> <li>Discrepancy: Current setup provides no gradient to Pairformer.</li> <li>Diffusion Module: Trained using a denoising objective (AF3 Eq 6 - weighted MSE, LDDT loss, etc.) on noisy coordinates, conditioned on trunk embeddings.</li> <li>Discrepancy: Current loss (MSE on Stage C output) is fundamentally different. Stage D isn't run.</li> </ul> </li> <li>Internal Design Docs (<code>Integrated_RNA_3D...</code>, <code>full_pipeline_spec...</code>, <code>core_framework...</code>):<ul> <li>Flow: Stages A/B -&gt; Merger -&gt; Stage D -&gt; Final Coords.</li> <li>Combined Loss: Explicitly mention <code>L_3D</code> (on Stage D output) + <code>L_angle</code> (for TorsionBERT) + optional <code>L_pair</code>.</li> <li>Discrepancy: Current loss is on Stage C output, lacks Stage D integration, and lacks <code>L_angle</code>.</li> </ul> </li> </ul> </li> <li> <p>Conclusion Confirmation:</p> <ul> <li>The current training loss setup is significantly flawed. It trains only TorsionBERT, and does so indirectly. It completely ignores Pairformer and the entire Stage D Diffusion model, which are critical components according to the design documents and underlying model papers.</li> </ul> </li> </ol> <p>Actionable Recommendations (Refined Implementation Plan):</p> <p>To correctly train the intended pipeline, the <code>RNALightningModule</code> needs substantial refactoring, primarily in the <code>training_step</code>.</p> <p>Phase 1: Prerequisites &amp; Direct Angle Supervision</p> <ol> <li> <p>Enable Ground Truth Angle Loading:</p> <ul> <li>File: <code>rna_predict/conf/data/default.yaml</code><ul> <li>Action: Set <code>load_ang: true</code>.</li> <li>Action: Ensure paths/patterns for loading angle data are correctly configured if not derivable from <code>index_csv</code>.</li> </ul> </li> <li>File: <code>rna_predict/dataset/loader.py</code> (<code>RNADataset</code>)<ul> <li>Action: Implement <code>_load_angles</code> to read angle data (e.g., from <code>.pt</code>, <code>.npy</code>). Handle missing data.</li> <li>Action: In <code>__getitem__</code>, call <code>_load_angles</code> and add the tensor as <code>\"angles_true\"</code> to the sample dictionary. Ensure correct padding/dtype.</li> </ul> </li> <li>File: <code>rna_predict/dataset/collate.py</code> (<code>rna_collate_fn</code>)<ul> <li>Action: Ensure the collate function correctly handles and batches the <code>\"angles_true\"</code> tensor.</li> </ul> </li> <li>Verification: Load a batch and confirm <code>\"angles_true\"</code> exists with shape <code>[B, N, 7]</code> (or similar) and correct dtype/device.</li> </ul> </li> <li> <p>Implement Direct Angle Loss (<code>L_angle</code>) in <code>training_step</code>:</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>)</li> <li>Action: Inside <code>training_step</code>, after <code>output = self.forward(batch)</code>:<ul> <li><code>predicted_angles_sincos = output[\"torsion_angles\"]</code> # Shape [B, N, 14]</li> <li><code>true_angles_rad = batch[\"angles_true\"]</code> # Shape [B, N, 7]</li> <li>Implement/import <code>angles_rad_to_sin_cos(true_angles_rad)</code> -&gt; <code>true_angles_sincos</code> # Shape [B, N, 14]</li> <li><code>loss_angle = F.mse_loss(predicted_angles_sincos, true_angles_sincos)</code> (apply masking if needed based on sequence length).</li> <li>Store <code>loss_angle</code>.</li> </ul> </li> <li>Verification: Temporarily return <code>{\"loss\": loss_angle}</code>. Train 1 step. Check <code>self.stageB_torsion</code> parameters have non-None <code>.grad</code>.</li> </ul> </li> </ol> <p>Phase 2: Integrating Stage D into Training</p> <ol> <li> <p>Prepare Inputs for Stage D within <code>training_step</code>:</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>)</li> <li>Action: Gather inputs needed by <code>self.stageD.forward</code> (or a dedicated train method).<ul> <li><code>coords_true = batch[\"coords_true\"]</code> # Shape [B, N_atom, 3]</li> <li>Sample noise level <code>sigma_t</code> (shape <code>[B]</code>) based on a schedule (e.g., AF3's exponential or simpler linear).</li> <li>Generate noise <code>epsilon</code> (shape like <code>coords_true</code>).</li> <li><code>coords_noisy = coords_true + epsilon * sigma_t.view(-1, 1, 1)</code>.</li> <li>Retrieve Stage B outputs: <code>s_embeddings</code> (<code>[B, N_res, C_s]</code>), <code>z_embeddings</code> (<code>[B, N_res, N_res, C_z]</code>).</li> <li>(Bridging Required): Bridge residue-level <code>s_embeddings</code> (and potentially <code>z_embeddings</code>) to atom-level using <code>rna_predict.utils.tensor_utils.embedding.residue_to_atoms</code> and <code>batch[\"atom_to_token_idx\"]</code>. Let the result be <code>s_embeddings_atom</code>, <code>z_embeddings_atom</code>. This is critical if Stage D expects atom-level conditioning.</li> <li>(Optional Merger): If using <code>self.latent_merger</code>, feed it the necessary (potentially bridged) embeddings, angles, adjacency to get <code>unified_latent</code>. Ensure its output level (residue/atom) matches Stage D's expectation.</li> <li>Define <code>conditioning_signal</code> (e.g., <code>s_embeddings_atom</code>, <code>z_embeddings_atom</code>, or <code>unified_latent</code>).</li> </ul> </li> </ul> </li> <li> <p>Execute Stage D Training Step:</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>)</li> <li>Action: Call <code>self.stageD</code>'s forward/training method.<ul> <li><code>stage_d_pred = self.stageD(coords_noisy=coords_noisy, conditioning=conditioning_signal, noise_level=sigma_t, **other_stageD_args)</code></li> </ul> </li> <li>Verification: Check input/output shapes and types. Ensure no runtime errors.</li> </ul> </li> <li> <p>Implement Diffusion Loss (<code>L_diffusion</code>):</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>)</li> <li>Action: Calculate loss based on <code>stage_d_pred</code>.<ul> <li>If <code>stage_d_pred</code> is predicted noise <code>\\hat{\\epsilon}</code>:<ul> <li><code>loss_diffusion = F.mse_loss(stage_d_pred, epsilon)</code> (apply masking).</li> </ul> </li> <li>If <code>stage_d_pred</code> is predicted denoised coords <code>\\hat{x}_0</code> (like AF3):<ul> <li>Requires implementing <code>weighted_rigid_align</code> (Algo 28) and <code>SmoothLDDTLoss</code> (Algo 27).</li> <li><code>aligned_coords_true = weighted_rigid_align(coords_true, stage_d_pred, weights)</code></li> <li><code>loss_mse = weighted_mse(stage_d_pred, aligned_coords_true, weights)</code></li> <li><code>loss_lddt = SmoothLDDTLoss(stage_d_pred, aligned_coords_true)</code></li> <li><code>loss_diffusion = weight_factor * (loss_mse + w_lddt * loss_lddt)</code> (weight_factor depends on <code>sigma_t</code> per AF3 Eq 6).</li> <li>Recommendation: Start with the simpler noise prediction objective if possible, unless Stage D is explicitly designed for coordinate prediction.</li> </ul> </li> </ul> </li> <li>Verification: Check <code>loss_diffusion</code> is scalar. Temporarily return <code>{\"loss\": loss_diffusion}</code>. Train 1 step. Verify gradients flow to <code>self.stageD</code> parameters and back to the conditioning parameters (Pairformer, TorsionBERT, Merger).</li> </ul> </li> </ol> <p>Phase 3: Final Integration</p> <ol> <li> <p>Combine and Log Losses:</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>)</li> <li>Action: Define loss weights in config (e.g., <code>cfg.training.w_diffusion</code>, <code>cfg.training.w_angle</code>).</li> <li>Action: Calculate <code>total_loss = cfg.training.w_diffusion * loss_diffusion + cfg.training.w_angle * loss_angle</code>.</li> <li>Action: Use <code>self.log(...)</code> to log <code>total_loss</code>, <code>loss_diffusion</code>, <code>loss_angle</code>.</li> <li>Action: Return <code>{\"loss\": total_loss}</code>.</li> </ul> </li> <li> <p>Configuration &amp; Dimension Review:</p> <ul> <li>Files: <code>rna_predict/conf/**</code></li> <li>Action: Systematically ensure all embedding dimensions (<code>c_s</code>, <code>c_z</code>, <code>c_atom</code>, <code>num_angles</code>, merger dims, Stage D conditioning dims) are consistent across stage configurations and the <code>config_schema.py</code>. Pay close attention to residue-vs-atom level dimensions during bridging.</li> </ul> </li> <li> <p>Testing Strategy:</p> <ul> <li>Unit Tests: For angle conversion, loss calculations (diffusion, angle), merger.</li> <li>Integration Test (<code>test_lightning_trainer.py</code>): Train for a few steps on 1-2 samples. Assert non-None gradients for parameters in TorsionBERT (LoRA), Pairformer (LoRA), and Stage D.</li> <li>Monitor Training: Use TensorBoard/W&amp;B to watch loss components decrease on a small dataset. Use loss weights to isolate components if debugging is needed.</li> </ul> </li> </ol>"},{"location":"guides/training_loss_hypothesis_analysis/#this-detailed-plan-addresses-the-identified-discrepancies-and-provides-a-concrete-path-to-implement-a-correct-and-effective-training-loop-for-your-multi-stage-rna-prediction-pipeline-the-most-complex-parts-will-be-the-residue-to-atom-bridging-for-stage-d-conditioning-and-implementing-the-chosen-diffusion-loss-objective-accurately","title":"This detailed plan addresses the identified discrepancies and provides a concrete path to implement a correct and effective training loop for your multi-stage RNA prediction pipeline. The most complex parts will be the residue-to-atom bridging for Stage D conditioning and implementing the chosen diffusion loss objective accurately.","text":"<p>Okay, let's perform a detailed, critical deep dive into Phase 1: Enable Ground Truth Angle Loading of the proposed plan. This is a foundational step, and getting the data loading right is crucial for implementing the direct angle loss later.</p> <p>Goal of Phase 1: Modify the data pipeline (<code>RNADataset</code> and <code>rna_collate_fn</code>) to reliably load, process (pad/truncate), and batch ground truth torsion angles, making them available as <code>batch[\"angles_true\"]</code> with the correct shape, dtype, and device in the <code>RNALightningModule.training_step</code>.</p> <p>1. File: <code>rna_predict/conf/data/default.yaml</code></p> <ul> <li> <p>Action 1.1: Enable Angle Loading Flag</p> <ul> <li>Code Change: <pre><code># rna_predict/conf/data/default.yaml\n# ... (other keys) ...\nload_adj: false\nload_ang: true   # &lt;&lt;&lt; CHANGE THIS LINE from false to true\n# ... (other keys like coord_fill_value, coord_dtype) ...\n# Optional: Add flag for detailed batch inspection during debugging\ndebug_inspect_batch: false # Set to true via CLI or test config to log first batch details\n</code></pre></li> <li>Rationale: This boolean flag acts as a switch for the <code>RNADataset</code> to activate the angle loading logic. It allows flexibility to train with or without angle supervision without changing code.</li> <li>Critical Consideration: This flag only enables the attempt to load angles. It doesn't guarantee the data exists or is correctly formatted.</li> </ul> </li> <li> <p>Action 1.2: Define Angle Data Location and Format Strategy</p> <ul> <li>Strategy Decision: Assume angle data for a structure (e.g., <code>data_root/1abc.cif</code>) is stored in a corresponding file (e.g., <code>data_root/1abc.pt</code>). This requires a consistent naming convention.<ul> <li>File Extension: Assume <code>.pt</code> (PyTorch tensor file).</li> <li>Data Format: Assume the <code>.pt</code> file directly contains a <code>torch.Tensor</code>.</li> <li>Tensor Shape: Assume shape <code>[L, 7]</code>, where <code>L</code> is the number of residues in the sequence, and 7 corresponds to the standard torsions (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7).</li> <li>Units: Assume angles are stored in radians. This is the standard for trigonometric functions in PyTorch/NumPy.</li> <li>Dtype: Assume <code>torch.float32</code>.</li> </ul> </li> <li>Configuration Impact: No new configuration keys are needed immediately if this convention is followed. The existing <code>root_dir</code> and the file path derived from <code>index_csv</code> (e.g., <code>row['filepath']</code> or similar) will be used to construct the angle file path.</li> <li>Alternative Strategies (If needed later):<ul> <li>Add <code>data.angle_dir</code> key if angles are in a separate directory.</li> <li>Add <code>data.angle_suffix</code> key if the extension isn't <code>.pt</code>.</li> <li>Add a column like <code>angle_filepath</code> to <code>index_csv</code>.</li> <li>Store angles within a larger <code>.pt</code> or <code>.hdf5</code> file containing multiple data types per structure.</li> </ul> </li> <li>Documentation: This chosen strategy (same base name, <code>.pt</code> extension, <code>[L, 7]</code> shape, radians) must be clearly documented for anyone preparing the training data.</li> </ul> </li> </ul> <p>2. File: <code>rna_predict/dataset/loader.py</code> (<code>RNADataset</code>)</p> <ul> <li> <p>Action 2.1: Imports</p> <ul> <li>Code Change: <pre><code>from pathlib import Path # Ensure Path is imported\nfrom typing import Optional # Add Optional for type hinting\n# ... other imports ...\n</code></pre></li> </ul> </li> <li> <p>Action 2.2: Implement <code>_load_angles</code> Method</p> <ul> <li>Code Implementation: <pre><code># Inside RNADataset class\n\ndef _get_structure_filepath(self, row) -&gt; Path:\n    \"\"\"Helper to consistently get the structure filepath.\"\"\"\n    # !! ADAPT THIS based on your actual index_csv structure !!\n    # Example 1: If CSV has a 'filepath' column\n    if 'filepath' in row and pd.notna(row['filepath']):\n        file_path_str = row['filepath']\n    # Example 2: If CSV has 'pdb_id' and 'chain' (e.g., \"1abc_A\")\n    elif 'target_id' in row and pd.notna(row['target_id']):\n         # Assuming target_id might be like \"PDBID_CHAIN\" or just \"PDBID\"\n         pdb_id = row['target_id'].split('_')[0]\n         # Assuming extension is .cif, could be .pdb\n         file_path_str = f\"{pdb_id}.cif\"\n    else:\n        raise ValueError(f\"Cannot determine structure file path from row: {row.to_dict()}\")\n\n    file_path = Path(file_path_str)\n    if not file_path.is_absolute():\n        file_path = Path(self.root_dir) / file_path # Assumes root_dir is absolute or relative to CWD\n\n    if not file_path.exists():\n         raise FileNotFoundError(f\"Structure file not found at resolved path: {file_path}\")\n    return file_path\n\ndef _load_angles(self, row) -&gt; Optional[torch.Tensor]:\n    \"\"\"\n    Loads ground truth torsion angles from a .pt file based on the\n    structure file path derived from the row.\n    Assumes angle file has the same base name as structure file but with .pt extension.\n    Returns None if file not found or data is invalid.\n    \"\"\"\n    angle_path = None # Initialize for error logging\n    try:\n        structure_path = self._get_structure_filepath(row)\n        angle_path = structure_path.with_suffix(\".pt\")\n\n        if not angle_path.is_file():\n            if self.verbose:\n                logger.warning(f\"Angle file not found for {structure_path.name} at expected location: {angle_path}\")\n            return None\n\n        # Load the tensor data\n        angles_data = torch.load(angle_path, map_location='cpu') # Load to CPU first\n\n        # --- Data Validation ---\n        if not isinstance(angles_data, torch.Tensor):\n            logger.error(f\"Invalid data type in angle file {angle_path}. Expected torch.Tensor, got {type(angles_data)}.\")\n            return None\n\n        if angles_data.dim() != 2:\n            logger.error(f\"Invalid dimensions in angle file {angle_path}. Expected 2D tensor [L, num_angles], got {angles_data.dim()}D.\")\n            return None\n\n        num_angles_loaded = angles_data.shape[1]\n        # Check if number of angles is reasonable (e.g., 7 standard, maybe more if pseudo included)\n        if num_angles_loaded &lt; 7: # Allow for more than 7 if pseudo-torsions are included\n             logger.warning(f\"Unexpected number of angles ({num_angles_loaded}) in {angle_path}. Expected at least 7. Using loaded data.\")\n             # Decide if this should be an error or just a warning\n\n        # Length check will happen in __getitem__ after sequence is loaded\n        # Ensure dtype is float32\n        return angles_data.to(torch.float32)\n\n    except FileNotFoundError as e:\n         # This is handled by _get_structure_filepath now, but keep as fallback\n         logger.error(f\"Structure file error for row {row.get('id', 'N/A')}: {e}\")\n         return None\n    except Exception as e:\n        logger.error(f\"Error loading or validating angles for {row.get('id', 'N/A')} from {angle_path if angle_path else 'unknown path'}: {e}\", exc_info=self.verbose)\n        return None\n</code></pre></li> <li>Critical Considerations:<ul> <li>Robust Path Finding: The <code>_get_structure_filepath</code> needs to correctly interpret your <code>index_csv</code> to find the base path. The example provided covers common scenarios but might need adjustment. Ensure it correctly resolves relative paths using <code>self.root_dir</code>.</li> <li>Validation: The checks for tensor type and dimensions are crucial. Decide how strictly to enforce the number of columns (e.g., exactly 7, or at least 7).</li> <li>Error Logging: Provide informative logs, especially when files are missing or data is invalid. Include <code>exc_info=self.verbose</code> for detailed tracebacks during debugging.</li> </ul> </li> </ul> </li> <li> <p>Action 2.3: Modify <code>__getitem__</code></p> <ul> <li>Code Implementation: <pre><code># Inside RNADataset.__getitem__:\ndef __getitem__(self, idx):\n    row = self.df.iloc[idx]\n    item_id = row.get('id', idx) # For logging\n\n    # --- Load Sequence (Assume _load_sequence exists) ---\n    sequence = self._load_sequence(row)\n    if sequence is None:\n         logger.error(f\"Failed to load sequence for item {item_id}. Skipping sample.\")\n         # Returning an empty dict or raising an error might be necessary depending on DataLoader robustness\n         return {} # Or raise appropriate error\n    L_res = len(sequence)\n\n    # --- Load Coords &amp; Mask (Assume _load_coords exists) ---\n    coords_true, atom_mask, L_atoms = self._load_coords(row) # Assume returns tuple\n    if coords_true is None or atom_mask is None:\n         logger.error(f\"Failed to load coordinates/mask for item {item_id}. Skipping sample.\")\n         return {} # Or raise\n\n    # --- Load Angles (Conditional) ---\n    angles_true_loaded = None\n    if self.load_ang:\n        angles_true_loaded = self._load_angles(row)\n\n    # --- Process Angles (Validation, Placeholder Creation, Padding) ---\n    angles_true_processed = None\n    expected_angle_dim = 7 # Define expected dimension\n\n    if angles_true_loaded is not None:\n        # Validate length against sequence length\n        if angles_true_loaded.shape[0] != L_res:\n            logger.error(f\"Angle length mismatch for {item_id}: Angles({angles_true_loaded.shape[0]}) != Sequence({L_res}). Using zero placeholder.\")\n            angles_true_processed = torch.zeros((L_res, expected_angle_dim), dtype=torch.float32)\n        else:\n            # Use loaded angles, ensure correct dim if needed\n            if angles_true_loaded.shape[1] != expected_angle_dim:\n                 logger.warning(f\"Angle dim mismatch for {item_id}: Expected {expected_angle_dim}, got {angles_true_loaded.shape[1]}. Slicing/Padding last dim.\")\n                 # Slice or pad the feature dimension (dim 1)\n                 if angles_true_loaded.shape[1] &gt; expected_angle_dim:\n                      angles_true_processed = angles_true_loaded[:, :expected_angle_dim]\n                 else:\n                      pad_needed = expected_angle_dim - angles_true_loaded.shape[1]\n                      padding = torch.zeros((L_res, pad_needed), dtype=torch.float32)\n                      angles_true_processed = torch.cat([angles_true_loaded, padding], dim=1)\n            else:\n                 angles_true_processed = angles_true_loaded.to(torch.float32) # Ensure dtype\n    elif self.load_ang:\n         # Loading was enabled but failed or file not found\n         logger.warning(f\"Could not load angles for {item_id}. Using zero placeholder.\")\n         angles_true_processed = torch.zeros((L_res, expected_angle_dim), dtype=torch.float32)\n    # else: load_ang is False, angles_true_processed remains None\n\n    # Apply padding/truncation to max_res\n    if angles_true_processed is not None:\n        if L_res &gt; self.max_res:\n            angles_true_final = angles_true_processed[:self.max_res, :]\n        elif L_res &lt; self.max_res:\n            pad_len = self.max_res - L_res\n            padding = torch.zeros((pad_len, angles_true_processed.shape[1]), dtype=torch.float32)\n            angles_true_final = torch.cat([angles_true_processed, padding], dim=0)\n        else:\n            angles_true_final = angles_true_processed\n    else:\n         angles_true_final = None # Keep as None if load_ang was false\n\n    # --- Construct Final Sample Dictionary ---\n    sample = {\n        'id': item_id,\n        'sequence': sequence, # Already loaded &amp; processed\n        'coords_true': coords_true, # Already loaded &amp; processed\n        'atom_mask': atom_mask, # Already loaded &amp; processed\n        # ... include ALL other keys needed by the model/collate_fn ...\n        # 'atom_to_token_idx': ...,\n        # 'ref_element': ...,\n        # 'ref_atom_name_chars': ...,\n        # 'residue_indices': ... # Add this if needed\n    }\n\n    if angles_true_final is not None:\n         sample['angles_true'] = angles_true_final\n\n    # Example: Add residue_indices if not already loaded\n    if 'residue_indices' not in sample and 'atom_to_token_idx' in sample:\n         # Assuming atom_to_token_idx maps atom index to residue index\n         # This is a placeholder - adapt based on your actual data structure\n         sample['residue_indices'] = sample['atom_to_token_idx']\n\n    return sample\n</code></pre></li> <li>Critical Considerations:<ul> <li>Order of Operations: Ensure <code>L_res</code> (sequence length) is known before creating placeholder tensors or performing length validation for angles.</li> <li>Placeholder Strategy: Using <code>torch.zeros</code> as a placeholder for missing/failed loads is common, but relies on the loss function correctly handling masking (e.g., using an attention mask derived from sequence length). Alternatively, samples with missing crucial data could be skipped entirely.</li> <li>Dimension Handling: Explicitly check and handle potential mismatches in the number of angles (dimension 1) if your data source might vary.</li> <li>Completeness: Ensure all keys required by the <code>rna_collate_fn</code> and the model are present in the final <code>sample</code> dictionary.</li> </ul> </li> </ul> </li> </ul> <p>3. File: <code>rna_predict/dataset/collate.py</code> (<code>rna_collate_fn</code>)</p> <ul> <li>Action 3.1: Add Handling for <code>angles_true</code> Key<ul> <li>Code Implementation: <pre><code>import torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef rna_collate_fn(batch, debug_logging=False):\n    collated_batch = {}\n    # Determine keys present in the first sample to handle optional keys\n    if not batch:\n        return {}\n    sample_keys = batch[0].keys()\n\n    # Keys expected to be stacked (typically already padded in dataset)\n    keys_to_stack = ['coords_true', 'atom_mask', 'angles_true'] # Add angles_true here\n    # Keys needing padding (if variable length in dataset, less common now)\n    keys_to_pad = []\n    # Keys to collect into lists\n    keys_to_list = ['id', 'sequence'] # Add other non-tensor keys\n\n    for key in sample_keys:\n        if key in keys_to_stack:\n            try:\n                # Check if all items have this key and it's a tensor\n                if all(key in item and isinstance(item[key], torch.Tensor) for item in batch):\n                     tensors = [item[key] for item in batch]\n                     # Check if shapes match before stacking (common error source)\n                     if len(set(t.shape for t in tensors)) &gt; 1:\n                         logger.error(f\"Collate Error: Inconsistent shapes for key '{key}'. Shapes: {[t.shape for t in tensors]}\")\n                         collated_batch[key] = None # Or raise error\n                         continue\n                     collated_batch[key] = torch.stack(tensors, dim=0)\n                else:\n                     logger.warning(f\"Collate Warning: Key '{key}' missing or not a tensor in some batch items. Skipping.\")\n                     collated_batch[key] = None\n            except Exception as e:\n                logger.error(f\"Error stacking key '{key}': {e}\", exc_info=debug_logging)\n                collated_batch[key] = None\n        elif key in keys_to_pad:\n             # Add padding logic here if needed\n             pass\n        elif key in keys_to_list:\n             collated_batch[key] = [item.get(key) for item in batch]\n        # Handle other keys if necessary, or ignore them\n        # else:\n        #    if debug_logging: logger.debug(f\"Collate: Ignoring key '{key}'\")\n\n    # --- Add Debug Logging for final batch shapes ---\n    if debug_logging:\n         logger.debug(\"--- Final Collated Batch Shapes/Types ---\")\n         for k, v in collated_batch.items():\n             if isinstance(v, torch.Tensor):\n                 logger.debug(f\"Key: '{k}', Shape: {v.shape}, Dtype: {v.dtype}, Device: {v.device}\")\n             elif isinstance(v, list):\n                  logger.debug(f\"Key: '{k}', Type: list, Length: {len(v)}\")\n             else:\n                 logger.debug(f\"Key: '{k}', Type: {type(v)}\")\n         logger.debug(\"------------------------------------------\")\n\n    return collated_batch\n</code></pre></li> <li>Critical Considerations:<ul> <li>Shape Consistency: The most common failure in collation is attempting to stack tensors of different shapes. The added check <code>len(set(t.shape for t in tensors)) &gt; 1</code> helps catch this. This relies on the <code>RNADataset</code> correctly padding/truncating all <code>angles_true</code> tensors to <code>[max_res, 7]</code>.</li> <li>Handling Missing Keys: The code now checks if the key exists and is a tensor before attempting to stack, logging a warning if it's inconsistent across the batch.</li> <li>Completeness: Ensure the collation handles all necessary keys from the dataset samples.</li> </ul> </li> </ul> </li> </ul> <p>4. Verification Plan:</p> <ul> <li> <p>Setup:</p> <ul> <li>Create a <code>test_data</code> directory.</li> <li>Inside, create <code>test_index.csv</code> with at least two rows pointing to structure files:     <pre><code>id,filepath\nsample1,test_data/sample1.pdb\nsample2,test_data/sample2.cif\n# Add a row that might lack an angle file if testing error handling\n# sample3,test_data/sample3.pdb\n</code></pre></li> <li>Create dummy <code>sample1.pdb</code> (e.g., 10 residues) and <code>sample2.cif</code> (e.g., 20 residues). Exact content doesn't matter much, just needs to be parsable by your <code>_load_coords</code> / <code>_load_sequence</code>.</li> <li>Create <code>sample1.pt</code> containing <code>torch.randn(10, 7, dtype=torch.float32)</code>.</li> <li>Create <code>sample2.pt</code> containing <code>torch.randn(20, 7, dtype=torch.float32)</code>.</li> <li>Do not create <code>sample3.pt</code> if testing missing file handling.</li> <li>Create <code>rna_predict/conf/test_load_angles.yaml</code>:     <pre><code>defaults:\n  - data: default\n  # Add minimal model configs ONLY if RNADataset depends on them\n  # - model: ...\ndata:\n  index_csv: test_data/test_index.csv # Relative to where test script runs\n  root_dir: ./ # Assumes test script runs from project root\n  load_ang: true\n  max_res: 50 # Choose a value &gt; largest L (e.g., 20)\n  batch_size: 2\n  num_workers: 0 # CRITICAL for debugging\n  verbose: true # Enable dataset verbose logging\n  debug_inspect_batch: true # Enable detailed batch logging in train.py (if used)\n# Add minimal training config if needed by LightningModule\n# training:\n#   w_diffusion: 1.0\n#   w_angle: 0.1\n</code></pre></li> </ul> </li> <li> <p>Execution Script (<code>tests/integration/test_angle_loading.py</code>): <pre><code>import hydra\nfrom omegaconf import DictConfig, OmegaConf\nfrom rna_predict.dataset.loader import RNADataset\nfrom rna_predict.dataset.collate import rna_collate_fn\nfrom torch.utils.data import DataLoader\nimport torch\nimport os\nimport pytest\n\n# Assuming this test runs from the project root directory\nCONFIG_PATH = \"../rna_predict/conf\"\nCONFIG_NAME = \"test_load_angles.yaml\"\n\n@pytest.fixture(scope=\"module\")\ndef hydra_config():\n    \"\"\"Loads the Hydra configuration.\"\"\"\n    with hydra.initialize(config_path=CONFIG_PATH, version_base=None):\n        cfg = hydra.compose(config_name=CONFIG_NAME)\n    # Resolve paths relative to original CWD if needed by dataset\n    # Example: cfg.data.index_csv = os.path.join(hydra.utils.get_original_cwd(), cfg.data.index_csv)\n    # Example: cfg.data.root_dir = os.path.join(hydra.utils.get_original_cwd(), cfg.data.root_dir)\n    # NOTE: RNADataset might resolve root_dir itself, check its implementation\n    print(\"Loaded Hydra Config:\\n\", OmegaConf.to_yaml(cfg))\n    return cfg\n\ndef test_dataset_loading(hydra_config):\n    \"\"\"Tests dataset instantiation and single item retrieval.\"\"\"\n    print(\"\\n--- Testing RNADataset ---\")\n    # Pass cfg directly to RNADataset\n    dataset = RNADataset(cfg=hydra_config, load_adj=False, load_ang=True, verbose=True)\n    print(f\"Dataset size: {len(dataset)}\")\n    assert len(dataset) &gt; 0, \"Dataset is empty!\"\n\n    print(\"\\nFetching first sample (sample1)...\")\n    sample1 = dataset[0] # Corresponds to sample1.pdb\n    print(\"Sample 1 keys:\", sample1.keys())\n    assert 'angles_true' in sample1, \"'angles_true' key missing from sample1\"\n    assert isinstance(sample1['angles_true'], torch.Tensor), \"'angles_true' is not a Tensor\"\n    assert sample1['angles_true'].shape == (hydra_config.data.max_res, 7), f\"Sample 1 shape mismatch: Expected {(hydra_config.data.max_res, 7)}, got {sample1['angles_true'].shape}\"\n    assert sample1['angles_true'].dtype == torch.float32, f\"Sample 1 dtype mismatch: Expected float32, got {sample1['angles_true'].dtype}\"\n    # Check that padded values are zero\n    assert torch.all(sample1['angles_true'][10:] == 0), \"Padding values are not zero for sample1\"\n    print(\"Sample 1 verification PASSED.\")\n\n    print(\"\\nFetching second sample (sample2)...\")\n    sample2 = dataset[1] # Corresponds to sample2.cif\n    assert 'angles_true' in sample2\n    assert isinstance(sample2['angles_true'], torch.Tensor)\n    assert sample2['angles_true'].shape == (hydra_config.data.max_res, 7)\n    assert torch.all(sample2['angles_true'][20:] == 0), \"Padding values are not zero for sample2\"\n    print(\"Sample 2 verification PASSED.\")\n\n    # Optional: Test missing angle file handling if sample3 was included\n    # print(\"\\nFetching third sample (sample3 - missing angle file)...\")\n    # sample3 = dataset[2]\n    # assert 'angles_true' in sample3\n    # assert isinstance(sample3['angles_true'], torch.Tensor)\n    # assert sample3['angles_true'].shape == (hydra_config.data.max_res, 7)\n    # assert torch.all(sample3['angles_true'] == 0), \"Placeholder for missing angle file is not zeros\"\n    # print(\"Sample 3 (missing file) verification PASSED.\")\n\n\ndef test_dataloader_collation(hydra_config):\n    \"\"\"Tests DataLoader and collation function.\"\"\"\n    print(\"\\n--- Testing DataLoader &amp; Collation ---\")\n    dataset = RNADataset(cfg=hydra_config, load_adj=False, load_ang=True, verbose=False) # Less verbose here\n    # Use debug_logging=True in collate_fn for verbose output\n    dataloader = DataLoader(dataset, batch_size=hydra_config.data.batch_size, collate_fn=lambda b: rna_collate_fn(b, debug_logging=True), num_workers=0)\n\n    print(\"\\nFetching first batch...\")\n    batch = next(iter(dataloader))\n    print(\"Batch keys:\", batch.keys())\n\n    assert 'angles_true' in batch, \"'angles_true' key missing from batch\"\n    assert isinstance(batch['angles_true'], torch.Tensor), f\"'angles_true' is not a Tensor in batch, got {type(batch['angles_true'])}\"\n    expected_shape = (hydra_config.data.batch_size, hydra_config.data.max_res, 7)\n    assert batch['angles_true'].shape == expected_shape, f\"Expected batch shape {expected_shape}, got {batch['angles_true'].shape}\"\n    assert batch['angles_true'].dtype == torch.float32, f\"Expected batch dtype torch.float32, got {batch['angles_true'].dtype}\"\n    print(\"Batch collation verification PASSED.\")\n\n# To run this test: pytest tests/integration/test_angle_loading.py -s\n</code></pre></p> </li> <li>Run: Execute the test script using <code>pytest -s tests/integration/test_angle_loading.py</code> (the <code>-s</code> shows print statements). Examine the output logs from the dataset and collate function, and check if the assertions pass.</li> </ul>"},{"location":"guides/training_loss_hypothesis_analysis/#this-detailed-breakdown-for-phase-1-provides-specific-code-changes-assumptions-critical-considerations-and-a-verification-plan-to-ensure-ground-truth-angles-are-correctly-loaded-and-batched-setting-the-stage-for-implementing-the-direct-angle-loss-in-the-next-phase","title":"This detailed breakdown for Phase 1 provides specific code changes, assumptions, critical considerations, and a verification plan to ensure ground truth angles are correctly loaded and batched, setting the stage for implementing the direct angle loss in the next phase.","text":"<p>Okay, you are absolutely correct. We cannot just assume the ground truth torsion angle data exists in the required format (<code>.pt</code> files with <code>[L, 7]</code> tensors in radians). That was a placeholder assumption in the previous plan. We need a concrete strategy to generate this ground truth data from the existing 3D coordinate files (PDB/CIF).</p> <p>This requires analyzing the candidate tools (X3DNA-DSSR, MDAnalysis) and selecting the best one for pre-computing highly accurate torsion angles for the entire dataset.</p> <p>Deep Dive: Selecting the Best Tool for Pre-computing Ground Truth Torsion Angles</p> <p>Goal: Choose a tool that accurately calculates standard RNA torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7), handles real-world PDB/CIF complexities (modified residues, missing atoms), is scriptable for batch processing, and provides output in a usable format (e.g., numerical values).</p> <p>Analysis of Candidate Tools:</p> <ol> <li> <p>X3DNA-DSSR:</p> <ul> <li>Pros:<ul> <li>Specialized for Nucleic Acids: Designed specifically for RNA/DNA, deeply understands conventions (torsion definitions, base pairing, Leontis-Westhof, Saenger).</li> <li>High Accuracy: Widely regarded as a gold standard for RNA structural analysis. The documentation explicitly mentions correcting errors from other tools.</li> <li>Robust Handling of Modifications: Automatically maps dozens of common modified residues to standard parents, ensuring calculations don't fail on non-standard bases.</li> <li>Handles Edge Cases: Specific flags for NMR ensembles (<code>--nmr</code>) and symmetric assemblies (<code>--symm</code>). Robust to missing atoms (outputs <code>---</code>).</li> <li>Scriptable: Excellent command-line interface.</li> <li>Convenient Output: <code>--json</code> flag provides structured, machine-readable output containing torsion angles (typically in degrees) which is easy to parse in Python. The <code>--torsion-file</code> provides a dedicated text file.</li> <li>Fast: As a compiled binary, it's very efficient for processing individual files.</li> </ul> </li> <li>Cons:<ul> <li>Licensing: Requires obtaining a license (free for academics, paid for commercial). This is a one-time setup step.</li> <li>Installation: Not a simple <code>pip install</code>. Requires downloading a binary and potentially setting PATH.</li> <li>Output Units: Outputs angles in degrees by default, requiring conversion to radians for use with PyTorch trigonometric functions (<code>torch.sin</code>, <code>torch.cos</code>).</li> </ul> </li> </ul> </li> <li> <p>MDAnalysis:</p> <ul> <li>Pros:<ul> <li>Python Native: Integrates seamlessly into Python data pipelines (<code>pip install mdanalysis</code>).</li> <li>Flexible: The general <code>analysis.dihedrals.Dihedral</code> class can calculate any dihedral if the 4 atoms are correctly selected.</li> <li>Open Source: LGPL license, easy installation.</li> <li>Trajectory Handling: Excellent for analyzing MD simulations (though not our primary need here).</li> <li>Output Units: Returns angles in radians directly, which is convenient for PyTorch.</li> </ul> </li> <li>Cons:<ul> <li>Not RNA-Specialized: Lacks built-in knowledge of RNA-specific conventions (\u03b1-\u03b6, \u03c7 definitions, sugar puckers \u03bd0-\u03bd4, modified base handling). The user must manually define the correct 4-atom selections for every angle type, including tricky inter-residue ones (\u03b1, \u03b5, \u03b6). This is error-prone.</li> <li>Robustness Dependent on User Code: Handling missing atoms, altLocs, or modified residues requires explicit, careful Python logic within the selection process. Less robust out-of-the-box compared to DSSR's built-in handling.</li> <li>Potential Complexity: Scripting the selection logic for all 7 standard torsions across all residues, handling termini and chain breaks correctly, is significantly more complex than using DSSR's automated calculation.</li> <li>Potential BAT Class Issue: The reported issue with the <code>BAT</code> class modifying coordinates unexpectedly (though potentially fixed or specific) raises a minor flag about tool subtleties.</li> </ul> </li> </ul> </li> </ol> <p>Conclusion: Recommendation</p> <p>For the specific task of pre-computing accurate ground truth torsion angles for a dataset of RNA structures, X3DNA-DSSR is the strongly recommended tool.</p> <ul> <li>Reasoning: Its specialization in nucleic acids ensures higher accuracy and robustness concerning RNA-specific conventions, modified residues, and common structural artifacts found in PDB/CIF files. While MDAnalysis can calculate these angles, the burden of correctly implementing and validating the selections for all RNA angles (especially inter-residue ones) falls entirely on the user, increasing the risk of errors in the \"ground truth\" data. DSSR automates this complex process reliably. The convenience of JSON output for parsing outweighs the minor inconvenience of the licensing step and degree-to-radian conversion. Speed is also likely superior for batch processing static files.</li> </ul> <p>Revised Plan for Phase 1: Pre-computation using DSSR</p> <p>This replaces the previous assumption of existing <code>.pt</code> files with a concrete generation step.</p> <p>Phase 1a: Generate Ground Truth Angle Data (One-Time Pre-computation)</p> <ol> <li>Obtain DSSR: Secure the appropriate DSSR binary and license for your operating system. Install it and ensure it's executable (e.g., in your PATH or provide the full path).</li> <li>Create a Pre-computation Script: Write a Python script (<code>scripts/preprocessing/compute_ground_truth_angles.py</code> or similar).<ul> <li>Input: Path to the main <code>index_csv</code> used by <code>RNADataset</code>, path to the data <code>root_dir</code>.</li> <li>Logic:<ul> <li>Read the <code>index_csv</code> into a pandas DataFrame.</li> <li>Iterate through each row of the DataFrame.</li> <li>For each row:<ul> <li>Determine the full path to the structure file (PDB or CIF) using the same logic as <code>RNADataset._get_structure_filepath</code>.</li> <li>Define the corresponding output <code>.pt</code> file path (e.g., <code>structure_path.with_suffix(\".pt\")</code>).</li> <li>Check if output <code>.pt</code> file already exists. If yes, skip (allows incremental runs).</li> <li>Construct the DSSR command: <code>dssr_cmd = [\"x3dna-dssr\", \"--json\", f\"--input={structure_path}\"]</code>.</li> <li>Run DSSR using <code>subprocess.run(dssr_cmd, capture_output=True, text=True, check=True)</code>. Use <code>check=True</code> to raise an error if DSSR fails.</li> <li>Use a <code>try...except subprocess.CalledProcessError</code> block to catch DSSR failures and log them (e.g., file parsing errors, DSSR internal errors).</li> <li>Inside the <code>try</code> block (after successful DSSR run):<ul> <li>Parse the JSON output: <code>dssr_data = json.loads(result.stdout)</code>.</li> <li>Check if <code>\"nts\"</code> (nucleotides) key exists and is not empty. If empty, log a warning (e.g., protein-only file) and continue to the next structure.</li> <li>Initialize an empty list <code>all_angles = []</code>.</li> <li>Iterate through the nucleotides in <code>dssr_data[\"nts\"]</code>:<ul> <li>Extract the 7 standard torsion angles: <code>alpha</code>, <code>beta</code>, <code>gamma</code>, <code>delta</code>, <code>epsilon</code>, <code>zeta</code>, <code>chi</code>.</li> <li>Handle Missing Values: DSSR JSON might use <code>null</code> for undefined angles (e.g., alpha/zeta at termini). Replace <code>null</code> with a chosen placeholder, typically <code>0.0</code> or <code>np.nan</code>. Using <code>0.0</code> is often simpler if subsequent masking in the loss function is handled correctly. Using <code>np.nan</code> requires handling NaNs during loss calculation. Let's choose <code>0.0</code> for simplicity initially.</li> <li>Convert Degrees to Radians: <code>angle_rad = angle_deg * np.pi / 180.0</code> for each extracted angle.</li> <li>Append the list/tuple of 7 radian values for this nucleotide to <code>all_angles</code>.</li> </ul> </li> <li>Convert the list of lists to a NumPy array: <code>angle_array_np = np.array(all_angles, dtype=np.float32)</code>. Shape should be <code>[L, 7]</code>.</li> <li>Convert to a PyTorch tensor: <code>angle_tensor = torch.from_numpy(angle_array_np)</code>.</li> <li>Save the Tensor: <code>torch.save(angle_tensor, output_pt_path)</code>.</li> <li>Log success for this file.</li> </ul> </li> </ul> </li> <li>Include overall progress reporting (e.g., using <code>tqdm</code> for the main loop).</li> </ul> </li> </ul> </li> <li>Run the Script: Execute this script once over your entire training/validation dataset. This populates the data directory with the necessary <code>.pt</code> angle files.</li> </ol> <p>Phase 1b: Adapt Data Loading (As Before, but Simpler)</p> <p>Now that the <code>.pt</code> files are guaranteed to exist (or were skipped with logs during pre-computation), the implementation in <code>RNADataset</code> becomes simpler.</p> <ol> <li> <p>File: <code>rna_predict/conf/data/default.yaml</code></p> <ul> <li>Action: Set <code>load_ang: true</code>. (No change from previous plan here).</li> </ul> </li> <li> <p>File: <code>rna_predict/dataset/loader.py</code> (<code>RNADataset</code>)</p> <ul> <li>Action: Implement <code>_load_angles</code> to only load the pre-computed <code>.pt</code> file. The complex DSSR call is removed.     <pre><code># Inside RNADataset class\ndef _load_angles(self, row) -&gt; Optional[torch.Tensor]:\n    \"\"\"Loads pre-computed ground truth torsion angles from a .pt file.\"\"\"\n    angle_path = None\n    try:\n        structure_path = self._get_structure_filepath(row) # Assumes this helper exists and works\n        angle_path = structure_path.with_suffix(\".pt\")\n\n        if not angle_path.is_file():\n            # This case should be less frequent now if pre-computation was run\n            if self.verbose:\n                logger.warning(f\"Pre-computed angle file not found: {angle_path}. Returning None.\")\n            return None\n\n        # Load the tensor data\n        angles_tensor = torch.load(angle_path, map_location='cpu')\n\n        # Basic validation (optional, as pre-computation should ensure this)\n        if not isinstance(angles_tensor, torch.Tensor) or angles_tensor.dim() != 2 or angles_tensor.shape[1] != 7:\n             logger.error(f\"Invalid data in pre-computed angle file {angle_path}. Shape: {angles_tensor.shape}, Type: {type(angles_tensor)}\")\n             return None\n\n        return angles_tensor.to(torch.float32) # Ensure correct dtype\n\n    except Exception as e:\n        logger.error(f\"Error loading pre-computed angles for {row.get('id', 'N/A')} from {angle_path if angle_path else 'unknown path'}: {e}\", exc_info=self.verbose)\n        return None\n</code></pre></li> <li>Action: Modify <code>__getitem__</code> to call the simplified <code>_load_angles</code> and handle padding/placeholder creation as outlined in the previous plan's step 2.3. The logic remains the same, but it now relies on the pre-computed files.</li> </ul> </li> <li> <p>File: <code>rna_predict/dataset/collate.py</code> (<code>rna_collate_fn</code>)</p> <ul> <li>Action: Ensure collation handles <code>angles_true</code> correctly (no change from previous plan's step 3.1).</li> </ul> </li> <li> <p>Verification Plan:</p> <ul> <li>Run the DSSR pre-computation script on your test data subset. Verify <code>.pt</code> files are created.</li> <li>Run the <code>tests/integration/test_angle_loading.py</code> script (from the previous plan). It should now pass by loading the pre-computed <code>.pt</code> files.</li> </ul> </li> </ol> <p>Summary of Refined Phase 1:</p> <ol> <li>Pre-computation: Use a dedicated script leveraging X3DNA-DSSR (<code>--json</code> output) to iterate through all structure files, calculate standard torsion angles (\u03b1-\u03b6, \u03c7), convert them to radians, and save them as <code>[L, 7]</code> <code>torch.float32</code> tensors in <code>.pt</code> files (e.g., <code>structure.pt</code>) alongside the original structure files. Handle errors and missing atoms gracefully during this step.</li> <li>Data Loading: Modify <code>RNADataset</code> to simply load these pre-computed <code>.pt</code> files when <code>cfg.data.load_ang</code> is true. Implement padding/truncation and placeholder logic for missing files within <code>__getitem__</code>.</li> <li>Collation: Ensure <code>rna_collate_fn</code> stacks the loaded <code>angles_true</code> tensors correctly into batches.</li> </ol>"},{"location":"guides/training_loss_hypothesis_analysis/#this-approach-ensures-accuracy-by-using-a-specialized-tool-dssr-improves-efficiency-by-pre-computing-angles-once-and-simplifies-the-data-loading-logic-within-rnadataset","title":"This approach ensures accuracy by using a specialized tool (DSSR), improves efficiency by pre-computing angles once, and simplifies the data loading logic within <code>RNADataset</code>.","text":"<p>Okay, let's perform a meticulous deep dive into Phase 2: Integrating Stage D into Training within the <code>RNALightningModule</code>. This phase is the most complex part of the refactoring, as it connects multiple stages and introduces the core diffusion training logic.</p> <p>Goal: Modify <code>RNALightningModule.training_step</code> to correctly prepare inputs for Stage D, execute its training step, compute the appropriate diffusion loss (<code>L_diffusion</code>), and ensure gradients propagate back to Stage D and the upstream conditioning modules (TorsionBERT, Pairformer, LatentMerger).</p> <p>Detailed Implementation Steps &amp; Considerations:</p> <p>3. File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>) - Prepare Inputs for Stage D</p> <ul> <li> <p>Action 3.1: Define Helpers for Noise Sampling and Application.</p> <ul> <li>Rationale: Encapsulate the logic for noise schedule sampling and adding noise to coordinates for better readability and potential reuse.</li> <li>Implementation: Add <code>_sample_noise_level</code> and <code>_add_noise</code> methods to <code>RNALightningModule</code>.     <pre><code># Inside RNALightningModule class\ndef _sample_noise_level(self, batch_size: int) -&gt; torch.Tensor:\n    \"\"\"Samples noise level sigma_t for each item in the batch based on config.\"\"\"\n    # Access noise schedule config safely\n    noise_schedule_cfg = getattr(getattr(getattr(self.cfg, 'model', {}), 'stageD', {}), 'diffusion', {}).get('noise_schedule', {})\n    p_mean = noise_schedule_cfg.get('p_mean', -1.2)\n    p_std = noise_schedule_cfg.get('p_std', 1.5)\n    # Access sigma_data safely from model_architecture\n    model_arch_cfg = getattr(getattr(getattr(self.cfg, 'model', {}), 'stageD', {}), 'diffusion', {}).get('model_architecture', {})\n    sigma_data = model_arch_cfg.get('sigma_data', 1.0) # Default to 1.0 for EDM-like schedule\n\n    # Log the parameters being used\n    logger.debug(f\"Noise sampling params: p_mean={p_mean}, p_std={p_std}, sigma_data={sigma_data}\")\n\n    # AF3-like exponential schedule (log-normal distribution for sigma)\n    # log_snr = torch.randn(batch_size, device=self.device_) * p_std + p_mean # SNR = 1/sigma^2 -&gt; log(SNR) = -2*log(sigma)\n    # log_sigma = -0.5 * log_snr\n    # sigma_t = sigma_data * torch.exp(log_sigma)\n\n    # Simpler Placeholder: Uniform sampling in log space\n    min_log_sigma = torch.log(torch.tensor(noise_schedule_cfg.get('s_min', 0.002), device=self.device_)) # Use s_min from config\n    max_log_sigma = torch.log(torch.tensor(noise_schedule_cfg.get('s_max', 80.0), device=self.device_)) # Use s_max from config\n    log_sigma_t = torch.rand(batch_size, device=self.device_) * (max_log_sigma - min_log_sigma) + min_log_sigma\n    sigma_t = torch.exp(log_sigma_t)\n\n    logger.debug(f\"Sampled sigma_t (noise levels, shape {sigma_t.shape}): {sigma_t}\")\n    return sigma_t.to(self.device_) # Ensure device\n\ndef _add_noise(self, coords_true: torch.Tensor, sigma_t: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Adds Gaussian noise based on sampled noise levels sigma_t.\"\"\"\n    if coords_true.numel() == 0: # Handle empty coordinates\n         return coords_true.clone(), torch.zeros_like(coords_true)\n    epsilon = torch.randn_like(coords_true)\n    # Ensure sigma_t can broadcast to coords_true shape [B, N_atom, 3]\n    # sigma_t is [B], need [B, 1, 1]\n    sigma_t_reshaped = sigma_t.view(-1, *([1] * (coords_true.dim() - 1)))\n    coords_noisy = coords_true + epsilon * sigma_t_reshaped\n    logger.debug(f\"Added noise: coords_true.shape={coords_true.shape}, sigma_t.shape={sigma_t.shape}, epsilon.shape={epsilon.shape}, coords_noisy.shape={coords_noisy.shape}\")\n    return coords_noisy, epsilon\n</code></pre></li> <li>Critical Check: The specific noise sampling strategy (<code>_sample_noise_level</code>) directly impacts training stability and performance. The AF3 schedule is complex; starting with a simpler schedule (like linear or uniform log-space) might be easier initially. Verify <code>sigma_data</code>, <code>s_min</code>, <code>s_max</code> are correctly read from the config.</li> </ul> </li> <li> <p>Action 3.2: Gather and Prepare Inputs within <code>training_step</code>.</p> <ul> <li>Code Snippet (Inside <code>training_step</code>): <pre><code># (Ensure self.forward(batch) has been called to get 'output' dict)\n# ... (L_angle calculation from Phase 1) ...\n\nlogger.debug(\"--- Preparing Stage D Inputs ---\")\n\n# 1. Ground Truth Coords &amp; Mask\n# Ensure keys exist in the batch, handle potential errors\nif \"coords_true\" not in batch or \"atom_mask\" not in batch:\n    logger.error(\"Batch missing 'coords_true' or 'atom_mask'. Cannot proceed with Stage D training.\")\n    # Return a minimal loss or raise error\n    return {\"loss\": loss_angle if 'loss_angle' in locals() else torch.tensor(0.0, device=self.device_, requires_grad=True)}\n\ncoords_true = batch[\"coords_true\"].to(self.device_) # Shape [B, N_atom_padded, 3]\natom_mask = batch[\"atom_mask\"].to(self.device_)     # Shape [B, N_atom_padded] bool\n\n# 2. Sample Noise Level &amp; Add Noise\nbatch_size = coords_true.shape[0]\nsigma_t = self._sample_noise_level(batch_size) # Shape [B]\ncoords_noisy, epsilon = self._add_noise(coords_true, sigma_t) # Shapes [B, N_atom_padded, 3]\n\n# 3. Retrieve Stage B Outputs (Residue Level) from `output` dict\nif \"s_embeddings\" not in output or \"z_embeddings\" not in output:\n     logger.error(\"Missing 's_embeddings' or 'z_embeddings' from self.forward output dict.\")\n     return {\"loss\": loss_angle} # Fallback\n\ns_embeddings_res = output[\"s_embeddings\"].to(self.device_) # Shape [B, N_res_padded, C_s]\nz_embeddings_res = output[\"z_embeddings\"].to(self.device_) # Shape [B, N_res_padded, N_res_padded, C_z]\n\n# 4. Bridging from Residue to Atom Level (CRITICAL)\nif 'atom_to_token_idx' not in batch:\n    raise KeyError(\"Batch missing 'atom_to_token_idx' needed for residue-to-atom bridging.\")\natom_to_token_idx = batch['atom_to_token_idx'].to(self.device_) # Shape [B, N_atom_padded]\n\n# --- Bridging s_embeddings ---\nfrom rna_predict.utils.tensor_utils import residue_to_atoms, derive_residue_atom_map\ns_embeddings_atom_list = []\nn_atoms_padded = coords_true.shape[1]\nfor b_idx in range(batch_size):\n    # Derive map for this specific item (using its actual atom mask)\n    current_atom_mask = atom_mask[b_idx]\n    # Provide sequence if available for better mapping\n    sequence_item = batch[\"sequence\"][b_idx] if \"sequence\" in batch else \"\"\n    residue_atom_map_item = derive_residue_atom_map(sequence=sequence_item, atom_mask=current_atom_mask)\n\n    if not residue_atom_map_item:\n         logger.warning(f\"Empty residue_atom_map derived for batch item {b_idx}. Using zeros.\")\n         s_embeddings_atom_list.append(torch.zeros((n_atoms_padded, s_embeddings_res.shape[-1]), device=self.device_, dtype=s_embeddings_res.dtype))\n         continue\n\n    # Bridge using the derived map\n    try:\n        # residue_to_atoms expects [N_res, C] and returns [N_atom_actual, C]\n        s_atom_actual = residue_to_atoms(s_embeddings_res[b_idx], residue_atom_map_item)\n        # Pad back to N_atom_padded\n        padded_s_atom = torch.zeros((n_atoms_padded, s_atom_actual.shape[-1]), device=self.device_, dtype=s_atom_actual.dtype)\n        real_indices = torch.where(current_atom_mask)[0] # Indices of real atoms\n        num_real_atoms = len(real_indices)\n        # Ensure slicing matches actual number of atoms returned by bridging\n        padded_s_atom[real_indices] = s_atom_actual[:num_real_atoms]\n        s_embeddings_atom_list.append(padded_s_atom)\n    except Exception as e:\n        logger.error(f\"Error bridging s_embeddings for batch item {b_idx}: {e}\", exc_info=True)\n        s_embeddings_atom_list.append(torch.zeros((n_atoms_padded, s_embeddings_res.shape[-1]), device=self.device_, dtype=s_embeddings_res.dtype))\n\ns_embeddings_atom = torch.stack(s_embeddings_atom_list, dim=0) # Shape [B, N_atom_padded, C_s]\nlogger.debug(f\"Bridged s_embeddings_atom shape: {s_embeddings_atom.shape}\")\n\n# --- Bridging z_embeddings (Optional but potentially needed by Stage D) ---\n# If Stage D needs atom-level pair features, implement bridging here.\n# This typically involves replicating residue-pair features to corresponding atom-pairs.\n# Example (Simple Replication - adapt if Stage D uses a more complex scheme):\n# z_embeddings_atom = torch.zeros(batch_size, n_atoms_padded, n_atoms_padded, z_embeddings_res.shape[-1], device=self.device_, dtype=z_embeddings_res.dtype)\n# for b_idx in range(batch_size):\n#    residue_map_item = derive_residue_atom_map(...) # Derive map again or reuse\n#    for res_i, atoms_i in enumerate(residue_map_item):\n#        for res_j, atoms_j in enumerate(residue_map_item):\n#            if atoms_i and atoms_j: # Only if both residues have atoms\n#                 # Get indices for atoms in res_i and res_j\n#                 idx_i = torch.tensor(atoms_i, device=self.device_)\n#                 idx_j = torch.tensor(atoms_j, device=self.device_)\n#                 # Assign residue-pair value to all atom-pairs\n#                 # Need meshgrid for multi-dim assignment\n#                 mesh_i, mesh_j = torch.meshgrid(idx_i, idx_j, indexing='ij')\n#                 z_embeddings_atom[b_idx, mesh_i, mesh_j, :] = z_embeddings_res[b_idx, res_i, res_j, :]\n# Placeholder: Keep z as residue-level for now, assume Stage D handles it\nz_embeddings_atom = None # Set to None if not bridged\nlogger.debug(f\"z_embeddings (residue level) shape: {z_embeddings_res.shape}\")\n\n# 5. Prepare Conditioning Signal Dictionary for StageDContext\n# Adapt keys based on what StageDContext / run_stageD expects\n# Typically: s_trunk (residue), s_inputs (atom), pair/z_trunk (residue or atom)\nconditioning_signal = {\n    \"s_trunk\": s_embeddings_res, # Keep residue level for potential use\n    \"pair\": z_embeddings_res,    # Keep residue level for potential use\n    \"s_inputs\": s_embeddings_atom,# Pass atom level\n    \"z_atom\": z_embeddings_atom, # Pass atom level if bridged, else None\n    # Pass other batch features needed by Stage D / Context\n    \"atom_mask\": atom_mask,\n    \"atom_to_token_idx\": atom_to_token_idx,\n    # Add input_feature_dict content needed by ProtenixDiffusionManager\n    # e.g., ref_element, ref_charge if used by conditioning\n    \"ref_element\": batch.get(\"ref_element\"),\n    \"ref_atom_name_chars\": batch.get(\"ref_atom_name_chars\"),\n    # etc.\n}\n\n# Optional: Unified Latent Merger\nunified_latent = None\nif getattr(self.cfg, \"merge_latent\", True): # Default to True based on design docs\n    logger.debug(\"--- Running Latent Merger ---\")\n    # Prepare inputs for the merger (ensure correct device)\n    # Note: SimpleLatentMerger expects residue-level inputs based on its forward sig\n    merge_inputs = LatentInputs(\n        adjacency=output[\"adjacency\"].to(self.device_),\n        angles=output[\"torsion_angles\"].to(self.device_), # Shape [B, N_res, C_angle]\n        s_emb=s_embeddings_res,                           # Shape [B, N_res, C_s]\n        z_emb=z_embeddings_res,                           # Shape [B, N_res, N_res, C_z]\n        # partial_coords is optional, needs careful handling if atom-level\n        partial_coords=None # Or pass output[\"coords\"] if merger handles atom-level coords\n    )\n    unified_latent = self.latent_merger(merge_inputs) # Expected output [B, N_res, C_latent]\n    conditioning_signal[\"unified_latent\"] = unified_latent # Add to conditioning dict\n    logger.debug(f\"Unified Latent shape: {unified_latent.shape if unified_latent is not None else None}\")\n    # CRITICAL: If Stage D needs atom-level unified_latent, bridge it here.\n    # Example: unified_latent_atom = residue_to_atoms(unified_latent, residue_map_list_of_lists_batched)\n    # conditioning_signal[\"unified_latent_atom\"] = unified_latent_atom\n\nlogger.debug(\"--- Stage D Input Preparation Complete ---\")\n</code></pre></li> <li>Critical Considerations:<ul> <li>Bridging Accuracy: The correctness of <code>derive_residue_atom_map</code> and <code>residue_to_atoms</code> is paramount. Errors here will propagate incorrect conditioning signals. Test this bridging extensively.</li> <li><code>z_embeddings</code> Bridging: Determine if Stage D requires atom-level pair embeddings (<code>z_atom</code>). If so, implement the bridging (e.g., the replication logic sketched above). If not, pass <code>z_embeddings_res</code>.</li> <li>Conditioning Dictionary Keys: Ensure the keys in <code>conditioning_signal</code> match precisely what <code>StageDContext</code> and <code>run_stageD</code> expect. Rename keys if necessary (e.g., maybe Stage D expects <code>\"pair\"</code> instead of <code>\"z_trunk\"</code>).</li> <li>Unified Latent Level: Decide if the <code>unified_latent</code> should be residue-level (as produced by <code>SimpleLatentMerger</code>) or atom-level. Bridge if needed. Ensure Stage D's conditioning logic handles whichever level is provided.</li> </ul> </li> </ul> </li> </ul> <p>4. File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>) - Execute Stage D</p> <ul> <li>Action 4.1: Call <code>run_stageD</code> using <code>StageDContext</code>.<ul> <li>Code Snippet (Inside <code>training_step</code>, after input prep): <pre><code>logger.debug(\"--- Executing Stage D ---\")\nfrom rna_predict.pipeline.stageD.context import StageDContext\nfrom rna_predict.pipeline.stageD.run_stageD import run_stageD\n\n# Create the context object, mapping prepared signals to context fields\n# Ensure atom_metadata contains at least residue_indices for bridging inside run_stageD\natom_metadata_for_stageD = {\n     \"residue_indices\": atom_to_token_idx.squeeze(0) # Assuming batch size 1 for now, needs adjustment if B &gt; 1\n     # Add other metadata like atom_names if available and needed\n}\nif 'atom_names' in batch: atom_metadata_for_stageD['atom_names'] = batch['atom_names'][0] # Assuming B=1\n\nstage_d_context = StageDContext(\n    cfg=self.cfg,                      # Pass the main Hydra config\n    coords=coords_noisy,               # Noisy coordinates are the primary input\n    s_trunk=conditioning_signal[\"s_trunk\"], # Residue-level trunk\n    z_trunk=conditioning_signal[\"pair\"],    # Residue-level pair\n    s_inputs=conditioning_signal[\"s_inputs\"],# ATOM-level inputs\n    input_feature_dict=conditioning_signal, # Pass the whole dict for flexibility\n    atom_metadata=atom_metadata_for_stageD,\n    unified_latent=unified_latent,     # Pass if merger is used\n    mode='train',                      # Explicitly set train mode\n    noise_level=sigma_t,               # Pass noise level for potential use in loss weighting\n    device=str(self.device_),          # Pass device string\n    debug_logging=getattr(self.cfg.model.stageD, 'debug_logging', False)\n)\n\ntry:\n    # Call run_stageD, which should handle training mode internally\n    stage_d_pred_output = run_stageD(stage_d_context) # Assuming run_stageD is adapted\n\n    # --- Determine what Stage D returned ---\n    # Check the type and content of stage_d_pred_output\n    # Ideally, it returns predicted noise epsilon_hat or denoised coords x_hat_0\n    if isinstance(stage_d_pred_output, tuple) and len(stage_d_pred_output) == 3:\n         # Example: Assume returns (x_denoised, sigma, x_gt_augment) like ProtenixDiffusionManager.train_diffusion_step\n         # We need either noise or denoised coords for loss calculation\n         # If it returns denoised coords:\n         stage_d_pred = stage_d_pred_output[1] # Assuming x_denoised is the second element\n         prediction_type = 'coords' # Flag for loss calculation\n    elif isinstance(stage_d_pred_output, torch.Tensor):\n         # Assume it returned the direct prediction (e.g., noise)\n         stage_d_pred = stage_d_pred_output\n         # ****** ASSUMPTION: Need to know if this tensor is noise or coords ******\n         # For now, assume noise prediction based on simpler loss path\n         prediction_type = 'noise'\n    else:\n         logger.error(f\"Unexpected output type from run_stageD in train mode: {type(stage_d_pred_output)}\")\n         stage_d_pred = None\n         prediction_type = None\n\n    if stage_d_pred is None:\n         raise ValueError(\"Stage D training step did not return a usable prediction.\")\n\n    logger.debug(f\"Stage D prediction type: {prediction_type}, shape: {stage_d_pred.shape if stage_d_pred is not None else 'None'}\")\n\nexcept Exception as e:\n    logger.error(f\"Error during Stage D execution (run_stageD call) in training_step: {e}\", exc_info=True)\n    stage_d_pred = None\n    prediction_type = None\n    # Assign a dummy loss that requires grad to avoid Lightning errors\n    loss_diffusion = torch.tensor(0.0, device=self.device_, requires_grad=True)\n\nlogger.debug(\"--- Stage D Execution Complete ---\")\n</code></pre></li> <li>Critical Considerations:<ul> <li>Adapt <code>run_stageD</code> for Training: The primary dependency is ensuring <code>run_stageD</code> (and <code>_run_stageD_impl</code>) correctly handles <code>mode='train'</code>. It needs to:<ul> <li>Recognize the training mode.</li> <li>Pass the <code>noise_level</code> (<code>sigma_t</code>) to the underlying <code>DiffusionModule</code> or loss calculation.</li> <li>Return the appropriate prediction (noise or denoised coords) needed for the loss. Currently, <code>_run_stageD_impl</code> seems inference-focused and might just return the final coordinates without loss calculation. This must be adapted. Consider adding a dedicated <code>train_stageD</code> function or modifying <code>run_stageD</code> with conditional logic based on <code>context.mode</code>.</li> </ul> </li> <li><code>StageDContext</code> Completeness: Double-check that <code>StageDContext</code> correctly bundles all information needed by the diffusion model's conditioning mechanism and loss function (including potentially required elements from the original <code>input_feature_dict</code> like masks, residue types, etc.).</li> <li>Batch Size &gt; 1: The current snippet assumes batch size 1 in places (like squeezing <code>atom_metadata['residue_indices']</code>). This needs generalization if <code>batch_size &gt; 1</code>. <code>StageDContext</code> should likely hold batched tensors.</li> </ul> </li> </ul> </li> </ul> <p>5. File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>) - Implement Diffusion Loss</p> <ul> <li> <p>Action 5.1: Calculate <code>L_diffusion</code>.</p> <ul> <li>Code Snippet (Inside <code>training_step</code>, after Stage D execution): <pre><code>import torch.nn.functional as F # Ensure F is imported\n\nlogger.debug(\"--- Calculating Diffusion Loss ---\")\nloss_diffusion = torch.tensor(0.0, device=self.device_) # Default zero loss\n\n# Use the prediction_type determined in Step 4\nif stage_d_pred is not None and prediction_type is not None:\n    if prediction_type == 'noise':\n        logger.debug(\"Calculating noise prediction loss (MSE)\")\n        target_noise = epsilon # The noise added in step 3.2\n\n        # Apply mask to compute loss only on non-padded atoms\n        # Ensure masks have compatible shapes (e.g., [B, N_atom_padded])\n        if atom_mask.shape != target_noise.shape[:-1]:\n             # Attempt to fix common case: mask is [B, N], target is [B, N, 3]\n             if atom_mask.shape == target_noise.shape[:-1]: # Should match if padding is correct\n                  mask_for_loss = atom_mask.unsqueeze(-1) # -&gt; [B, N, 1]\n             else:\n                  logger.warning(f\"Atom mask shape {atom_mask.shape} incompatible with target noise shape {target_noise.shape}. Using unmasked loss.\")\n                  mask_for_loss = torch.ones_like(target_noise[..., 0], dtype=torch.bool) # Unmasked\n        else:\n             mask_for_loss = atom_mask.unsqueeze(-1) # -&gt; [B, N, 1]\n\n        # Ensure prediction shape matches target shape\n        if stage_d_pred.shape != target_noise.shape:\n             logger.error(f\"Shape mismatch between predicted noise {stage_d_pred.shape} and target noise {target_noise.shape}\")\n             # Fallback to zero loss\n             loss_diffusion = torch.tensor(0.0, device=self.device_, requires_grad=True)\n        else:\n             # Calculate masked MSE loss\n             error = (stage_d_pred - target_noise)**2\n             masked_error = error * mask_for_loss # Apply boolean mask\n             loss_diffusion = masked_error.sum() / (mask_for_loss.sum() * 3 + 1e-8) # Normalize by number of valid atom coordinates\n\n        logger.debug(f\"Loss Diffusion (Noise MSE): {loss_diffusion.item()}\")\n\n    elif prediction_type == 'coords':\n        logger.debug(\"Calculating coordinate prediction loss (AF3-style - Placeholder MSE)\")\n        target_coords = coords_true # Ground truth coordinates\n\n        # --- Placeholder: Implement AF3 Loss Here ---\n        # Requires:\n        # 1. weighted_rigid_align function (from AF3 supplement Algo 28)\n        # 2. Atom weights wl (from AF3 Eq 4 - needs atom type info)\n        # 3. weighted_mse loss\n        # 4. SmoothLDDTLoss function (from AF3 supplement Algo 27)\n        # 5. sigma_data from config\n        # 6. Weight factor based on sigma_t (from AF3 Eq 6)\n\n        # Simplified Placeholder: Masked MSE on *unaligned* coordinates (less accurate)\n        logger.warning(\"Using simplified coordinate MSE loss (NO ALIGNMENT). Implement AF3 loss for best results.\")\n        mask_for_loss = atom_mask.unsqueeze(-1) # [B, N, 1]\n        if stage_d_pred.shape != target_coords.shape:\n             logger.error(f\"Shape mismatch between predicted coords {stage_d_pred.shape} and target coords {target_coords.shape}\")\n             loss_diffusion = torch.tensor(0.0, device=self.device_, requires_grad=True)\n        else:\n             error = (stage_d_pred - target_coords)**2\n             masked_error = error * mask_for_loss\n             loss_diffusion = masked_error.sum() / (mask_for_loss.sum() * 3 + 1e-8)\n\n        logger.debug(f\"Loss Diffusion (Coord MSE - No Align): {loss_diffusion.item()}\")\n    else:\n        logger.error(f\"Internal error: Unknown prediction_type '{prediction_type}'\")\n        loss_diffusion = torch.tensor(0.0, device=self.device_, requires_grad=True) # Dummy grad\n\n# Ensure loss requires grad if Stage D requires grad\nif not loss_diffusion.requires_grad and stage_d_pred is not None and stage_d_pred.requires_grad:\n     loss_diffusion = loss_diffusion.clone().requires_grad_(True)\n\nlogger.debug(f\"--- Diffusion Loss Calculation Complete: loss={loss_diffusion.item()} ---\")\n</code></pre></li> <li>Critical Considerations:<ul> <li>Loss Choice: Noise prediction loss is significantly simpler to implement than the full AF3 coordinate loss. Start with noise prediction unless the <code>ProtenixDiffusionManager</code> is specifically designed only for coordinate prediction.</li> <li>Masking: Correctly applying <code>atom_mask</code> is essential. Ensure its shape is broadcastable to the error tensor before multiplication and normalization.</li> <li>AF3 Loss Implementation: If implementing the coordinate loss, <code>weighted_rigid_align</code> and <code>SmoothLDDTLoss</code> are non-trivial and must be implemented correctly based on the AF3 supplement. Atom type information is needed for <code>wl</code>. The noise-level weighting (<code>weight_factor</code>) is also important.</li> </ul> </li> </ul> </li> <li> <p>Action 5.2: Verification of Gradient Flow.</p> <ul> <li>Details: Perform the gradient check meticulously.</li> <li>Procedure:<ol> <li>Ensure <code>self.stageD</code> and relevant parts of <code>self.stageB_pairformer</code> and <code>self.stageB_torsion</code> (e.g., LoRA adapters if used) have <code>requires_grad=True</code>. Freeze other parts if necessary (<code>self.stageA</code>, base weights of LoRA models).</li> <li>In <code>training_step</code>, temporarily set <code>total_loss = loss_diffusion</code>.</li> <li>Run <code>trainer.fit(model, dataloader, max_steps=1)</code>.</li> <li>Iterate through <code>model.stageD.named_parameters()</code>: Check <code>p.grad is not None</code> and <code>p.grad.abs().sum() &gt; 0</code>.</li> <li>Iterate through relevant conditioning parameters (e.g., <code>model.stageB_pairformer.lora_params()</code>, <code>model.stageB_torsion.lora_params()</code>, <code>model.latent_merger.parameters()</code>): Check <code>p.grad is not None</code> and <code>p.grad.abs().sum() &gt; 0</code>.</li> </ol> </li> <li>Debugging Missing Gradients:<ul> <li>Check for <code>.detach()</code> calls anywhere in the path from conditioning inputs -&gt; Stage D -&gt; loss.</li> <li>Ensure the <code>conditioning_signal</code> passed to Stage D is correctly derived from trainable upstream modules.</li> <li>Verify <code>run_stageD</code> preserves the computation graph when in training mode.</li> <li>Check the diffusion loss calculation maintains the graph connection to <code>stage_d_pred</code>.</li> </ul> </li> </ul> </li> </ul> <p>Final Checkpoints for Phase 2:</p> <ol> <li>Data Flow Confirmed: Stage B outputs -&gt; Bridging -&gt; Conditioning Signal -&gt; Stage D -&gt; Prediction (<code>stage_d_pred</code>).</li> <li>Stage D Training Mode: <code>run_stageD</code> or equivalent must correctly execute the training step of the diffusion model, using <code>coords_noisy</code> and <code>sigma_t</code>, and returning the appropriate prediction (noise or denoised coords).</li> <li>Loss Function Correctness: The chosen diffusion loss (<code>L_diffusion</code>) must match the <code>prediction_type</code> returned by Stage D and correctly use the target (<code>epsilon</code> or <code>coords_true</code>) and <code>atom_mask</code>.</li> <li>Gradient Propagation Verified: Gradients from <code>L_diffusion</code> must successfully reach parameters in Stage D and the upstream modules providing the conditioning signal.</li> </ol>"},{"location":"guides/training_loss_hypothesis_analysis/#completing-these-steps-thoroughly-is-essential-for-successfully-training-the-stage-d-component-and-enabling-end-to-end-learning-driven-by-the-diffusion-process","title":"Completing these steps thoroughly is essential for successfully training the Stage D component and enabling end-to-end learning driven by the diffusion process.","text":"<p>Okay, let's perform a deep dive into the inputs required for the Stage D diffusion process in the <code>RNA_PREDICT</code> pipeline, paying close attention to the role of Stage A's input embedding.</p> <p>1. Stage D Overview</p> <p>Stage D (<code>rna_predict/pipeline/stageD/</code>) is responsible for refining the 3D coordinates of the RNA structure, likely starting from coordinates generated by Stage C. It employs a diffusion model, inspired by techniques like those in AlphaFold 3, which iteratively denoises coordinates conditioned on various structural and sequence features.</p> <p>2. Main Entry Point and Top-Level Inputs (<code>run_stageD_unified.py</code>)</p> <p>The primary entry point seems to be <code>run_stageD_diffusion</code> within <code>run_stageD_unified.py</code>. This function expects a configuration object, specifically an instance of <code>DiffusionConfig</code> (defined in <code>rna_predict/pipeline/stageD/diffusion/utils/config_types.py</code>), which bundles the main inputs:</p> <ul> <li><code>partial_coords</code> (torch.Tensor): The initial 3D coordinates to be refined. These likely come from Stage C's reconstruction. Crucially, the diffusion model itself (<code>DiffusionModule</code>) operates at the atom level. Shape is expected to be <code>[Batch, N_atom, 3]</code>.</li> <li><code>trunk_embeddings</code> (Dict[str, torch.Tensor]): A dictionary containing core feature embeddings. These are expected before the bridging step and are likely residue-level. The bridging function converts them to atom-level. Key embeddings typically include:<ul> <li><code>s_trunk</code>: Single representation from upstream (e.g., Pairformer in Stage B). Expected shape before bridging: <code>[Batch, N_residue, C_s]</code>.</li> <li><code>s_inputs</code>: Single representation derived from input features (potentially including Stage A output). Expected shape before bridging: <code>[Batch, N_residue, C_s_inputs]</code>.</li> <li><code>pair</code> (or <code>z_trunk</code>): Pair representation from upstream (e.g., Pairformer). Expected shape before bridging: <code>[Batch, N_residue, N_residue, C_z]</code>.</li> </ul> </li> <li><code>diffusion_config</code> (DictConfig / Dict): A nested configuration structure containing parameters specific to the diffusion process itself (noise schedule, inference steps, model architecture details, etc.).</li> <li><code>input_features</code> (Optional[Dict[str, Any]]): A dictionary containing various other input features, potentially a mix of atom-level and residue-level data before bridging. The bridging step standardizes these. Important keys include:<ul> <li><code>atom_metadata</code>: Contains <code>residue_indices</code>, <code>atom_names</code>, etc. Crucial for mapping atoms to residues.</li> <li><code>ref_element</code>, <code>ref_atom_name_chars</code>, <code>ref_charge</code>, <code>ref_mask</code>: Atom-level reference features.</li> <li><code>restype</code>, <code>profile</code>: Residue-level features.</li> <li><code>sequence</code>: The RNA sequence string.</li> </ul> </li> <li><code>mode</code> (str): \"inference\" or \"train\".</li> <li><code>device</code> (str): \"cpu\", \"cuda\", or \"mps\".</li> <li><code>debug_logging</code> (bool): Controls logging level.</li> <li><code>cfg</code> (DictConfig): The overall Hydra configuration object, passed down for accessing various parameters.</li> <li><code>atom_metadata</code> (Optional[Dict[str, Any]]): Explicitly passed metadata (overlaps with <code>input_features</code>). Required for bridging.</li> </ul> <p>3. The Crucial Bridging Step (<code>bridging/residue_atom_bridge.py</code>)</p> <p>Before the core diffusion model (<code>DiffusionModule</code>) sees the inputs, the <code>bridge_residue_to_atom</code> function is called within <code>run_stageD_unified.py</code>. This is a critical step:</p> <ul> <li>Input: Takes <code>BridgingInput</code> containing the residue-level <code>trunk_embeddings</code> (<code>s_trunk</code>, <code>s_inputs</code>, <code>pair</code>/<code>z_trunk</code>), <code>partial_coords</code> (atom-level), <code>input_features</code>, and <code>sequence</code>.</li> <li>Process:<ul> <li>Uses <code>atom_metadata</code> (specifically <code>residue_indices</code>) to create or validate a <code>residue_atom_map</code>.</li> <li>Calls <code>process_trunk_embeddings</code>: Expands the residue-level <code>s_trunk</code>, <code>s_inputs</code>, and <code>pair</code>/<code>z_trunk</code> embeddings into atom-level embeddings using the map. For example, <code>s_trunk</code> <code>[B, N_residue, C]</code> becomes <code>[B, N_atom, C]</code>. <code>pair</code> <code>[B, N_residue, N_residue, C]</code> becomes <code>[B, N_atom, N_atom, C]</code>.</li> <li>Calls <code>process_input_features</code>: Ensures other features (like <code>restype</code>, <code>profile</code>) are expanded to the atom level and creates the essential <code>atom_to_token_idx</code> tensor (mapping each atom to its residue index).</li> </ul> </li> <li>Output: Returns atom-level <code>partial_coords</code>, atom-level <code>trunk_embeddings</code>, and atom-level <code>input_features</code>.</li> </ul> <p>Therefore, the inputs directly consumed by the core diffusion machinery (<code>ProtenixDiffusionManager</code> -&gt; <code>DiffusionModule</code>) are primarily at the atom level, having been transformed by this bridging step.</p> <p>4. Core Diffusion Inputs (<code>ProtenixDiffusionManager</code> &amp; <code>DiffusionModule</code>)</p> <p>The <code>ProtenixDiffusionManager</code> orchestrates the diffusion steps, calling the <code>DiffusionModule</code>. The effective inputs to the <code>DiffusionModule</code> (after bridging and internal processing by the manager/conditioning layers) are:</p> <ul> <li><code>x_noisy</code> (torch.Tensor): Noisy atom coordinates. Shape <code>[Batch, N_sample, N_atom, 3]</code>. Derived from the initial <code>partial_coords</code>.</li> <li><code>t_hat_noise_level</code> (torch.Tensor): The noise level for the current step. Shape <code>[Batch, N_sample]</code> or broadcastable.</li> <li><code>input_feature_dict</code> (Dict[str, Any]): Contains atom-level features after bridging. Crucially includes:<ul> <li><code>atom_to_token_idx</code>: Map from atom index to residue index. Shape <code>[Batch, N_atom]</code>.</li> <li><code>ref_pos</code>: Reference positions (can be the initial <code>partial_coords</code>). Shape <code>[Batch, N_atom, 3]</code>.</li> <li><code>ref_mask</code>: Mask indicating valid atoms. Shape <code>[Batch, N_atom, 1]</code>.</li> <li><code>ref_element</code>, <code>ref_atom_name_chars</code>, <code>ref_charge</code>: Atom properties.</li> <li><code>restype</code>, <code>profile</code>: Residue-level features broadcasted to the atom level.</li> </ul> </li> <li><code>s_inputs</code> (torch.Tensor): Single representation derived from input features, now at the atom level. Shape <code>[Batch, N_sample, N_atom, C_s_inputs]</code>.</li> <li><code>s_trunk</code> (torch.Tensor): Single representation from upstream stages, now at the atom level. Shape <code>[Batch, N_sample, N_atom, C_s]</code>.</li> <li><code>z_trunk</code> (torch.Tensor): Pair representation from upstream stages, now at the atom level. Shape <code>[Batch, N_sample, N_atom, N_atom, C_z]</code>. (Note: Referred to as <code>pair</code> in some parts of the bridging code, but likely corresponds to <code>z_trunk</code> conceptually).</li> <li><code>unified_latent</code> (Optional[torch.Tensor]): An optional conditioning vector potentially combining information from multiple upstream stages (A, B, C). Its exact processing depends on how <code>DiffusionConditioning</code> uses it.</li> </ul> <p>5. The Role of Stage A Input Embedding</p> <p>Now, let's address the connection to Stage A's input embedding:</p> <ul> <li>Stage A Output: The <code>InputFeatureEmbedder</code> (in <code>rna_predict/pipeline/stageA/input_embedding/current/embedders.py</code>), which uses the <code>AtomAttentionEncoder</code>, processes atom features (like element type, charge) and produces a token-level (residue-level) embedding. Its output dimension is <code>c_token</code>.</li> <li>Indirect Connection: This Stage A output does not directly feed into the core Stage D <code>DiffusionModule</code> which operates at the atom level.</li> <li>How it Feeds In: The token-level embedding produced by Stage A likely contributes to one or both of the residue-level <code>trunk_embeddings</code> (<code>s_trunk</code>, <code>s_inputs</code>) before they are passed to the Stage D bridging function.<ul> <li>It might be directly used as <code>s_inputs</code> (residue-level).</li> <li>It might be passed to Stage B (e.g., Pairformer) which then produces the <code>s_trunk</code> and <code>pair</code>/<code>z_trunk</code> embeddings (still residue-level).</li> </ul> </li> <li>Transformation: The key is that the residue-level information, potentially influenced by Stage A's output, is explicitly converted to the atom-level representations required by Stage D's <code>DiffusionModule</code> via the <code>bridge_residue_to_atom</code> function.</li> </ul> <p>In Summary:</p> <p>Stage D diffusion requires atom-level inputs:</p> <ol> <li>Initial/Noisy Atom Coordinates: (<code>partial_coords</code> -&gt; <code>x_noisy</code>)</li> <li>Atom-Level Single Embeddings: (<code>s_trunk</code>, <code>s_inputs</code> after bridging) - These originated as residue-level embeddings potentially derived from Stage A/B/C outputs.</li> <li>Atom-Level Pair Embeddings: (<code>pair</code>/<code>z_trunk</code> after bridging) - Originated as residue-level embeddings from Stage B/C.</li> <li>Atom-Level Reference Features: (<code>ref_mask</code>, <code>ref_element</code>, etc. from <code>input_features</code> after bridging).</li> <li>Atom-to-Residue Mapping: (<code>atom_to_token_idx</code> from <code>input_features</code> after bridging).</li> <li>Noise Level: (<code>t_hat_noise_level</code>).</li> <li>Optional Unified Latent Conditioning: (From merger).</li> </ol>"},{"location":"guides/training_loss_hypothesis_analysis/#stage-as-input-embedding-produces-residue-level-features-these-features-influence-the-residue-level-trunk_embeddings-passed-into-the-stage-d-module-but-they-must-first-be-transformed-into-atom-level-representations-by-the-crucial-bridging-step-bridge_residue_to_atom-before-being-consumed-by-the-core-atom-level-diffusion-model-diffusionmodule","title":"Stage A's input embedding produces residue-level features. These features influence the residue-level <code>trunk_embeddings</code> passed into the Stage D module, but they must first be transformed into atom-level representations by the crucial bridging step (<code>bridge_residue_to_atom</code>) before being consumed by the core atom-level diffusion model (<code>DiffusionModule</code>).","text":"<p>Okay, let's perform a deep dive into Phase 3: Final Integration, focusing on combining the losses, reviewing configurations, and establishing a robust testing strategy.</p> <p>Goal: Finalize the <code>RNALightningModule.training_step</code> by combining the angle and diffusion losses, ensure the entire configuration is consistent (especially dimensions), and define tests to verify end-to-end gradient flow and training stability.</p> <p>6. File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule</code>) - Combine and Log Losses</p> <ul> <li> <p>Action 6.1: Define Loss Weights in Configuration.</p> <ul> <li>Rationale: Loss weights should be configurable hyperparameters for easy tuning.</li> <li>File: <code>rna_predict/conf/config_schema.py</code><ul> <li>Action: Add a <code>TrainingConfig</code> dataclass (if not already present) to hold training-specific hyperparameters like loss weights.     <pre><code>from dataclasses import dataclass, field\n# ... other imports ...\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for training-related parameters.\"\"\"\n    checkpoint_dir: str = field(default=\"outputs/checkpoints\", metadata={\"help\": \"Directory to save checkpoints\"})\n    w_diffusion: float = field(default=1.0, metadata={\"help\": \"Weight for the diffusion loss term\"})\n    w_angle: float = field(default=0.1, metadata={\"help\": \"Weight for the direct angle loss term\"})\n    # Add other training params like learning_rate, epochs etc. if managing them here\n\n# ... Add TrainingConfig to the main RNAConfig ...\n@dataclass\nclass RNAConfig:\n    # ... other fields ...\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    # ... other fields ...\n\n# ... Add TrainingConfig registration in register_configs() ...\ndef register_configs() -&gt; None:\n    # ... other cs.store calls ...\n    cs.store(group=\"training\", name=\"default\", node=TrainingConfig)\n    # ...\n</code></pre></li> </ul> </li> <li>File: Create <code>rna_predict/conf/training/default.yaml</code><ul> <li>Action: Define the default weights.     <pre><code># rna_predict/conf/training/default.yaml\ndefaults:\n  - _self_\n\ncheckpoint_dir: outputs/checkpoints # Default checkpoint dir\nw_diffusion: 1.0\nw_angle: 0.1\n# Add other training defaults (e.g., learning_rate: 0.001)\n</code></pre></li> </ul> </li> <li>File: <code>rna_predict/conf/default.yaml</code><ul> <li>Action: Ensure the training config group is included in the main defaults list.     <pre><code># rna_predict/conf/default.yaml\ndefaults:\n  - _self_\n  - data: default\n  - model/stageA@model.stageA\n  # ... other model stages ...\n  - training: default # &lt;&lt;&lt; ADD THIS LINE\n  - test_data@test_data\n# ... rest of the file ...\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Action 6.2: Calculate Combined Loss in <code>training_step</code>.</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>RNALightningModule.training_step</code>)</li> <li>Action: Combine <code>loss_angle</code> and <code>loss_diffusion</code> using weights from <code>self.cfg.training</code>. Handle cases where a loss might not have been computed (e.g., <code>load_ang=False</code>).</li> <li>Code Snippet (End of <code>training_step</code>): <pre><code># (Assuming loss_angle and loss_diffusion are calculated above if enabled/successful)\n# Initialize total loss\ntotal_loss = torch.tensor(0.0, device=self.device_, requires_grad=True) # Start with a zero tensor that requires grad\n\n# Access weights safely with defaults\nw_diffusion = getattr(getattr(self.cfg, 'training', {}), 'w_diffusion', 1.0)\nw_angle = getattr(getattr(self.cfg, 'training', {}), 'w_angle', 0.1)\n\n# Add diffusion loss if computed\ndiffusion_loss_computed = 'loss_diffusion' in locals() and loss_diffusion is not None and torch.is_tensor(loss_diffusion)\nif diffusion_loss_computed:\n    # Ensure loss_diffusion requires grad if its inputs did\n    if not loss_diffusion.requires_grad and 'stage_d_pred' in locals() and stage_d_pred is not None and stage_d_pred.requires_grad:\n         loss_diffusion = loss_diffusion.clone().requires_grad_(True)\n    if w_diffusion &gt; 0:\n         total_loss = total_loss + w_diffusion * loss_diffusion\n         self.log('train/loss_diffusion', loss_diffusion, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n    else:\n         # Log zero if weight is zero but loss was computed\n         self.log('train/loss_diffusion', 0.0, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n\n# Add angle loss if computed (depends on load_ang and successful calculation)\nangle_loss_computed = 'loss_angle' in locals() and loss_angle is not None and torch.is_tensor(loss_angle)\nif angle_loss_computed:\n     # Ensure loss_angle requires grad if its inputs did\n     if not loss_angle.requires_grad and 'output' in locals() and output[\"torsion_angles\"].requires_grad:\n          loss_angle = loss_angle.clone().requires_grad_(True)\n     if w_angle &gt; 0:\n          total_loss = total_loss + w_angle * loss_angle\n          self.log('train/loss_angle', loss_angle, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n     else:\n         # Log zero if weight is zero but loss was computed\n         self.log('train/loss_angle', 0.0, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n\n# Log total loss\nself.log('train/loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n\n# Return the total loss for the optimizer\nreturn {\"loss\": total_loss}\n</code></pre></li> <li>Critical Considerations:<ul> <li>Gradient Requirement: Ensure <code>total_loss</code> retains <code>requires_grad=True</code> if any component loss had it. Starting <code>total_loss</code> as a zero tensor with <code>requires_grad=True</code> helps if the first added loss term happens to be zero but had graph history. Cloning component losses before adding might also be necessary if they were potentially modified in-place during logging.</li> <li>Logging: Use <code>sync_dist=True</code> for distributed training environments.</li> <li>Zero Weights: The logic handles cases where a weight is zero, ensuring the corresponding component isn't added to <code>total_loss</code> but is still logged (as 0.0) for monitoring consistency.</li> </ul> </li> </ul> </li> </ul> <p>7. File: <code>rna_predict/conf/**</code> - Configuration &amp; Dimension Review</p> <ul> <li> <p>Action 7.1: Establish Central Dimension Definitions.</p> <ul> <li>File: <code>rna_predict/conf/config_schema.py</code></li> <li>Action: Define a <code>DimensionsConfig</code> dataclass to hold shared dimensions.     <pre><code>@dataclass\nclass DimensionsConfig:\n    \"\"\"Centralized dimensions for consistency across stages.\"\"\"\n    # --- Stage B / Pairformer Output ---\n    c_s: int = 384  # Single representation dimension (Pairformer -&gt; Merger -&gt; Stage D)\n    c_z: int = 128  # Pair representation dimension (Pairformer -&gt; Merger -&gt; Stage D)\n\n    # --- Stage B / TorsionBERT Output ---\n    num_angles: int = 7 # Standard torsions (alpha-zeta, chi)\n    angle_rep_dim: int = field(init=False) # Calculated: 7 or 14\n\n    # --- Stage D Conditioning / Internal ---\n    c_s_inputs: int = 449 # Dimension of initial token features (InputFeatureEmbedder output)\n    c_token: int = 768 # Token dimension within Stage D Transformer\n    c_atom: int = 128 # Atom embedding dimension within Stage D\n    c_atompair: int = 16 # Atom pair embedding dimension within Stage D\n    c_noise_embedding: int = 32 # Noise level embedding dimension\n\n    # --- Latent Merger ---\n    latent_merger_hidden_dim: int = 256 # Example hidden dim for merger MLP/Transformer\n    latent_merger_output_dim: int = 512 # Example output dim for unified latent\n\n    def __post_init__(self):\n         # Example: calculate angle_rep_dim based on TorsionBERT config (needs access to it)\n         # This is tricky with pure dataclasses. Better to handle in RNALightningModule __init__\n         # Or require angle_mode to be passed here. Let's assume 14 for sin/cos default.\n         self.angle_rep_dim = self.num_angles * 2 # Default assumes sin/cos output\n\n# Add DimensionsConfig registration\ndef register_configs() -&gt; None:\n     # ... other registrations ...\n     cs.store(group=\"dimensions\", name=\"default\", node=DimensionsConfig)\n\n# In RNAConfig:\n@dataclass\nclass RNAConfig:\n     # ...\n     dimensions: DimensionsConfig = field(default_factory=DimensionsConfig)\n     # ...\n</code></pre></li> <li>File: <code>rna_predict/conf/dimensions/default.yaml</code> (New file)     <pre><code># Default dimension values\nc_s: 384\nc_z: 128\nnum_angles: 7\nc_s_inputs: 449\nc_token: 768\nc_atom: 128\nc_atompair: 16\nc_noise_embedding: 32\nlatent_merger_hidden_dim: 256\nlatent_merger_output_dim: 512\n</code></pre></li> <li>File: <code>rna_predict/conf/default.yaml</code> <pre><code>defaults:\n  - _self_\n  - data: default\n  - dimensions: default # &lt;&lt;&lt; ADD THIS\n  # ... model stages ...\n  - training: default\n  - test_data@test_data\n</code></pre></li> </ul> </li> <li> <p>Action 7.2: Use Interpolation in Stage YAMLs.</p> <ul> <li>Rationale: Link stage-specific dimensions back to the central <code>dimensions</code> config group.</li> <li>Example (<code>rna_predict/conf/model/stageB_pairformer.yaml</code>): <pre><code># Core model parameters\nn_blocks: 48 # Example specific value\nn_heads: 8   # Example specific value\nc_z: ${dimensions.c_z}          # Interpolated\nc_s: ${dimensions.c_s}          # Interpolated\n# ... other pairformer params ...\n</code></pre></li> <li>Example (<code>rna_predict/conf/model/stageD_diffusion.yaml</code>): <pre><code># ... other diffusion keys ...\nmodel_architecture:\n  c_s: ${dimensions.c_s}          # Interpolated\n  c_z: ${dimensions.c_z}          # Interpolated\n  c_s_inputs: ${dimensions.c_s_inputs} # Interpolated\n  c_token: ${dimensions.c_token}    # Interpolated\n  c_atom: ${dimensions.c_atom}      # Interpolated\n  c_atompair: ${dimensions.c_atompair} # Interpolated\n  c_noise_embedding: ${dimensions.c_noise_embedding} # Interpolated\n  sigma_data: 1.0 # Example specific value\n# ... other model_arch keys ...\n</code></pre></li> <li>Example (<code>rna_predict/conf/model/latent_merger.yaml</code> - If created): <pre><code>defaults:\n  - _self_\n\nmerge_method: \"concat\" # Example\nangle_dim: ${dimensions.angle_rep_dim} # Interpolated (Needs calculation logic)\ns_dim: ${dimensions.c_s}             # Interpolated\nz_dim: ${dimensions.c_z}             # Interpolated\nhidden_dim: ${dimensions.latent_merger_hidden_dim} # Interpolated\noutput_dim: ${dimensions.latent_merger_output_dim} # Interpolated\n# ... other merger params ...\n</code></pre></li> <li>Action: Apply this interpolation pattern systematically to all relevant dimension parameters in all stage <code>.yaml</code> files and corresponding <code>config_schema.py</code> dataclasses.</li> </ul> </li> <li> <p>Action 7.3: Verify Bridging Logic Dimensions.</p> <ul> <li>File: <code>rna_predict/training/rna_lightning_module.py</code> (<code>training_step</code>)</li> <li>Action: When preparing the <code>conditioning_signal</code> for Stage D, ensure the expected dimensions match those defined in <code>cfg.dimensions</code>.<ul> <li>Check <code>s_embeddings_atom</code> has feature dim <code>cfg.dimensions.c_s</code>.</li> <li>Check <code>z_embeddings_res</code> (or <code>z_embeddings_atom</code>) has feature dim <code>cfg.dimensions.c_z</code>.</li> <li>If using <code>unified_latent</code>, ensure its dimension matches <code>cfg.dimensions.latent_merger_output_dim</code> and that Stage D conditioning expects this dimension.</li> </ul> </li> <li>File: <code>rna_predict/pipeline/stageD/diffusion/components/diffusion_conditioning.py</code> (<code>DiffusionConditioning</code>)</li> <li>Action: Ensure the dimensions passed during initialization (<code>c_s</code>, <code>c_z</code>, <code>c_s_inputs</code>) are derived from the interpolated config values, ultimately linking back to <code>cfg.dimensions</code>.</li> </ul> </li> <li> <p>Verification:</p> <ul> <li>Run <code>python rna_predict/training/train.py --cfg job</code>. Examine the output YAML. Verify that dimensions like <code>c_s</code>, <code>c_z</code> are identical across <code>model.stageB_pairformer</code>, <code>model.stageD.diffusion.model_architecture</code>, etc.</li> <li>Instantiate <code>RNALightningModule</code> with the config and print the shapes of internal layers (e.g., <code>self.latent_merger.mlp[0].in_features</code>) to confirm they match the resolved config dimensions.</li> </ul> </li> </ul> <p>8. Testing Strategy</p> <ul> <li> <p>Action 8.1: Unit Tests.</p> <ul> <li>Files: <code>tests/unit/test_losses.py</code>, <code>tests/unit/test_merger.py</code>, <code>tests/unit/test_bridging.py</code>.</li> <li>Actions:<ul> <li>Write tests for <code>L_angle</code> calculation: include sin/cos conversion, MSE logic, masking.</li> <li>Write tests for <code>L_diffusion</code> calculation:<ul> <li>If noise prediction: test MSE with masking.</li> <li>If coordinate prediction: unit test <code>weighted_rigid_align</code>, <code>SmoothLDDTLoss</code>, weighted MSE, and the final combination with the <code>sigma_t</code> factor.</li> </ul> </li> <li>Write tests for <code>SimpleLatentMerger</code> (or chosen merger) checking output shape given input shapes matching <code>cfg.dimensions</code>.</li> <li>Refine tests for <code>residue_to_atoms</code> and bridging logic in <code>training_step</code> to ensure atom-level outputs have correct shapes based on <code>cfg.dimensions</code>.</li> </ul> </li> </ul> </li> <li> <p>Action 8.2: Integration Test (<code>tests/integration/test_lightning_trainer.py</code>).</p> <ul> <li>Setup:<ul> <li>Use a minimal dataset (1-2 samples with pre-computed angles).</li> <li>Use a minimal config (<code>test_config.yaml</code>) with small dimensions (e.g., <code>c_s=16, c_z=8</code>), 1 block per stage, <code>load_ang=true</code>, and loss weights <code>w_diffusion=1.0, w_angle=1.0</code>.</li> </ul> </li> <li>Execution: <pre><code>import pytest\nimport torch\nimport lightning as L\nfrom hydra import compose, initialize\nfrom rna_predict.training.rna_lightning_module import RNALightningModule\nfrom rna_predict.dataset.loader import RNADataset\nfrom rna_predict.dataset.collate import rna_collate_fn\nfrom torch.utils.data import DataLoader\n\n@pytest.mark.slow # Mark as slow integration test\ndef test_end_to_end_gradient_flow():\n    # Assumes test_config.yaml exists and points to minimal data\n    with initialize(config_path=\"../../rna_predict/conf\", version_base=None):\n        # Compose with minimal overrides for training\n        cfg = compose(config_name=\"test_config.yaml\", overrides=[\n            \"data.load_ang=true\",\n            \"training.w_diffusion=1.0\",\n            \"training.w_angle=1.0\",\n            \"data.batch_size=1\", # Use batch size 1 for easier debugging\n            \"data.max_res=30\", # Small max length\n            # Minimal model dimensions override example\n            \"dimensions.c_s=16\",\n            \"dimensions.c_z=8\",\n            \"dimensions.c_atom=4\",\n            \"dimensions.c_atompair=2\",\n            \"dimensions.c_token=32\",\n            \"dimensions.c_s_inputs=10\",\n            \"dimensions.c_noise_embedding=4\",\n            \"model.stageB_pairformer.n_blocks=1\",\n            \"model.stageD.diffusion.transformer.n_blocks=1\",\n            # Ensure Stage D runs in train mode for the test\n            \"model.stageD.mode=train\",\n            \"model.stageD.diffusion.mode=train\",\n        ])\n\n    # Setup Model, Dataset, DataLoader\n    model = RNALightningModule(cfg)\n    dataset = RNADataset(cfg=cfg, load_adj=False, load_ang=True, verbose=True)\n    # Ensure dataset is not empty\n    if len(dataset) == 0:\n         pytest.skip(\"Skipping gradient flow test: Dataset is empty.\")\n    dataloader = DataLoader(dataset, batch_size=cfg.data.batch_size, collate_fn=rna_collate_fn, num_workers=0)\n\n    # Define parameters to check gradients for\n    params_to_check = {\n        \"stageB_torsion\": list(model.stageB_torsion.parameters()),\n        \"stageB_pairformer\": list(model.stageB_pairformer.parameters()), # Check all if not using LoRA\n        \"latent_merger\": list(model.latent_merger.parameters()),\n        \"stageD\": list(model.stageD.parameters()) # Check diffusion manager/module params\n    }\n    # Filter for trainable params only (requires_grad=True)\n    trainable_params = {name: [p for p in params if p.requires_grad] for name, params in params_to_check.items()}\n\n    # Ensure there are trainable parameters before proceeding\n    total_trainable = sum(len(p) for p in trainable_params.values())\n    if total_trainable == 0:\n         pytest.skip(\"Skipping gradient flow test: No trainable parameters found in relevant modules.\")\n\n    # Simple Training Loop (instead of Trainer.fit for direct control)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    model.train() # Set model to training mode\n    batch = next(iter(dataloader))\n\n    # --- Manual Training Step ---\n    optimizer.zero_grad()\n    loss_dict = model.training_step(batch, 0) # Get loss dict { \"loss\": total_loss }\n    total_loss = loss_dict[\"loss\"]\n    assert torch.is_tensor(total_loss) and not torch.isnan(total_loss), \"Loss is NaN or not a tensor\"\n    total_loss.backward()\n    # --- End Manual Step ---\n\n    # Check Gradients\n    print(\"\\n--- Checking Gradients ---\")\n    all_grads_present = True\n    for name, params in trainable_params.items():\n        has_grad = any(p.grad is not None and p.grad.abs().sum() &gt; 0 for p in params)\n        print(f\"Module: {name}, Trainable Params: {len(params)}, Has Non-Zero Grad: {has_grad}\")\n        if not params:\n             print(f\"  (No trainable params found for {name})\")\n        elif not has_grad:\n            all_grads_present = False\n            print(f\"  WARNING: No gradients flowed to trainable parameters in {name}!\")\n            # Optional: Print grad norms for debugging\n            # for i, p in enumerate(params):\n            #     print(f\"    Param {i}: grad={p.grad.abs().sum().item() if p.grad is not None else None}\")\n\n    assert all_grads_present, \"Gradients did not flow back to all expected trainable components (TorsionBERT, Pairformer, Merger, StageD).\"\n    print(\"--- Gradient Check Passed ---\")\n</code></pre></li> <li>Action: Run this integration test. Debug any assertion failures related to missing gradients or NaN losses.</li> </ul> </li> <li> <p>Action 8.3: Training Monitoring.</p> <ul> <li>Setup:<ul> <li>Use a representative subset of data (e.g., 100-500 structures).</li> <li>Configure <code>lightning.pytorch.loggers.TensorBoardLogger</code>.</li> <li>Set <code>Trainer(logger=tb_logger, max_epochs=10)</code>.</li> </ul> </li> <li>Execution: Run <code>python rna_predict/training/train.py</code> with appropriate overrides for the dataset subset and logger.</li> <li>Analysis: Open TensorBoard (<code>tensorboard --logdir ./lightning_logs</code>). Examine the plots for <code>train/loss</code>, <code>train/loss_diffusion</code>, <code>train/loss_angle</code>. Look for decreasing trends and stability. Experiment with different <code>w_diffusion</code> and <code>w_angle</code> values (e.g., <code>[1.0, 0.0]</code>, <code>[0.0, 1.0]</code>, <code>[1.0, 0.1]</code>, <code>[1.0, 1.0]</code>) in the config to understand their relative impact.</li> </ul> </li> </ul> <p>Summary of Phase 3:</p> <p>This final phase ensures the components trained in Phases 1 &amp; 2 work together correctly. 1.  Loss Combination: Implemented a weighted sum of <code>L_angle</code> and <code>L_diffusion</code> in <code>training_step</code>, controlled by Hydra config. Added robust logging. 2.  Config Review: Emphasized using <code>config_schema.py</code> and Hydra interpolation (<code>${dimensions...}</code>) to enforce dimensional consistency across all YAML files and code. Outlined key dimensions to track. 3.  Testing: Defined specific unit tests for loss components and bridging, a critical integration test checking end-to-end gradient flow to all trainable modules (TorsionBERT, Pairformer, Merger, StageD), and a strategy for monitoring training stability and loss convergence using TensorBoard.</p>"},{"location":"guides/training_loss_hypothesis_analysis/#completing-this-phase-successfully-validates-that-the-entire-trainable-pipeline-stages-b-d-and-merger-is-correctly-connected-and-receiving-appropriate-learning-signals","title":"Completing this phase successfully validates that the entire trainable pipeline (Stages B, D, and Merger) is correctly connected and receiving appropriate learning signals.","text":"<p>Excellent points! These considerations highlight the practical nuances and potential friction points involved in implementing the proposed training plan. Let's analyze each one:</p> <ol> <li> <p>External Tool Dependency (DSSR) in Phase 1a:</p> <ul> <li>Analysis: You are correct. Relying on DSSR introduces an external dependency that requires separate installation and licensing (albeit free for academics). This contrasts with a pure Python dependency like MDAnalysis.</li> <li>Rationale Recap: The choice favoured DSSR because of its specialized accuracy and robustness in handling RNA complexities (conventions, modifications) out-of-the-box. The hypothesis is that the effort saved in not having to manually implement and rigorously validate angle calculations for all edge cases (especially inter-residue angles like \u03b1, \u03b5, \u03b6) using a general tool like MDAnalysis outweighs the one-time setup cost of DSSR. The risk of introducing subtle errors into the \"ground truth\" angles via manual implementation with MDAnalysis was deemed higher.</li> <li>Plan's Mitigation: The plan isolates this dependency into a single, explicit pre-computation script (<code>Phase 1a</code>).</li> <li>Refinement/Action:<ul> <li>Documentation: The pre-computation script's README/documentation must clearly state the DSSR version dependency and link to its installation/licensing instructions.</li> <li>Error Handling: The script needs robust error handling to detect DSSR failures (e.g., binary not found, license issue, file parsing error) and provide informative messages.</li> <li>Reproducibility: Store the exact DSSR version used alongside the generated <code>.pt</code> files (e.g., in a manifest file) to ensure future reproducibility.</li> </ul> </li> </ul> </li> <li> <p>Complexity of AF3 Loss in Phase 2 (Action 5.1):</p> <ul> <li>Analysis: Agreed. Implementing the full AlphaFold 3 coordinate loss (Eq 6, involving Algo 27 <code>SmoothLDDTLoss</code> and Algo 28 <code>weighted_rigid_align</code>) is considerably more complex than a standard MSE or even a noise-prediction MSE.</li> <li>Rationale Recap: The plan explicitly recommends starting with the simpler noise prediction MSE loss (<code>loss_diffusion = F.mse_loss(predicted_noise, actual_noise, reduction='none'); loss_diffusion = (loss_diffusion * mask_for_loss).sum() / (mask_for_loss.sum() * 3 + 1e-8)</code>) as the default <code>L_diffusion</code>, unless the <code>ProtenixDiffusionManager</code> is definitively designed to output denoised coordinates (<code>\\hat{x}_0</code>).</li> <li>Plan's Mitigation: The plan flags the coordinate loss path as complex and requiring specific algorithm implementations.</li> <li>Refinement/Action:<ul> <li>Prioritize Noise Prediction: Strongly emphasize implementing and testing the noise prediction loss first. Only switch to the coordinate prediction loss if absolutely necessary and after the rest of the pipeline is stable.</li> <li>Sub-tasking: If coordinate loss is required, break down its implementation into distinct sub-tasks: (a) implement <code>weighted_rigid_align</code>, (b) implement <code>SmoothLDDTLoss</code>, (c) ensure atom-type data is available for weights <code>wl</code>, (d) implement the noise-level weighting factor from Eq 6. Treat this as a separate, significant development effort.</li> </ul> </li> </ul> </li> <li> <p><code>run_stageD</code> Implementation Details (Training Mode):</p> <ul> <li>Analysis: Correct. The plan identifies that <code>run_stageD</code> needs modification but not the precise internal changes. <code>_run_stageD_impl</code> currently calls <code>run_diffusion_and_handle_output</code>, which in turn calls <code>ProtenixDiffusionManager.multi_step_inference</code>. This inference path is unsuitable for training.</li> <li>Rationale Recap: The plan flags this as needing investigation.</li> <li>Refinement/Action:<ul> <li>Recommended Approach: Modify <code>ProtenixDiffusionManager</code> to have a distinct <code>train_step</code> method (or adapt its <code>forward</code> method based on <code>self.training</code> or a <code>mode</code> flag). This method should take noisy coordinates, conditioning signals, and the noise level (<code>sigma_t</code>) as input, perform one step of the diffusion model's forward pass (predicting noise or denoised coords), and return that prediction.</li> <li>Modify <code>run_stageD</code> / <code>_run_stageD_impl</code>: Add conditional logic based on <code>context.mode</code>. If <code>'train'</code>, it should call the new <code>diffusion_manager.train_step</code> method instead of <code>multi_step_inference</code>.</li> <li>Interface Clarity: Ensure the return value of <code>run_stageD</code> in training mode is clearly defined (e.g., just the predicted noise tensor, or a tuple/dict including it) so <code>RNALightningModule.training_step</code> knows what to expect for the loss calculation.</li> </ul> </li> </ul> </li> <li> <p>Bridging Function Robustness (<code>residue_to_atoms</code>, <code>derive_residue_atom_map</code>):</p> <ul> <li>Analysis: Valid concern. The entire conditioning signal's correctness hinges on these utilities mapping residue-level information (like <code>s_embeddings</code>) to the correct atom-level representation expected by Stage D.</li> <li>Rationale Recap: The plan uses these functions but implicitly assumes their correctness.</li> <li>Refinement/Action:<ul> <li>Dedicated Unit Tests: Implement specific unit tests for <code>rna_predict.utils.tensor_utils.embedding.residue_to_atoms</code> and <code>rna_predict.utils.tensor_utils.residue_mapping.derive_residue_atom_map</code>.</li> <li>Test Cases: These tests should cover:<ul> <li>Standard RNA sequences.</li> <li>Sequences with varying lengths (requiring padding/truncation relative to <code>max_res</code>).</li> <li>Cases with <code>atom_mask</code> indicating missing atoms.</li> <li>Batch handling (if the functions are designed to be batched).</li> <li>Correct output shapes and dtypes.</li> <li>Correct mapping verification for a small, known example.</li> </ul> </li> <li>Placement: Add these tests under <code>tests/unit/utils/</code> or similar. Run them as part of the prerequisites before or during Phase 2 implementation.</li> </ul> </li> </ul> </li> <li> <p>Hyperparameter Tuning (Loss Weights):</p> <ul> <li>Analysis: Correct. The optimal <code>w_diffusion</code> and <code>w_angle</code> are unknown and will impact training dynamics.</li> <li>Rationale Recap: The plan sets up the structure (configurable weights) but defers the tuning.</li> <li>Refinement/Action:<ul> <li>Acknowledge as Future Work: Explicitly state in documentation (e.g., a <code>docs/training_guide.md</code>) that loss weights are key hyperparameters requiring tuning after initial functionality is established.</li> <li>Suggest Initial Values: Recommend starting points (e.g., <code>w_diffusion=1.0</code>, <code>w_angle=0.1</code> or <code>w_angle=0.0</code> initially to isolate diffusion training).</li> <li>Tuning Strategy: Briefly mention potential tuning methods (manual adjustment based on validation loss curves, grid search, random search, Bayesian optimization).</li> </ul> </li> </ul> </li> </ol> <p>Revised Hypothesis (Implicit):</p>"},{"location":"guides/training_loss_hypothesis_analysis/#while-the-implementation-plan-addresses-the-core-requirements-for-fixing-the-training-loop-successful-execution-hinges-on-careful-handling-of-the-identified-minor-considerations-specifically-managing-the-dssr-dependency-choosing-the-appropriate-diffusion-loss-complexity-correctly-adapting-the-run_staged-function-for-training-mode-rigorously-testing-the-bridging-utilities-and-planning-for-subsequent-hyperparameter-tuning-these-are-not-blockers-but-represent-key-areas-requiring-developer-attention-and-potentially-dedicated-effort-during-or-after-the-main-implementation-phases","title":"While the implementation plan addresses the core requirements for fixing the training loop, successful execution hinges on careful handling of the identified \"Minor Considerations,\" specifically: managing the DSSR dependency, choosing the appropriate diffusion loss complexity, correctly adapting the <code>run_stageD</code> function for training mode, rigorously testing the bridging utilities, and planning for subsequent hyperparameter tuning. These are not blockers but represent key areas requiring developer attention and potentially dedicated effort during or after the main implementation phases.","text":"<p>Okay, let's analyze those MDAnalysis timings in the context of using it for on-the-fly angle calculation during training data loading.</p> <p>Analysis of Timings:</p> <ul> <li>Short Sequences (10-24 residues): 20-53 milliseconds per structure. This is very fast and unlikely to be a bottleneck.</li> <li>Longer Sequence (240 residues): 373 milliseconds (0.37 seconds) per structure. This is considerably longer.</li> </ul> <p>Context: DataLoader Performance</p> <ul> <li><code>__getitem__</code> Impact: This calculation happens every time <code>RNADataset.__getitem__</code> is called for a sample.</li> <li><code>num_workers</code>: PyTorch's <code>DataLoader</code> uses multiple worker processes (<code>num_workers</code> in your config, default is 8) to prepare batches in parallel ahead of time. The goal is to have the next batch ready the moment the GPU finishes processing the current one.</li> <li>Bottleneck Identification: The overall throughput of the DataLoader is limited by the time it takes for workers to prepare samples. If the average time per sample (including file I/O, sequence loading, coordinate loading, and angle calculation) multiplied by the batch size, divided by the number of workers, is significantly longer than the time your model takes for one training step on the GPU, then data loading becomes the bottleneck, and your GPU will sit idle waiting for data.</li> </ul> <p>Is 0.37s \"Fast Enough\"?</p> <ul> <li>For a Single Item: Maybe.</li> <li>For Training Throughput: Potentially problematic, especially for longer sequences.<ul> <li>If your dataset has many sequences around the 200-500 residue mark, each taking 0.3-0.5+ seconds just for angle calculation within <code>__getitem__</code>.</li> <li>Even with 8 workers, if a few workers happen to be processing long sequences simultaneously, the time to assemble a full batch could easily exceed the time for a GPU training step (which might be &lt; 0.5s or even &lt; 0.1s depending on model size and hardware).</li> <li>This leads to the GPU waiting, drastically slowing down your overall training time.</li> </ul> </li> </ul> <p>Comparison with Pre-computation:</p> <ul> <li>Loading <code>.pt</code> file: Typically takes single-digit milliseconds (1-10ms), dominated by disk I/O.</li> <li>Difference: Pre-computation removes the 20ms-370ms+ calculation cost entirely from the training loop's data loading path.</li> </ul> <p>Conclusion &amp; Recommendation:</p> <p>While you can proceed with on-the-fly MDAnalysis calculation, and it will work functionally (especially if your dataset is mostly short sequences or your <code>num_workers</code> is high relative to GPU speed), it is highly likely to become a performance bottleneck during training, potentially slowing it down significantly.</p> <p>Strong Recommendation:</p> <p>Leverage the pre-computation script you already built (<code>compute_ground_truth_angles.py</code>).</p> <ol> <li> <p>Run Pre-computation with MDAnalysis: Execute your script once for your entire dataset using the MDAnalysis backend:     <pre><code># Example command - adjust paths as needed\nuv run rna_predict/dataset/preprocessing/compute_ground_truth_angles.py \\\n    --input_dir /path/to/your/pdb_cif_files \\\n    --output_dir /path/to/store/angle_pt_files \\\n    --chain_id &lt;your_default_or_logic&gt; \\\n    --backend mdanalysis\n</code></pre> (Ensure the output directory is accessible or adjust paths in <code>RNADataset</code> accordingly)</p> </li> <li> <p>Revert <code>_load_angles</code> in <code>loader.py</code>: Change the <code>RNADataset._load_angles</code> method back to the simpler version designed in the refined plan, which just loads the pre-computed <code>.pt</code> file:     <pre><code># Inside RNADataset class\ndef _load_angles(self, row) -&gt; Optional[torch.Tensor]:\n    \"\"\"Loads pre-computed ground truth torsion angles from a .pt file.\"\"\"\n    angle_path = None\n    try:\n        # --- Logic to determine the ANGLE file path ---\n        # This MUST match how compute_ground_truth_angles.py saves files.\n        # Example: Assuming output_dir from compute script is accessible\n        # and uses naming like &lt;pdb_id&gt;_&lt;chain_id&gt;_angles.pt\n\n        # !! ADAPT THIS PATH LOGIC !!\n        # 1. Get base name (e.g., from row['id'] or derived from row['filepath'])\n        base_name = row.get('id', 'unknown') # Or derive from filepath\n\n        # 2. Construct expected .pt filename\n        #    This assumes the compute script saved with _&lt;chain&gt;_angles.pt suffix\n        chain_id = row.get('chain_id', 'A') # Get chain if available, else default\n        angle_fname = f\"{base_name}_{chain_id}_angles.pt\"\n\n        # 3. Define the directory where compute_script saved the files\n        #    This might need to be configurable via cfg.data.angle_dir\n        angle_dir = Path(self.cfg.data.angle_dir) # Add angle_dir to your config!\n\n        angle_path = angle_dir / angle_fname\n        # --- End Path Logic ---\n\n        if not angle_path.is_file():\n            if self.verbose:\n                logger.warning(f\"Pre-computed angle file not found: {angle_path}. Returning None.\")\n            return None\n\n        # Load the tensor data\n        angles_tensor = torch.load(angle_path, map_location='cpu')\n\n        # Basic validation (optional)\n        if not isinstance(angles_tensor, torch.Tensor) or angles_tensor.dim() != 2 or angles_tensor.shape[1] != 7:\n             logger.error(f\"Invalid data in pre-computed angle file {angle_path}. Shape: {angles_tensor.shape}, Type: {type(angles_tensor)}\")\n             return None\n\n        return angles_tensor.to(torch.float32) # Ensure correct dtype\n\n    except Exception as e:\n        logger.error(f\"Error loading pre-computed angles for {row.get('id', 'N/A')} from {angle_path if angle_path else 'unknown path'}: {e}\", exc_info=self.verbose)\n        return None\n</code></pre></p> </li> </ol> <p>Advantages of this approach:</p> <ul> <li>Best Performance: Ensures fastest possible data loading during training.</li> <li>Leverages Existing Work: Uses the <code>compute_ground_truth_angles.py</code> script you already wrote and tested.</li> <li>Clean Separation: Keeps calculation (pre-computation) separate from loading (<code>RNADataset</code>).</li> <li>Future-Proof: When DSSR is ready, you simply re-run the same pre-computation script with <code>--backend dssr</code> without changing the DataLoader.</li> </ul> <p>While on-the-fly calculation is tempting as a temporary measure, investing the time now to run the pre-computation (even with MDAnalysis) will likely save significant training time later.</p>"},{"location":"guides/x3dna_dssr_setup/","title":"Setting Up X3DNA-DSSR for RNA Structure Analysis","text":""},{"location":"guides/x3dna_dssr_setup/#overview","title":"Overview","text":"<p>This guide outlines the process for obtaining and setting up X3DNA-DSSR, which is required for generating ground truth torsion angle data for our RNA prediction pipeline.</p>"},{"location":"guides/x3dna_dssr_setup/#step-1-obtain-x3dna-dssr-license-and-binary","title":"Step 1: Obtain X3DNA-DSSR License and Binary","text":"<ol> <li>Request an Academic License:</li> <li>Visit the Columbia Technology Ventures DSSR page</li> <li>Click on \"Express Licensing\"</li> <li>Sign in or create an account</li> <li>Request the \"DSSR-Basic, Academic (1 seat)\" license, which is now free for academic users thanks to NIH funding</li> <li>Complete the license request form with your academic credentials</li> <li> <p>Wait for approval (usually within 1-2 business days)</p> </li> <li> <p>Download the Binary:</p> </li> <li>After your license is approved, you'll receive an email with download instructions</li> <li>Download the appropriate binary for your operating system (Linux, macOS, or Windows)</li> <li>The current version as of this writing is DSSR v2.5.2-2025apr03</li> </ol>"},{"location":"guides/x3dna_dssr_setup/#step-2-install-x3dna-dssr","title":"Step 2: Install X3DNA-DSSR","text":""},{"location":"guides/x3dna_dssr_setup/#for-macoslinux","title":"For macOS/Linux:","text":"<ol> <li> <p>Create a directory for the binary:    <pre><code>mkdir -p ~/bin/x3dna-dssr\n</code></pre></p> </li> <li> <p>Move the downloaded binary to this directory:    <pre><code>mv /path/to/downloaded/x3dna-dssr ~/bin/x3dna-dssr/\n</code></pre></p> </li> <li> <p>Make the binary executable:    <pre><code>chmod +x ~/bin/x3dna-dssr/x3dna-dssr\n</code></pre></p> </li> <li> <p>Add to your PATH by adding the following line to your <code>~/.bashrc</code>, <code>~/.zshrc</code>, or equivalent shell configuration file:    <pre><code>export PATH=\"$HOME/bin/x3dna-dssr:$PATH\"\n</code></pre></p> </li> <li> <p>Apply the changes:    <pre><code>source ~/.bashrc  # or source ~/.zshrc\n</code></pre></p> </li> </ol>"},{"location":"guides/x3dna_dssr_setup/#for-windows","title":"For Windows:","text":"<ol> <li>Create a folder for the binary, e.g., <code>C:\\Program Files\\x3dna-dssr</code></li> <li>Move the downloaded binary to this folder</li> <li>Add the folder to your system PATH:</li> <li>Right-click on \"This PC\" or \"My Computer\" and select \"Properties\"</li> <li>Click on \"Advanced system settings\"</li> <li>Click on \"Environment Variables\"</li> <li>Under \"System variables\", find and select \"Path\", then click \"Edit\"</li> <li>Click \"New\" and add the path to the folder (e.g., <code>C:\\Program Files\\x3dna-dssr</code>)</li> <li>Click \"OK\" on all dialogs to save the changes</li> </ol>"},{"location":"guides/x3dna_dssr_setup/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<p>Test that X3DNA-DSSR is properly installed and accessible:</p> <pre><code>x3dna-dssr --version\n</code></pre> <p>You should see the version information displayed.</p>"},{"location":"guides/x3dna_dssr_setup/#next-steps","title":"Next Steps","text":"<p>Once X3DNA-DSSR is installed and working, we'll proceed with:</p> <ol> <li>Creating a script to automate running DSSR over our dataset structure files</li> <li>Saving the results as <code>.pt</code> files containing the radian angle tensors</li> <li>Integrating these ground truth angles into our training pipeline</li> </ol>"},{"location":"guides/x3dna_dssr_setup/#references","title":"References","text":"<ul> <li>X3DNA-DSSR Official Website</li> <li>DSSR Documentation</li> <li>DSSR Paper</li> </ul>"},{"location":"guides/x3dna_dssr_setup/#license","title":"License","text":"<p>DSSR: an integrated software tool for dissecting the spatial structure of RNA Overview Request Info More Like This Express Licensing DSSR-Basic, Academic (1 seat), Now Free!</p> <p>First name\u2009* Last name\u2009* Organization name\u2009* Job title\u2009* Address\u2009* Apartment, suite, etc. (optional) City\u2009* Country\u2009* State, territory, or province\u2009* Postal code\u2009* Telephone number\u2009* Agreement You must accept the terms of the agreement using the checkbox at the bottom of the page.</p> <p>DSSR-Basic, Academic (1 seat), Now Free! CU20391 -- DSSR: an integrated software tool for dissecting the spatial structure of RNA</p> <p>You: Tom Riddle CU Biology END USER LICENSE AGREEMENT</p> <p>BY DOWNLOADING, INSTALLING OR USING THE ABOVE-NAMED SOFTWARE PROGRAM OR ITS RELATED DOCUMENTATION (COLLECTIVELY, THE \"PROGRAM\"), YOU ACKNOWLEDGE THAT YOU HAVE READ ALL OF THE TERMS AND CONDITIONS OF THIS AGREEMENT, UNDERSTAND THEM, AND AGREE TO BE BOUND BY THEM. WE RECOMMEND THAT YOU PRINT A COPY OF THIS AGREEMENT FOR YOUR RECORDS.</p> <p>IF YOU DO NOT AGREE TO ALL OF THE TERMS OF THIS AGREEMENT, YOU MUST NOT DOWNLOAD, INSTALL OR USE THE PROGRAM.</p> <p>YOU HEREBY REPRESENT AND WARRANT THAT YOU HAVE THE LEGAL AUTHORITY TO BIND THE ORGANIZATION NAMED IN YOUR REGISTRATION FORM, IF ANY, AND IF SUCH AN ORGANIZATION IS NAMED, SUCH ORGANIZATION SHALL BE DEEMED TO BE \"YOU\" FOR THE PURPOSE OF THIS AGREEMENT. IF NO SUCH ORGANIZATION IS NAMED, THEN \"YOU\" SHALL REFER TO YOU INDIVIDUALLY.</p> <p>This Software License Agreement (the \"Agreement\") is between The Trustees of Columbia University in the City of New York, a non-profit private educational institution, having a principal place of business at 116th St. and Broadway, New York, New York 10027, U.S.A. (\"Columbia\") and You (as defined above).</p> <ol> <li> <p>License Grant. Under Columbia\u2019s rights, Columbia grants You a non-exclusive and non-transferable license to install, display, and use one (1) copy of the Program. Columbia reserves the right to make corrections, improvements, or enhancements to the Program without notice to You and without obligation to furnish the said corrections, improvements, or enhancements to You.</p> </li> <li> <p>Restrictions. You will not (i) reproduce or copy the Program, except that You may make one (1) copy of the Program solely for archival purposes, provided that You agree to reproduce all copyright and other proprietary right notices on the archival copy; (ii) use, or cause or permit the use of, the Program in whole or in part for any purpose other than as permitted under this Agreement; (iii) distribute, sell, lease, sublicense or otherwise transfer rights to the Program to any third party; (iv) reverse engineer, decompile, disassemble or otherwise attempt to derive the source code for the Program (except to the extent applicable laws specifically prohibit such restriction); (v) modify or create any derivative works of the Program, including translation or localization; or (vi) remove or alter any patent, trademark, logo, copyright or other proprietary notices, legends, symbols or labels in the Program.</p> </li> <li> <p>License Fee. Now free, as</p> </li> <li> <p>Term and Termination. The term of this Agreement shall continue until terminated in accordance with this Section 4. You may terminate this Agreement at any time by destroying all copies of the Program. This Agreement and the rights granted under this Agreement will terminate automatically, and without any further notice from or action by Columbia, if You fail to comply with any obligation set forth herein. Upon termination, You must immediately cease use and destroy all copies of the Program and verify such destruction in writing. Columbia shall have the right to disable electronically Your unauthorized use of the Program and resort to other \"self-help\" measures Columbia deems appropriate. Sections 2, 4-10, and 12-14 shall survive expiration or termination of this Agreement.</p> </li> <li> <p>No Obligation to Support. It is understood and agreed that Columbia will provide no maintenance or installation services of any kind, error corrections, bug fixes, patches, updates or other modifications hereunder. In the event that Columbia, at its sole option, provides updates, error corrections, bug fixes, patches or other modifications to the Program to You (\"Program Updates\"), the Program Updates will be considered part of the Program, and subject to the terms and conditions of this Agreement.</p> </li> <li> <p>Proprietary Rights. Title to the Program, and patents, copyrights, trademarks, and all other intellectual property rights applicable thereto, shall at all times remain solely and exclusively with Columbia and its suppliers, and You shall not take any action inconsistent with such ownership. Any rights not expressly granted herein are reserved to Columbia and its suppliers. You will not use or display any trademark, trade name, insignia, or symbols of Columbia, its faculties or departments, or any variation or combination thereof, or the name of any trustee, faculty member, other employee, or student of Columbia, for any purpose whatsoever without Columbia's prior written consent.</p> </li> <li> <p>NO WARRANTY. TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, COLUMBIA DISCLAIMS ALL WARRANTIES AND CONDITIONS, EITHER EXPRESS OR IMPLIED, WITH RESPECT TO THE PROGRAM, INCLUDING BUT NOT LIMITED TO ALL IMPLIED WARRANTIES AND CONDITIONS OF MERCHANTABILITY, TITLE, FITNESS, ADEQUACY OR SUITABILITY FOR A PARTICULAR PURPOSE, USE OR RESULT, OR ARISING FROM A COURSE OF DEALING, USAGE OR TRADE PRACTICE, AND ANY WARRANTIES OF FREEDOM FROM INFRINGEMENT OF ANY DOMESTIC OR FOREIGN PATENTS, COPYRIGHTS, TRADE SECRETS OR OTHER PROPRIETARY RIGHTS OF ANY PARTY. COLUMBIA SPECIFICALLY DISCLAIMS ANY WARRANTY THAT THE FUNCTIONS CONTAINED IN THE PROGRAM WILL MEET YOUR REQUIREMENTS OR WILL OPERATE IN COMBINATIONS OR IN A MANNER SELECTED FOR USE BY YOU, OR THAT THE OPERATION OF THE LICENSED SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.</p> </li> <li> <p>LIMITATION OF LIABILITY. IN NO EVENT SHALL COLUMBIA BE LIABLE TO YOU FOR ANY DAMAGES RESULTING FROM LOSS OF DATA, LOST PROFITS, LOSS OF USE OF EQUIPMENT OR LOST CONTRACTS OR FOR ANY SPECIAL, INDIRECT, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL DAMAGES IN ANY WAY ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THE PROGRAM OR RELATING TO THIS AGREEMENT, HOWEVER CAUSED, EVEN IF COLUMBIA HAS BEEN MADE AWARE OF THE POSSIBILITY OF SUCH DAMAGES. COLUMBIA'S ENTIRE LIABILITY TO YOU, REGARDLESS OF THE FORM OF ANY CLAIM OR ACTION OR THEORY OF LIABILITY (INCLUDING CONTRACT, TORT, OR WARRANTY), SHALL NOT EXCEED IN THE AGGREGATE THE SUM OF TEN U.S. DOLLARS ($10.00).</p> </li> <li> <p>Exports. Each party agrees not to take any action, directly or indirectly, that would violate or cause the other party to violate United States laws and regulations, including, without limitation, regulations and rules regarding sponsored research, trade and import and export controls (the \u201cExport Laws\u201d). In that connection, You confirm to be each of the following: (a) not a Restricted Party and that no agency of the U.S. Government has denied, suspended, or otherwise abridged the Company\u2019s export or import privileges. A \u201cRestricted Party\u201d means any company or individual on the Department of Treasury Office of Foreign Assets Control list of Specially Designated Nationals and Blocked Persons or List of Foreign Sanctions Evaders, on the Denied Persons List, the Entity List, or the Unverified List maintained by the U.S. Department of Commerce\u2019s Bureau of Industry and Security or on any other list maintained by any governmental agency restricting the export of any items to or other transactions with specific individuals, companies or other entities; (b) not directly or indirectly owned or controlled by or acting on behalf of others whose interests taken in the aggregate make them subject to U.S. trade sanctions or restrictions; (c) not directly or indirectly owned or controlled by or acting on behalf of a government of or entity located in a country subject to economic sanctions programs that are or may be maintained by the U.S. Government; and (d) not otherwise restricted, embargoed, or prohibited under applicable law from entering into agreements with U.S. entities and individuals. You shall not export, re-export or otherwise transfer to any individuals or entities identified in items (i)-(iv) above any hardware, software, technology or services provided by Columbia under this Agreement. You confirm that you do not intend for the hardware, software, technology or services that Columbia provides under this Agreement to be used for any purposes prohibited by U.S. export control laws and regulations, including without limitation nuclear, chemical, or biological weapons proliferation, or for military end-uses or military end-users. The provisions of this section will remain in full force and effect during the term of this Agreement, and the You will immediately notify Columbia of any events or changes that may conflict with the assurances and statements provided hereunder. You agree to comply with all applicable export laws and regulations of all jurisdictions with respect to the Program and obtain, at your own expense, any required permits or export clearances, copies of which you shall provide to Columbia prior to such export.</p> </li> <li> <p>U.S. Government Agencies. If You are an agency of the United States Government, the Program constitutes \"commercial computer software\" or \"commercial computer software documentation.\" Absent a written agreement to the contrary, the Government's rights with respect to the Program are limited by the terms of this Agreement, pursuant to FAR 12.212(a) and/or DFARS 227.7202-4, as applicable.</p> </li> <li> <p>Assignment. Neither this Agreement nor any rights, obligations, or licenses granted hereunder may be assigned or delegated by You without the prior written consent of Columbia. This Agreement shall inure to the benefit of the parties and their permitted successors and assigns.</p> </li> <li> <p>Governing Law; Jurisdiction and Venue. This Agreement shall be governed by New York law applicable to agreements made and to be fully performed in New York, without reference to the conflict of laws principles of any jurisdiction. The parties agree that any and all claims arising under this Agreement or relating thereto shall be heard and determined either in the United States District Court for the Southern District of New York or in the Courts of the State of New York located in the City and County of New York, and the parties agree to submit themselves to the personal jurisdiction of those Courts and to waive any objections as to the convenience of the forum.</p> </li> <li> <p>Severability. If any provision of this Agreement shall be held by a court of competent jurisdiction to be illegal, invalid or unenforceable, the remaining provisions shall remain in full force and effect.</p> </li> <li> <p>Miscellaneous. (a) This Agreement and its exhibits contain the entire understanding and agreement between the parties respecting the subject matter hereof. (b) This Agreement may not be supplemented, modified, amended, released or discharged except by an instrument in writing signed by each party\u2019s duly authorized representative. (c) All captions and headings in this Agreement are for purposes of convenience only and shall not affect the construction or interpretation of any of its provisions. (d) Any waiver by either party of any default or breach hereunder shall not constitute a waiver of any provision of this Agreement or of any subsequent default or breach of the same or a different kind. (e) This Agreement shall be binding upon and shall inure to the benefit of the parties, their successors, and permitted assigns.</p> </li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/","title":"Code Quality Improvement: Best Practices and Lessons Learned (Revised)","text":"<p>This document outlines key lessons and best practices for improving code quality in complex projects, based on our team's experiences with refactoring and code analysis.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#quick-start-code-quality-improvement-workflow","title":"Quick Start: Code Quality Improvement Workflow","text":"<p>This guide outlines a step-by-step process for improving code quality, based on real-world experiences and lessons learned from previous failures.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#step-1-baseline-assessment","title":"Step 1: Baseline Assessment","text":"<ol> <li>Clean Slate First:    ```bash    rm analysis_results_*.txt</li> </ol> <p>Why: Old analysis files have misled developers into thinking they were looking at current results.     2.  Full Analysis:</p> <p>bash rna_predict/scripts/analyze_code.sh path/to/file.py</p> <p>Expected output: Creates analysis_results_file.py.txt with comprehensive analysis</p> <pre><code>3.  Review Complete Report:\n</code></pre> <p>cat analysis_results_file.py.txt</p> <p>Why: Skipping this step and using only filtered views has caused teams to miss critical context.</p> <pre><code>4.  Check Current Score:\n</code></pre> <p>cat analysis_results_file.py.txt | grep \"score\"</p> <p>Rule of thumb:     \u2022   Scores below 5.0: Critical improvement needed     \u2022   Scores 5.0-7.0: Significant refactoring required     \u2022   Scores 7.0-8.5: Targeted improvements needed     \u2022   Scores above 8.5: Fine-tuning only</p> <pre><code>5.  \u2705 Save Baseline Metrics:\n</code></pre> <p>cp analysis_results_file.py.txt analysis_results_file.py.baseline.txt</p> <p>Why: Without a preserved baseline, you cannot objectively measure improvement.</p> <p>Step 2: Identify Root Issues (Not Just Symptoms)     1.  Analyze File Size:</p> <p>wc -l path/to/file.py</p> <p>Decision point: If file exceeds 500 lines, prioritize splitting it before detailed refactoring.</p> <pre><code>2.  Find all methods that have code smell issue identified by codescene (cs):\n</code></pre> <p>Focus on: Methods with cyclomatic complexity &gt; 15 or &gt; 50 lines</p> <pre><code>3.  Identify Type Issues:\n</code></pre> <p>mypy --strict path/to/file.py</p> <p>Critical insight: Type errors often reveal deeper design problems, not just annotation issues.</p> <pre><code>4.  Build a Type-Issue Map:\n</code></pre> <p>Create a simple table with these columns:</p> Error Location Error Type Root Cause Fix Strategy <p>Why: This creates a roadmap for fixing structural issues, not just symptoms.</p> <pre><code>5.  Check Test Coverage:\n</code></pre> <p>python -m pytest --cov=module.path tests/path/to/test_file.py -v</p> <p>Rule of thumb: Don\u2019t refactor code with &lt; 60% test coverage without adding tests first</p> <p>Step 3: Create Tactical Plan</p> <p>Based on the assessment, create a plan following this priority order (learned from past failures):     1.  If file &gt; 500 lines: Split into logical modules first</p> <p>Example problem: - Original: ml_utils.py (900 lines) - Better: atom_operations.py, tensor_utils.py, sequence_processing.py</p> <pre><code>2.  If high cyclomatic complexity: Extract helper functions\n</code></pre> <p>Example problem: - Original: process_data() function with complexity of 25 - Better: Five helper functions with complexity of 5 each</p> <pre><code>3.  If deep nesting: Invert conditions or extract blocks\n</code></pre> <p>Example problem: - Original: 5 levels of nested if-statements - Better: Early returns or extracted conditional blocks</p> <pre><code>4.  If duplicate logic: Create shared utilities\n</code></pre> <p>Example problem: - Original: Same tensor manipulation in 6 different functions - Better: Shared utility function with clear documentation</p> <p>Step 4: Implementation (With Verification)</p> <p>For each change, follow this strict verification protocol:     1.  \u2705 MANDATORY: Run component tests before making changes:</p> <p>python -m pytest tests/stageX/component_tests/test_specific_component.py -v</p> <p>Why: Establishes a known-working baseline for comparison.</p> <pre><code>2.  Make targeted changes (one issue at a time):\n</code></pre> <p>Example: - Extract ONE complex function - Fix ONE type error pattern - NOT overhauling multiple systems at once</p> <pre><code>3.  \u2705 MANDATORY: Run component tests after each change:\n</code></pre> <p>python -m pytest tests/stageX/component_tests/test_specific_component.py -v</p> <p>Verification rule: If tests fail, revert immediately and re-evaluate approach.</p> <pre><code>4.  \u2705 MANDATORY: Check for new linter/type issues:\n</code></pre> <p>mypy --strict path/to/file.py</p> <p>Warning: Changes often introduce new type errors that must be addressed.</p> <pre><code>5.  \u2705 MANDATORY: After multiple related changes, run stage pipeline tests:\n</code></pre> <p>python -m pytest tests/stageX/test_stage_x_*.py -v</p> <p>CRITICAL CHECKPOINT: Never skip integration tests after module-level changes.</p> <pre><code>6.  \u2705 Complete Test Verification Checklist:\n\u2022   Component tests passing\n\u2022   No new type errors introduced\n\u2022   Integration tests passing\n\u2022   Code review by another team member (if available)\n</code></pre> <p>Step 5: Verify Improvement     1.  Re-run full analysis:</p> <p>bash rna_predict/scripts/analyze_code.sh path/to/file.py</p> <pre><code>2.  Compare score improvement:\n</code></pre> <p>echo \"BEFORE:\" &amp;&amp; cat analysis_results_file.py.baseline.txt | grep \"score\" echo \"AFTER:\" &amp;&amp; cat analysis_results_file.py.txt | grep \"score\"</p> <p>Expected improvements:     \u2022   File splitting: +1.0-2.0 points     \u2022   Complexity reduction: +0.5-1.0 points per function     \u2022   Type fixes: +0.3-0.7 points</p> <pre><code>3.  \u2705 Measure Specific Improvements:\n</code></pre> <p>python -m scripts.compare_code_quality analysis_results_file.py.baseline.txt analysis_results_file.py.txt</p> <p>Expected output: Detailed metrics showing concrete improvements across categories.</p> <pre><code>4.  If score improved &lt; 0.3 points:\n\u2022   You likely addressed symptoms, not root causes\n\u2022   The most common reason is keeping complex code in the same file\n\u2022   Consider more aggressive module separation\n</code></pre> <p>Step 6: Document Lessons</p> <p>After completing refactoring, add a brief comment to your commit message:</p> <p>Refactored X to improve code quality (score: 5.2 \u2192 7.8) - Split large file into 3 modules - Reduced complexity of function Y from 25 to 8 - Fixed 12 type errors</p> <p>Refactoring Decision Tree</p> <p>Use this decision tree to determine which refactoring approach to apply based on code quality issues:</p> <p>START \u2502 \u251c\u2500 Is file &gt; 500 lines? \u2502  \u251c\u2500 YES \u2192 Split into logical modules (Step 3.1) \u2502  \u2502        \u2193 \u2502  \u2502        Run dependency analysis first (Step 3.5) \u2502  \u2502        \u2193 \u2502  \u2502        Verify cross-module interactions (Step 4.5) \u2502  \u2502 \u2502  \u2514\u2500 NO \u2192 Continue \u2502 \u251c\u2500 Any functions with complexity &gt; 15? \u2502  \u251c\u2500 YES \u2192 Extract helper functions (Step 3.2) \u2502  \u2502        \u2193  \u2502  \u2502        Verify each function works independently (Step 4.3) \u2502  \u2502 \u2502  \u2514\u2500 NO \u2192 Continue \u2502 \u251c\u2500 Any nested conditionals &gt; 3 levels deep? \u2502  \u251c\u2500 YES \u2192 Invert conditions or extract blocks (Step 3.3) \u2502  \u2502        \u2193 \u2502  \u2502        Verify logic remains equivalent (Step 4.3) \u2502  \u2502 \u2502  \u2514\u2500 NO \u2192 Continue \u2502 \u251c\u2500 Any duplicate logic patterns? \u2502  \u251c\u2500 YES \u2192 Create shared utilities (Step 3.4) \u2502  \u2502        \u2193 \u2502  \u2502        Verify all callers work with new utility (Step 4.5) \u2502  \u2502 \u2502  \u2514\u2500 NO \u2192 Continue \u2502 \u2514\u2500 Any type errors/warnings?    \u251c\u2500 YES \u2192 Apply type-driven refactoring (New section below)    \u2502        \u2193    \u2502        Verify type correctness (Step 4.4)    \u2502    \u2514\u2500 NO \u2192 Apply style improvements and documentation</p> <p>Type-Driven Refactoring Guide</p> <p>Type errors reveal structural issues in your code. Use this guide to address common patterns:</p> <p>Common Type Error Patterns and Their Root Causes</p> <p>Error Pattern   Likely Root Cause   Refactoring Approach Union types with None   Inconsistent return types   Standardize return type with proper error handling Complex Union types Function doing too many things  Split function by return type Type ignores (# type: ignore)   Weak abstractions   Create proper interfaces with Protocol classes Shape mismatches in tensors Missing shape validation    Add shape adapter utilities Any types   Lack of proper typing   Define custom TypedDict or dataclass</p> <p>Tensor Shape Handling Best Practices</p> <p>For machine learning codebases, tensor shape issues are common sources of runtime errors:     1.  Create shape adapter functions:</p> <p>def ensure_shape_compatibility(     tensor: torch.Tensor,      expected_shape: Tuple[int, ...],      dim: int = -1 ) -&gt; torch.Tensor:     \"\"\"Ensure tensor has expected shape along specified dimension.\"\"\"     actual_shape = tensor.shape[dim]     if actual_shape != expected_shape[dim]:         # Implement adaptation logic here         pass     return tensor</p> <pre><code>2.  Use runtime shape assertions:\n</code></pre> <p>def process_batch(batch: torch.Tensor) -&gt; torch.Tensor:     assert batch.dim() == 3, f\"Expected 3D tensor, got shape {batch.shape}\"     # Processing logic     return batch</p> <pre><code>3.  Standardize error handling:\n</code></pre> <p>def safe_tensor_operation(a: torch.Tensor, b: torch.Tensor) -&gt; torch.Tensor:     try:         return a @ b  # Matrix multiplication     except RuntimeError as e:         # Log shapes and operation         raise ValueError(f\"Shape mismatch: {a.shape} @ {b.shape}\") from e</p> <p>Dependency Management Protocol</p> <p>When splitting files, follow this protocol to avoid runtime errors:</p> <ol> <li>Map Dependencies Before Splitting</li> </ol> <p>mkdir -p analysis/deps pydeps path/to/file.py --max-bacon=2 --cluster --output analysis/deps/file_deps.svg</p> <ol> <li>Identify Dependency Patterns</li> </ol> <p>Look for:     \u2022   Circular dependencies     \u2022   Functions that share many variables     \u2022   Functions called from many locations</p> <ol> <li>Create Module Boundaries</li> </ol> <p>Create new files based on:     \u2022   Functional cohesion (functions that work together)     \u2022   Data cohesion (functions that operate on the same data structures)     \u2022   Minimal cross-module dependencies</p> <ol> <li>Handle Import Order</li> </ol> <p>Create files in this order:     1.  Base utilities with no project dependencies     2.  Domain-specific utilities that depend only on base utilities     3.  Core functionality that may depend on both</p> <ol> <li>Cross-Module Verification</li> </ol> <p>After splitting:</p>"},{"location":"guides/best_practices/code_quality_best_practices/#check-for-circular-imports","title":"Check for circular imports","text":"<p>pylint --disable=all --enable=cyclic-import path/to/module/</p>"},{"location":"guides/best_practices/code_quality_best_practices/#verify-imports-resolve-correctly","title":"Verify imports resolve correctly","text":"<p>python -c \"import path.to.new.module\"</p>"},{"location":"guides/best_practices/code_quality_best_practices/#run-tests-to-verify-functionality","title":"Run tests to verify functionality","text":"<p>python -m pytest tests/path/to/module_tests.py -v</p> <ol> <li>Common Dependency Mistakes</li> </ol> <p>Mistake Detection Method    Prevention Strategy Circular imports    pylint --enable=cyclic-import   Use dependency injection or move shared code to a common module Import errors   Run module directly Create proper package structure with init.py files Runtime type errors Component tests Add runtime type checking on module boundaries</p> <p>Verification Checklist Template</p> <p>Copy this checklist into your refactoring ticket/issue and complete each step:</p>"},{"location":"guides/best_practices/code_quality_best_practices/#pre-refactoring","title":"Pre-Refactoring","text":"<ul> <li>[ ] Baseline code quality score captured</li> <li>[ ] Component tests running and passing</li> <li>[ ] Integration tests running and passing</li> <li>[ ] Type errors documented</li> <li>[ ] Dependency map created</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#during-refactoring-for-each-change","title":"During Refactoring (for each change)","text":"<ul> <li>[ ] Component tests pass after change</li> <li>[ ] No new type errors introduced</li> <li>[ ] Changes address root causes, not just symptoms</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#post-refactoring","title":"Post-Refactoring","text":"<ul> <li>[ ] Component tests passing</li> <li>[ ] Integration tests passing</li> <li>[ ] Code quality score improved by expected amount</li> <li>[ ] No new type errors or linter warnings</li> <li>[ ] Code review completed (if applicable)</li> <li>[ ] Documentation updated</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#metrics","title":"Metrics","text":"<ul> <li>Initial code quality score: ___</li> <li>Final code quality score: ___</li> <li>Number of type errors before: ___</li> <li>Number of type errors after: ___</li> <li>Lines of code before: ___</li> <li>Lines of code after: ___</li> </ul> <p>Implementation Approach     1.  Module Separation: Split large files into smaller, focused modules with clear responsibilities before deep refactoring.     2.  Preserve Interfaces: When extracting functionality, maintain the same public interfaces to minimize ripple effects.     3.  Incremental Verification: Run tests after each significant change to catch regressions early.     4.  Address Type Errors: Fix typing issues as they often reveal deeper design problems.     5.  Track Metrics: Measure concrete improvements at each step to ensure progress.</p> <p>Measuring Progress     1.  Track Multiple Metrics: Monitor all relevant quality indicators, not just a single score.     2.  Understand Score Components: Know how different issues contribute to the overall score.     3.  Verify Real Improvements: Ensure changes improve maintainability, not just metrics.     4.  Watch for Negative Side Effects: Monitor if improvements in one area cause degradation in others.</p> <p>Common Pitfalls to Avoid     1.  Function Extraction Without Reorganization: Creating helper functions in the same file can increase file size and complexity.     2.  Over-focusing on Individual Functions: Missing forest-level issues while fixing tree-level problems.     3.  Losing Context in Analysis: Using narrow grep filters that miss important contextual information.     4.  Missing Low-Hanging Fruit: Overlooking simple fixes with high impact (like file splitting).     5.  Ignoring Global Patterns: Missing repeated patterns that could be addressed with a single systemic change.     6.  Skipping Integration Tests: Failing to verify cross-module functionality after making changes.     7.  Superficial Changes Without Validation: Making cosmetic improvements without measuring impact.     8.  Type Annotation Without Design Improvement: Adding types without addressing underlying design issues.</p> <p>Testing Strategy     1.  Leverage Modular Testing: In a modular architecture, run tests specific to the component or pipeline stage you\u2019re refactoring. This is more efficient than running the entire test suite for every change and provides faster feedback.     2.  Stage Integration Testing: Run integration tests only after component tests pass to verify that changes haven\u2019t broken cross-module interactions.     3.  Create Tests for Uncovered Code: Add tests for previously uncovered code paths before refactoring.     4.  Validate Edge Cases: Ensure that refactored code handles edge cases correctly.     5.  Test Performance Impact: Check that refactoring doesn\u2019t introduce performance regressions.</p> <p>Implementation Workflow     1.  Baseline: Run tests and analysis     2.  Plan: Identify highest-priority issues using score breakdown     3.  Change: Make one targeted improvement     4.  Verify: Run tests and quick analysis     5.  Repeat: Until target score is reached     6.  Validate: Run comprehensive analysis and full test suite</p> <p>Remember to review complete analysis output rather than relying solely on filtered views for critical decisions.</p> <p>Conclusion</p> <p>Improving code quality is an iterative process that requires both technical skill and strategic thinking. By following the guidelines outlined in this document and leveraging the provided commands, you can systematically transform complex, difficult-to-maintain code into cleaner, more robust implementations.</p> <p>The most successful refactoring efforts combine these key elements:     \u2022   Comprehensive baseline metrics     \u2022   Clear understanding of dependencies     \u2022   Root cause analysis of issues (not just symptoms)     \u2022   Strict verification at each step     \u2022   Measurement of concrete improvements</p> <p>By adhering to these principles and using the provided checklists, you\u2019ll avoid common refactoring pitfalls and achieve sustainable code quality improvements.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#debugging-protocol","title":"Debugging Protocol","text":"<p>When tests fail during refactoring, immediately switch to this systematic debugging workflow:</p>"},{"location":"guides/best_practices/code_quality_best_practices/#cohesive-systematic-debugging-workflow-version-5","title":"Cohesive, Systematic Debugging Workflow (Version 5)","text":""},{"location":"guides/best_practices/code_quality_best_practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction  </li> <li>Phase A: Capture, Triage &amp; Control  </li> <li>Phase B: Reproduce &amp; Simplify  </li> <li>Phase C: Hypothesis Generation &amp; Verification  </li> <li>Phase D: Systematic Cause Isolation  </li> <li>Phase E: Fix, Verify &amp; Learn  </li> <li>References</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#1-introduction","title":"1. Introduction","text":"<p>Debugging is both a critical and time-consuming aspect of software development. Despite decades of research, finding the root cause of a failure remains challenging due to issues like non-reproducibility, overcomplicated failure scenarios, and difficulty in correctly formulating hypotheses.</p> <p>This workflow integrates insights from three pillars: - Andreas Zeller\u2019s Why Programs Fail, which provides a systematic, scientific approach (the TRAFFIC model, defect\u2013infection\u2013failure chain, delta debugging, and dynamic slicing). - Alaboudi &amp; LaToza\u2019s research, which emphasizes that formulating explicit, correct hypotheses early in the debugging process is essential for success. - LLM-driven scientific debugging (AutoSD), which shows that modern tools, including large language models, can assist in hypothesis generation, interact with debuggers, and produce explainable reasoning traces.</p> <p>Our goal is to provide a robust, repeatable, and efficient process that not only finds the defect causing the failure but also generates a clear, documented reasoning trail for future learning and improved processes.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#2-phase-a-capture-triage-control","title":"2. Phase A: Capture, Triage &amp; Control","text":""},{"location":"guides/best_practices/code_quality_best_practices/#a1-purpose-background","title":"A.1 Purpose &amp; Background","text":"<p>Before any technical analysis begins, you must have a clear, reproducible description of the failure. Zeller\u2019s \u201cTrack\u201d phase underscores the importance of a thorough bug report. Alaboudi &amp; LaToza further stress that incomplete or ambiguous information makes it extremely difficult to formulate correct hypotheses later. Additionally, modern debugging approaches (such as those using LLMs) depend on having accurate, well-organized initial context.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#a2-what-why","title":"A.2 What &amp; Why","text":"<ul> <li>Capture the Bug: Record the issue with all necessary details (environment, steps to reproduce, logs, etc.).</li> <li>Triage &amp; Classify: Determine severity and priority; ensure everyone is on the same page regarding the failure.</li> <li>Control Environment: Establish the precise conditions (software version, OS, configuration) under which the bug occurs.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#a3-detailed-steps","title":"A.3 Detailed Steps","text":"<ol> <li>Record the Issue: </li> <li>Log the bug in your issue tracker (e.g., Jira, Bugzilla, GitHub Issues).  </li> <li>Include:<ul> <li>A clear, concise summary.</li> <li>Detailed steps to reproduce the failure.</li> <li>Observed behavior versus expected behavior.</li> <li>Diagnostic data: error messages, stack traces, logs, and screenshots.</li> <li>Environment details (OS, hardware, software versions, configurations).</li> </ul> </li> <li>Triage: </li> <li>Assess the impact, assign severity (e.g., blocker, critical, major) and priority.  </li> <li>This prioritization helps focus efforts on the most impactful defects.</li> <li>Establish Control: </li> <li>Ensure that all relevant context is available for subsequent debugging steps.  </li> <li>Use clear, unambiguous language (preferably distinguishing between Failure\u2014the observable error, Infection\u2014the erroneous internal state, and Defect\u2014the underlying code error).</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#a4-practical-tips","title":"A.4 Practical Tips","text":"<ul> <li>Keep Reports Concise Yet Complete: Aim for a minimal but sufficient reproduction.</li> <li>Attach Artifacts: Provide logs, screenshots, and stack traces to improve context.</li> <li>Standardize Terminology: Clearly define \u201cdefect,\u201d \u201cinfection,\u201d and \u201cfailure\u201d for the team.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#3-phase-b-reproduce-simplify","title":"3. Phase B: Reproduce &amp; Simplify","text":""},{"location":"guides/best_practices/code_quality_best_practices/#b1-purpose-background","title":"B.1 Purpose &amp; Background","text":"<p>Reproducibility is the foundation of systematic debugging (WPF Chapters 3\u20135). You must reliably trigger the failure under controlled conditions and then simplify the scenario to isolate the essential elements of the bug. A minimal test case not only speeds up iterations but also makes it easier to generate and test hypotheses (a critical point from A&amp;L and AutoSD).</p>"},{"location":"guides/best_practices/code_quality_best_practices/#b2-what-why","title":"B.2 What &amp; Why","text":"<ul> <li>Reproduce: Ensure you can trigger the failure consistently in a controlled environment.</li> <li>Automate: Convert the steps into an automated test for repeatable experimentation.</li> <li>Simplify: Reduce extraneous factors until you have the smallest possible test case that still reproduces the failure.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#b3-detailed-steps","title":"B.3 Detailed Steps","text":"<ol> <li>Reproduce the Failure Deterministically: </li> <li>Set up a controlled environment (local machine, container, or CI environment) that matches the bug report.</li> <li>Incrementally adjust configurations (files, dependencies, OS) to replicate the conditions.</li> <li>Ensure determinism by controlling randomness (fixed seeds, static time settings) and using capture/replay tools if necessary.</li> <li>Automate the Test Case: </li> <li>Write a script or unit test that automates the reproduction of the failure.</li> <li>Store the test case in version control as a permanent artifact.</li> <li>Simplify the Test Case (Delta Debugging): </li> <li>Apply automated delta debugging (e.g., <code>ddmin</code>) or manual binary search to remove unnecessary parts of the input/configuration.</li> <li>Aim for a \u201c1-minimal\u201d test case where removing any element causes the failure to vanish.</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#b4-practical-tips","title":"B.4 Practical Tips","text":"<ul> <li>Version Control the Test: The minimal test case will be invaluable for verifying future fixes.</li> <li>Ensure Fast Execution: A small, simplified test case enables rapid iterations.</li> <li>LLM Input Considerations: A concise, well-defined test is ideal when feeding context into LLM-based debugging tools.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#4-phase-c-hypothesis-generation-verification","title":"4. Phase C: Hypothesis Generation &amp; Verification","text":""},{"location":"guides/best_practices/code_quality_best_practices/#c1-purpose-background","title":"C.1 Purpose &amp; Background","text":"<p>At the heart of efficient debugging is the formulation of explicit, testable hypotheses about the bug\u2019s root cause. Alaboudi &amp; LaToza\u2019s research indicates that the earlier a correct hypothesis is formed, the more likely the defect will be resolved successfully. Zeller\u2019s Scientific Debugging (Chapter 6) prescribes a methodical loop of hypothesize, predict, experiment, and conclude. Modern LLM-based systems (like AutoSD) can assist by automatically suggesting potential hypotheses and experiments.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#c2-what-why","title":"C.2 What &amp; Why","text":"<ul> <li>Generate Hypotheses: Formulate a short list of plausible causes based on observed behavior.</li> <li>Test Quickly: Design micro-experiments to validate or refute each hypothesis.</li> <li>Iterate: Use the scientific method to refine your understanding until a promising lead is found.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#c3-detailed-steps","title":"C.3 Detailed Steps","text":"<ol> <li>Observe &amp; Brainstorm: </li> <li>Run the minimal test case and observe program state via debuggers, logs, or tracing tools.</li> <li>Compare failing and passing runs to spot anomalies.</li> <li>Brainstorm potential causes (e.g., \u201can off-by-one error,\u201d \u201cnull pointer exception due to uninitialized variable,\u201d \u201cmisuse of an external API\u201d).</li> <li>Leverage Tool Assistance: </li> <li>If available, use an LLM to generate additional hypotheses by providing it with the minimal test case, code snippet, and failure details.</li> <li>Alternatively, consult static analysis tools to highlight suspicious patterns.</li> <li>Record Hypotheses: </li> <li>Log each hypothesis in a dedicated \u201cdebug log\u201d along with your rationale.</li> <li>Example entry: \u201cHypothesis #1: The array index in loop X is off by one. Expected behavior: iterate from 0 to N\u20131; observed: iterating from 0 to N.\u201d</li> <li>Design &amp; Execute Experiments: </li> <li>For each hypothesis, predict what change would fix the issue.  </li> <li>Temporarily modify the code or state:<ul> <li>Use a debugger to change variable values or step through suspect code.</li> <li>Insert temporary code modifications (e.g., adjust loop bounds, add null checks).</li> <li>Add assertions to verify expected state (WPF Chapter 10).</li> </ul> </li> <li>Run the automated test case to see if the failure is resolved.</li> <li>Conclude &amp; Iterate: </li> <li>If the test passes after your change, the hypothesis is supported.</li> <li>If not, discard or refine the hypothesis and repeat the experiment.</li> <li>Update your debug log with the outcome of each experiment.</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#c4-practical-tips","title":"C.4 Practical Tips","text":"<ul> <li>Emphasize Correctness: A&amp;L\u2019s studies show that the success of debugging hinges on getting the correct hypothesis early.</li> <li>Keep Experiments Small: Test one small change at a time.</li> <li>Interactive LLM Use: If using LLM tools, ask for specific debugger commands or small code snippets and integrate them into your test cycle.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#5-phase-d-systematic-cause-isolation","title":"5. Phase D: Systematic Cause Isolation","text":""},{"location":"guides/best_practices/code_quality_best_practices/#d1-purpose-background","title":"D.1 Purpose &amp; Background","text":"<p>Even if a hypothesis is validated through small experiments, it might address only a symptom rather than the earliest point of failure in the infection chain. Zeller\u2019s methodology stresses the importance of isolating the defect\u2014the point where a correct state first becomes \u201cinfected.\u201d This phase uses static and dynamic analysis to trace back through code dependencies, ensuring that the root cause is identified.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#d2-what-why","title":"D.2 What &amp; Why","text":"<ul> <li>Trace the Infection Chain: Identify where the program state first deviated from correctness.</li> <li>Use Advanced Analysis: Employ static slicing, dynamic slicing, and omniscient debugging tools to determine dependencies.</li> <li>Why: Finding the earliest infection ensures you correct the true defect rather than applying a superficial fix.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#d3-detailed-steps","title":"D.3 Detailed Steps","text":"<ol> <li>Static &amp; Dynamic Analysis: </li> <li>Static Slicing: Generate a control and data-dependence graph (WPF Chapter 7) to see all statements that could have affected the failing variable.</li> <li>Dynamic Slicing: Use dynamic slicing tools (WPF Chapter 9) to analyze the execution trace of the failing run, focusing on the actual path taken.</li> <li>Omniscient Debugging: If available, use tools that record full execution history to step backward and pinpoint the first moment of deviation.</li> <li>Delta Debugging on State: </li> <li>Compare the state of the failing run with a passing run.  </li> <li>Use delta debugging techniques on program states (WPF Chapters 11\u201314) to isolate the minimal difference that triggers the failure.</li> <li>Iterative Refinement: </li> <li>Based on the slicing and state comparison, refine your hypotheses and perform targeted experiments (refer back to Phase C).</li> <li>Focus on identifying a specific line or block of code (the defect) where correct inputs produce an infected output.</li> <li>Validate the Defect: </li> <li>Temporarily patch or correct the identified location.  </li> <li>Re-run the minimal test case to confirm that the failure is resolved.</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#d4-practical-tips","title":"D.4 Practical Tips","text":"<ul> <li>Systematic Documentation: Update your debug log with slices, comparisons, and experimental outcomes.</li> <li>Tool Integration: Consider integrating advanced static/dynamic analysis tools to assist with slicing.</li> <li>Be Wary of Multiple Causes: Some bugs may involve multiple interacting factors; isolate the most critical infection point first.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#6-phase-e-fix-verify-learn","title":"6. Phase E: Fix, Verify &amp; Learn","text":""},{"location":"guides/best_practices/code_quality_best_practices/#e1-purpose-background","title":"E.1 Purpose &amp; Background","text":"<p>Once the true defect has been identified, it is time to implement a robust fix. Zeller\u2019s later chapters (Chapters 15\u201316) emphasize that the fix should address the root cause and not just mask symptoms. Furthermore, reflecting on the debugging process and documenting the reasoning trace helps prevent future occurrences.</p>"},{"location":"guides/best_practices/code_quality_best_practices/#e2-what-why","title":"E.2 What &amp; Why","text":"<ul> <li>Implement the Fix: Correct the defect at its source.</li> <li>Verify Thoroughly: Ensure that the fix resolves the failure and does not introduce new issues.</li> <li>Document &amp; Learn: Capture the debugging reasoning, update tests, and reflect on process improvements.</li> <li>Why: A robust fix, combined with proper documentation, reduces recurrence and aids team learning, closing the feedback loop.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#e3-detailed-steps","title":"E.3 Detailed Steps","text":"<ol> <li>Implement the Fix: </li> <li>Apply the minimal change needed at the defect location to restore correct behavior.</li> <li>Prefer the simplest, most localized change that corrects the logic.</li> <li>Verify the Fix: </li> <li>Re-run the Minimal Test Case: Confirm the failure is gone.</li> <li>Run Regression Tests: Execute the full test suite to ensure no new issues have been introduced.</li> <li>Peer Review: Have another developer review the fix for accuracy and potential side effects.</li> <li>Document the Outcome: </li> <li>Update the bug report with the fix details, linking the commit(s) to the original issue.</li> <li>Archive the full debugging log and explanation trace (this \u201creasoning trace\u201d is akin to AutoSD\u2019s output), providing insights for future reference.</li> <li>Reflect &amp; Improve: </li> <li>Conduct a root cause analysis: Why was the defect introduced? What process or design gaps allowed it?</li> <li>Enhance Quality Assurance:<ul> <li>Add assertions or invariant checks to catch similar issues earlier.</li> <li>Expand or refine the test suite based on the minimal test case.</li> <li>Consider code refactoring or improved code review practices if systemic patterns are observed.</li> </ul> </li> <li>Update any predictive risk models if used.</li> </ol>"},{"location":"guides/best_practices/code_quality_best_practices/#e4-practical-tips","title":"E.4 Practical Tips","text":"<ul> <li>Consolidate Learning: Encourage team discussions on what was learned from the debugging session.</li> <li>Capture the Reasoning Trace: Ensure that the final explanation\u2014whether generated manually or via an LLM tool\u2014is stored in an accessible repository for onboarding or future troubleshooting.</li> <li>Iterate on Process: Use each debugging experience to continuously refine the workflow.</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#7-references","title":"7. References","text":"<ul> <li>Zeller, A. Why Programs Fail: A Guide to Systematic Debugging. Morgan Kaufmann. (Referenced Chapters: 2\u201316)</li> <li>Alaboudi, A., &amp; LaToza, T. Using Hypotheses as a Debugging Aid. (Key insights on hypothesis formulation and its impact on debugging success)</li> <li>Kang, S., Chen, B., Yoo, S., &amp; Lou, J-G. Explainable Automated Debugging via Large Language Model-Driven Scientific Debugging (AutoSD). (Insights on LLM-driven debugging, interactive hypothesis testing, and explanation generation)</li> </ul>"},{"location":"guides/best_practices/code_quality_best_practices/#final-thoughts","title":"Final Thoughts","text":"<p>This Version 5 Cohesive Debugging Workflow represents a synthesis of the best practices from established debugging methodologies and modern, automated tools. By following these five phases\u2014Capture &amp; Triage, Reproduce &amp; Simplify, Hypothesis Generation &amp; Verification, Systematic Cause Isolation, and Fix, Verify &amp; Learn\u2014developers gain both the technical rigor and the practical efficiency necessary to address defects thoroughly. Explicit emphasis on hypothesis formulation and testing (as shown by Alaboudi &amp; LaToza) combined with automated assistance (AutoSD) ensures that the root cause is identified accurately and that the solution is both robust and well-documented for continuous learning.</p> <p>This comprehensive document is designed to serve as a technical guide for teams and individuals seeking a methodical approach to debugging\u2014one that is more powerful than the sum of its parts.</p> <p>CodeScene Command Reference cs delta</p> <p>DESCRIPTION   Run a delta analysis. A delta analysis compares your current work against a   previous state of your code. See also <code>cs docs git-hooks</code>.</p> <p>USAGE   $ cs delta [] [ []] <p>OPTIONS       --staged        Examine only staged content.       --interactive   Run command with interactive mode, forcing user input for findings.  For more info try <code>cs docs interactive</code>.       --git-hook      Run as a git hook. For more info try <code>cs docs git-hooks</code>.       --output-format Output in json or edn format. Default is human-readable.       --pretty        Use in conjunction with --format to pretty-print the output.       --verbose       Verbose output   -h, --help          Show this help</p> <p>EXAMPLES   $ cs delta                       Analyse all non-committed changes   $ cs delta --output-format json  As above, but output in json format. Note that                                    specifying any output-format will report new issues                                    only - i.e. no improvents or explanations   $ cs delta main                  Analyse changes against the main branch   $ cs delta main feat             Analyse changes between two branches   $ cs delta main~30 main          Analyse the latest 30 commits on main</p> <p>cs review</p> <p>DESCRIPTION   Check a file for code health issues and print the results in a JSON structure</p> <p>USAGE   $ cs review [] [] <p>OPTIONS       --file-name     Specify the file-name when reading from stdin       --output-format Output in json or edn format. Default is human-readable.       --pretty        Use in conjunction with --format to pretty-print the output.       --verbose       Verbose output   -h, --help          Show this help</p> <p>EXAMPLES   $ cs review test.c                       Check the file test.c   $ cs review master:./test.c              Check the file test.c on the master branch   $ cs review 801b0c0f:./test.c            Check the file test.c at the given commit   $ cs review --file-name test.c &lt; test.c  Read file data from stdin</p> <p>cs check</p> <p>DESCRIPTION   Check a file for code health issues and print the results in a lint-like manner.</p> <p>USAGE   $ cs check [] [] <p>OPTIONS       --file-name Specify the file-type/extension when reading from stdin       --verbose   Verbose output   -h, --help      Show this help</p> <p>EXAMPLES   $ cs check test.c                       Check the file test.c   $ cs check master:./test.c              Check the file test.c on the master branch   $ cs check 801b0c0f:./test.c            Check the file test.c at the given commit   $ cs check --file-name test.c &lt; test.c  Read file data from stdin</p> <p>DESCRIPTION   CodeScene CLI documentation topics</p> <p>cs docs</p> <p>USAGE   $ cs docs    example <code>cs docs git-hooks</code> <p>AVAILABLE TOPICS</p> <p>Delta analysis    \ud83d\udcc4 git-hooks                Using the delta command in a git hook    \ud83d\udcc4 interactive          Using the delta command in interactive mode to prompt user input    \ud83d\udcc4 interactive-pre-commit-hook-example  Outputs an example pre-commit hook using interactive</p> <p>\ud83d\udcc4 pre-commit-hook-example  Outputs an example pre-commit hook</p> <p>Editor integration    \ud83d\udcc4 vim  Integrating the \"check\" command in (neo)vim</p> <p>Miscellaneous    \ud83d\udcc4 license                     Setting up a license    \ud83d\udcc4 file-name                   File-name and language support    \ud83d\udcc4 code-health-rules           Customizing code health rules    \ud83d\udcc4 code-health-rules-template  Outputs a code health rules template</p> <p>cs check-rules</p> <p>DESCRIPTION   Find out which of your custom rules, if any, matches the given file   This use useful when creating custom code-health-rules.json</p> <p>USAGE   $ cs check-rules [] <p>EXAMPLES   $ cs check-rules test.c             Check which code health rule that matches test.c</p> <p>cs version</p> <p>DESCRIPTION   Displays the version</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/","title":"Cohesive, Systematic Debugging Workflow (Version 5)","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction  </li> <li>Phase A: Capture, Triage &amp; Control  </li> <li>Phase B: Reproduce &amp; Simplify  </li> <li>Phase C: Hypothesis Generation &amp; Verification  </li> <li>Phase D: Systematic Cause Isolation  </li> <li>Phase E: Fix, Verify &amp; Learn  </li> <li>References</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#1-introduction","title":"1. Introduction","text":"<p>Debugging is both a critical and time-consuming aspect of software development. Despite decades of research, finding the root cause of a failure remains challenging due to issues like non-reproducibility, overcomplicated failure scenarios, and difficulty in correctly formulating hypotheses.</p> <p>This workflow integrates insights from three pillars: - Andreas Zeller\u2019s Why Programs Fail, which provides a systematic, scientific approach (the TRAFFIC model, defect\u2013infection\u2013failure chain, delta debugging, and dynamic slicing). - Alaboudi &amp; LaToza\u2019s research, which emphasizes that formulating explicit, correct hypotheses early in the debugging process is essential for success. - LLM-driven scientific debugging (AutoSD), which shows that modern tools, including large language models, can assist in hypothesis generation, interact with debuggers, and produce explainable reasoning traces.</p> <p>Our goal is to provide a robust, repeatable, and efficient process that not only finds the defect causing the failure but also generates a clear, documented reasoning trail for future learning and improved processes.</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#2-phase-a-capture-triage-control","title":"2. Phase A: Capture, Triage &amp; Control","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#a1-purpose-background","title":"A.1 Purpose &amp; Background","text":"<p>Before any technical analysis begins, you must have a clear, reproducible description of the failure. Zeller\u2019s \u201cTrack\u201d phase underscores the importance of a thorough bug report. Alaboudi &amp; LaToza further stress that incomplete or ambiguous information makes it extremely difficult to formulate correct hypotheses later. Additionally, modern debugging approaches (such as those using LLMs) depend on having accurate, well-organized initial context.</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#a2-what-why","title":"A.2 What &amp; Why","text":"<ul> <li>Capture the Bug: Record the issue with all necessary details (environment, steps to reproduce, logs, etc.).</li> <li>Triage &amp; Classify: Determine severity and priority; ensure everyone is on the same page regarding the failure.</li> <li>Control Environment: Establish the precise conditions (software version, OS, configuration) under which the bug occurs.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#a3-detailed-steps","title":"A.3 Detailed Steps","text":"<ol> <li>Record the Issue: </li> <li>Log the bug in your issue tracker (e.g., Jira, Bugzilla, GitHub Issues).  </li> <li>Include:<ul> <li>A clear, concise summary.</li> <li>Detailed steps to reproduce the failure.</li> <li>Observed behavior versus expected behavior.</li> <li>Diagnostic data: error messages, stack traces, logs, and screenshots.</li> <li>Environment details (OS, hardware, software versions, configurations).</li> </ul> </li> <li>Triage: </li> <li>Assess the impact, assign severity (e.g., blocker, critical, major) and priority.  </li> <li>This prioritization helps focus efforts on the most impactful defects.</li> <li>Establish Control: </li> <li>Ensure that all relevant context is available for subsequent debugging steps.  </li> <li>Use clear, unambiguous language (preferably distinguishing between Failure\u2014the observable error, Infection\u2014the erroneous internal state, and Defect\u2014the underlying code error).</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#a4-practical-tips","title":"A.4 Practical Tips","text":"<ul> <li>Keep Reports Concise Yet Complete: Aim for a minimal but sufficient reproduction.</li> <li>Attach Artifacts: Provide logs, screenshots, and stack traces to improve context.</li> <li>Standardize Terminology: Clearly define \u201cdefect,\u201d \u201cinfection,\u201d and \u201cfailure\u201d for the team.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#3-phase-b-reproduce-simplify","title":"3. Phase B: Reproduce &amp; Simplify","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#b1-purpose-background","title":"B.1 Purpose &amp; Background","text":"<p>Reproducibility is the foundation of systematic debugging (WPF Chapters 3\u20135). You must reliably trigger the failure under controlled conditions and then simplify the scenario to isolate the essential elements of the bug. A minimal test case not only speeds up iterations but also makes it easier to generate and test hypotheses (a critical point from A&amp;L and AutoSD).</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#b2-what-why","title":"B.2 What &amp; Why","text":"<ul> <li>Reproduce: Ensure you can trigger the failure consistently in a controlled environment.</li> <li>Automate: Convert the steps into an automated test for repeatable experimentation.</li> <li>Simplify: Reduce extraneous factors until you have the smallest possible test case that still reproduces the failure.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#b3-detailed-steps","title":"B.3 Detailed Steps","text":"<ol> <li>Reproduce the Failure Deterministically: </li> <li>Set up a controlled environment (local machine, container, or CI environment) that matches the bug report.</li> <li>Incrementally adjust configurations (files, dependencies, OS) to replicate the conditions.</li> <li>Ensure determinism by controlling randomness (fixed seeds, static time settings) and using capture/replay tools if necessary.</li> <li>Automate the Test Case: </li> <li>Write a script or unit test that automates the reproduction of the failure.</li> <li>Store the test case in version control as a permanent artifact.</li> <li>Simplify the Test Case (Delta Debugging): </li> <li>Apply automated delta debugging (e.g., <code>ddmin</code>) or manual binary search to remove unnecessary parts of the input/configuration.</li> <li>Aim for a \u201c1-minimal\u201d test case where removing any element causes the failure to vanish.</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#b4-practical-tips","title":"B.4 Practical Tips","text":"<ul> <li>Version Control the Test: The minimal test case will be invaluable for verifying future fixes.</li> <li>Ensure Fast Execution: A small, simplified test case enables rapid iterations.</li> <li>LLM Input Considerations: A concise, well-defined test is ideal when feeding context into LLM-based debugging tools.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#4-phase-c-hypothesis-generation-verification","title":"4. Phase C: Hypothesis Generation &amp; Verification","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#c1-purpose-background","title":"C.1 Purpose &amp; Background","text":"<p>At the heart of efficient debugging is the formulation of explicit, testable hypotheses about the bug\u2019s root cause. Alaboudi &amp; LaToza\u2019s research indicates that the earlier a correct hypothesis is formed, the more likely the defect will be resolved successfully. Zeller\u2019s Scientific Debugging (Chapter 6) prescribes a methodical loop of hypothesize, predict, experiment, and conclude. Modern LLM-based systems (like AutoSD) can assist by automatically suggesting potential hypotheses and experiments.</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#c2-what-why","title":"C.2 What &amp; Why","text":"<ul> <li>Generate Hypotheses: Formulate a short list of plausible causes based on observed behavior.</li> <li>Test Quickly: Design micro-experiments to validate or refute each hypothesis.</li> <li>Iterate: Use the scientific method to refine your understanding until a promising lead is found.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#c3-detailed-steps","title":"C.3 Detailed Steps","text":"<ol> <li>Observe &amp; Brainstorm: </li> <li>Run the minimal test case and observe program state via debuggers, logs, or tracing tools.</li> <li>Compare failing and passing runs to spot anomalies.</li> <li>Brainstorm potential causes (e.g., \u201can off-by-one error,\u201d \u201cnull pointer exception due to uninitialized variable,\u201d \u201cmisuse of an external API\u201d).</li> <li>Leverage Tool Assistance: </li> <li>If available, use an LLM to generate additional hypotheses by providing it with the minimal test case, code snippet, and failure details.</li> <li>Alternatively, consult static analysis tools to highlight suspicious patterns.</li> <li>Record Hypotheses: </li> <li>Log each hypothesis in a dedicated \u201cdebug log\u201d along with your rationale.</li> <li>Example entry: \u201cHypothesis #1: The array index in loop X is off by one. Expected behavior: iterate from 0 to N\u20131; observed: iterating from 0 to N.\u201d</li> <li>Design &amp; Execute Experiments: </li> <li>For each hypothesis, predict what change would fix the issue.  </li> <li>Temporarily modify the code or state:<ul> <li>Use a debugger to change variable values or step through suspect code.</li> <li>Insert temporary code modifications (e.g., adjust loop bounds, add null checks).</li> <li>Add assertions to verify expected state (WPF Chapter 10).</li> </ul> </li> <li>Run the automated test case to see if the failure is resolved.</li> <li>Conclude &amp; Iterate: </li> <li>If the test passes after your change, the hypothesis is supported.</li> <li>If not, discard or refine the hypothesis and repeat the experiment.</li> <li>Update your debug log with the outcome of each experiment.</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#c4-practical-tips","title":"C.4 Practical Tips","text":"<ul> <li>Emphasize Correctness: A&amp;L\u2019s studies show that the success of debugging hinges on getting the correct hypothesis early.</li> <li>Keep Experiments Small: Test one small change at a time.</li> <li>Interactive LLM Use: If using LLM tools, ask for specific debugger commands or small code snippets and integrate them into your test cycle.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#5-phase-d-systematic-cause-isolation","title":"5. Phase D: Systematic Cause Isolation","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#d1-purpose-background","title":"D.1 Purpose &amp; Background","text":"<p>Even if a hypothesis is validated through small experiments, it might address only a symptom rather than the earliest point of failure in the infection chain. Zeller\u2019s methodology stresses the importance of isolating the defect\u2014the point where a correct state first becomes \u201cinfected.\u201d This phase uses static and dynamic analysis to trace back through code dependencies, ensuring that the root cause is identified.</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#d2-what-why","title":"D.2 What &amp; Why","text":"<ul> <li>Trace the Infection Chain: Identify where the program state first deviated from correctness.</li> <li>Use Advanced Analysis: Employ static slicing, dynamic slicing, and omniscient debugging tools to determine dependencies.</li> <li>Why: Finding the earliest infection ensures you correct the true defect rather than applying a superficial fix.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#d3-detailed-steps","title":"D.3 Detailed Steps","text":"<ol> <li>Static &amp; Dynamic Analysis: </li> <li>Static Slicing: Generate a control and data-dependence graph (WPF Chapter 7) to see all statements that could have affected the failing variable.</li> <li>Dynamic Slicing: Use dynamic slicing tools (WPF Chapter 9) to analyze the execution trace of the failing run, focusing on the actual path taken.</li> <li>Omniscient Debugging: If available, use tools that record full execution history to step backward and pinpoint the first moment of deviation.</li> <li>Delta Debugging on State: </li> <li>Compare the state of the failing run with a passing run.  </li> <li>Use delta debugging techniques on program states (WPF Chapters 11\u201314) to isolate the minimal difference that triggers the failure.</li> <li>Iterative Refinement: </li> <li>Based on the slicing and state comparison, refine your hypotheses and perform targeted experiments (refer back to Phase C).</li> <li>Focus on identifying a specific line or block of code (the defect) where correct inputs produce an infected output.</li> <li>Validate the Defect: </li> <li>Temporarily patch or correct the identified location.  </li> <li>Re-run the minimal test case to confirm that the failure is resolved.</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#d4-practical-tips","title":"D.4 Practical Tips","text":"<ul> <li>Systematic Documentation: Update your debug log with slices, comparisons, and experimental outcomes.</li> <li>Tool Integration: Consider integrating advanced static/dynamic analysis tools to assist with slicing.</li> <li>Be Wary of Multiple Causes: Some bugs may involve multiple interacting factors; isolate the most critical infection point first.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#6-phase-e-fix-verify-learn","title":"6. Phase E: Fix, Verify &amp; Learn","text":""},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#e1-purpose-background","title":"E.1 Purpose &amp; Background","text":"<p>Once the true defect has been identified, it is time to implement a robust fix. Zeller\u2019s later chapters (Chapters 15\u201316) emphasize that the fix should address the root cause and not just mask symptoms. Furthermore, reflecting on the debugging process and documenting the reasoning trace helps prevent future occurrences.</p>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#e2-what-why","title":"E.2 What &amp; Why","text":"<ul> <li>Implement the Fix: Correct the defect at its source.</li> <li>Verify Thoroughly: Ensure that the fix resolves the failure and does not introduce new issues.</li> <li>Document &amp; Learn: Capture the debugging reasoning, update tests, and reflect on process improvements.</li> <li>Why: A robust fix, combined with proper documentation, reduces recurrence and aids team learning, closing the feedback loop.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#e3-detailed-steps","title":"E.3 Detailed Steps","text":"<ol> <li>Implement the Fix: </li> <li>Apply the minimal change needed at the defect location to restore correct behavior.</li> <li>Prefer the simplest, most localized change that corrects the logic.</li> <li>Verify the Fix: </li> <li>Re-run the Minimal Test Case: Confirm the failure is gone.</li> <li>Run Regression Tests: Execute the full test suite to ensure no new issues have been introduced.</li> <li>Peer Review: Have another developer review the fix for accuracy and potential side effects.</li> <li>Document the Outcome: </li> <li>Update the bug report with the fix details, linking the commit(s) to the original issue.</li> <li>Archive the full debugging log and explanation trace (this \u201creasoning trace\u201d is akin to AutoSD\u2019s output), providing insights for future reference.</li> <li>Reflect &amp; Improve: </li> <li>Conduct a root cause analysis: Why was the defect introduced? What process or design gaps allowed it?</li> <li>Enhance Quality Assurance:<ul> <li>Add assertions or invariant checks to catch similar issues earlier.</li> <li>Expand or refine the test suite based on the minimal test case.</li> <li>Consider code refactoring or improved code review practices if systemic patterns are observed.</li> </ul> </li> <li>Update any predictive risk models if used.</li> </ol>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#e4-practical-tips","title":"E.4 Practical Tips","text":"<ul> <li>Consolidate Learning: Encourage team discussions on what was learned from the debugging session.</li> <li>Capture the Reasoning Trace: Ensure that the final explanation\u2014whether generated manually or via an LLM tool\u2014is stored in an accessible repository for onboarding or future troubleshooting.</li> <li>Iterate on Process: Use each debugging experience to continuously refine the workflow.</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#7-references","title":"7. References","text":"<ul> <li>Zeller, A. Why Programs Fail: A Guide to Systematic Debugging. Morgan Kaufmann. (Referenced Chapters: 2\u201316)</li> <li>Alaboudi, A., &amp; LaToza, T. Using Hypotheses as a Debugging Aid. (Key insights on hypothesis formulation and its impact on debugging success)</li> <li>Kang, S., Chen, B., Yoo, S., &amp; Lou, J-G. Explainable Automated Debugging via Large Language Model-Driven Scientific Debugging (AutoSD). (Insights on LLM-driven debugging, interactive hypothesis testing, and explanation generation)</li> </ul>"},{"location":"guides/best_practices/debugging/comprehensive_debugging_guide/#final-thoughts","title":"Final Thoughts","text":"<p>This Version 5 Cohesive Debugging Workflow represents a synthesis of the best practices from established debugging methodologies and modern, automated tools. By following these five phases\u2014Capture &amp; Triage, Reproduce &amp; Simplify, Hypothesis Generation &amp; Verification, Systematic Cause Isolation, and Fix, Verify &amp; Learn\u2014developers gain both the technical rigor and the practical efficiency necessary to address defects thoroughly. Explicit emphasis on hypothesis formulation and testing (as shown by Alaboudi &amp; LaToza) combined with automated assistance (AutoSD) ensures that the root cause is identified accurately and that the solution is both robust and well-documented for continuous learning.</p> <p>This comprehensive document is designed to serve as a technical guide for teams and individuals seeking a methodical approach to debugging\u2014one that is more powerful than the sum of its parts.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/","title":"Progressive Test Coverage System","text":"<p>This document explains the progressive test coverage system implemented in <code>rna_predict/scripts/run_failing_tests.sh</code>, which aligns test coverage goals with the Kaggle competition timeline.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#overview","title":"Overview","text":"<p>The progressive test coverage system:</p> <ol> <li>Automatically adjusts coverage requirements based on the current Kaggle competition phase</li> <li>Implements a gradual day-by-day increase in coverage goals within each phase</li> <li>Sets higher standards for critical modules compared to utility modules</li> <li>Provides clear visibility into upcoming milestones and coverage goals</li> <li>Supports both overall and module-specific coverage testing</li> </ol>"},{"location":"guides/best_practices/testing/progressive_coverage/#kaggle-competition-timeline","title":"Kaggle Competition Timeline","text":"<p>The system is aligned with the Stanford RNA 3D Folding Competition timeline:</p> Phase Date Range Description Exploration Feb 27 \u2013 Mar 27, 2025 Initial exploration and model development Development Mar 28 \u2013 Apr 22, 2025 Core implementation and refinement Optimization Apr 23 \u2013 May 15, 2025 Performance tuning after leaderboard refresh Final Submission May 16 \u2013 May 29, 2025 Final preparations for submission"},{"location":"guides/best_practices/testing/progressive_coverage/#coverage-goals-by-phase","title":"Coverage Goals by Phase","text":"<p>The coverage goals increase progressively through each phase, with a gradual day-by-day increase within each phase:</p> Phase Starting Coverage Target Coverage Critical Modules Standard Modules Utility Modules Exploration 80% 80% 85% 75% 70% Development 80% 85% 90% 85% 75% Optimization 85% 90% 95% 90% 80% Final Submission 90% 95% 98% 95% 85% <p>Rather than jumping immediately to the target coverage when a new phase begins, the system calculates a daily incremental increase based on:</p> <ol> <li>The current day within the phase</li> <li>The total duration of the phase</li> <li>The starting coverage and target coverage for the phase</li> </ol> <p>This ensures a smooth, gradual progression that gives the team time to adapt and improve test coverage at a sustainable pace.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#module-categories","title":"Module Categories","text":"<p>Modules are categorized based on their importance to the RNA prediction pipeline:</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#critical-modules","title":"Critical Modules","text":"<ul> <li><code>rna_predict.pipeline.stageB</code> - Torsion angle prediction (TorsionBERT)</li> <li><code>rna_predict.pipeline.stageD.diffusion</code> - Diffusion-based refinement</li> </ul>"},{"location":"guides/best_practices/testing/progressive_coverage/#standard-modules","title":"Standard Modules","text":"<ul> <li><code>rna_predict.pipeline.stageA</code> - RNA 2D structure prediction</li> <li><code>rna_predict.pipeline.stageC</code> - Forward kinematics</li> <li><code>rna_predict.pipeline.stageD.tensor_fixes</code> - Shape handling utilities</li> </ul>"},{"location":"guides/best_practices/testing/progressive_coverage/#utility-modules","title":"Utility Modules","text":"<ul> <li><code>rna_predict.utils</code> - General utilities</li> <li><code>rna_predict.scripts</code> - Scripts and tools</li> <li><code>rna_predict.dataset</code> - Dataset handling</li> </ul>"},{"location":"guides/best_practices/testing/progressive_coverage/#usage","title":"Usage","text":""},{"location":"guides/best_practices/testing/progressive_coverage/#standard-usage","title":"Standard Usage","text":"<p>Run all tests with the phase-appropriate overall coverage goal:</p> <pre><code>./rna_predict/scripts/run_failing_tests.sh\n</code></pre>"},{"location":"guides/best_practices/testing/progressive_coverage/#module-specific-testing","title":"Module-Specific Testing","text":"<p>Run tests with different coverage thresholds for each module category:</p> <pre><code>./rna_predict/scripts/run_failing_tests.sh --module-specific\n</code></pre>"},{"location":"guides/best_practices/testing/progressive_coverage/#configuration","title":"Configuration","text":"<p>The system uses a JSON configuration file (<code>.coverage_config.json</code>) to store:</p> <ol> <li>Phase dates and coverage goals</li> <li>Module categorization</li> <li>Current coverage status and base coverage</li> </ol> <p>This file is automatically created if it doesn't exist, but can be manually edited to adjust goals or module categorization.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#requirements","title":"Requirements","text":"<ul> <li><code>jq</code> command-line JSON processor (for parsing the configuration)</li> <li><code>bc</code> command-line calculator (for calculating coverage goals)</li> <li>For macOS users: GNU coreutils (<code>brew install coreutils</code>) for accurate date calculations</li> <li><code>pytest</code> with the following plugins:</li> <li><code>pytest-xdist</code> (for parallel testing)</li> <li><code>pytest-cov</code> (for coverage reporting)</li> <li><code>pytest-memray</code> (for memory profiling)</li> <li><code>pytest-timeout</code> (for test timeouts)</li> </ul>"},{"location":"guides/best_practices/testing/progressive_coverage/#benefits-for-kaggle-competition","title":"Benefits for Kaggle Competition","text":"<ol> <li>Focus on Critical Components: Higher coverage for core prediction modules ensures reliability where it matters most</li> <li>Gradual Progression: Realistic coverage goals that increase at a sustainable pace</li> <li>Milestone Awareness: Keeps the team aware of upcoming competition phases and deadlines</li> <li>Adaptability: Can be adjusted if competition dates change or if you need to prioritize different modules</li> </ol>"},{"location":"guides/best_practices/testing/progressive_coverage/#implementation-details","title":"Implementation Details","text":"<p>The script:</p> <ol> <li>Determines the current competition phase based on the date</li> <li>Calculates the appropriate coverage goal based on days into the phase</li> <li>Applies phase transition smoothing to prevent abrupt jumps between phases</li> <li>Caps daily coverage increases to ensure sustainable progress</li> <li>Shows days remaining until the next milestone</li> <li>Runs tests with the appropriate coverage threshold</li> <li>Generates detailed coverage reports</li> <li>Updates the stored coverage goal and last run date for the next run</li> </ol> <p>For module-specific testing, it applies different thresholds to different module categories based on their criticality to the competition goals.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/best_practices/testing/progressive_coverage/#phase-transition-smoothing","title":"Phase Transition Smoothing","text":"<p>When transitioning between phases, the system checks if: 1. We're within the first 3 days of a new phase 2. The current coverage is significantly below the target of the previous phase</p> <p>If both conditions are met, it adjusts the starting point to be closer to the actual coverage, preventing unrealistic jumps in requirements.</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#maximum-daily-increase-cap","title":"Maximum Daily Increase Cap","text":"<p>To prevent large jumps in coverage requirements after periods of inactivity: 1. The system tracks the last time tests were run 2. It caps the maximum daily increase to 0.5% per day 3. For longer gaps between runs, it allows a proportional increase (e.g., 1.5% after 3 days)</p>"},{"location":"guides/best_practices/testing/progressive_coverage/#configuration-backup-and-recovery","title":"Configuration Backup and Recovery","text":"<p>To prevent data loss and ensure continuity: 1. The system creates a backup of the configuration file before each run 2. It validates the JSON structure and restores from backup if corrupted 3. It falls back to creating a new configuration if no valid backup exists</p>"},{"location":"guides/best_practices/testing/test_coverage_strategy/","title":"Strategy: Improving Test Coverage using <code>show_coverage.py</code>","text":"<p>This document outlines a systematic approach to using the <code>rna_predict/scripts/show_coverage.py</code> script for assessing and enhancing test coverage within the <code>RNA_PREDICT</code> codebase.</p> <p>Prerequisites:</p> <ul> <li>Ensure all test dependencies are installed: <code>pip install -r requirements-test.txt</code></li> <li>Note: The script uses <code>pytest-memprof</code> flags. Verify that the <code>pytest-memprof</code> plugin is installed. If not, install it (<code>pip install pytest-memprof</code>).</li> </ul> <p>1. Running the <code>show_coverage.py</code> Script</p> <p>The script serves as a wrapper around <code>pytest --cov</code> and <code>coverage report</code>. Execute it from the project root directory (<code>/Users/tomriddle1/RNA_PREDICT</code>).</p> <ul> <li> <p>Baseline / Full Report: Get the overall coverage picture.     <pre><code>python rna_predict/scripts/show_coverage.py\n</code></pre>     This runs all tests, combines coverage data, and displays the standard <code>coverage report -m</code> output, including missing line numbers and branch coverage details, plus a memory usage summary.</p> </li> <li> <p>Filtered Report: Focus on specific modules or sub-packages using a path pattern.     <pre><code># Example: Show coverage only for files within the stageA pipeline\npython rna_predict/scripts/show_coverage.py --filter rna_predict/pipeline/stageA\n\n# Example: Show coverage for utility functions\npython rna_predict/scripts/show_coverage.py --filter rna_predict/utils\n</code></pre>     This helps narrow down investigation to specific areas.</p> </li> <li> <p>Least Covered Report: Quickly identify the single file with the lowest coverage percentage (ignoring files with 0 statements) and see its untested lines directly.     <pre><code>python rna_predict/scripts/show_coverage.py --least-covered\n</code></pre>     This is useful for pinpointing the most critical area needing immediate attention.</p> </li> </ul> <p>2. Interpreting Script Outputs</p> <p>Understanding the reports is key to taking action.</p> <ul> <li> <p>Full/Filtered Report (<code>coverage report -m</code> format):</p> <ul> <li><code>Name</code>: The path to the source file.</li> <li><code>Stmts</code>: Total number of executable statements in the file.</li> <li><code>Miss</code>: Number of statements not executed by any test.</li> <li><code>Branch</code>: Total number of possible execution branches (e.g., each side of an <code>if</code>/<code>else</code>).</li> <li><code>BrPart</code>: Number of branch paths not taken by any test (Branch Partial).</li> <li><code>Cover</code>: Overall statement coverage percentage (<code>100 * (Stmts - Miss) / Stmts</code>). Branch coverage is implicitly included via <code>BrPart</code>. A line is only fully covered if all its branches are taken.</li> <li><code>Missing</code>: Specific line numbers or ranges of statements not executed. Also shows partial branches (e.g., <code>35-&gt;37</code> means the branch from line 35 to line 37 was never taken).</li> </ul> </li> <li> <p><code>--least-covered</code> Report:</p> <ul> <li>Displays the standard report line for the single file with the lowest <code>Cover %</code>.</li> <li>Prints the <code>File with lowest coverage</code>, <code>Coverage %</code>, and raw <code>Missing lines</code> string.</li> <li>Crucially, it then reads the source file and prints the actual code for each <code>Untested line</code>, prefixed by its line number. This provides immediate context for writing new tests.</li> </ul> </li> <li> <p>Memory Usage Summary:</p> <ul> <li>Lists the top 10 tests consuming the most memory (as reported by <code>pytest-memprof</code>). Useful for identifying potentially inefficient tests, but separate from code coverage analysis.</li> </ul> </li> </ul> <p>3. Systematic Process for Identifying Coverage Gaps</p> <p>Use the script outputs strategically:</p> <ol> <li>Establish Baseline: Run the full report (<code>python rna_predict/scripts/show_coverage.py</code>) to understand the current overall coverage percentage and identify modules/files with low scores (e.g., below a target like 80% or 90%).</li> <li>Prioritize: Focus efforts based on:<ul> <li>Lowest Coverage: Files identified by <code>--least-covered</code> or those at the bottom of the full report.</li> <li>Criticality: Core logic, complex algorithms (e.g., pipeline stages, key utils), or frequently modified modules are higher priority than simple scripts or peripheral code.</li> <li>High <code>Miss</code> / <code>BrPart</code> Count: Files with many missed statements or branches indicate significant untested logic.</li> </ul> </li> <li>Investigate:<ul> <li>Use <code>--filter</code> to drill down into specific low-coverage modules identified in the baseline report.</li> <li>Use <code>--least-covered</code> repeatedly. After fixing the lowest, rerun to find the next lowest.</li> </ul> </li> <li>Analyze Missing Lines: For prioritized files, examine the <code>Missing</code> column (full/filtered report) or the <code>Untested lines</code> output (<code>--least-covered</code>) to understand exactly what code paths are not being tested.</li> </ol> <p>4. Translating Missing Lines to Targeted Tests</p> <p>The <code>Missing</code> lines/branches directly guide test creation:</p> <ul> <li>Missing Statement Line (e.g., <code>Missing: 42</code>): The code on line 42 was never executed. Write a test case whose execution path includes line 42.</li> <li>Missing Range (e.g., <code>Missing: 50-55</code>): Lines 50 through 55 were not executed. This often happens inside conditional blocks (<code>if</code>, <code>elif</code>, <code>else</code>) or loops that weren't entered. Write tests that satisfy the conditions to enter that block/loop.</li> <li>Missing Line in <code>except</code> Block (e.g., <code>Missing: 65</code> where line 65 is <code>print(\"Error\")</code> inside <code>except ValueError:</code>): The <code>ValueError</code> was never triggered. Write a test that specifically causes a <code>ValueError</code> in the <code>try</code> block.</li> <li>Missing Branch (e.g., <code>Missing: 30-&gt;32</code>): The condition on line 30 evaluated such that execution did not proceed directly to line 32 (it likely went to an <code>else</code> or jumped past the block). Write a test where the condition on line 30 does lead to line 32. Conversely, if <code>Missing: 30-&gt;exit</code> appears, it means the condition on line 30 always evaluated the same way, never taking the other path.</li> <li><code>--least-covered</code> Untested Lines Output: This makes it even easier. Look at the printed code snippet for line <code>N</code>. Determine what input or state is needed for that line to be executed, and create a test case for it.</li> </ul> <p>Example: If <code>--least-covered</code> shows: <pre><code>File with lowest coverage: rna_predict/pipeline/stageC/mp_nerf/utils.py\nCoverage: 65%\nMissing lines: 88-92, 105-&gt;107\n\nUntested lines:\n  88 |     if not data:\n  89 |         logger.warning(\"Received empty data.\")\n  90 |         return None\n  91 |     # ... more processing\n  92 |     result = process(data)\n ...\n 105 |     if config.get(\"use_fallback\"):\n ...\n</code></pre> This tells you: *   Need a test for <code>utils.py</code> where the input <code>data</code> is empty or <code>None</code> (to cover lines 88-90). *   Need a test where <code>data</code> is not empty (to cover line 92 and potentially subsequent lines). *   Need a test where <code>config.get(\"use_fallback\")</code> is <code>True</code> (to cover the branch into line 107, assuming line 105 is the <code>if</code> statement).</p> <p>5. Iterative Workflow for Coverage Improvement</p> <p>Adopt a cyclical approach:</p> <pre><code>graph TD\n    A[Baseline: Run `show_coverage.py`] --&gt; B(Identify &amp; Prioritize: Low coverage files/modules);\n    B --&gt; C{Focus: Use `--filter` / `--least-covered`};\n    C --&gt; D[Analyze Missing Lines/Branches];\n    D --&gt; E[Implement: Write Targeted Tests];\n    E --&gt; F[Verify: Rerun `show_coverage.py`];\n    F --&gt; G{Goal Met?};\n    G -- No --&gt; B;\n    G -- Yes --&gt; H(Set New Goal / Maintain);\n    H --&gt; A;\n</code></pre> <ol> <li>Baseline: Run the script, note the overall percentage and key low points.</li> <li>Target: Set a specific, achievable goal (e.g., \"Increase overall coverage by 5%\", \"Ensure <code>rna_predict/utils/</code> reaches 90%\", \"Eliminate missing lines in <code>least-covered</code> file\").</li> <li>Implement: Write the necessary tests based on the analysis (Step 4). Focus on the prioritized areas.</li> <li>Verify: Rerun the script (<code>show_coverage.py</code> with appropriate flags) to confirm the new tests cover the intended lines/branches and that the coverage percentage has increased. Check for any unexpected decreases elsewhere (regressions).</li> <li>Repeat: Continue the cycle, iteratively improving coverage.</li> <li>Maintain: Once a satisfactory level is reached, monitor coverage regularly. Consider integrating a coverage check into CI/CD pipelines, potentially by setting a <code>fail_under</code> threshold in <code>.coveragerc</code> in the future.</li> </ol>"},{"location":"guides/best_practices/testing/test_generation_prompt/","title":"Test Generation","text":"<p>Prompt: Synthesized Test File Generation for High Coverage</p> <p>\u2e3b</p> <p>Mission Craft a comprehensive, maintainable Python test file covering at least 80% of the target module\u2019s code. Use the specified generated_tests file as an instructional guide, without copying it verbatim. Integrate standard unit tests, property-based tests, mocks (if needed), and round-trip checks (if relevant), aiming to deliver clear, PEP 8-compliant code.</p> <p>\u2e3b</p> <p>Context You have a folder, /Users/tomriddle1/RNA_PREDICT/generated_tests/, containing multiple automatically generated instruction files. Each file details how one might build a suite of Python tests for a corresponding code module. However, these instruction files may be repetitive, incomplete, or contain extraneous content. Your job is to do the following for the current file being processed:     1.  Parse &amp; Interpret the Instructions     \u2022   Identify the primary module or functionality these instructions reference.     \u2022   Extract important details: function/class coverage goals, potential edge cases, or example test patterns.     \u2022   Avoid copying large blocks of text verbatim. Instead, translate the instructions into actionable test code concepts.     2.  Design the Final Test File     \u2022   Write a single Python test module (e.g., test_.py or similar).     \u2022   Use unittest or pytest (choose whichever best suits the instructions or your style).     \u2022   Include a logical structure (e.g., classes or fixtures) that organizes tests by functionality or feature area.     3.  Key Testing Elements a. Behavioral (Unit) Tests     \u2022   Thoroughly cover the public methods and classes of the target module: normal cases, boundary conditions, and anticipated error paths.     \u2022   Adhere to best practices: clear naming, setup/teardown or fixtures, PEP 8 compliance. b. Property-Based Tests     \u2022   When the instructions mention fuzzing, or automatically generated inputs, incorporate hypothesis strategies to cover a wide input range.     \u2022   Deploy relevant hypothesis decorators and strategies (st.integers, st.floats, st.builds, etc.) to ensure robust coverage.     \u2022   Add at least one or two property-based tests that systematically probe edge cases. c. Round-Trip &amp; Integration Checks     \u2022   If the instructions reference transformations, parsing/serialization, or encode/decode logic, include a round-trip test verifying that data remains consistent.     \u2022   For multi-step processes, consider an integration-style test to confirm components work together as expected. d. Mocking &amp; Isolation     \u2022   For dependencies on external services or nondeterministic behavior (randomness, time, I/O), use unittest.mock (or equivalent) to isolate the logic under test.     \u2022   Confirm that tests remain fast, self-contained, and repeatable. e. Coverage Emphasis     \u2022   Seek to exercise every relevant branch or path. Check that each public function/class is included in some test scenario.     \u2022   Write additional test cases for unusual inputs or corner cases to lift coverage over 80%.     4.  Deliver a Polished Python Test File     \u2022   The final output is one complete .py test file, which can be run directly (e.g., via python -m unittest or pytest).     \u2022   Use concise, readable code that a new contributor could quickly follow or a seasoned developer would trust.     \u2022   Avoid duplicating large text from the generated instructions; rework them into a clear, consolidated test suite. <p>\u2e3b</p> <p>Constraints &amp; Style     \u2022   Do not merely replicate the entire instructions from the generated tests. Instead, interpret them to produce a concise but comprehensive test file.     \u2022   Maintain standard Python coding style (PEP 8) and typical test naming conventions.     \u2022   Include docstrings or inline comments where beneficial for clarity, but keep them focused.     \u2022   Ensure the final test code stands on its own\u2014no extra harness or extraneous scaffolding.</p> <p>\u2e3b</p> <p>Your Output     1.  A well-structured Python code block representing the final test file.     2.  Immediately runnable under unittest or pytest.     3.  Incorporates all test categories (behavioral, property-based, round-trip, mocking as needed), fulfilling the coverage objectives.     4.  Does not quote or copy instruction text verbatim, but instead uses it as reference to produce well-crafted tests.</p> <p>\u2e3b</p> <p>Example Prompt Usage When you invoke this prompt for a specific file\u2014say test_wrapped_common.md\u2014you ask:</p> <p>\u201cRead the instructions in test_wrapped_common.md. Then produce a single Python test module that covers the code module it references, achieving &gt;=80% coverage, and integrating best practices, property-based tests, and potential mocking, following the guidelines above.\u201d</p> <p>The response should be a valid Python file with high-quality tests, not a discussion or commentary.</p> <p>\u2e3b</p> <p>End of Refined Prompt</p>"},{"location":"guides/getting_started/","title":"RNA_PREDICT Documentation","text":"<p>Welcome to the RNA_PREDICT documentation homepage! This comprehensive guide provides detailed navigation and insights into all documentation files within the <code>docs/</code> directory. Organized clearly, this resource aims to quickly familiarize users and collaborators with the documentation, ensuring efficient and targeted exploration of topics essential for RNA 3D prediction workflows.</p>"},{"location":"guides/getting_started/#quickstart-project-commands","title":"\ud83d\udccc Quickstart Project Commands","text":"<p>Easily manage your MkDocs documentation using these essential commands:</p> <ul> <li><code>mkdocs new [dir-name]</code>: Initialize a new MkDocs project.</li> <li><code>mkdocs serve</code>: Launch a local live-reloading documentation server.</li> <li><code>mkdocs build</code>: Generate static HTML documentation for deployment.</li> <li><code>mkdocs -h</code>: Display help information for MkDocs.</li> </ul>"},{"location":"guides/getting_started/#comprehensive-documentation-overview","title":"\ud83d\udcc2 Comprehensive Documentation Overview","text":""},{"location":"guides/getting_started/#rna-prediction-pipeline","title":"\ud83e\uddec RNA Prediction Pipeline","text":"<ul> <li> <p><code>AlphaFold3_progress.md</code></p> <ul> <li> <p>Tracks progress in adapting AlphaFold 3 methodologies specifically for RNA.</p> </li> <li> <p>Highlights implemented components, remaining modules, and clearly defined next steps.</p> </li> <li> <p>Recommended for developers and researchers working on AlphaFold-inspired RNA prediction.</p> </li> </ul> </li> <li> <p><code>Multi_Stage_Implementation_Plan.md</code></p> <ul> <li>Details the technical architecture and phased rollout plan of the RNA 3D prediction pipeline.</li> <li>Ideal for technical architects and project managers.</li> </ul> </li> <li> <p>Stage-specific Documentation:</p> <ul> <li> <p>StageA_RFold.md: Covers RNA folding stage (Stage A) using the RFold approach.</p> </li> <li> <p>Stage_B.md: Describes the intermediate torsion-angle generation from 2D structures.</p> </li> <li> <p>Stage_C.md: Explains final generation of Cartesian coordinates from torsion angles.</p> </li> <li> <p>Useful for clearly understanding the modular responsibilities and dependencies within the pipeline.</p> </li> </ul> </li> <li> <p><code>core_framework.md</code></p> <ul> <li>Outlines the pipeline\u2019s three-stage process clearly:<ol> <li>Sequence \u2192 2D structure</li> <li>2D structure \u2192 Torsion angles</li> <li>Torsion angles \u2192 3D coordinates</li> </ol> </li> <li>Recommended for new team members and collaborators for a clear pipeline overview.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#reference-and-research-materials","title":"\ud83d\udcd1 Reference and Research Materials","text":""},{"location":"guides/getting_started/#torsion-angles-geometric-calculations","title":"\ud83d\udcd0 Torsion Angles &amp; Geometric Calculations","text":"<ul> <li> <p><code>torsion_angles.md</code></p> <ul> <li>Comprehensive overview of RNA torsion angles, calculation methodologies, software tools (e.g., PyMOL, Chimera), and theoretical considerations.</li> <li>Essential for researchers and developers working with RNA geometry.</li> </ul> </li> <li> <p><code>torsion_angle_Latent_Manifold_Representation.md</code></p> <ul> <li>Proposes innovative methods using lower-dimensional latent manifolds (autoencoders, VAEs) for RNA conformation representation.</li> <li>Targeted at advanced researchers exploring next-gen dimensionality reduction techniques.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#external-literature-references","title":"\ud83d\udcd6 External Literature &amp; References","text":"<ul> <li> <p><code>RNA_papers.md</code></p> <ul> <li>Compares different curated reference lists, highlighting critical RNA 3D structure prediction papers relevant to competitions and practical applications.</li> </ul> </li> <li> <p><code>2d_structure_prediction_papers.md</code></p> <ul> <li>Specialized compilation of references specifically focusing on RNA secondary (2D) structure prediction methods.</li> </ul> </li> <li> <p><code>RNA_STRUCTURE_PREDICTION_Categorized.csv</code></p> <ul> <li>Structured, categorized references to facilitate efficient literature review and method benchmarking.</li> </ul> </li> <li> <p><code>ConnectedPapers-for-RNA-secondary-structure-prediction-using-an-ensemble-of-two_20dimensional-deep-neural-networks-and-transfer-learning.txt</code></p> <ul> <li>Provides curated insights from Connected Papers about RNA secondary prediction using advanced ensemble and transfer-learning methods.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#isostericity-and-sequence-conservation","title":"\ud83d\udd04 Isostericity and Sequence Conservation","text":"<ul> <li><code>RNA_isostericity.md</code><ul> <li>Details principles of RNA isostericity, substitution algorithms, and their significance in structural modeling and preservation.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#advanced-modeling-and-techniques","title":"\ud83d\ude80 Advanced Modeling and Techniques","text":""},{"location":"guides/getting_started/#diffusion-and-state-space-modeling","title":"\ud83c\udf0a Diffusion and State-Space Modeling","text":"<ul> <li> <p><code>s4_diffusion.md</code></p> <ul> <li>Introduces Liquid-S4 state-space models and their empirical performance advantages in long-sequence modeling.</li> <li>Offers integration strategies within AlphaFold-inspired pipelines.</li> </ul> </li> <li> <p><code>test_time_scaling.md</code></p> <ul> <li>Discusses practical strategies to optimize inference speed and quality using flexible step-count adjustments in diffusion-based models.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#alphafold-adaptation","title":"\ud83e\udde0 AlphaFold Adaptation","text":"<ul> <li><code>AF3_paper.md</code></li> <li>Summarizes AlphaFold 3 foundational concepts, pipeline structure, key innovations, and applicability to RNA prediction.</li> </ul>"},{"location":"guides/getting_started/#competition-context-and-practical-applications","title":"\ud83c\udfaf Competition Context and Practical Applications","text":"<ul> <li><code>kaggle_competition.md</code><ul> <li>Comprehensive overview of the Stanford RNA 3D Folding Kaggle challenge.</li> <li>Explains datasets, submission criteria, scoring metrics, and critical FAQs.</li> <li>Indispensable resource for competitors and teams preparing for Kaggle submissions.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/#interconnections-and-recommended-usage","title":"\ud83c\udf10 Interconnections and Recommended Usage","text":""},{"location":"guides/getting_started/#pipeline-integration","title":"\ud83d\udd17 Pipeline Integration","text":"<ul> <li>Core pipeline documents (<code>core_framework.md</code>, stage-specific files) articulate clear progression paths from theoretical concepts (<code>torsion_angles.md</code>) to practical competitive applications (<code>kaggle_competition.md</code>).</li> </ul>"},{"location":"guides/getting_started/#cutting-edge-explorations","title":"\ud83c\udf1f Cutting-edge Explorations","text":"<ul> <li>Advanced documentation (<code>s4_diffusion.md</code>, <code>torsion_angle_Latent_Manifold_Representation.md</code>) outlines future enhancements and novel research trajectories, ensuring the project remains aligned with frontier research.</li> </ul>"},{"location":"guides/getting_started/#comprehensive-reference-resources","title":"\ud83d\udcd8 Comprehensive Reference Resources","text":"<ul> <li>External literature documents form a robust base for validating methodologies, guiding strategic implementation, and establishing scientific rigor in RNA structure prediction.</li> </ul>"},{"location":"guides/getting_started/#recommended-exploration-pathways","title":"\ud83d\udea9 Recommended Exploration Pathways","text":"<ul> <li> <p>New Users &amp; Team Members:</p> <ul> <li>Begin with the pipeline overview (<code>core_framework.md</code>), progress to geometric foundational knowledge (<code>torsion_angles.md</code>), and conclude with practical competition context (<code>kaggle_competition.md</code>).</li> </ul> </li> <li> <p>Advanced Practitioners &amp; Researchers:</p> <ul> <li>Explore cutting-edge representation and modeling methods in <code>torsion_angle_Latent_Manifold_Representation.md</code> and state-space diffusion strategies outlined in <code>s4_diffusion.md</code>.</li> </ul> </li> <li> <p>Research-oriented Users:</p> <ul> <li>Leverage curated external literature resources (<code>RNA_papers.md</code>, <code>2d_structure_prediction_papers.md</code>) for academic rigor, benchmarking, and method validation.</li> </ul> </li> </ul> <p>This enhanced documentation overview aims to optimize user engagement, foster efficient navigation, and encourage collaborative contribution to the RNA_PREDICT project. Happy exploring and contributing!</p>"},{"location":"guides/getting_started/Orchestrator_Task_Template_ROO/","title":"Orchestrator Task Template","text":"<p>1. Immediate Priority &amp; Scope: The immediate priority is to [Clearly state the primary goal for this interaction, e.g., \"address all actionable CodeRabbit feedback provided below\", \"complete Task [Task_ID] from the task management system\", \"implement the basic structure for the [Module/Class Name] module\"]. All subsequent actions in this current phase must focus solely on achieving this priority.</p> <p>2. Contextual Review: To begin, thoroughly review project documentation (paying close attention to [Specify relevant documents, e.g., <code>docs/pseudocode.md</code>, <code>scripts/prd.txt</code>, <code>README.md</code>]) and meticulously examine relevant code files (such as [Specify relevant files/dirs, e.g., <code>src/[module_name].py</code>, <code>tests/</code>, <code>run-task-master.bat</code>]) specifically to gather the context needed for the stated priority. Take brief notes for reference.</p> <p>3. Task System Interaction (Optional but Recommended): [Optional: If using TaskMaster and the Task ID is known] Access the project task management system by running <code>[Command to view specific task, e.g., .\\run-task-master.bat show [Task_ID]]</code> to confirm the detailed requirements and status of the current priority task: [Task_ID / Name].</p> <p>4. Scoped Decomposition: Based on the review and the requirements of the immediate priority, decompose only the work needed to achieve this priority into granular, manageable sub-tasks. Establish a systematic and logical workflow for these sub-tasks only.</p> <p>5. Delegation: For each sub-task identified, utilize the most appropriate specialized mode (e.g., 'code', 'architect') via the <code>new_task</code> tool. Provide clear, specific instructions and well-defined acceptance criteria for each delegated task. [Optional: If providing context below, add: \"Ensure the delegated tasks explicitly reference and address the relevant items from the context provided below.\"]</p> <p>6. Autonomy &amp; Constraints: Operate with autonomy within the defined scope of completing [Restate the immediate priority/Task ID]. User approval for decisions within this scope is pre-granted. Do not initiate decomposition or delegation for other features, tasks, or general improvements beyond this immediate priority until explicitly instructed.</p>"},{"location":"guides/getting_started/Orchestrator_Task_Template_ROO/#7-supporting-context-if-applicable","title":"7. Supporting Context (If Applicable):","text":"<p>[Context specific to the immediate priority, e.g., CodeRabbit Comments, specific requirements, error logs]</p> <p>[Paste Context Here]</p> <p>====</p>"},{"location":"guides/getting_started/windows_compatibility_plan/","title":"Technical Implementation Plan: Windows Compatibility for RNA_PREDICT Scripts","text":"<p>Objective: Modify the <code>rna_predict</code> project codebase to allow <code>rna_predict/scripts/run_all_pipeline.py</code> and <code>rna_predict/scripts/run_failing_tests.py</code> to execute successfully on a Windows operating system.</p> <p>Overall Approach: Identify and refactor platform-specific code, primarily focusing on file path manipulation, while verifying external dependencies. The goal is cross-platform compatibility rather than Windows-only scripts.</p>"},{"location":"guides/getting_started/windows_compatibility_plan/#1-task-breakdown","title":"1. Task Breakdown","text":"<ol> <li>Modify path string definitions in <code>rna_predict/pipeline/stageA/run_stageA.py</code> to use <code>os.path.join</code>.</li> <li>Modify path string construction in <code>rna_predict/scripts/run_failing_tests.py</code> to use <code>os.path.join</code>.</li> <li>Document external dependency requirements for Windows (Java).</li> <li>Verify core library installation instructions (PyTorch, Pandas, NumPy, potentially Transformers).</li> <li>Test execution of both target scripts (<code>run_all_pipeline.py</code>, <code>run_failing_tests.py</code>) on Windows.</li> </ol>"},{"location":"guides/getting_started/windows_compatibility_plan/#2-file-modifications","title":"2. File Modifications","text":"<ul> <li><code>rna_predict/pipeline/stageA/run_stageA.py</code></li> <li><code>rna_predict/scripts/run_failing_tests.py</code></li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#3-code-section-adjustments-logic","title":"3. Code Section Adjustments &amp; Logic","text":""},{"location":"guides/getting_started/windows_compatibility_plan/#file-rna_predictpipelinestagearun_stageapy","title":"File: <code>rna_predict/pipeline/stageA/run_stageA.py</code>","text":"<ul> <li>Location: Lines 120, 122, 124, 144, 145 (and potentially others defining relative paths).</li> <li>Change: Replace hardcoded forward slashes (<code>/</code>) in path strings with <code>os.path.join()</code>.</li> <li>Rationale: Ensures paths are constructed using the correct OS-specific separator (<code>\\</code> on Windows, <code>/</code> on Unix-like systems), improving robustness, especially when paths are passed to external processes or libraries.</li> <li>Example (Line 120): <pre><code># Before\ncheckpoint_zip = \"RFold/checkpoints.zip\"\n# After\ncheckpoint_zip = os.path.join(\"RFold\", \"checkpoints.zip\")\n</code></pre></li> <li>Example (Line 144): <pre><code># Before\nvarna_jar_path = \"RFold/VARNAv3-93.jar\"\n# After\nvarna_jar_path = os.path.join(\"RFold\", \"VARNAv3-93.jar\")\n</code></pre></li> <li>Impact: Minimal impact expected. Ensures correct path resolution on Windows. Requires <code>import os</code>.</li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#file-rna_predictscriptsrun_failing_testspy","title":"File: <code>rna_predict/scripts/run_failing_tests.py</code>","text":"<ul> <li>Location: Line 397 (<code>module_path = module.replace(\".\", \"/\")</code>)</li> <li>Change: Replace the string replacement with <code>os.path.join</code>.</li> <li>Rationale: Correctly converts Python module dot notation (e.g., <code>rna_predict.utils</code>) into an OS-specific file path (e.g., <code>rna_predict\\utils</code> on Windows).</li> <li>Example: <pre><code># Before\nmodule_path = module.replace(\".\", \"/\")\n# After (assuming 'os' is imported)\nmodule_path = os.path.join(*module.split('.'))\n</code></pre></li> <li> <p>Impact: Corrects potential path issues when specifying coverage sources (<code>--cov={module}</code>) or finding related test files on Windows.</p> </li> <li> <p>Location: Line 421 (f-string for <code>--cov-report</code> argument)</p> </li> <li>Change: Construct the HTML report path using <code>os.path.join</code>.</li> <li>Rationale: Ensures the path provided to pytest for the HTML coverage report uses the correct OS separator.</li> <li>Example: <pre><code># Before\nf\"--cov-report=html:coverage/{os.path.basename(module_path)}\",\n# After (assuming 'os' is imported)\nf\"--cov-report=html:{os.path.join('coverage', os.path.basename(module_path))}\",\n</code></pre></li> <li>Impact: Ensures pytest-cov can correctly create the HTML report directory structure on Windows.</li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#4-new-functionsclasses","title":"4. New Functions/Classes","text":"<ul> <li>No new functions or classes are anticipated for these specific changes.</li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#5-dependencies-configuration-updates","title":"5. Dependencies &amp; Configuration Updates","text":"<ul> <li>External Software:<ul> <li>Java: Required only for the optional VARNA visualization feature in <code>run_stageA.py</code>. If visualization is needed on Windows, a JRE must be installed and the <code>java</code> executable must be in the system's PATH environment variable.</li> </ul> </li> <li>Python Libraries:<ul> <li>Ensure <code>requirements.txt</code> (and potentially <code>requirements-test.txt</code>) list versions of core libraries (PyTorch, NumPy, Pandas, pytest, pytest-cov, potentially transformers) that are known to be compatible with Windows and the target Python version. No changes to the requirements files are planned unless testing reveals an incompatible library version. Installation should use standard <code>pip install -r requirements.txt</code>.</li> </ul> </li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#6-data-structure-interface-changes","title":"6. Data Structure / Interface Changes","text":"<ul> <li>No changes to data structures or function/class interfaces are anticipated.</li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#7-potential-side-effects","title":"7. Potential Side Effects","text":"<ul> <li>The changes are minor refactorings aimed at improving cross-platform compatibility. The primary risk is introducing a typo when modifying path construction; careful checking is required.</li> <li>The dependency on Java for VARNA remains; if Java is not configured correctly on Windows, the visualization step in <code>run_stageA.py</code> will fail (gracefully, based on the code's checks), but the rest of the script should proceed.</li> </ul>"},{"location":"guides/getting_started/windows_compatibility_plan/#8-success-criterion","title":"8. Success Criterion","text":"<ul> <li>Successful, error-free execution of <code>python rna_predict/scripts/run_all_pipeline.py</code> on Windows.</li> <li>Successful, error-free execution of <code>python rna_predict/scripts/run_failing_tests.py</code> (both with and without <code>--module-specific</code>) on Windows, including generation of coverage reports.</li> </ul>"},{"location":"pipeline/Energy_Minimization_%26_Molecular_Dynamics_%28MD%29_for_RNA_Structure_Refinement/","title":"Energy & MD","text":"<p>Energy Minimization &amp; Molecular Dynamics (MD) for RNA Structure Refinement</p> <p>Since Stage C (torsion \u2192 3D) places RNA atoms in Cartesian space, small errors in predicted torsion angles can accumulate and cause:</p> <p>Bond strains (small deviations in bond lengths/angles).</p> <p>Clashes (atoms too close together).</p> <p>Deviations from known conformations (e.g., incorrect sugar pucker).</p> <p>A small, local refinement using energy minimization or a short MD run can help correct these without over-distorting the structure.</p> <p>1) Energy Minimization: Quick Local Refinement</p> <p>What It Does</p> <p>Energy minimization adjusts atomic positions without drastic motion.</p> <p>It finds the nearest local energy minimum in the molecular energy landscape.</p> <p>Works by iteratively moving atoms to reduce steric clashes and optimize bond lengths/angles.</p> <p>How It Works</p> <ol> <li> <p>You provide an initial 3D structure (output of your forward kinematics step).</p> </li> <li> <p>A force field computes forces on atoms:</p> </li> </ol> <p>Bonds should be near ideal lengths (P\u2013O, O\u2013C, C\u2013C, etc.).</p> <p>Bond angles should be near known values.</p> <p>Torsions should match expected low-energy states.</p> <ol> <li> <p>A minimizer (gradient descent-like) moves atoms slightly to reduce strain.</p> </li> <li> <p>The final output has fewer distortions but still retains the original overall shape.</p> </li> </ol> <p>Common Force Fields for RNA</p> <p>AMBER (OL3, bsc1): Well-validated for nucleic acids.</p> <p>CHARMM (C36): Another strong choice.</p> <p>OPLS-AA: Used in some RNA modeling but less common.</p> <p>Implementation Options</p> <p>GROMACS: (gmx energymin)</p> <p>AMBER: (sander -minimize)</p> <p>OpenMM: Python-based (good for scripting refinements).</p> <p>Run Time</p> <p>Fast: Just 1,000\u201310,000 steps of minimization (a few seconds to minutes on a CPU/GPU).</p> <p>Typically keeps the backbone largely intact but fixes bad sterics.</p> <p>2) Short Molecular Dynamics (MD) Simulation: Adds Small Motions</p> <p>What It Does</p> <p>Instead of just finding a minimum, MD lets the structure move under thermal energy.</p> <p>This allows small adjustments to base-pairing, sugar puckers, and stacking.</p> <p>Key Steps</p> <ol> <li> <p>Minimize energy first (so the structure is not highly strained).</p> </li> <li> <p>Run a short MD simulation (e.g., 10\u2013100 picoseconds) at low temperature (~100 K\u2013300 K).</p> </li> <li> <p>Apply weak restraints on known positions (so the RNA does not unfold completely).</p> </li> <li> <p>Let the structure \"relax\" slightly under real physical forces.</p> </li> </ol> <p>What It Helps With</p> <p>\u2705 Fixes non-physical bond lengths/angles. \u2705 Lets sugar rings adopt a valid pucker. \u2705 Slightly adjusts base stacking &amp; orientation. \u2705 Refines backbone conformations to be more native-like.</p> <p>What It Does Not Do</p> <p>\u274c Does not \u201cpredict\u201d new folds (it assumes the structure is already close). \u274c Does not fix large-scale misfolds. \u274c Does not drastically change helices (just smooths them).</p> <p>Software for MD</p> <p>GROMACS (gmx mdrun)</p> <p>AMBER (pmemd / sander)</p> <p>OpenMM (Python interface, easier for deep-learning-based refinements).</p> <p>Example: Short 100 ps MD Run</p>"},{"location":"pipeline/Energy_Minimization_%26_Molecular_Dynamics_%28MD%29_for_RNA_Structure_Refinement/#example-in-gromacs","title":"Example in GROMACS:","text":"<p>gmx grompp -f short_md.mdp -c minimized.pdb -o short_md.tpr gmx mdrun -deffnm short_md -nsteps 50000  # ~100 ps simulation</p> <p>This brief simulation lets the RNA relax while maintaining known constraints.</p> <p>3) Hybrid: Refinement With Restraints (Best for RNA Pipeline)</p> <p>A good compromise is:</p> <ol> <li> <p>Run energy minimization to remove steric clashes.</p> </li> <li> <p>Run a short MD, but with weak restraints:</p> </li> </ol> <p>Keep base-pairs restrained to avoid unfolding.</p> <p>Keep some torsion angles slightly restrained (if needed).</p> <p>Let sugar puckers &amp; sterics relax naturally.</p> <p>Restraint Options</p> <p>Positional restraints: Keep certain atoms near their input positions.</p> <p>Distance restraints: Keep base-pairs at realistic hydrogen-bonding distances.</p> <p>Torsion restraints: Keep backbone angles in valid ranges.</p> <p>4) When Should You Use Energy Minimization or MD?</p> <p>5) What\u2019s the Best Choice for Your RNA Stage C?</p> <p>\u2705 Always do at least energy minimization after forward kinematics. \u2705 If the RNA has distortions, run 10\u2013100 ps of restrained MD. \u2705 If Stage D (diffusion) is already adjusting structure, MD is optional (only minimize). \u2705 If you want fully realistic structures, restrained MD is a good addition.</p> <p>6) Summary</p> <p>Energy minimization is a fast, physics-based way to clean up small bond strains after forward kinematics.</p> <p>Short MD (10\u2013100 ps) can slightly refine local structure while keeping the RNA stable.</p> <p>Weak restraints (on base-pairs, torsion angles) prevent unwanted unfolding while still allowing some natural movement.</p> <p>These methods complement forward kinematics, helping correct small errors in torsion-angle-based placement.</p> <p>\u2705 Recommendation for Stage C: 1\ufe0f\u20e3 Run forward kinematics to get initial coordinates. 2\ufe0f\u20e3 Minimize energy to fix bond-length/angle deviations. 3\ufe0f\u20e3 (Optional) Run short MD (~100 ps) with weak constraints for slight improvements.</p> <p>This keeps RNA structures realistic without introducing excessive noise.</p> <p>======== RNA Structure Refinement: Software Comparison and Best Practices</p> <p>Overview of Energy Minimization and MD in RNA Refinement</p> <p>Refining RNA 3D models with energy minimization and molecular dynamics (MD) helps relieve steric clashes and improve stereochemistry, driving the structure toward a local energy minimum \ufffc. In energy minimization, the atomic coordinates are adjusted to reduce the force field energy (fixing bad contacts without large atomic displacements). MD simulations then sample the molecule\u2019s conformational space by integrating Newton\u2019s equations of motion, allowing the RNA to relax and refine in a physically realistic environment. This approach is often applied after initial RNA modeling (e.g. from homology or coarse methods) to enhance model quality \ufffc \ufffc. Key factors in successful refinement include selecting an appropriate force field, preparing a proper simulation environment (solvent and ions), and following best practices in simulation setup and analysis.</p> <p>Key Concepts in Simulation Setup and Execution</p> <p>Figure: (a) The basic MD simulation loop involves generating an initial model, calculating forces using a chosen force field, integrating Newton\u2019s equations of motion, and updating atomic positions. This iterative process (with femtosecond time steps) lets the RNA explore conformational space under the defined potential. (b) MD simulations include a realistic environment; for example, a biomolecule (blue) might be simulated in explicit water (red) with neutralizing ions (green/purple) and even a membrane (brown) if applicable \ufffc. Such environments are critical for RNA due to its high negative charge density, which requires counter-ions to stabilize folded conformations \ufffc.</p> <p>Force Field Selection and Parameterization</p> <p>Choosing the right force field is crucial for RNA simulations. Modern all-atom force fields for nucleic acids (e.g. Amber or CHARMM families) provide parameter sets for RNA that have been optimized to reproduce experimentally observed geometries and dynamics \ufffc \ufffc. Notably, older Amber force fields (ff94, ff99) were later improved with RNA-specific corrections (ff99bsc0 and \u03c7_OL3) to fix issues like overly rigid sugar puckers or helix unwinding \ufffc. The current Amber RNA force field (ff99bsc0+OL3, sometimes called ff99OL3) is recommended for accuracy \ufffc. CHARMM36 is another widely used force field that includes RNA parameters and is continually refined. When setting up a simulation, ensure the selected force field is up-to-date and suitable for RNA (e.g. Amber\u2019s OL3 update or latest CHARMM nucleic acid parameters) \ufffc \ufffc.</p> <p>If the RNA includes modified nucleotides or ligands, parameterization may be needed. Tools like Amber\u2019s Antechamber (for small molecules) can generate partial charges and bonded parameters to integrate non-standard residues into the force field \ufffc \ufffc. Similarly, the CHARMM General Force Field (CGenFF) or GAFF (General Amber Force Field) can be used for RNA modifications. It\u2019s important to validate any custom parameters (e.g. check that geometries remain reasonable). In summary, use reliable force fields (Amber or CHARMM families are most common for RNA \ufffc) and include all necessary parameters for any non-canonical moieties before simulation.</p> <p>Hardware Requirements and Optimization Techniques</p> <p>Computational performance is a major consideration for MD refinement. Modern MD engines can take advantage of GPUs and parallel CPUs to accelerate simulations. For instance, GROMACS and Amber (pmemd) both offer GPU acceleration that yields fast simulation rates (on the order of tens to hundreds of nanoseconds per day for typical systems) \ufffc. Amber\u2019s GPU-accelerated MD code (pmemd.cuda) and GROMACS are both highly optimized, with GROMACS known as a \u201cworkhorse\u201d that is fast, highly parallelized, GPU accelerated and very efficient \ufffc. NAMD also supports GPUs and excels at scaling on large CPU clusters, making it well-suited for very large RNA systems (millions of atoms, such as ribosomes) \ufffc. OpenMM offers a Python API and GPU backend which is great for custom workflows, though it\u2019s not known for the top speed or scalability compared to the specialized engines \ufffc. In practice, if you have a single workstation with one or a few GPUs, Amber or GROMACS will deliver excellent performance; if you are running on a multi-node supercomputer with thousands of CPU cores, NAMD\u2019s scaling may shine \ufffc.</p> <p>To optimize performance, take advantage of features like constraint algorithms (e.g. SHAKE/RATTLE) on bonds involving hydrogens, allowing a larger integration timestep (commonly 2\u00a0fs in all-atom MD, or up to 4\u20135\u00a0fs if using hydrogen mass repartitioning). Ensure you compile/run the MD code with hardware-specific optimizations (CUDA for NVIDIA GPUs, MPI for multi-core CPUs). Parallel scaling efficiency can drop if too many processors are used for a small system \u2013 it\u2019s often best to run multiple shorter simulations in parallel rather than one simulation on an excessively large number of threads \ufffc \ufffc. Monitor CPU/GPU utilization to adjust the number of threads or domain decomposition settings for optimal efficiency. Memory is usually not a bottleneck for MD (RAM needs scale roughly with system size, e.g. a 100k-atom RNA system might require a few hundred MB). Trajectory storage can be significant, so plan for disk space if running long simulations or many replicas. In summary, match the MD software to your available hardware (use GPU acceleration if available \ufffc) and follow vendor or community guides for performance tuning (e.g. using the right number of MPI ranks vs OpenMP threads in GROMACS, or enabling GPU PME offload). This ensures simulations run faster and makes longer refinement simulations feasible.</p> <p>Common Pitfalls and Error Handling</p> <p>Refining RNA structures via MD can present some common pitfalls. A frequent issue is simulation instability (blow-ups), often manifesting as errors like SHAKE or LINCS warnings. These usually indicate bad contacts or extreme forces. To avoid this, always perform a gentle energy minimization first \ufffc to relieve atomic clashes. If an MD run still diverges, consider a more gradual equilibration: e.g. start with heavy atom position restraints and slowly relax them. Also verify that the system is properly neutralized and solvated \u2013 RNA is highly negatively charged, so forgetting counter-ions can cause the structure to rapidly expand or disintegrate due to coulombic repulsion \ufffc. Including the correct concentration of Na\u207a/Mg\u00b2\u207a is critical for stability of folded RNA, as ions shield the phosphate charges and allow the RNA to maintain a compact conformation \ufffc \ufffc.</p> <p>Another pitfall is using an inappropriate force field or wrong parameterization. An outdated force field can bias the RNA toward non-native conformations (for example, a known issue with older parameters was unwinding of helices due to over-stabilized anti \u03c7 torsions \ufffc). Always use the latest recommended parameters for RNA \ufffc. If your RNA has uncommon modifications, a missing parameter can cause a crash (the MD program might report a missing improper or an undefined atom type). Tools like Amber\u2019s tleap will warn if any atom types are unparametrized. In such cases, one must add the missing parameters (via Antechamber, paramchem, etc.) before running MD.</p> <p>During the run, keep an eye on temperature and pressure stability if using thermostats/barostats. Incorrect coupling constants or a poor equilibration can lead to large temperature spikes or pressure instabilities that crash the simulation. If the simulation reports \u201cvibration or integration error\u201d, try using a smaller time step (e.g. 1\u00a0fs) for a short period or re-minimize the structure. Floating-point errors or segmentation faults can sometimes occur with faulty GPU drivers or if the simulation domain is excessively large relative to cutoffs; updating software or adjusting cut-off distances can help. When errors occur, consult community forums or documentation: for example, GROMACS will output an error message (like \u201cLINCS constraint broken\u201d) which can be looked up in their user forum, and Amber has an active mailing list where many common errors (e.g. \u201cvlimit exceeded\u201d) are explained. In short, the main error-handling strategy is to backtrack to a safer state (minimize more, apply restraints, reduce step size) and gradually reintroduce the conditions, while ensuring the physical setup (force field, solvation, etc.) is sound.</p> <p>Recommended Best Practices for High-Quality RNA Refinements</p> <p>Following established best practices will improve the outcomes of RNA refinement simulations:     \u2022   System Preparation: Start from the best model available. If the RNA has missing loops or unresolved regions, model those (with tools like ModeRNA or by homology) before MD. Protonate appropriately (RNAs typically carry deprotonated phosphate groups at neutral pH). Use a solvation model consistent with the force field (e.g. TIP3P water with Amber RNA force fields) and add neutralizing counter-ions (and additional salt if needed to mimic physiological conditions). Many use web-based builders (like CHARMM-GUI) to set up solvated RNA systems with the desired ion concentrations in a standardized way.     \u2022   Energy Minimization and Equilibration: Always run a staged relaxation. First, perform energy minimization (500\u20135000 steps) to remove bad contacts \ufffc. Then, equilibrate with restraints: for example, restrain the RNA heavy atoms and let water and ions relax around it (run MD for a short period, e.g. 100 ps, at constant volume). Gradually heat the system from 0 K to the target temperature (300 K) over several tens of picoseconds under restraint, to prevent shocking the RNA. Then switch to constant pressure (NPT) to equilibrate density. Over a few nanoseconds, reduce and remove the positional restraints so the RNA can relax fully. This approach prevents abrupt disturbances that could denature the RNA.     \u2022   Production MD and Sampling: Run multiple short MD simulations rather than one very long simulation. This helps sample different local minima (since each trajectory with different initial velocities may explore slightly different paths). For refinement purposes, simulations on the order of a few nanoseconds to tens of nanoseconds may be sufficient to improve local geometry and adjust incorrect torsions \ufffc. If seeking larger-scale conformational adjustments, longer simulations or enhanced sampling might be needed (see Alternative Approaches below). Always save periodic snapshots of the trajectory (e.g. every 10\u2013100 ps) so that you can analyze the path and also restart from intermediate points if needed.     \u2022   Monitoring and Verification: Throughout the refinement, monitor key properties. Check that base-pairing interactions remain intact (if they should be) \u2013 sometimes applying weak distance restraints on known base pairs or tertiary contacts can help preserve them during the early equilibration. Monitor RMSD of the structure over time to ensure it stabilizes. After refinement, use structure validation tools like MolProbity or RNAvalid to evaluate geometry (backbone torsions, contacts, etc.) \ufffc. A successful refinement should improve geometry (fewer steric clashes, better bond angles) and ideally maintain the RNA\u2019s overall fold near the experimental or predicted model.     \u2022   Documentation and Reproducibility: Use scripting to your advantage \u2013 many MD packages allow you to script the whole setup and run (shell scripts, Python for OpenMM, TCL scripts for NAMD). Record the exact parameters used (time step, thermostat, etc.) and random seeds for reproducibility. Community forums and tutorials are abundant for these workflows; for example, Amber\u2019s official tutorials provide step-by-step protocols for RNA duplex MD refinement \ufffc, and tools like QwikMD (a VMD plugin) offer a GUI wizard to set up NAMD simulations with best practices, lowering the learning curve for novices \ufffc.</p> <p>By adhering to these best practices \u2013 careful setup, gradual equilibration, and thorough validation \u2013 you can achieve high-quality RNA structural refinements that are more likely to be physically realistic and closer to the true native conformation.</p> <p>Comparison of Software Packages for RNA Refinement</p> Software Ease of Use (User-Friendliness &amp; Support) Computational Efficiency (CPU/GPU Performance &amp; Scalability) Integration (Workflow Compatibility &amp; I/O) Amber (AmberTools + AMBER MD) Extensive documentation and tutorials available (e.g. official Amber RNA tutorials). Command-line driven (text input files); well-supported via active mailing list. AmberTools (free) provides setup and analysis utilities, though the full GPU-optimized engine (pmemd) is licensed. Lacks a built-in GUI. Highly optimized on GPUs \u2013 pmemd.cuda delivers very fast simulations for biomolecules. Good multicore CPU performance; can use multiple GPUs (scaling to 2\u20134 GPUs efficiently, with diminishing returns beyond a single node). Suitable for systems up to million-atom scale. Memory footprint is moderate. Strong support for Amber force fields (ff14SB, RNA OL3, etc.). Input format uses prmtop/inpcrd, while outputs (topologies, trajectories) are readable by other tools (e.g. VMD, MDAnalysis). Scripting is possible via Python (pyAmber, ParmEd) for custom workflows. GROMACS Command-line toolset with a stepwise workflow (preprocessing, running, analysis). Considered very user-friendly once the learning curve is overcome \u2013 many tutorials and an active user forum available. No official GUI, but community examples and built-in analysis tools ease use. Among the fastest MD engines available \u2013 highly efficient CPU core usage and excellent GPU acceleration. Particularly strong performance on single-node or few-node setups; strong scaling with multiple GPUs. Low memory overhead. Flexible with force fields \u2013 supports Amber, CHARMM, OPLS, etc. via provided parameter files. Outputs trajectories in portable formats (.xtc, .trr) and coordinates in PDB/GRO; widely compatible with analysis libraries. Integration with external tools (e.g. CHARMM-GUI) is straightforward. NAMD Setup via text configuration (TCL syntax) can be complex for newcomers. However, the QwikMD GUI plugin for VMD provides a point-and-click interface that enhances ease of use. Documentation is comprehensive (NAMD User Guide) and VMD integration aids preparation and visualization. Designed for high-performance parallel computing. Scales impressively to thousands of CPU cores and multi-GPU setups. GPU support (CUDA) offloads computations, though single-GPU performance may be slightly behind Amber/GROMACS for small systems. Excels in multi-node environments. Very interoperable \u2013 can natively read both CHARMM (PSF/PARAM) and Amber (prmtop/crd) files. Outputs trajectories in DCD format (compatible with VMD, MDAnalysis, etc.). TCL-based scripting allows custom operations and smooth integration with VMD for both pre- and post-processing. OpenMM Python-based toolkit with programmatic control. Ideal for users comfortable with Python. Lacks a traditional GUI but offers transparency via Jupyter notebooks. Well-documented with an active community, making it excellent for prototyping custom forces or integrators. Utilizes GPU acceleration via CUDA or OpenCL, achieving good performance but typically slightly slower than specialized codes like GROMACS or Amber for mainstream simulations. Best used on a single GPU or single node. Offers flexibility over massive parallel scaling. Highly interoperable \u2013 can directly consume force field and topology files from Amber, GROMACS, and CHARMM. Outputs standard trajectory formats (PDB, DCD) and easily integrates into Python workflows for seamless scripting and analysis. CHARMM A powerful but historically complex program. Uses its own scripting language and requires careful setup of topology/parameter files. The CHARMM-GUI web server can lower the barrier. Extensive documentation exists, though some features are expert-oriented. Good performance on CPUs with some GPU support (via interfaces like OpenMM). Efficiency is generally lower than GROMACS/Amber for pure MD runs, but its vast feature set (advanced sampling methods, polarizable force fields) is a strength. Runs in parallel (MPI) for large systems. Native to CHARMM force fields. Often used in conjunction with CHARMM-GUI for system preparation, though outputs and inputs may require conversion for use with other MD engines. Scripting is available via the CHARMM input language or Python interfaces (e.g. PyCHARMM). Others Desmond (Schr\u00f6dinger) provides a user-friendly GUI via Maestro and very fast MD core; proprietary with academic limitations. LAMMPS is highly modular and scriptable but requires more manual setup, making it less friendly for biomolecular simulations for novices. Desmond offers excellent GPU performance and scaling with protocols optimized for drug-design workflows (applicable to RNA). LAMMPS can perform well with relevant accelerator packages, though it is not specifically optimized for biomolecular MD like the specialized engines. Desmond integrates with Schr\u00f6dinger\u2019s suite (Maestro format, exportable to standard formats). LAMMPS requires manual definition of force field parameters; can ingest data from tools like CHARMM-GUI. Typically used by advanced users for niche applications in RNA refinement. <p>Multiple MD software packages are available for RNA refinement, each with its own strengths. The table below compares some of the most widely used packages on ease of use, computational efficiency, and integration with RNA modeling workflows:</p> <p>Software    Ease of Use (User-Friendliness &amp; Support)   Computational Efficiency (CPU/GPU Performance &amp; Scalability)    Integration (Workflow Compatibility &amp; I/O) Amber (AmberTools + AMBER MD)   Extensive documentation and tutorials available (e.g. official Amber RNA tutorials) \ufffc. Command-line driven (text input files); well-supported via active mailing list. AmberTools (free) provides setup and analysis utilities, though the full GPU-optimized engine (pmemd) is licensed. Overall user-friendly for those following tutorials, but lacking a built-in GUI.  Highly optimized on GPUs \u2013 pmemd.cuda delivers very fast simulations for biomolecules \ufffc. Good multicore CPU performance; can use multiple GPUs (scaling to 2\u20134 GPUs efficiently, with diminishing returns beyond a single node). Suitable for systems up to million-atom scale with adequate hardware (has been used in very large simulations via NAMD interfacing \ufffc). Memory footprint is moderate.   Integration: Strong support for Amber force fields (ff14SB, RNA OL3, etc.). Input format uses prmtop/inpcrd, but Amber outputs (topologies, trajectories) are readable by other tools (e.g. VMD, MDAnalysis). NAMD can directly read Amber prmtop coordinate files \ufffc, enabling workflow flexibility. Scripting possible via Python (pyAmber, ParmEd) for custom workflows. GROMACS Command-line toolset with a stepwise workflow (preprocessing, running, analysis). Considered very user-friendly once the learning curve is overcome \u2013 many tutorials and an active user forum available \ufffc. No official GUI, but well-documented commands and community examples (e.g. Justin Lemkul\u2019s tutorials) ease use. Inbuilt analysis tools simplify common tasks, reducing need for external scripts \ufffc. Large user community and support.    Among the fastest MD engines available \u2013 highly efficient CPU core usage and excellent GPU acceleration \ufffc. Particularly strong performance on single-node or few-node setups; can achieve strong scaling with multiple GPUs. Parallelization uses domain decomposition \u2013 good for medium systems (10^5\u201310^6 atoms). For very large systems, scaling to many nodes is possible but NAMD/Charmm may outperform. Low memory overhead.  Integration: Flexible with force fields \u2013 supports Amber, CHARMM, OPLS, etc. via provided parameter files. Outputs trajectories in portable formats (.xtc, .trr) and coordinates in PDB/GRO; widely compatible with analysis libraries. Setup can be integrated with external tools (e.g. CHARMM-GUI can output GROMACS inputs). Scripting typically done via shell or Python wrappers; no native script language, but GROMACS tools can be chained easily. NAMD    Setup via text configuration (TCL syntax) which can be complex for newcomers. However, the QwikMD GUI plugin for VMD provides a point-and-click interface to set up and run NAMD simulations, greatly enhancing ease of use for novices \ufffc. Documentation is comprehensive (NAMD User Guide), and VMD integration (for preparation and visualization) is a plus. Community support via mailing list is available.    Designed for high-performance parallel computing. NAMD can handle very large RNA systems efficiently \u2013 it has shown impressive scaling to thousands of CPU cores and multi-GPU setups \ufffc. GPU support (CUDA) offloads most computations, though single-GPU speed is slightly behind Amber/GROMACS for small systems \ufffc. Excels in multi-node environments due to load-balanced runtime, making it ideal for massive simulations (e.g. whole ribosomes) \ufffc. Integration: Very interoperable \u2013 NAMD can natively read both CHARMM (PSF/PARAM) and Amber (prmtop/crd) files \ufffc, allowing one to prepare structures in CHARMM or AmberTools and then run NAMD. Outputs trajectories in DCD format (readable by VMD, MDAnalysis, etc.). TCL-based scripting enables on-the-fly custom operations (e.g. applied forces, steered MD), and its close coupling with VMD streamlines pre- and post-processing. OpenMM  Python-based toolkit with programmatic control. Ease of use depends on programming comfort: for researchers familiar with Python, OpenMM is extremely flexible and \u201cinteractive\u201d (you write a Python script to set up and run simulation). It lacks a traditional GUI but the Python API and Jupyter notebooks make setup transparent. Good documentation and an active community on forums. Ideal for prototyping custom forces or integrators \ufffc.  Utilizes GPU acceleration via CUDA or OpenCL, achieving good performance but typically slightly slower than specialized codes like GROMACS or Amber for mainstream simulations \ufffc. Not designed for massive parallel scaling; best used on a single GPU or single node. Performance is still sufficient for refining moderate-size RNA models, and the trade-off is greater flexibility. Integration: Highly interoperable \u2013 OpenMM can directly consume force field and topology files from other programs (Amber, GROMACS, CHARMM) \ufffc. For example, you can load an Amber prmtop and coordinate file into OpenMM and run MD \ufffc. This makes it easy to integrate OpenMM as a refinement step in various workflows. Trajectory outputs (PDB, DCD) are standard. One can embed OpenMM in Python scripts that also perform analysis, enabling seamless scripted workflows (e.g. iteratively refine models and evaluate scoring functions all within one script). CHARMM  A powerful but historically complex program. CHARMM (the engine) uses its own scripting language and requires careful setup of topology/parameter files. This steep learning curve makes it less \u201cfriendly\u201d to new users compared to Amber or GROMACS. However, CHARMM-GUI web server can generate CHARMM input files for you, lowering the barrier. Documentation is extensive but some features are expert-oriented. Community support exists, though the user base is smaller now relative to GROMACS/Amber. Good performance on CPUs and some GPU support (CHARMM can interface with OpenMM or use its own accelerated routines for certain force fields). Efficiency is generally lower than GROMACS/Amber for purely MD runs, but CHARMM\u2019s strength lies in its vast feature set (extensive sampling methods, polarizable force fields, etc.). It can run in parallel (MPI) and handle large systems but is not typically the first choice for high-speed production MD.  Integration: Native to CHARMM force fields (although it can use others). Often used in preparation steps (through CHARMM-GUI) then one might switch to faster MD engines for production. CHARMM outputs and inputs can be converted (e.g. psf/pdb to Amber/Gromacs formats via tools), but it\u2019s generally used in its own ecosystem. Scripting is via the CHARMM input script language or via Python interfaces (e.g. PyCHARMM). Suitable for refinement protocols that might combine MD with advanced analysis within one environment. Others  Desmond (Schr\u00f6dinger) provides a user-friendly GUI via Maestro and very fast MD core; however, it\u2019s proprietary (free for academics with limitations) and less commonly used for RNA. LAMMPS is another engine, highly modular and scriptable, but using it for biomolecules requires more manual setup; it\u2019s powerful for custom simulations (and can handle coarse-grained models) but has a steep learning curve for novices.    Desmond: excellent GPU performance and scaling, with protocols optimized for drug-design workflows (proteins/DNA; should handle RNA similarly). LAMMPS: performance can be good if using relevant accelerator packages, but not specifically optimized for biomolecular MD like specialized codes. GENESIS and ACEMD are other engines focusing on performance (GENESIS for HPC, ACEMD for GPUs) that might be encountered in literature.   Integration: Desmond integrates with Schr\u00f6dinger\u2019s suite (inputs/outputs in Maestro format, though it can export trajectories in standard formats). LAMMPS requires users to define force field parameters in input scripts; it can ingest data from tools like CHARMM-GUI (which now has options to output to LAMMPS \ufffc). These are niche choices for RNA and typically used by advanced users or specific projects.</p> <p>Table: Comparison of software packages for RNA structure refinement via MD. Each package has unique advantages: for example, Amber and GROMACS are renowned for speed and well-rounded tools (Amber with extensive built-in features like QM/MM \ufffc, and GROMACS with easy analysis utilities \ufffc), while NAMD shines in large-scale simulations \ufffc. OpenMM offers unmatched flexibility for custom methods \ufffc, and CHARMM provides a one-stop environment for advanced sampling at the cost of complexity. The best choice often depends on user familiarity, available hardware, and specific needs (e.g. need for a GUI, system size, or specific force field requirements).</p> <p>Additional Notes on Software Integration</p> <p>All major MD packages accept PDB files for initial coordinates (often with accompanying topology files). Workflow integration is facilitated by the fact that force fields are generally portable: one can, for instance, use CHARMM-GUI to build a solvated RNA model and then run the simulation in GROMACS or NAMD by selecting the appropriate output format \ufffc. Many researchers prepare RNA systems with Amber\u2019s tleap (leveraging Amber\u2019s RNA force field) and then use GROMACS or NAMD to run the production MD \u2013 this is possible because Amber\u2019s force field parameters can be converted or read directly \ufffc. The analysis stage is also largely code-agnostic: tools like VMD, MDAnalysis, and cpptraj can read trajectories from any of these programs, so one can use whichever analysis or visualization tool preferred regardless of the MD engine used. This interoperability means the choice of MD software for refinement can be made based on practical considerations (speed, ease) without sacrificing compatibility in a modeling pipeline.</p> <p>In terms of community and support: GROMACS and Amber have very large user communities (with decades of collective experience on forums and mailing lists), which can be reassuring for newcomers. NAMD\u2019s close relationship with VMD also means a lot of combined documentation is available. OpenMM\u2019s community is growing, especially in the Python-savvy computational biology sphere, and CHARMM, while older, is backed by a rich history of publications and protocols. Each of these packages has been successfully used in RNA refinement studies in the literature, so they are all viable \u2013 selecting one is often a matter of matching the tool to the task and the user\u2019s comfort.</p> <p>Alternative Approaches Beyond Standard MD Refinement</p> <p>While energy minimization and conventional MD are the primary tools for RNA structure refinement, several alternative or complementary approaches can enhance RNA model quality:     \u2022   Knowledge-Based Monte Carlo Refinement: Methods like Rosetta utilize Monte Carlo sampling with sophisticated scoring functions to refine RNA structures. Rosetta\u2019s RNA approach (e.g. FARFAR) assembles structures from fragments using a hybrid physics/statistical potential, then applies full-atom refinement with an all-atom energy function \ufffc. These techniques can sometimes overcome force-field deficiencies by incorporating knowledge-based terms. For example, Rosetta\u2019s ERRASER (Enumerative Real-Space Refinement ASsisted by Electron density) protocol is used to automatically fix local errors in RNA crystal structures, optimizing geometries with a physically realistic all-atom model \ufffc. Monte Carlo refinement is especially useful when the starting model is far from the native state or contains severe local misfolds that MD might have trouble correcting (due to getting trapped in nearby minima). These methods can be used before MD (to generate a better starting model) or after/with MD (e.g. alternating between Rosetta refinement and MD relaxation).     \u2022   Coarse-Grained and Enhanced Sampling Simulations: Standard all-atom MD can be slow to escape local minima, so coarse-grained (CG) models and enhanced sampling techniques provide alternatives. In a CG model, each nucleotide is represented by a few \u201cpseudo-atoms\u201d instead of all atoms, smoothing the energy landscape \ufffc \ufffc. Tools like SimRNA (which uses a five-bead model for RNA \ufffc) or NAST (one bead per nucleotide) allow broader conformational exploration with lower computational cost. One strategy is to run a coarse-grained simulation to sample large-scale reorientations or folding, then convert selected structures to all-atom detail and refine with standard MD. Discrete molecular dynamics (DMD), a simplified fast-sampling MD variant using stepwise potentials, has also been applied to RNA (for example, to fold RNAs with experimental constraints) \ufffc \ufffc. On the enhanced sampling front, methods like replica-exchange MD (REMD), metadynamics, or accelerated MD can be employed to improve sampling of RNA conformational space beyond what a single MD trajectory would visit. These approaches help in finding alternative low-energy conformations that plain MD might miss, thus providing better candidates for refinement.     \u2022   Experimentally Restrained Refinement: Often, additional experimental data can guide refinement in ways pure MD cannot. For instance, cryo-EM density-guided MD (such as MDFF \u2013 MD Flexible Fitting) introduces a potential term to pull the structure into agreement with a cryo-EM map \ufffc. This can refine RNA loops or tertiary contacts to better fit density while MD ensures the result is physically plausible. Similarly, NMR-driven refinement uses distance and angle restraints (e.g. NOE distances between protons, residual dipolar couplings) during MD or simulated annealing to enforce agreement with experimental constraints \u2013 programs like Xplor-NIH or the integrated use of Amber\u2019s NMR restraint facility can do this. These hybrid approaches have enabled refinement of very large RNA-protein complexes (even whole ribosomal subunits) by heavily restraining the simulation with experimental data \ufffc \ufffc. The end result is a model that not only has good geometry due to MD, but also is consistent with experimental observations. Even chemical probing data (SHAPE reactivities, etc.) can be incorporated via pseudo-energy terms to favor RNA conformations that agree with experiments.     \u2022   Normal Mode and Structural Deformation Methods: If only small adjustments are needed (e.g. resolving minor clashes or improving bond geometry), one might not require full MD. Normal mode analysis (NMA) can identify collective low-frequency motions of an RNA structure; moving along these modes and performing energy minimization can sometimes relieve strain without lengthy simulations. Likewise, elastic network models (coarse-grained harmonic models) are orders of magnitude faster than MD and can be used to relax structures slightly \ufffc \ufffc. These methods won\u2019t account for detailed atomic interactions, but they can be useful for generating an ensemble of slightly perturbed conformations that MD can then refine locally. Another simple approach is using energy minimization in internal coordinate space (torsion angle space) \u2013 for example, adjusting backbone torsions to ideal values and then minimizing. Some refinement tools (like QRNAS) adopt this strategy: QRNAS drags backbone torsions to nearest known rotamers from a database upon minimization, improving geometry in a knowledge-based way \ufffc \ufffc.     \u2022   AI and Data-Driven Prediction: An emerging area (though still maturing for RNA) is the use of machine learning to predict or refine structures. For proteins, tools like AlphaFold have revolutionized model accuracy; for RNA, analogous efforts (using graph neural networks or statistical potentials trained on known RNA structures) might soon provide initial models that require less refinement. Already, some pipelines use predicted base pairing probabilities or secondary structures as restraints in MD to maintain plausible folds. As these tools improve, they could be integrated with MD \u2013 for instance, using an ML-predicted distance map as a restraint matrix in an MD refinement. While not yet standard, it\u2019s worth noting that the landscape of refinement is expanding beyond traditional physics-based simulations.</p> <p>In practice, a hybrid strategy often works best. One might use Rosetta or another knowledge-based method to resolve gross errors in an RNA model, then perform MD in explicit solvent to fine-tune the geometry. Alternatively, run a short MD refinement and then use a scoring function (like Rosetta\u2019s) to evaluate if the structure improved, iterating as needed. The combination of different approaches can compensate for the weaknesses of any single method. For example, MD might preserve realistic local geometry but not sample a corrected global fold, whereas a coarse-grained search could find the right fold but with imperfect local details \u2013 using both in sequence can yield a high-quality structure.</p> <p>Conclusion</p> <p>Refining RNA structures via energy minimization and molecular dynamics is a crucial step in obtaining realistic 3D models. By carefully choosing software tools and following best practices in setup (force field selection, adequate solvation and ion placement, gradual equilibration), researchers can significantly improve initial RNA models. Modern MD engines like Amber, GROMACS, NAMD, OpenMM, and others provide robust platforms for these refinements \u2013 each with different trade-offs in usability and performance, but all capable of producing quality results when used properly. It\u2019s important to remain aware of common pitfalls (force field limitations, simulation instabilities) and to leverage community knowledge and documentation when troubleshooting. Furthermore, RNA refinement need not be limited to straightforward MD: integrating alternative approaches such as Monte Carlo sampling, coarse-grained simulations, and experimental restraints can dramatically enhance refinement outcomes, especially for complex or large RNAs. By integrating these tools and approaches into existing RNA modeling workflows, one can minimize the need for custom code and instead rely on well-established protocols to achieve high-quality, biologically meaningful RNA structures.</p> <p>References: The comparison and recommendations above draw on established benchmarks and user experiences in the literature. For instance, performance characteristics are summarized from community assessments of MD engines \ufffc \ufffc, and force field considerations from RNA-specific studies \ufffc. Many protocols have been described in Methods in Molecular Biology and other sources \ufffc \ufffc, which provide further reading for interested users.</p>"},{"location":"pipeline/test_time_scaling/","title":"Test Time Scaling","text":"<p>This document outlines the test time scaling for the RNA 3D prediction pipeline.</p>"},{"location":"pipeline/test_time_scaling/#test-cases","title":"Test Cases:","text":"<pre><code>\u2022 Small RNA (20-30 nucleotides)\n\u2022 Medium RNA (50-100 nucleotides)\n\u2022 Large RNA (150-200 nucleotides)\n\u2022 Very Large RNA (300+ nucleotides)\n</code></pre>"},{"location":"pipeline/test_time_scaling/#metrics-to-measure","title":"Metrics to Measure:","text":"<pre><code>\u2022 Total pipeline runtime\n\u2022 Time per major stage\n\u2022 Memory usage peaks\n\u2022 Disk space requirements\n\u2022 CPU/GPU utilization\n</code></pre>"},{"location":"pipeline/test_time_scaling/#testing-protocol","title":"Testing Protocol:","text":"<pre><code>1. Run each test case 3 times to get average metrics\n2. Record system specifications:\n    \u2022 CPU model and cores\n    \u2022 RAM capacity\n    \u2022 GPU model (if applicable)\n    \u2022 Storage type (SSD/HDD)\n3. Monitor resource usage during runs\n4. Document any bottlenecks or failures\n</code></pre>"},{"location":"pipeline/test_time_scaling/#expected-scaling","title":"Expected Scaling:","text":"<pre><code>\u2022 Runtime should scale approximately O(n\u00b2) with sequence length\n\u2022 Memory usage expected to scale linearly O(n)\n\u2022 Storage requirements scale linearly with sequence length\n</code></pre>"},{"location":"pipeline/test_time_scaling/#optimization-targets","title":"Optimization Targets:","text":"<pre><code>\u2022 Identify stages that don't scale well\n\u2022 Find opportunities for parallelization\n\u2022 Reduce memory footprint where possible\n\u2022 Optimize I/O operations\n</code></pre>"},{"location":"pipeline/integration/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/","title":"Design","text":"<p>Below is a comprehensive, \"best-of-all-worlds\" architectural design document that consolidates the strengths of earlier versions (V1, V2, V3, V4), addresses their criticisms, and clarifies optional vs. required steps to ensure synergy between (1) a torsion-based pipeline, (2) an AlphaFold 3\u2013style pairwise trunk, and (3) a final Diffusion module for 3D structure generation. This design is meant to serve as a robust piece of technical documentation\u2014verbose and detailed enough to guide implementation.</p> <p>\u2e3b</p> <p>Integrated RNA 3D Prediction Pipeline: Final Comprehensive Design</p> <ol> <li>High-Level Goal</li> </ol> <p>Objective: Accurately predict RNA 3D coordinates by unifying:     1.  A torsion-based pipeline (stages for 2D adjacency \u2192 torsion angles \u2192 optionally forward kinematics).     2.  An AlphaFold 3\u2013style pairwise trunk (MSA-based or single-sequence-based Pairformer with triangular updates, pair embeddings).     3.  A unified latent that merges local geometry (torsion + adjacency) with global pairwise constraints.     4.  A Diffusion model that conditions on that unified latent to iteratively refine or generate final 3D coordinates.     5.  A short Energy Minimization step (plus multi-sample approach) to yield a final ensemble and choose the best structure(s).</p> <p>Key Emphasis     \u2022   Preventing \"too many optional pieces\" that undermine synergy.     \u2022   Ensuring adjacency is used effectively in both torsion and pairwise modules.     \u2022   Aligning residue indexing so the staged pipeline's angles match the Pairformer's pair embeddings.     \u2022   Using a single final generator (Diffusion) that sees both local angle constraints and global pair embeddings, delivering more accurate final 3D structures.</p> <p>\u2e3b</p> <ol> <li>Detailed Pipeline Diagram</li> </ol> <p>Below is a textual flow with recommended mandatory vs. optional steps clearly noted. Boxes represent major modules; arrows indicate data/feature flow.</p> <pre><code>                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502 (1) INPUTS &amp; INITIAL SETUP                              \u2502\n                   \u2502  \u2022 RNA sequence [REQUIRED]                              \u2502\n                   \u2502  \u2022 2D adjacency from Stage A [HIGHLY RECOMMENDED]       \u2502\n                   \u2502  \u2022 MSA data (for Pairformer) [IF AVAILABLE]             \u2502\n                   \u2502  \u2022 Possibly external templates or partial 3D            \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       v\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (2) TORSION-BASED SUBPIPELINE (Stages A/B)                                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Use adjacency + sequence to predict backbone torsion angles:            \u2502  \u2502      \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, \u2026 plus adjacency-based features.                   \u2502  \u2502   b) Potentially do an MLP or GNN that merges adjacency signals.             \u2502  \u2502   c) Output: \"Torsion Representation\" =&gt; angles for each residue,            \u2502  \u2502      adjacency features (like base-pair partner indices).                    \u2502  \u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  \u2502   (Optional) Stage C: Forward Kinematics                                     \u2502  \u2502      If used, produce partial 3D coords from those angles.                   \u2502  \u2502      Align indexing with rest of pipeline.                                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; Torsion-based representation (angles, adjacency, partial coords)  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (3) ALPHAFOLD 3\u2013STYLE PAIRFORMER (MSA \u2192 Pair embeddings \u2192 Triangular Updates)\u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Optionally embed an MSA. Single-sequence possible if MSA is unavailable.\u2502  \u2502   b) Pass embeddings through ~48-block Pairformer trunk (like AF3)           \u2502  \u2502      - Triangular multiplication, attention, pair-bias.                      \u2502  \u2502   c) Possibly incorporate adjacency as a bias or input to pair embeddings.   \u2502  \u2502   d) Output: pair embeddings z\u1d62\u2c7c + single embeddings s\u1d62 for each residue.    \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; \"Pairwise Representation\" from final trunk pass.                  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           v                                 v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (4) UNIFIED LATENT MERGER / COMPRESSION                                      \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   Merge:                                                                     \u2502  \u2502   1) Torsion pipeline output (angles, adjacency data, optional partial 3D).  \u2502  \u2502   2) Pairwise trunk output (z\u1d62\u2c7c, s\u1d62).                                        \u2502  \u2502   Possibly a small Transformer or MLP that aligns residue indices,           \u2502  \u2502   creating a single \"latent\" that captures local + global constraints.       \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; \"Compressed Latent\" for the Diffusion.                            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (5) DIFFUSION MODULE                                                         \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Initialize random/noised 3D coords for each residue (heavy atoms).      \u2502  \u2502      Or optionally start from partial coords from Stage C.                   \u2502  \u2502   b) Condition on the \"Compressed Latent\" to guide iterative denoising.      \u2502  \u2502   c) Generate final 3D coordinates after X diffusion steps.                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; multiple 3D structure samples (e.g., 5 or 10).                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (6) ENERGY MINIMIZATION &amp; ENSEMBLE SELECTION                                 \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Short local minimization (e.g., Amber/CHARMM) to fix small geometry.    \u2502  \u2502   b) Evaluate &amp; rank each sample by geometry score or internal confidence.   \u2502  \u2502   c) Return top N (like 5) final structures, or a single best structure.      \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; Final 3D ensemble or single best structure.                       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>\u2e3b</p> <ol> <li>Addressing Previous Criticisms<ol> <li>Undermining synergy by making everything \"optional.\" \u2022   Here, torsion angles (Stage B) and pair embeddings (AF3 trunk) are both mandatory for synergy. \u2022   Adjacency is strongly recommended (it's the entire reason the torsion pipeline works effectively). \u2022   Forward Kinematics (Stage C) is labeled optional but we provide a rationale for skipping or using it. \u2022   The final Diffusion cannot skip either local or global constraints, because they are merged at step 4 by design.</li> <li>Using adjacency only in the torsion pipeline \u2022   We now highlight that adjacency can also feed into the Pairformer trunk as a pair-bias in attention. \u2022   This ensures adjacency is not underused or stuck in a corner; it can influence both local angle modeling and the global pairwise network.</li> <li>Residue indexing mismatch \u2022   We explicitly define a single consistent indexing scheme that all pipeline stages must share. \u2022   If the torsion pipeline re-maps or discards residues, we do a bridging \"residue index alignment\" prior to the \"Unified Latent Merger.\"</li> <li>Weak merging of torsion + pair embeddings \u2022   Previously, we said \"small MLP.\" Now we specify that a \"Latent Merger\" might be a minimal Transformer or GNN that can properly unify node-level angles with pair-level embeddings z\u1d62\u2c7c. \u2022   This is a richer approach, preserving structure. Or simpler solutions are possible, but we highlight the need to handle (i, j) pairs carefully.</li> <li>Energy Minimization \u2022   We reaffirm that short local minimization is strongly advised for final geometry polishing, especially in a multi-sample scenario. \u2022   This step addresses lingering steric or bond-angle issues not fully solved by the neural pipeline.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Mandatory vs. Optional Steps</li> </ol> <p>To avoid confusion about synergy:     \u2022   Mandatory:     1.  Stage B Torsion: adjacency + sequence \u2192 angles.     2.  Pairformer trunk: MSA or single-sequence \u2192 pair embeddings.     3.  Unified Latent (merger) so the Diffusion sees both.     4.  Diffusion as the final generator.     \u2022   Strongly Recommended:     \u2022   Stage A adjacency: Typically required if you want a torsion pipeline.     \u2022   Energy Minimization at the end.     \u2022   Truly Optional:     1.  Stage C forward kinematics: If you prefer letting Diffusion handle initial coords from random noise, you can skip. But giving it a partial 3D \"warm start\" can help.     2.  MSA: If you lack multiple sequences, the Pairformer can run single-sequence mode, though results may degrade.     3.  Templates: Could be integrated but not mandatory.</p> <p>Thus, the pipeline always merges local angles and pair embeddings for synergy. Adjacency is recommended so the torsion pipeline has meaningful constraints.</p> <p>\u2e3b</p> <ol> <li>Explanation of Each Module</li> </ol> <p>(A) Torsion-Based Pipeline (Stage B)     1.  Input:     \u2022   RNA sequence of length N.     \u2022   Adjacency/2D structure from Stage A (each residue i has a potential base-pair partner j).     2.  Angle Prediction:     \u2022   A GNN or MLP that sees adjacency and predicts \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi for each residue i. Possibly sugar pucker angles if needed.     3.  Output:     \u2022   An angle vector per residue; adjacency-based features (like \"which j is i paired with?\").     \u2022   (Optional) partial coordinates via forward kinematics if Stage C is invoked.</p> <p>(B) AlphaFold 3\u2013Style Pairformer     1.  MSA / Single Sequence:     \u2022   Construct initial single representation from an MSA embedding or a single-sequence embedding if MSA is unavailable.     2.  Pairformer Trunk:     \u2022   Triangular multiplication &amp; triangular attention to refine a pair representation \\mathbf{z}{ij} and single representation \\mathbf{s}_i.     \u2022   Possibly incorporate adjacency in the pair-bias or in the initial pair embedding to nudge the trunk about known base pairs.     3.  Output:     \u2022   Final pair embeddings z for all pairs i,j, plus single embeddings s_i.</p> <p>(C) Unified Latent Merger     1.  Combining:     \u2022   Residue-level data from the torsion pipeline (angles, adjacency info, partial coords).     \u2022   Pair-level data from the Pairformer trunk (z\u1d62\u2c7c, plus single s\u1d62).     2.  Technique:     \u2022   A small Transformer or GNN can unify node-level (angles, single s\u1d62) with edge-level (z\u1d62\u2c7c, adjacency). Or a simpler MLP if resource-limited.     \u2022   Ensure residue indexing matches between both modules (especially if partial coords skip or reorder some residues).     3.  Output:     \u2022   A single \"latent representation\" fed to the diffusion model for conditioning.</p> <p>(D) Diffusion Module     1.  Input: random or partially noised 3D coordinates for each residue's heavy atoms.     2.  Conditioning: the \"Compressed Latent\" from step (C).     3.  Process: iterative denoising (like standard 2D/3D diffusion). Each step sees the latent, adjusting coordinates accordingly.     4.  Output: final 3D coordinates after X steps. Because it's generative, we can sample multiple times (multiple seeds).</p> <p>(E) Energy Minimization + Ensemble     1.  Sampling:     \u2022   We produce ~5\u201310 final 3D samples from the diffusion to cover multiple solutions.     2.  Short Minimization:     \u2022   For each sample, do a short local minimization (1\u201310k steps) with an RNA-friendly force field. This corrects bond angles/lengths or steric clashes.     3.  Scoring &amp; Ranking:     \u2022   Possibly adapt a pLDDT-like network, or do geometry checks. We select top N structures (like top 5).     4.  Final:     \u2022   Provide the best structure for a single guess, or an ensemble of top solutions if the application (e.g., Kaggle) allows multiple submissions.</p> <p>\u2e3b</p> <ol> <li>Potential Implementation Details</li> </ol> <p>Residue Index Alignment     \u2022   Mapping: We keep a dictionary or table, \"ResidueIndexMap,\" that ensures if the torsion pipeline discards residues or re-labeled them, the Pairformer still references the same i, j.     \u2022   Practical: The adjacency is typically a matrix [N\u00d7N]; the Pairformer is also [N\u00d7N]. They must have identical dimension N, consistent ordering.</p> <p>Adjacency Integration in Pairformer     \u2022   Option 1: Modify pair embedding init: z_init[i,j] += Linear(adjacency[i,j]).     \u2022   Option 2: Add a logit bias: attention_logits(i,j) += w * adjacency(i,j).     \u2022   Either ensures the pair trunk is aware of known base pairs.</p> <p>Forward Kinematics (Stage C)     \u2022   If used, we do a standard NeRF or MP-NeRF approach to place atoms by the predicted torsion angles. This yields partial 3D we either feed to the diffusion as an initialization or as an extra conditioning channel.     \u2022   Potential advantage: Diffusion starts from a not-too-random conformation, possibly speeding convergence.</p> <p>Diffusion Model     \u2022   Implementation: Could be e.g. a score-based generative model or discrete time-step diffusion.     \u2022   Condition: We pass in \"unified latent\" each step. The network learns to correct or \"denoise\" coordinates in alignment with both local angles and global pair constraints.     \u2022   Training: We'd need training data of known 3D structures plus adjacency or MSA (where available) to supervise the diffusion.</p> <p>Ensemble &amp; Minimization     \u2022   Often done in a separate script:     1.  Run each predicted structure in a local MD environment.     2.  Evaluate geometry.     3.  Keep best.     \u2022   For large RNAs, you might reduce the sample size or do partial minimization.</p> <p>\u2e3b</p> <ol> <li>Advantages Over Previous \"Versioned\" Designs<ol> <li>No \"lost synergy\": We do not allow the torsion pipeline or the Pairformer to be fully bypassed. Both feed the final Diffusion, ensuring we incorporate adjacency and MSA-like global constraints.</li> <li>Clarity on optional: Stage C is optional for a well-understood reason (some may prefer random initialization in the diffusion if partial coords are too inaccurate or if computation time is short).</li> <li>Improved Merging: We no longer say \"just a small MLP.\" We highlight a purposeful \"Latent Merger\" that can handle node-edge data properly. This solves the prior critique of \"weak merging.\"</li> <li>Residue alignment: Addressed explicitly with a recommendation to keep a consistent indexing or bridging step.</li> <li>Energy Minimization: Elevated to recommended status, explaining how it polishes final geometry in a multi-sample scenario.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Implementation Caveats     \u2022   Complex Development: This pipeline is non-trivial\u2014four major modules (Torsion, AF3 trunk, Merger, Diffusion) plus optional forward kinematics and a final minimization script.     \u2022   Performance: A full Pairformer (~48 blocks) + GNN or MLP for torsions + a big diffusion network can be memory-heavy. Minimization for 5\u201310 structures also costs some CPU/GPU time.     \u2022   Data Gaps: If you lack good adjacency or an MSA, performance could degrade. Single-sequence Pairformer plus no adjacency is effectively a partial pipeline.     \u2022   Indexing: Potentially the biggest source of bugs. Must ensure consistent labeling from start to finish.</li> </ol> <p>\u2e3b</p> <ol> <li>Example Implementation Roadmap<ol> <li>Data Preprocessing \u2022   Gather RNA sequence(s). \u2022   Predict or obtain adjacency (2D structure) from a standard method (Stage A). \u2022   If available, compile an MSA. \u2022   Create a \"ResidueIndexMap\" to unify indexing across pipeline steps.</li> <li>Torsion Pipeline \u2022   Use adjacency + sequence \u2192 predict angles. \u2022   (Optional) run forward kinematics \u2192 partial 3D. \u2022   Store angles, adjacency-based features, partial coords if used.</li> <li>AF3 Pairformer \u2022   Load MSA or single sequence. \u2022   Run ~48-block trunk. \u2022   Possibly incorporate adjacency as a pair-bias. \u2022   Output final z\u1d62\u2c7c, s\u1d62.</li> <li>Unified Latent Merger \u2022   For each residue i, gather angles, adjacency info, partial coords, single embed s\u1d62. \u2022   For each pair (i,j), gather z\u1d62\u2c7c, adjacency bits. \u2022   Construct a single \"latent graph\" or \"multi-dimensional array\" the diffusion can read.</li> <li>Diffusion \u2022   Condition on that latent. \u2022   Start from random/noisy coords or from the partial 3D in step 2. \u2022   Iteratively generate final coords. Possibly produce multiple samples.</li> <li>Energy Minimization &amp; Ranking \u2022   For each sample, run short local minimization. \u2022   Evaluate geometry, pick top structures.</li> <li>Output \u2022   Store or submit the best structure(s). Possibly keep an ensemble.</li> </ol> </li> </ol> <p>By following this plan, you ensure the final design is robust, synergy is retained, adjacency is used effectively, and each step is well-defined in terms of \"mandatory vs. optional.\"</p> <p>\u2e3b</p> <ol> <li>Concluding Remarks</li> </ol> <p>This \"best-of-all-worlds\" pipeline:     1.  Merges the local knowledge (torsion angles + adjacency from Stage A/B) and the global perspective (AlphaFold's pair embeddings) in a single final generator (Diffusion).     2.  Maintains synergy by systematically requiring both the torsion pipeline and the pair trunk to feed into a single \"unified latent\" stage.     3.  Leverages an optional forward kinematics step (Stage C) only if beneficial.     4.  Recommends energy minimization to refine final coordinates from the diffusion model, especially beneficial in an ensemble context (e.g., picking the best 1\u20135 out of multiple predictions).     5.  Addresses earlier criticisms about optional synergy, adjacency usage, residue alignment, and shallow merging.</p> <p>Hence, you get a holistic RNA 3D prediction system that can handle small to moderately large RNAs, incorporate base-pair constraints, exploit MSA-driven pair embeddings, and finalize coordinates through a powerful diffusion framework\u2014ultimately yielding more consistent and accurate 3D structures than the sum of the earlier partial designs.</p> <p>==== Below is a high-level architectural plan detailing how backpropagation flows through this entire end-to-end RNA 3D prediction system\u2014integrating (1) TorsionBERT (or analogous angle predictor), (2) RFold for 2D adjacency, (3) an AlphaFold 3\u2013style Pairformer trunk, (4) a \"unified latent merger\", (5) MP-NeRF or forward kinematics for partial 3D (optional), (6) a Diffusion module for final coordinate generation, and (7) an energy-minimization or short MD pass. We also address how to apply LoRA (Low-Rank Adapters) or QLoRA techniques to adaptively train subsets of pre-initialized weights without exploding GPU memory.</p> <p>\u2e3b</p> <ol> <li>Overall Model Flow &amp; Backprop Considerations</li> </ol> <p>A. Forward Pass Summary     1.  Stage A (Adjacency, if not provided externally):     \u2022   If adjacency is predicted by something like RFold or another 2D method, we can treat that as either a frozen or partially trainable module. Usually, adjacency is not strongly backpropagated from final 3D coordinates because it's more of a discrete 2D structure.     \u2022   However, if we want adjacency differentiability, we'd need a differentiable base-pair \"soft assignment\" approach. Typically, we freeze adjacency or treat it as an input.     2.  TorsionBERT (Stage B):     \u2022   Takes the RNA sequence (and possibly adjacency features as input).     \u2022   Produces predicted torsion angles \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi (plus sugar pucker if desired).     \u2022   LoRA Application: Because TorsionBERT is large and partially pre-trained, we can freeze the base BERT-like layers and insert LoRA adapters on top. This ensures a small rank update for angles.     3.  Pairformer Trunk (AlphaFold 3\u2013style):     \u2022   Takes an MSA or single sequence, plus possibly adjacency/2D constraints as \"pair-bias.\"     \u2022   Outputs final pair embeddings z_{ij} and single embeddings s_i.     \u2022   LoRA Application: Similarly, we can place LoRA adapters in the Pairformer's attention layers. We typically freeze the main trunk weights from a pre-trained model and only train the low-rank updates.     4.  Unified Latent Merger:     \u2022   Combines TorsionBERT angles + adjacency-based features with the Pairformer embeddings (z_{ij}, s_i). Possibly done via a small merger subnetwork or autoencoder.     \u2022   LoRA Application: The merger is typically new code; if it's large, we can apply LoRA. But it might be a small MLP/Transformer, so we can fully train it from scratch if it's not too big.     5.  Optional Forward Kinematics (MP-NeRF):     \u2022   If we feed partial 3D coords into the Diffusion model, we do a differentiable forward pass from torsion angles \u2192 partial Cartesian.     \u2022   Backprop: MP-NeRF is fully differentiable with respect to torsion angles, so gradients flow from final 3D error signals back into TorsionBERT angles.     \u2022   LoRA Application: Typically none, as MP-NeRF is mostly geometry code, but if it's large (rarely is), we could also do minimal parameterization if needed.     6.  Diffusion Module:     \u2022   Input: random/noised coordinates or partial coords from MP-NeRF, plus the \"unified latent.\"     \u2022   Iteratively denoises to final 3D.     \u2022   LoRA Application: If we use a big diffusion U-Net or Transformer, we can freeze the backbone and add LoRA adapters in its attention layers or feed-forward blocks.     7.  Energy Minimization (Post-hoc):     \u2022   Typically not differentiable with respect to earlier modules. This step is outside the main gradient flow. We only do local minimization for final \"polish.\"</p> <p>Thus: The main backprop path is:</p> <p>Final 3D coordinate predictions \u2192 compute losses \u2192 backprop \u2192 (Diffusion model) \u2192 (Unified Latent Merger) \u2192 (Pairformer trunk's LoRA, TorsionBERT's LoRA) \u2192 adjacency is likely frozen or partially updated if we adopt a \"soft adjacency\" approach.</p> <p>\u2e3b</p> <p>B. Loss Functions</p> <p>We'll likely have two primary supervised loss signals:     1.  3D Coordinate Loss \\mathcal{L}{3D}:     \u2022   Compare final predicted 3D coords \\mathbf{X}{pred} (after the Diffusion stage) to known ground truth \\mathbf{X}{true}.     \u2022   Could be RMSD-based or a distribution-based loss (like Chamfer or L1 in Cartesian space).     \u2022   If partial coords from MP-NeRF are also available, we can also apply a direct partial 3D loss earlier in the pipeline.     2.  Torsion Angle Loss \\mathcal{L}:     \u2022   Compare predicted angles from TorsionBERT to known angles from real structures.     \u2022   This ensures TorsionBERT remains consistent with ground truth angles.}</p> <p>Optionally, one can combine: \\mathcal{L}{\\text{final}} = \\lambda{3D} \\cdot \\mathcal{L}{3D} \\;+\\; \\lambda{\\text{angle}} \\cdot \\mathcal{L}_{\\text{angle}}.</p> <p>Additionally, if the Pairformer trunk is trained for some contact/distance supervision, we might add pairwise distance or distogram losses \\mathcal{L}_{\\text{pair}}. But typically, we rely on the final 3D or angle constraints. Overall:</p> <p>\\mathcal{L}{\\text{end-to-end}} = \\lambda{3D}\\,\\mathcal{L}{3D} + \\lambda{\\text{angle}}\\,\\mathcal{L}{\\text{angle}} + \\lambda{\\text{pair}}\\,\\mathcal{L}_{\\text{pair}}.</p> <p>Backprop:     \u2022   The gradient from \\mathcal{L}{3D} flows through the diffusion model \u2192 merges into the unified latent \u2192 modifies the TorsionBERT &amp; Pairformer parameters (via LoRA) \u2192 updates adjacency if we let it.     \u2022   The gradient from \\mathcal{L} directly updates TorsionBERT's LoRA parameters, ensuring it accurately matches known angles.}</p> <p>Validation:     \u2022   Usually track final 3D RMSD or TM-score, plus angle-level MCQ or MAE.</p> <p>\u2e3b</p> <ol> <li>Detailed Implementation Plan for LoRA / QLoRA</li> </ol> <p>A. TorsionBERT with LoRA</p> <p>File(s) Potentially Affected: rna_predict/pipeline/stageB/torsion_bert_predictor.py     1.  Inject LoRA into BERT:     \u2022   If we use Hugging Face peft or a custom LoRA approach, we wrap the TorsionBert model to add \"low-rank adapters\" in attention and/or feed-forward layers.     \u2022   Keep a config like lora_r=4 or lora_alpha=16 to define the rank updates.     2.  Activating Grad for LoRA:     \u2022   Freeze all standard BERT parameters, let only LoRA adapter parameters have requires_grad=True.     \u2022   _init_lora_layers() function inserts the additional weight matrices for \\Delta W.     3.  Forward pass remains the same: input sequence \u2192 token embedding \u2192 [BERT + LoRA] \u2192 final hidden \u2192 regression for angles.     4.  Backward:     \u2022   Grad from \\mathcal{L}{\\text{angle}} and \\mathcal{L}{3D} flows into LoRA adapters.     \u2022   Weight updates occur only in the small rank modifications, saving memory.</p> <p>Architectural Decision:     \u2022   We must ensure the dimensionality of angle outputs remains the same. The top linear layer that projects hidden states to angle sin/cos can remain fully trainable or also get partial LoRA. Usually, we let it be fully trainable since it's small.</p> <p>\u2e3b</p> <p>B. Pairformer Trunk with LoRA</p> <p>File(s) Potentially Affected: Possibly a new subfolder models/pairformer_trunk/ or integrated in rna_predict/models/...     1.  Insert LoRA into Triangular Attention:     \u2022   For each block of the 48-block trunk, we freeze base attention weights (W_q, W_k, W_v, W_out) but add low-rank adapter layers that approximate the attention transformations.     \u2022   If we had a partial \"pretrained pairformer,\" we only adapt the \"LoRA-lized\" heads.     2.  Pair-bias:     \u2022   The adjacency bias can be a small linear transform. We can train that fully or also apply LoRA if it's large. Usually, it's small, so no LoRA needed.     3.  Output:     \u2022   Still produces \\mathbf{z}_{ij} and \\mathbf{s}_i.     \u2022   Grad from final 3D or pairwise constraints flows through these embeddings \u2192 modifies LoRA adapters.</p> <p>Architectural Decision:     \u2022   If the Pairformer is big, carefully define which layers get LoRA. Possibly only the later blocks for memory efficiency.</p> <p>\u2e3b</p> <p>C. Unified Latent Merger (ULM)</p> <p>File: possibly models/unified_latent_merger.py     1.  Combining Torsion + Pair:     \u2022   We parse a node-level embedding for each residue i from TorsionBERT. Another node-level embedding from Pairformer's s\u1d62. Possibly an edge-level embedding from z\u1d62\u2c7c.     \u2022   If we have adjacency, we either feed it in as a feature or let Pairformer handle it.     2.  LoRA:     \u2022   If this \"ULM\" is a small MLP or Transformer, we can either fully train it or embed LoRA if we want to keep it partially frozen. Typically we train it from scratch since it's a new bridging component.     3.  Output:     \u2022   A per-residue \"condition embedding\" fed into the diffusion, plus an optional per-(i,j) side channel for constraints.</p> <p>\u2e3b</p> <p>D. Diffusion Model with LoRA</p> <p>File: Possibly models/diffusion/angle_diffusion.py or rna_predict/models/diffusion.py     1.  Architecture:     \u2022   A UNet or Transformer-based diffusion. We apply it to 3D coordinates.     \u2022   Takes random/noisy coords + the merged latent. Each step refines coords.     2.  LoRA:     \u2022   If the diffusion model is large (like some advanced 3D Transformer), we can freeze the backbone and add LoRA. This is beneficial if we have a large checkpoint for diffusion pretrained on something else (e.g. a generative model from prior data).     3.  Loss:     \u2022   Typically a Denoising Score Matching or noise-prediction-based loss (like stable diffusion). The final step output can also be directly compared to ground-truth 3D coords.</p> <p>\u2e3b</p> <p>E. MP-NeRF or Forward Kinematics (Optional Stage C)     1.  Implementation:     \u2022   If used, each call is a simple geometry transform from angles to partial coords. Doesn't have big learnable parameters (just standard references).     \u2022   If you do have \"learnable geometry hack,\" it'd be minimal and likely not require LoRA.     2.  Backprop:     \u2022   The gradient from \\mathcal{L}{3D} or \\mathcal{L}{\\text{angle}} flows back through the trigonometric or matrix multiplication steps, ultimately reaching TorsionBERT's angle outputs.</p> <p>\u2e3b</p> <p>F. Energy Minimization (Post-Diffusion)     \u2022   Typically no direct backprop from the local minimization.     \u2022   We treat it as a separate script that polishes final coords or short MD runs.     \u2022   Because it's not integrated in the computational graph, it doesn't produce gradient signals upstream.</p> <p>\u2e3b</p> <ol> <li>Data Structures &amp; Configuration</li> </ol> <p>A. LoRA Parameterization</p> <p>Approach:     1.  For each major pretrained model (TorsionBERT, Pairformer, Diffusion trunk), we define a small config dict:</p> <p>lora:   r: 4   alpha: 16   dropout: 0.1   target_modules: [attention.W_q, attention.W_k, ...]</p> <pre><code>2.  We attach LoRA adapters using something like peft.LoraModel or a custom wrapper.\n</code></pre> <p>B. Residue Index &amp; Adjacency Storage</p> <p>Definition:     \u2022   ResidueIndexMap: List[int] to unify each stage's indexing if needed.     \u2022   adjacency: torch.Tensor shape [N, N], store base-pair probability or one-hot. Possibly use adj_soft for partial differentiability.</p> <p>\u2e3b</p> <ol> <li>Step-by-Step Backprop Flow<ol> <li>Diffusion final coords vs. ground-truth: \u2022   \\mathcal{L}_{3D} = RMSD(\\hat{X}, X_true). \u2022   The partial derivatives w.r.t. \\hat{X} pass back into the diffusion's UNet (some layers are LoRA).</li> <li>Unified Latent: \u2022   The UNet's gradient also flows into the latent that conditioned the diffusion. That triggers grads in the \"Latent Merger.\"</li> <li>Pairformer: \u2022   The portion of the latent derived from Pairformer's (z_ij, s_i) is updated. Because Pairformer is partially frozen except the LoRA layers, only LoRA weights get updated.</li> <li>TorsionBERT: \u2022   The portion from Torsion angles also sees grad if we used partial coords or if the final 3D is influenced by the torsion angles. \u2022   TorsionBERT's LoRA adapters update to better produce angles that yield correct final 3D coords.</li> <li>Angle Loss: \u2022   If we have direct angle supervision, that also updates TorsionBERT's LoRA weights.</li> </ol> </li> </ol> <p>Hence: We can effectively unify all sub-modules in a single graph, with local or global losses. The majority of large pretrained parameters remain frozen, while small rank-limited LoRA adapter weights get updated.</p> <p>\u2e3b</p> <ol> <li>Potential Implementation Steps</li> </ol> <p>(A) Codebase Reorganization (Optional):     \u2022   Create rna_predict/peft/ directory to store custom LoRA logic or integrate HF peft.     \u2022   For TorsionBERT, modify torsion_bert_predictor.py to wrap the BERT model with LoRA.</p> <p>(B) Pairformer Integration:     \u2022   If you have a partial \"pretrained Pairformer,\" define a PairformerLoRAAdapter that wraps each attention block.</p> <p>(C) Add a \"UnifiedPipeline\" script or class that orchestrates:     1.  Adjacency input     2.  TorsionBERT (LoRA) \u2192 angles     3.  Pairformer (LoRA) \u2192 pair embeddings     4.  Merger \u2192 latent     5.  Diffusion (LoRA optional) \u2192 final 3D     6.  Minimization is post-run</p> <p>(D) End-to-End Loss:     \u2022   Decide how to weigh angle-based vs. 3D-based terms.     \u2022   Possibly create small config in pyproject.toml or a JSON specifying the different \\lambda coefficients.</p> <p>(E) GPU/Memory:     \u2022   Because all these modules can be large, LoRA helps drastically.     \u2022   Double-check that you only keep the big pretrained weights in half precision or bfloat16, with minimal overhead for rank-limited updates.</p> <p>\u2e3b</p> <ol> <li>Summary of Architectural Decisions<ol> <li>Where to Insert LoRA: \u2022   TorsionBERT: good idea to freeze base, add LoRA to attention or feed-forward layers. \u2022   Pairformer trunk: same approach. \u2022   Diffusion model: only if it's large or pre-trained; else train from scratch if it's modest in size.</li> <li>Single vs. Multi-Loss: \u2022   Typically combine angle-level supervision with final 3D loss, to stabilize training.</li> <li>Optional Stage C: \u2022   The partial 3D from MP-NeRF is differentiable; backprop can refine angles. But if it's inaccurate or slow, skip it and let diffusion handle raw 3D from noise.</li> <li>Computational Efficiency: \u2022   We freeze 95% of parameters in TorsionBERT, Pairformer, and (optionally) Diffusion. We only train a small set of LoRA adapter parameters. This keeps VRAM usage manageable.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Conclusion &amp; Next Steps</li> </ol> <p>By combining LoRA-based partial fine-tuning of TorsionBERT, the Pairformer trunk, and (optionally) a diffusion generator, we enable an end-to-end pipeline where final 3D errors drive updates back into the angle and pair embeddings. The main steps are:     1.  Wrap TorsionBERT with LoRA adapters (freezing base).     2.  Wrap Pairformer with LoRA adapters (48-block attention).     3.  Optionally do the same for a large diffusion model or train a smaller diffusion from scratch.     4.  Construct a single composite forward pass hooking them together with a \"unified latent merger.\"     5.  Define \\mathcal{L}_{\\text{end-to-end}} with coordinate and/or angle supervision.     6.  Backward: Because each subcomponent is in the same computational graph, gradients reach the LoRA adapters.     7.  Energy Minimization is done offline, polishing final coordinates.</p> <p>This approach yields a memory-efficient training procedure thanks to LoRA's low-rank adaptation, letting you harness large pretrained models within a multi-stage, synergy-focused RNA 3D pipeline.</p> <p>==== Below is a fully updated, comprehensive design document that merges all four versions (V1\u2013V4) into a single, cohesive guide\u2014addressing their strengths, mitigating their weaknesses, and clarifying past criticisms. It is meant as technical documentation for building an end-to-end, LoRA-friendly RNA 3D structure prediction pipeline with:     1.  Torsion-based subpipeline (TorsionBERT).     2.  AlphaFold 3\u2013style Pairformer trunk.     3.  A Unified Latent Merger combining local angles + global pair embeddings.     4.  An optional forward kinematics step (Stage C) for partial 3D (using MP-NeRF or similar).     5.  A Diffusion model for final coordinate generation/refinement.     6.  A post-inference energy minimization pass.     7.  Support for LoRA (or QLoRA) to only finetune a small fraction of parameters in large pretrained networks.</p> <p>The result is more robust, synergistic, and memory-efficient than any single prior version\u2014truly a \"best-of-all-worlds\" solution.</p> <p>\u2e3b</p> <ol> <li>Grand Overview</li> </ol> <p>1.1 Core Objective</p> <p>Construct a single end-to-end RNA 3D predictor that:     \u2022   Generates local torsion angles from (sequence + adjacency).     \u2022   Extracts global pairwise constraints via an AF3-like Pairformer trunk (optionally leveraging an MSA).     \u2022   Merges these two representations into a \"unified latent.\"     \u2022   Optionally uses forward kinematics to produce partial 3D from the torsion angles.     \u2022   Employs a Diffusion model to produce final 3D coordinates, guided by both local angles and global pair embeddings.     \u2022   (Optionally) runs energy minimization or short MD to polish final geometry.</p> <p>Critically, large pretrained modules (TorsionBERT, Pairformer) remain frozen except for LoRA or QLoRA adapter layers\u2014thus drastically reducing memory usage.</p> <p>1.2 Data and Stage Flow</p> <p>(A) [Sequence + Adjacency + (Optional MSA)]            \u2514\u2500\u2500 TorsionBERT (LoRA) \u2192 angles                   \u2514\u2500\u2500 (Optional) Forward Kinematics \u2192 partial 3D            \u2514\u2500\u2500 Pairformer (LoRA) \u2192 pair embeddings z\u1d62\u2c7c + single s\u1d62            \u2514\u2500\u2500 Unified Latent Merger \u2192 \"merged latent\"            \u2514\u2500\u2500 Diffusion (LoRA optional) \u2192 final 3D coords            \u2514\u2500\u2500 (Optional) Energy Minimization \u2192 final polished coords</p> <pre><code>\u2022   Stage A: Adjacency can come from \"RFold\" or any other 2D structure method. Usually not backpropagated.\n\u2022   Stage B: TorsionBERT (LoRA) \u2192 angles.\n\u2022   Stage C (optional): Forward kinematics (MP-NeRF or standard NeRF) \u2192 partial 3D.\n\u2022   Stage D: Pairformer trunk (LoRA) \u2192 global pair embeddings.\n\u2022   Merger: Combines angles + adjacency + pair embeddings \u2192 final \"latent\" for diffusion.\n\u2022   Diffusion: Denoises random/noised coords into final 3D. Possibly partially or fully trained.\n\u2022   Energy Minimization: Polishing step with no direct gradient to the pipeline.\n</code></pre> <p>\u2e3b</p> <ol> <li>Mandatory vs. Optional Steps<ol> <li>Mandatory: \u2022   TorsionBERT for angles (Stage B). \u2022   Pairformer for pair embeddings. \u2022   Unified Latent so that Diffusion sees both local + global constraints. \u2022   Diffusion to generate final 3D coordinates.</li> <li>Strongly Recommended: \u2022   Adjacency from Stage A (or external) to feed TorsionBERT. \u2022   Energy Minimization at the end to correct small bond or steric issues.</li> <li>Truly Optional: \u2022   Forward Kinematics (Stage C) if you want partial 3D from torsion angles. \u2022   MSA: if available. Otherwise, Pairformer can run single-sequence mode. \u2022   Templates: Could also be integrated but not mandatory.</li> </ol> </li> </ol> <p>This ensures synergy: Torsion angles (local) + pair embeddings (global) must meet in the same pipeline. Adjacency is key for local angle constraints, though not strictly forced if you truly have no 2D data.</p> <p>\u2e3b</p> <ol> <li>Detailed Modules &amp; Design Choices</li> </ol> <p>3.1 TorsionBERT (Stage B) with LoRA     \u2022   Purpose: Predict backbone torsion angles {\\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi} from sequence + adjacency (optionally sugar pucker).     \u2022   Why Pretrained: TorsionBERT is typically a BERT-like language model, adapted for angle regression.     \u2022   LoRA:     \u2022   Freeze base weights, insert low-rank adapters in attention Q/K/V, or feed-forward blocks.     \u2022   Only these small adapter parameters get updated, keeping GPU memory usage modest.     \u2022   Output: (N, #angles) for an RNA of length N (plus sugar angles if you want \\nu_0..\\nu_4).</p> <p>Backprop Flow     \u2022   If we have an angle-level sub-loss (\\mathcal{L}{\\mathrm{angle}}), it directly updates LoRA layers in TorsionBERT.     \u2022   If we rely on final 3D loss (\\mathcal{L}), that gradient can also flow back to TorsionBERT via the diffusion \u2192 unify \u2192 angles chain.     \u2022   TorsionBERT indexing must remain consistent with the Pairformer's residue indexing (Residue 0..N\u20131).</p> <p>3.2 Pairformer (AF3-like) with LoRA     \u2022   Purpose: Provide global pair embeddings \\mathbf{z}_{ij} and single embeddings \\mathbf{s}_i from MSA or single sequence, optionally incorporating adjacency as a bias.     \u2022   LoRA:     \u2022   Large trunk (e.g. 48 blocks). We freeze the main trunk and only adapt a rank-limited set of parameters in each attention or feed-forward sub-layer.     \u2022   Output:     \u2022   (N\u00d7N, pair_dim) for pair embeddings,     \u2022   (N, single_dim) for single embeddings.</p> <p>Backprop Flow     \u2022   Gradients from \\mathcal{L}{3D} or from a pairwise sub-loss (\\mathcal{L}{\\mathrm{pair}}) update only the LoRA adapter weights, leaving the rest frozen.</p> <p>3.3 Unified Latent Merger     \u2022   Purpose: Combine TorsionBERT angles + adjacency + Pairformer embeddings \\mathbf{z}_{ij}, \\mathbf{s}_i into one \"conditioning latent\" for diffusion.     \u2022   Implementation:     \u2022   Possibly a small Transformer or MLP.     \u2022   We can train it fully (no need to freeze) or also apply LoRA if it's large.     \u2022   Output:     \u2022   A final latent representation for each residue (and possibly for residue pairs).     \u2022   Feeds the Diffusion as a \"condition.\"</p> <p>Indexing     \u2022   Must ensure TorsionBERT's residue i lines up with Pairformer's residue i, etc.     \u2022   Use a \"ResidueIndexMap\" if needed.</p> <p>3.4 Diffusion Module (Stage D)     \u2022   Purpose: Iteratively transform random/noised 3D coords (or partial coords from forward kinematics) into final 3D structure.     \u2022   Implementation:     \u2022   E.g. a 3D U-Net or GNN that at each step sees the \"unified latent\" as a condition.     \u2022   If it's large, partial freeze with LoRA. If smaller, train from scratch.     \u2022   Loss:     \u2022   Typically a diffusion denoising objective or a final L1/RMSD on the coordinates.     \u2022   The final \\mathcal{L}_{3D} is the main synergy enabler: it pushes all upstream modules to produce coherent angles, adjacency constraints, and pair embeddings.</p> <p>3.5 (Optional) Forward Kinematics     \u2022   Goal: Use predicted angles from TorsionBERT to compute partial 3D via MP-NeRF or standard NeRF.     \u2022   Pros: The diffusion starts from a partially folded conformation, possibly reducing required diffusion steps.     \u2022   Cons: If the angles are inaccurate, the diffusion might need to \"unfold\" it.     \u2022   Backprop:     \u2022   This geometry pipeline is fully differentiable, so final 3D errors can adjust TorsionBERT's angles.</p> <p>3.6 Energy Minimization     \u2022   Goal: Post-hoc local minimization or short MD run in a force field (Amber, CHARMM, or OpenMM).     \u2022   No gradient flows back; purely to fix small steric or bond-length errors.     \u2022   Typically done after inference, possibly across multiple diffusion samples to pick best.</p> <p>\u2e3b</p> <ol> <li>Multi-Level Loss Functions &amp; Training Approach</li> </ol> <p>4.1 Potential Loss Terms     1.  \\mathcal{L}{3D} (Final coordinate-based):     \u2022   Compare final predicted 3D to ground-truth, e.g.: \\mathcal{L}{3D} = \\mathrm{RMSD}\\bigl(\\hat{X}{\\mathrm{final}}, X\\bigr), or a distribution-based approach (like FAPE, distogram cross-entropy).     \u2022   Usually the primary synergy driver.     2.  \\mathcal{L}}{\\mathrm{angle}} (optional torsion supervision):     \u2022   If you have ground-truth angles for each residue, use a circular MSE or MCQ-based measure.     \u2022   Helps TorsionBERT remain physically consistent.     3.  \\mathcal{L} (optional adjacency or pair-distances):     \u2022   If the Pairformer is partially finetuned, we can guide it by known contact/distance constraints.}</p> <p>Weighted Sum:</p> <p>\\mathcal{L}{\\mathrm{total}} = \\lambda{3D}\\,\\mathcal{L}{3D}     \u2022   \\lambda}}\\,\\mathcal{L{\\mathrm{angle}}     \u2022   \\lambda Typical emphasis is on \\lambda{3D}.}}\\,\\mathcal{L}{\\mathrm{pair}</p> <p>4.2 End-to-End Backprop Path     1.  The final 3D error flows from the diffusion outputs \u2192 diffusion parameters \u2192 the unified latent \u2192 Pairformer + TorsionBERT LoRA \u2192 (optionally adjacency if we adopt a soft adjacency approach).     2.  If we do \\mathcal{L}{\\mathrm{angle}} or \\mathcal{L}{\\mathrm{pair}}, they also feed gradients directly into TorsionBERT or Pairformer LoRA layers.</p> <p>Hence: The entire pipeline can learn from 3D data alone, or from a combination of angles, pair constraints, and final coordinate errors.</p> <p>\u2e3b</p> <ol> <li>LoRA / QLoRA: Minimizing Memory</li> </ol> <p>5.1 Why LoRA     \u2022   TorsionBERT and the AF3 Pairformer trunk might each have hundreds of millions of parameters.     \u2022   LoRA adds a small rank-limited set of trainable weights to each large linear transform, drastically reducing GPU memory usage while still enabling backprop.     \u2022   For extremely large models, QLoRA can also quantize the base model to 4-bit or 8-bit, further shrinking memory footprint.</p> <p>5.2 Where to Insert LoRA     \u2022   TorsionBERT: Typically in each Transformer block's multi-head attention Q/K/V, or feed-forward layers.     \u2022   Pairformer: Similarly, in the triangular attention or pairwise transformations.     \u2022   Diffusion: Only if the diffusion model is large or partially pretrained. Otherwise, we can train it fully from scratch.     \u2022   Unified Merger: Usually small enough to train fully, but LoRA is optional if it's big.</p> <p>Example (pseudo-HF approach)</p> <p>from peft import LoraConfig, get_peft_model</p> <p>lora_cfg = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\",\"v_proj\"], ...) torsion_bert_lora = get_peft_model(pretrained_torsion_bert, lora_cfg)</p>"},{"location":"pipeline/integration/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/#freeze-base-weights-only-lora-adapters-are-trainable","title":"freeze base weights, only LoRA adapters are trainable","text":"<p>\u2e3b</p> <ol> <li>Implementation Steps: Putting It All Together</li> </ol> <p>Below is a unified approach that merges the deeper code-level detail (Version 1), synergy perspective (Version 2), stepwise memory/LoRA usage (Version 3), and final indexing clarity (Version 4).</p> <p>6.1 Overall Pipeline Construction     1.  Load the adjacency (Stage A) from an external predictor (RFold) or from data.     2.  Load TorsionBERT (with LoRA), freeze base weights:     \u2022   torsion_bert_lora = get_peft_model(...).     3.  (Optional) load or define a forward kinematics function (MP-NeRF):     \u2022   If used, produce partial 3D from TorsionBERT angles.     4.  Load Pairformer trunk (with LoRA):     \u2022   Possibly also freeze the main trunk.     5.  Implement a \"UnifiedLatentMerger\" that merges angles + adjacency + pair embeddings \u2192 final \"latent.\"     6.  Build or load the Diffusion model (LoRA if large, or train from scratch if small).     7.  In a single forward pass:     \u2022   Torsion angles \u2192 (FK \u2192 partial coords?).     \u2022   Pair embeddings z\u1d62\u2c7c.     \u2022   Merge into a final latent.     \u2022   Diffusion yields final 3D.     \u2022   Compare to ground-truth 3D (RMSD, L1, or distance-based).     8.  Loss is backpropagated:     \u2022   Only LoRA adapters in TorsionBERT + Pairformer + (optionally) Diffusion are updated.</p> <p>6.2 Example Training Loop</p> <p>model = FullRNA3DPipeline(     torsion_bert_lora,     pairformer_lora,     unify_module,     diffusion_module,     forward_kinematics=(use_fk) ) optimizer = torch.optim.AdamW(model.lora_params(), lr=1e-4)</p> <p>for batch in train_loader:     seq, adjacency, coords_gt, angles_gt, MSA = batch</p> <pre><code># Forward pass\nfinal_coords, loss_dict = model(seq, adjacency, MSA, coords_gt, angles_gt)\n\n# Weighted sum\ntotal_loss = (lambda_3D * loss_dict[\"3D_loss\"]\n              + lambda_angles * loss_dict[\"angle_loss\"])\ntotal_loss.backward()\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> <p>Memory is drastically reduced because we only keep gradient states for LoRA adapter matrices.</p> <p>\u2e3b</p> <ol> <li>Validation &amp; Ensemble Refinement</li> </ol> <p>7.1 Validation Metrics     \u2022   Angle-level: circular MSE or MCQ.     \u2022   Pair-level: if pair supervision is available, contact or distogram accuracy.     \u2022   3D-level: RMSD, GDT, or specialized RNA metrics. Possibly sugar pucker accuracy.     \u2022   Possibly break down by region (stems vs. loops vs. single-stranded segments).</p> <p>7.2 Ensemble &amp; Minimization     \u2022   After training, we can generate multiple final 3D structures by random seeding the diffusion or sampling.     \u2022   Energy Minimization: run each through a short local MD or minimization to fix small geometry.     \u2022   Rank by a geometry score or an internal model confidence measure.     \u2022   Output top 5 or a single best structure.</p> <p>\u2e3b</p> <ol> <li>Advantages Over Earlier \"Versions\"<ol> <li>Multi-Loss synergy (from V1): We incorporate angle, pair, and final 3D constraints in a single pipeline.</li> <li>High-level clarity (from V2): Emphasizes that final 3D backprop unifies TorsionBERT + Pairformer.</li> <li>Implementation practicalities (from V3): We detail LoRA injection points, stagewise training, memory usage tips, micro-batching, etc.</li> <li>Indexing and adjacency (from V4): We specify the importance of consistent residue numbering, highlight optional forward kin, and mention adjacency usage in both TorsionBERT and Pairformer.</li> <li>Explicit mention of sugar ring angles, potential re-labeled residues, partial or full training approach, plus chunking or micro-batching for large RNAs.</li> </ol> </li> </ol> <p>By combining all these points, we address the criticisms from earlier versions:     \u2022   We keep the pipeline synergy (no \"lost synergy\" from making everything optional).     \u2022   We specify how adjacency can feed both TorsionBERT and Pairformer.     \u2022   We highlight a single \"ResidueIndexMap\" for alignment.     \u2022   We detail how the \"Unified Latent Merger\" is more robust than a simple MLP approach.     \u2022   We incorporate a short local minimization step at the end for geometry polishing.</p> <p>\u2e3b</p> <ol> <li>Conclusions &amp; Best Practices</li> </ol> <p>Key Guidance:     1.  Use LoRA: It's essential for large pretrained TorsionBERT / Pairformer. Keep them in half precision or even 4-bit QLoRA if extremely large.     2.  Define Weighted Loss: Typically \\mathcal{L}{3D} is the main driver. If you have angle ground truth, do \\mathcal{L}{\\mathrm{angle}} to speed convergence.     3.  Forward Kinematics: Optionally do partial 3D from angles. If the angles are decent, it helps the diffusion. If they're poor, it might hamper training.     4.  Energy Minimization: Great as a final step, not part of backprop.     5.  Indexing: Absolutely ensure consistent residue numbering across TorsionBERT and Pairformer.     6.  Sampling: If the pipeline is large, do gradient checkpointing or micro-batching to avoid out-of-memory issues.</p> <p>Outcome: A single end-to-end pipeline that merges local angle constraints, global pair constraints, a final generative diffusion, and a partial post-processing step for geometry smoothing\u2014maximizing synergy and controlling memory via LoRA.</p>"},{"location":"pipeline/integration/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/#thus-this-final-design-stands-as-a-verbose-cohesive-and-thoroughly-integrated-architecture-that-surpasses-any-individual-version-14-by-merging-their-best-features-and-clarifications-into-one-complete-technical-document","title":"Thus, this final design stands as a verbose, cohesive, and thoroughly integrated architecture that surpasses any individual \"Version 1\u20134\" by merging their best features and clarifications into one complete technical document.","text":""},{"location":"pipeline/integration/full_pipeline_specification/","title":"Full Specification","text":"<p>Unified RNA 3D Prediction Pipeline: \u201cBest-of-All-Worlds\u201d Technical Documentation</p> <p>Below is a comprehensive, high-level-to-concrete technical guide that merges and improves upon the previous versions (V1\u2013V5). It addresses criticisms from each, consolidates their strengths, and incorporates a cohesive plan for seamlessly integrating LoRA, Pairformer embeddings, a Unified Latent Merger, and Diffusion (Stage\u202fD). It is verbose and detailed\u2014designed to serve as a robust blueprint for developers working on an end-to-end RNA structure prediction pipeline.</p> <p>\u2e3b</p> <ol> <li>Introduction &amp; Objectives</li> </ol> <p>This \u201cbest-of-all-worlds\u201d architecture aims to unify the entire multi-stage RNA 3D pipeline:     1.  Stage A: 2D adjacency predictor (e.g., RFold or an external method).     2.  Stage B:     \u2022   TorsionBERT (with LoRA) \u2192 local angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, \u2026).     \u2022   Pairformer (with LoRA) \u2192 global pairwise embeddings \\mathbf{z}_{ij} + single embeddings \\mathbf{s}_i.     3.  (Optional) Stage C: MP-NeRF or partial 3D geometry reconstruction from angles.     4.  Unified Latent Merger: Merges adjacency, angles, partial coords, Pairformer outputs into a single \u201cunified latent\u201d representation.     5.  Stage D: Diffusion-based refinement (with LoRA optional), which takes the unified latent as conditioning to generate final 3D coordinates.     6.  (Optional) Energy Minimization: A post-hoc step for short local refinement in a force field.</p> <p>Key motivations:     \u2022   Bring local angle predictions and global pair embeddings into a single representation.     \u2022   Support LoRA to minimize GPU memory usage in large pretrained models (TorsionBERT, Pairformer).     \u2022   Achieve synergy by letting the Diffusion step see both adjacency-based local constraints and Pairformer-driven global context.     \u2022   Provide a single top-level pipeline function so users can run from sequence \u2192 final 3D with minimal confusion.</p> <p>\u2e3b</p> <ol> <li>High-Level Data Flow<pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Stage A \u2502\n    \u2502Adjacency\u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 adjacency_matrix\n         v\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Stage B Combined Runner (Torsion+Pair)    \u2502 \u2502  - TorsionBERT (LoRA) -&gt; angles      \u2502 \u2502  - Pairformer (LoRA) -&gt; (s, z)       \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 {torsion_angles, s, z}  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 (Optional) Stage C: MP-NeRF/ partial 3D \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 partial_coords (optional)  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Unified Latent Merger (angles, s, z,  \u2502 \u2502 adjacency, partial_coords) -&gt; unified \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 unified_latent  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Stage D Diffusion (LoRA optional)        \u2502 \u2502  Condition on 'unified_latent'           \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 final_3D_coords  v  (Optional) Energy Minimization &amp; Output</p> </li> </ol> <p>\u2e3b</p> <ol> <li>LoRA Integration</li> </ol> <p>3.1 Where LoRA is Applied     1.  TorsionBERT: We load a base BERT-like model (for angle regression) and freeze its main weights. Insert LoRA in attention or feed-forward layers.     2.  Pairformer: The large trunk (TriangleAttention, TriangularMultiplication blocks). We freeze base layers, attach LoRA to minimal modules.     3.  (Optional) Diffusion: If we have a large pretrained diffusion model, we can also freeze and inject LoRA.</p> <p>3.2 Implementation Sketch</p> <p>In practice, you might create rna_predict/peft/lora_utils.py with a function like:</p> <p>from peft import LoraConfig, get_peft_model</p> <p>def apply_lora(model, lora_cfg):     # lora_cfg might contain r, alpha, dropout, target_modules, etc.     lora_config = LoraConfig(**lora_cfg)     return get_peft_model(model, lora_config)</p> <p>Then in TorsionBertPredictorWithLoRA:</p> <p>class TorsionBertPredictorWithLoRA:     def init(self, model_name_or_path, lora_cfg, device=\"cpu\", angle_mode=\"sin_cos\"):         # load base TorsionBERT         self.base_model = TorsionBertModel(model_name_or_path, device=device)         # apply LoRA to self.base_model.model (the underlying HF model)         self.model = apply_lora(self.base_model.model, lora_cfg)         # freeze base         for name, param in self.base_model.model.named_parameters():             if \"lora\" not in name.lower():                 param.requires_grad = False         # store angle_mode, device, etc.         self.angle_mode = angle_mode         self.device = device</p> <pre><code>def __call__(self, sequence, adjacency=None):\n    # same logic as StageBTorsionBertPredictor\n    out = self.base_model.predict_angles_from_sequence(sequence)\n    # convert sin/cos if needed\n    return {\"torsion_angles\": out}  # e.g. shape [N, 2 * num_angles]\n</code></pre> <p>A similar approach is used for PairformerWithLoRA.</p> <p>\u2e3b</p> <ol> <li>Stage-by-Stage Implementation Outline</li> </ol> <p>4.1 Stage A: Adjacency</p> <p>We assume a script (e.g. rna_predict/pipeline/stageA/run_stageA.py) that can produce an [N, N] adjacency. Possibly:</p> <p>def run_stageA(sequence: str, predictor, device=\"cpu\") -&gt; torch.Tensor:     # predictor might be StageARFoldPredictor or a wrapper     adjacency_matrix = predictor.predict_adjacency(sequence)     return adjacency_matrix.to(device)</p> <p>In the advanced pipeline: We only need the adjacency as a 2D float/bool tensor for the subsequent steps.</p> <p>4.2 Stage B: Combined Runner</p> <p>Create run_stageB_combined.py. This merges TorsionBERT + Pairformer:</p> <p>def run_stageB_combined(     sequence: str,     adjacency_matrix: torch.Tensor,     torsion_bert_model,  # TorsionBertPredictorWithLoRA     pairformer_model,     # PairformerWithLoRA     device=\"cpu\" ) -&gt; dict:     # 1) Torsion angles     torsion_output = torsion_bert_model(sequence, adjacency=adjacency_matrix)     angles = torsion_output[\"torsion_angles\"].to(device)  # shape [N, ...]</p> <pre><code># 2) Prepare input for Pairformer\n# e.g. create initial_s [1, N, c_s], initial_z [1, N, N, c_z], pair_mask [1, N, N]\n# possibly incorporate adjacency as bias\n\n# 3) Run Pairformer -&gt; s_embeddings, z_embeddings\ns_updated, z_updated = pairformer_model(initial_s, initial_z, pair_mask)\n\nreturn {\n  \"torsion_angles\": angles,   # [N, angle_dim]\n  \"s_embeddings\": s_updated.squeeze(0), # [N, c_s]\n  \"z_embeddings\": z_updated.squeeze(0)  # [N, N, c_z]\n}\n</code></pre> <p>Key Points:     \u2022   You must define how initial_s and initial_z are constructed (some shape [1, N, c_s], [1, N, N, c_z]).     \u2022   If you have an MSA, incorporate that or fallback to single-sequence mode.     \u2022   If adjacency is used for Pairformer attention bias, you add it to z_init or inside the Pairformer code.</p> <p>4.3 (Optional) Stage C: MP-NeRF</p> <p>If the pipeline uses partial 3D from angles:</p> <p>def run_stageC(sequence, torsion_angles, method=\"mp_nerf\", device=\"cpu\", **kwargs):     # e.g., build_scaffolds_rna_from_torsions -&gt; rna_fold -&gt; coords     # Return {\"coords\": shape [N, #atoms, 3], \"atom_count\": ...}     ...</p> <p>Note: This step can be skipped if you let the diffusion start from random noise.</p> <p>4.4 Unified Latent Merger</p> <p>We create a flexible module, e.g. in merger/unified_latent_merger.py.</p> <p>class SimpleUnifiedLatentMerger(nn.Module):     def init(self, angle_dim, s_dim, z_dim, hidden_dim, output_dim):         super().init()         # Various sub-layers         # Potential adjacency + partial_coords processing</p> <pre><code>def forward(self, angles, adjacency, s_embeddings, z_embeddings, partial_coords=None):\n    # merges into unified_latent\n    return unified_latent\n</code></pre> <p>Implementation Details:     \u2022   Possibly embed angles with nn.Linear(angle_dim, hidden_dim).     \u2022   Convert adjacency [N,N] into node features [N,1] by row sum or a GNN layer.     \u2022   Pool z [N,N,c_z] -&gt; [N,c_z].     \u2022   Concatenate all. You get [N, \\text{some_total_dim}].     \u2022   Possibly run a small MLP or Transformer block.     \u2022   Produce final shape [N, output_dim] or a single global [output_dim].</p> <p>4.5 Stage D: Diffusion</p> <p>Adapt your diffusion manager to accept \u201cunified_latent\u201d:</p> <p>def run_stageD_diffusion(     partial_coords: Optional[torch.Tensor],     unified_latent: torch.Tensor,     diffusion_manager,     device=\"cpu\",     inference_steps=20 ):     # 1) if partial_coords is None, create random noise     # 2) pass unified_latent as condition     final_coords = diffusion_manager.inference_conditioned(         coords_init=partial_coords,         conditioning_latent=unified_latent,         steps=inference_steps     )     return final_coords</p> <p>In practice: If your code uses s_trunk, z_trunk, you might do {\"unified_latent\": ...} or merge them inside. The actual method in ProtenixDiffusionManager must be changed to handle that single latent.</p> <p>\u2e3b</p> <ol> <li>A \u201cFull Pipeline\u201d Orchestrator</li> </ol> <p>5.1 Proposed File: rna_predict/run_full_pipeline.py</p> <p>Pseudocode:</p> <p>def run_full_pipeline(sequence, config, device=\"cuda\"):     # Stage A     adjacency = run_stageA(sequence, stageA_predictor, device=device)</p> <pre><code># Stage B\nb_outputs = run_stageB_combined(\n    sequence, adjacency,\n    torsion_bert_model,\n    pairformer_model,\n    device=device\n)\nangles = b_outputs[\"torsion_angles\"]\ns_emb = b_outputs[\"s_embeddings\"]\nz_emb = b_outputs[\"z_embeddings\"]\n\n# Stage C (optional)\npartial_coords = None\nif config[\"use_stageC\"]:\n    partial_coords_out = run_stageC(sequence, angles, device=device)\n    partial_coords = partial_coords_out[\"coords\"]\n\n# Merger\nunified_latent = merger_module(\n    angles, adjacency, s_emb, z_emb, partial_coords\n)\n\n# Stage D\nfinal_coords = run_stageD_diffusion(\n    partial_coords, unified_latent, diffusion_manager, device=device,\n    inference_steps=config[\"stageD\"][\"n_steps\"]\n)\n\n# (Optional) Minimization\nif config.get(\"run_minimization\"):\n    final_coords = run_energy_minimization(final_coords, config[\"minimization\"])\n\nreturn final_coords\n</code></pre> <p>Config can store file paths for TorsionBERT &amp; Pairformer LoRA checkpoints, adjacency predictor settings, etc.</p> <p>\u2e3b</p> <ol> <li>Key Architectural Details and Decisions<ol> <li>Index Consistency \u2022   Ensure Stage A\u2019s adjacency matrix matches sequence indices used by TorsionBERT, Pairformer, MP-NeRF, etc. A single reference for residue indexing is critical.</li> <li>LoRA Implementation \u2022   Each sub-model is loaded in a partially-frozen mode with small LoRA adapters. \u2022   Keep a method like get_trainable_parameters() to only optimize LoRA layers, or rely on PEFT\u2019s built-in parameter filtering.</li> <li>Pooling of z-embeddings \u2022   In the simplest approach, we do a row-wise average of z_{ij} across j to get a node-level feature for each residue i. More advanced methods might do specialized GNN or attention.</li> <li>Stage D Condition \u2022   By default, older code might expect s_trunk or z_trunk. We now unify them with unified_latent. The diffusion code must be updated accordingly\u2014 e.g., if it had a line like:</li> </ol> </li> </ol> <p>condition = self.model.build_condition(s_trunk, z_trunk, ...)</p> <p>it becomes:</p> <p>condition = self.model.build_condition(unified_latent=unified_latent, ...)</p> <p>The details are up to the existing diffusion architecture.</p> <pre><code>5.  Memory\n\u2022   The pipeline can get large. Use gradient checkpointing or micro-batching in Pairformer.\n\u2022   Keep angles as minimal float32 or float16.\n\u2022   LoRA helps reduce training memory by only learning small rank updates.\n6.  Angle Format\n\u2022   TorsionBERT might output sin/cos pairs or direct angles in degrees. The MP-NeRF pipeline might require radians. Either unify them or carefully convert in the Stage B \u2192 Stage C handoff.\n7.  Energy Minimization\n\u2022   If you have PyRosetta, OpenMM, or MDAnalysis, define a function run_energy_minimization(coords, config). This is purely optional, but recommended for final polishing.\n</code></pre> <p>\u2e3b</p> <ol> <li>Example Repository Layout</li> </ol> <p>rna_predict/ \u251c\u2500\u2500 pipeline \u2502   \u251c\u2500\u2500 stageA \u2502   \u2502   \u2514\u2500\u2500 run_stageA.py \u2502   \u251c\u2500\u2500 stageB \u2502   \u2502   \u251c\u2500\u2500 run_stageB_combined.py    # NEW \u2502   \u2502   \u251c\u2500\u2500 torsion \u2502   \u2502   \u2502   \u2514\u2500\u2500 torsion_bert_lora.py  # LoRA-enabled TorsionBERT \u2502   \u2502   \u2514\u2500\u2500 pairwise \u2502   \u2502       \u251c\u2500\u2500 pairformer_lora.py    # LoRA-enabled Pairformer \u2502   \u2502       \u2514\u2500\u2500 pairformer_wrapper.py # existing code, adapted \u2502   \u251c\u2500\u2500 stageC \u2502   \u2502   \u2514\u2500\u2500 stage_c_reconstruction.py \u2502   \u251c\u2500\u2500 stageD \u2502   \u2502   \u2514\u2500\u2500 run_stageD_unified.py     # or run_stageD_diffusion.py \u2502   \u2514\u2500\u2500 merger \u2502       \u2514\u2500\u2500 unified_latent_merger.py  # NEW \u251c\u2500\u2500 run_full_pipeline.py              # NEW orchestrator \u251c\u2500\u2500 peft \u2502   \u2514\u2500\u2500 lora_utils.py \u2514\u2500\u2500 postprocess     \u2514\u2500\u2500 energy_minimization.py</p> <p>\u2e3b</p> <ol> <li>Final Guidance, Next Steps<ol> <li>Implementation: \u2022   Start by creating placeholders for each new file. Copy in the pseudocode or code stubs from above. \u2022   Validate shapes at each step by printing tensor shapes\u2014especially adjacency, angles, s/z embeddings, partial coords, final coords. \u2022   Integrate your actual adjacency predictor code from Stage A. \u2022   Integrate real TorsionBERT &amp; Pairformer model loading with LoRA. \u2022   Flesh out the Diffusion manager so it can read unified_latent.</li> <li>Testing: \u2022   Write unit tests for each stage (A, B, Merger, C, D). Then add an integration test that calls run_full_pipeline on a short synthetic sequence (like \u201cACGUACG\u201d) and checks for shape correctness.</li> <li>Performance: \u2022   If training, ensure you only optimize LoRA parameters. Double-check memory usage. \u2022   If your pipeline is large, use half precision or BF16 on a modern GPU.</li> <li>Refinement: \u2022   Once you get the pipeline working on small RNAs, do QA metrics (RMSD, pLDDT equivalents). \u2022   Tweak adjacency usage, z pooling, or unify angle modes if the geometry fails.</li> </ol> </li> </ol> <p>\u2e3b</p> <p>Conclusion</p> <p>This document merges the strengths of Versions 1\u20135:     \u2022   It provides file-by-file structural guidance (V1, V3, V4).     \u2022   It includes concrete code snippets and final orchestrator pseudocode (V2, V5).     \u2022   It addresses LoRA hooking points, data shape alignment, synergy with partial 3D, and an integrated pipeline function\u2014all in a single, verbose reference.</p> <p>Next Steps: Implement the placeholders, ensure shapes align, confirm the diffusion model sees the unified_latent, and incorporate LoRA in your TorsionBERT &amp; Pairformer code. Once complete, you\u2019ll have a fully functional Stage A \u2192 B \u2192 (C) \u2192 Merger \u2192 D pipeline that reflects the advanced synergy described in your original design documents\u2014truly better than the sum of its parts.</p> <p>Below is a comprehensive technical document that unifies the best qualities of all four previous versions (V1\u2013V4) while addressing their criticisms. It includes a detailed, color-coded Mermaid diagram referencing specific code modules (as in Version\u202f3), visual clarity (Version\u202f2), top-down flow (Version\u202f1 &amp; V4), optional stages, LoRA integration points, shape details, and unified latent synergy\u2014ultimately creating a verbose, all-in-one technical overview of the multistage RNA 3D structure prediction pipeline.</p> <p>\u2e3b</p> <p>Multistage RNA 3D Structure Prediction Pipeline (Comprehensive Final Version)</p> <p>High-Level Summary</p> <p>We have a five-step pipeline for generating RNA 3D structures:     1.  Stage A \u2013 Predict 2D Adjacency (e.g., via RFold).     2.  Stage B \u2013 Torsion &amp; Pair Embeddings:     \u2022   TorsionBERT (LoRA-enabled) to get backbone torsion angles.     \u2022   Pairformer (LoRA-enabled) to generate single-residue and pairwise embeddings.     3.  Stage C \u2013 (Optional) Partial 3D Reconstruction (e.g., MP-NeRF), using torsions to build backbone coords.     4.  Unified Latent Merger \u2013 Combine adjacency, torsions, partial coords, single/pair embeddings into a single \u201clatent.\u201d     5.  Stage D \u2013 Diffusion (LoRA-optional) for final 3D refinement, optionally followed by short energy minimization.</p> <p>Implementation Files:     \u2022   Stage A: rna_predict/pipeline/stageA/run_stageA.py, rfold_predictor.py, model.py     \u2022   Stage B:     \u2022   Torsion: torsion/torsion_bert_predictor.py, torsionbert_inference.py     \u2022   Pairformer: pairwise/pairformer_wrapper.py, pairformer.py     \u2022   Stage C: stage_c_reconstruction.py + mp_nerf/rna.py + final_kb_rna.py     \u2022   Unified Latent: could be a simple MLP or attention block (not always singled out in code)     \u2022   Stage D: stageD/diffusion/, protenix_diffusion_manager.py, generator.py, diffusion.py</p> <p>Shape Conventions:     \u2022   Adjacency: [N, N] (binary or real-valued).     \u2022   Torsion Angles: [N, K] (e.g., K=7 for alpha..zeta + chi) or [N, 2*K] if sin/cos.     \u2022   Single Embeddings: [N, c_s]     \u2022   Pair Embeddings: [N, N, c_z]     \u2022   Partial 3D coords: [N, #atoms, 3]     \u2022   Diffusion: might internally handle [batch, N_sample, N_atom, 3] arrays, plus trunk embeddings.</p> <p>\u2e3b</p> <p>Comprehensive Mermaid Diagram</p> <p>Below is a left-to-right color-coded diagram with subgraphs for each stage, referencing code modules, shape details, LoRA usage, and optional steps. You can copy this into a Mermaid-compatible environment to view a rendered version.</p> <p>flowchart LR</p> <pre><code>%% -------------------------------------------------\n%% STYLES\n%% -------------------------------------------------\nclassDef stageA fill:#bbdefb,stroke:#1a237e,stroke-width:2px,color:#000\nclassDef stageB fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\nclassDef stageC fill:#fff9c4,stroke:#fdd835,stroke-width:2px,color:#000\nclassDef unify fill:#e1bee7,stroke:#6a1b9a,stroke-width:2px,color:#000\nclassDef stageD fill:#f8bbd0,stroke:#ad1457,stroke-width:2px,color:#000\nclassDef optional fill:#cfd8dc,stroke:#455a64,stroke-width:2px,color:#000,stroke-dasharray:5 5\nclassDef data fill:#ffffff,stroke:#999999,stroke-width:1px,color:#000,rx:5,ry:5\nclassDef code fill:#f5f5f5,stroke:#999999,stroke-width:1px,color:#000,stroke-dasharray:3 3\n\n%% -------------------------------------------------\n%% INPUTS\n%% -------------------------------------------------\nS((RNA Sequence)):::data\n\n%% ========== Stage A Subgraph ==========\nsubgraph A_subgraph [**Stage A**: 2D Adjacency Prediction (RFold)]\ndirection TB\nclass A_subgraph stageA\n\nA1[[run_stageA.py\\n(rfold_predictor.py / model.py)]:::code]\nA2((Adjacency NxN)):::data\nS --&gt; A1\nA1 --&gt; A2\nend\n\n%% ========== Stage B Subgraph ==========\nsubgraph B_subgraph [**Stage B**: TorsionBERT &amp; Pairformer (LoRA)]\ndirection TB\nclass B_subgraph stageB\n\nB1[[TorsionBert Predictor\\n(torsion_bert_predictor.py)\\nLoRA-enabled]]:::code\nB2((Torsion Angles\\n[N,K or N,2K])):::data\n\nB3[[Pairformer\\n(pairformer_wrapper.py)\\nLoRA-enabled]]:::code\nB4((Single Embs s:\\n[N, c_s])):::data\nB5((Pair Embs z:\\n[N,N,c_z])):::data\n\nS --&gt; B1\nA2 -. optional .-&gt; B1\nB1 --&gt; B2\n\nS --&gt; B3\nA2 -. optional .-&gt; B3\nB3 --&gt; B4\nB3 --&gt; B5\n\nend\n\n%% ========== Stage C Subgraph ==========\nsubgraph C_subgraph [**Stage C** (Optional): MP-NeRF Partial 3D]\ndirection TB\nclass C_subgraph stageC\n\nC1[[stage_c_reconstruction.py\\n+ mp_nerf/rna.py\\n+ final_kb_rna.py]]:::code\nC2((Partial 3D Coords\\n[N, #atoms, 3])):::data\n\nB2 --&gt; C1\nS --&gt; C1\nC1 --&gt; C2\nend\nclass C_subgraph optional\n\n%% ========== Unified Latent Subgraph ==========\nsubgraph M_subgraph [Unified Latent Merger]\nclass M_subgraph unify\n\nM1[[Merge angles,\\nadjacency, s, z,\\npartial coords]]:::code\nM2((Unified\\nLatent)):::data\nend\n\n%% Connect them\nA2 --&gt; M1\nB2 --&gt; M1\nB4 --&gt; M1\nB5 --&gt; M1\nC2 -. optional .-&gt; M1\nM1 --&gt; M2\n\n%% ========== Stage D Subgraph ==========\nsubgraph D_subgraph [**Stage D**: Diffusion Refinement]\ndirection TB\nclass D_subgraph stageD\n\nD1[[ProtenixDiffusionManager\\n(protenix_diffusion_manager.py)\\n+ DiffusionModule\\nLoRA optional]]:::code\nD2((Final 3D Structures\\n(N, #atoms, 3)\\nor multiple)):::data\nM2 --&gt; D1\nD1 --&gt; D2\nend\n\n%% ========== Post-Processing Subgraph ==========\nsubgraph PP_subgraph [Optional Post-Processing: Energy Minimization]\nclass PP_subgraph optional\ndirection TB\nPP1[[Local MD / Minimization\\ne.g. OpenMM, GROMACS]]:::code\nPP2((Polished 3D\\nStructure(s))):::data\nD2 --&gt; PP1\nPP1 --&gt; PP2\nend\n\n%% -------------------------------------------------\n%% STYLING\n%% -------------------------------------------------\nlinkStyle default stroke-width:2px,fill:none,stroke:#888\n</code></pre> <p>Diagram Explanation     1.  Input     \u2022   RNA Sequence S((\u2026)): A raw string representing nucleotides.     2.  Stage\u202fA: Adjacency (RFold)     \u2022   In run_stageA.py + rfold_predictor.py, the pipeline obtains an adjacency matrix [N, N].     \u2022   This matrix typically indicates base-pair contacts. Optionally fed into Stage\u202fB if the TorsionBERT or Pairformer uses adjacency as a feature.     3.  Stage\u202fB: TorsionBERT + Pairformer     1.  TorsionBERT (LoRA)     \u2022   Reads the RNA sequence and optionally adjacency, producing backbone torsion angles [N, K] or [N, 2K] if sin/cos.     \u2022   Code references: torsion_bert_predictor.py, torsionbert_inference.py.     2.  Pairformer (LoRA)     \u2022   Potentially uses the same sequence and adjacency to generate single [N, c_s] and pair [N, N, c_z] embeddings (like z_trunk).     \u2022   Code references: pairformer_wrapper.py, pairformer.py.     4.  Stage\u202fC (Optional): Partial 3D     \u2022   If used, we pass torsion angles + sequence + standard geometry from final_kb_rna.py into mp_nerf/rna.py or stage_c_reconstruction.py.     \u2022   Produces partial 3D coords [N, #atoms, 3], typically backbone only.     \u2022   This is optional; the pipeline can skip it and rely purely on Diffusion or initial random coords.     5.  Unified Latent Merger     \u2022   Merges everything: adjacency, angles, partial coords, single/pair embeddings.     \u2022   This synergy can be an MLP or a small attention block. Usually not a separate file, but references are integrated in stageD or a separate \u201cmerger\u201d class.     \u2022   Yields a single \u201cunified latent\u201d vector or array [N, ...] used by the next stage.     6.  Stage\u202fD: Diffusion     \u2022   The ProtenixDiffusionManager plus the DiffusionModule (optionally LoRA-enabled) use the unified latent as a condition to refine or generate final 3D coordinates.     \u2022   The code references: rna_predict/pipeline/stageD/diffusion/.py (including generator.py, protenix_diffusion_manager.py, diffusion.py).     \u2022   Produces final 3D coordinates [N, #atoms, 3] or an ensemble from multiple samples.     7.  Optional Energy Minimization     \u2022   Tools like OpenMM or GROMACS for local minimization or short MD runs, producing a final polished structure.     \u2022   Often done in a separate script or environment, not strictly part of the Python pipeline code.</p> <p>\u2e3b</p> <p>Detailed Stage-by-Stage Description</p> <p>Stage A: Adjacency (2D)     \u2022   Code: run_stageA.py, rfold_predictor.py, referencing an RFold_Model in model.py.     \u2022   Input: RNA sequence (e.g. \"AUGCA...\").     \u2022   Output: adjacency \u2208 \u211d^(N\u00d7N) (binary or probability).     \u2022   Comment: Typically no LoRA is used here, though you could do so if your adjacency predictor is large.</p> <p>Stage B: Torsion &amp; Pair Embeddings     1.  TorsionBERT     \u2022   LoRA: partial fine-tuning if model_name is huge (\"sayby/rna_torsionbert\").     \u2022   Produces angles in either sin/cos or direct rad/deg.     \u2022   Key shapes: [N, 2\u00d7num_angles] or [N, num_angles].     2.  Pairformer     \u2022   LoRA: partial fine-tuning again.     \u2022   Generates single embeddings s [N, c_s] and pair embeddings z [N, N, c_z].     \u2022   Possibly uses adjacency as a \u201cbias\u201d to handle base-pair info or skip if not needed.</p> <p>Stage C: (Optional) Partial 3D Reconstruction     \u2022   Code: stage_c_reconstruction.py \u2192 calls mp_nerf/rna.py, plus geometry from final_kb_rna.py.     \u2022   Input: Torsion angles + (optionally) adjacency or other constraints.     \u2022   Output: partial coords, typically [N, #atoms, 3] if building a backbone. This can be used as an initial conformation for Diffusion or a final fallback if Stage\u202fD is skipped.     \u2022   Sugar Pucker: default \u201cC3\u2032-endo\u201d for standard A-form. Could also handle \u201cC2\u2032-endo\u201d or ring closure logic.</p> <p>Unified Latent Merger     \u2022   Combines:     1.  Torsion angles     2.  Adjacency [N, N]     3.  Single embeddings [N, c_s] + pair embeddings [N, N, c_z]     4.  Possibly partial coords [N, #atoms, 3]     \u2022   Typically an MLP or small attention-based aggregator that outputs a single \u201cconditioning latent.\u201d Not always singled out as a separate .py, but recognized conceptually for synergy.</p> <p>Stage D: Diffusion Refinement     \u2022   Code: stageD/diffusion/, e.g. protenix_diffusion_manager.py, generator.py, diffusion.py.     \u2022   LoRA: optional if the base diffusion model is large.     \u2022   Process:     1.  Possibly start from partial coords (Stage\u202fC) or random noise.     2.  Condition on the \u201cunified latent.\u201d     3.  Iteratively denoise to generate refined 3D coords [N, #atoms, 3]. Possibly produce multiple samples.     \u2022   Output: final or near-final 3D structure(s).</p> <p>Post-Processing (Optional)     \u2022   Might run short local MD in OpenMM or GROMACS to fix minor geometry or steric issues.     \u2022   Not strictly in the code, but invoked for final polishing.     \u2022   If used, it yields an improved structure (lowest-energy or an ensemble).</p> <p>\u2e3b</p> <p>Why This Comprehensive Diagram Excels     1.  Complete Flow:     \u2022   We integrate the straightforward top-down approach (V1, V4) with color-coded subgraphs (V2) plus code references and shape details (V3).     2.  LoRA Markings:     \u2022   TorsionBERT &amp; Pairformer are explicitly shown as LoRA-enabled; Diffusion\u2019s LoRA is noted.     3.  Implementation Mapping:     \u2022   We reference .py files and configuration references (like model.py, protenix_diffusion_manager.py), for a developer-friendly approach.     4.  Optional Paths:     \u2022   Stage C (MP-NeRF) is visually \u201coptional,\u201d connected with a dashed arrow.     \u2022   Post-processing is also a separate optional subgraph.     5.  Shape / Data:     \u2022   Key data artifacts (adjacency NxN, angles NxK, single embeddings Nx c_s, pair NxNx c_z, partial coords Nx(#atoms), final coords Nx(#atoms), etc.) are all labeled.     6.  Unified Latent Synergy:     \u2022   The \u201cMerger\u201d is singled out as a subgraph, clarifying we combine adjacency, angles, partial coords, s, z, etc., exactly how we want.</p> <p>\u2e3b</p> <p>Key Configuration Points     1.  LoRA:     \u2022   TorsionBERT in Stage B: set model_name_or_path to a large pretrained model and insert LoRA adapters with \u201crank=8\u201d or so.     \u2022   Pairformer: similarly add LoRA to attention or feed-forward layers.     \u2022   Diffusion: optionally insert LoRA if the diffusion model is huge.     2.  Stage A:     \u2022   rfold_predictor.py might load RNAStralign_trainset_pretrained.pth.     \u2022   Output adjacency is used in Stages B and the Unified Merger, if we want adjacency-based synergy.     3.  Stage B:     \u2022   torsion_bert_predictor.py has angle_mode=\"degrees\" or \"sin_cos\".     \u2022   pairformer_wrapper.py config might specify n_blocks=48, c_z=128, c_s=384.     4.  Stage C:     \u2022   mp_nerf/rna.py: Usually sets sugar pucker to \u201cC3\u2032-endo.\u201d     \u2022   If do_ring_closure is False, we skip ring closure.     5.  Stage D:     \u2022   protenix_diffusion_manager.py might define a schedule in generator.py (InferenceNoiseScheduler) for 50\u2013100 denoising steps.     \u2022   We unify single embeddings (s_trunk), pair embeddings (z_trunk), partial coords, etc., in a single \u201cconditioning\u201d dictionary.</p> <p>\u2e3b</p> <p>Recommended Usage Flow     1.  Obtain adjacency from Stage A.     2.  Run Stage B to get angles + single/pair embeddings. Possibly pass adjacency in to TorsionBERT or Pairformer if they require it.     3.  (Optional) Stage C: If you want an initial 3D for diffusion, run MP-NeRF.     4.  Merge all data (angles, adjacency, partial coords, single/pair embeddings) into a unified latent.     5.  Stage D: Condition the diffusion model on that unified latent to refine final 3D.     6.  Optionally do a short local MD to minimize bond strains or fix sterics.</p> <p>You can skip Stage C if you want to let diffusion start from random noise. You can skip energy minimization if you trust the final diffusion geometry. However, each step may improve the final structure\u2019s accuracy.</p> <p>\u2e3b</p> <p>Conclusion</p> <p>This final combined architectural document:     \u2022   Merges the clarity of a color-coded flow (V2) with the concrete code references and shape details (V3).     \u2022   Incorporates the straightforward top-down perspective (V1) plus an emphasis on synergy and the \u201cunified latent\u201d concept (V4).     \u2022   Highlights LoRA usage in TorsionBERT, Pairformer, and (optionally) Diffusion.     \u2022   Shows optional partial 3D (Stage\u202fC) and optional energy minimization.</p> <p>Hence, this pipeline covers everything from adjacency (2D) \u2192 torsion + pair embeddings \u2192 (optional) partial 3D \u2192 unified synergy \u2192 diffusion-based refinement \u2192 final or post-processed 3D. It should serve as a verbose and complete reference for both developers and advanced users interested in each module\u2019s role, shapes, code references, and how they integrate to produce high-quality RNA 3D structures. Multistage RNA 3D Structure Prediction Pipeline (Comprehensive Final Version, ASCII Edition)</p> <p>Below is a comprehensive technical document unifying the strengths of previous versions (V1\u2013V4), addressing their criticisms, and presenting a verbose ASCII-based diagram clearly referencing code modules, shapes, optional paths, LoRA integration, and unified latent synergy.</p> <p>\u2e3b</p> <p>High-Level Summary</p> <p>The RNA 3D prediction pipeline consists of five primary stages:</p> <ol> <li>Stage A \u2013 2D Adjacency Prediction (RFold)</li> <li>Stage B \u2013 Torsion &amp; Pair Embeddings:</li> <li>TorsionBERT (LoRA-enabled) \u2192 backbone torsion angles</li> <li>Pairformer (LoRA-enabled) \u2192 single-residue and pairwise embeddings</li> <li>Stage C \u2013 (Optional) Partial 3D Reconstruction (MP-NeRF)</li> <li>Unified Latent Merger \u2013 Combines adjacency, angles, partial coords, embeddings</li> <li>Stage D \u2013 Diffusion-based Refinement (LoRA-optional)</li> <li>Optional Post-processing (Energy Minimization)</li> </ol> <p>\u2e3b</p> <p>Implementation Files</p> <p>Stage A:   - rna_predict/pipeline/stageA/run_stageA.py   - rfold_predictor.py   - model.py</p> <p>Stage B:   - TorsionBERT:     - torsion/torsion_bert_predictor.py     - torsionbert_inference.py   - Pairformer:     - pairwise/pairformer_wrapper.py     - pairformer.py</p> <p>Stage C:   - stage_c_reconstruction.py   - mp_nerf/rna.py   - final_kb_rna.py</p> <p>Unified Latent:   - Typically a small MLP or attention block (often within Stage D code)</p> <p>Stage D:   - stageD/diffusion/   - protenix_diffusion_manager.py   - generator.py   - diffusion.py</p> <p>\u2e3b</p> <p>Shape Conventions</p> <ul> <li>Adjacency:         [N, N] (binary/probability)</li> <li>Torsion Angles:    [N, K] or [N, 2K] if sin/cos encoding</li> <li>Single Embeddings: [N, c_s]</li> <li>Pair Embeddings:   [N, N, c_z]</li> <li>Partial 3D Coords: [N, #atoms, 3]</li> <li>Diffusion:         [batch, N_sample, N_atom, 3] + trunk embeddings</li> </ul> <p>\u2e3b</p> <p>Detailed ASCII Diagram of the Pipeline</p> <p>RNA Sequence (String: \"ACGU...\")          |          v +---------------------------------------------+ | Stage A: 2D Adjacency Prediction (RFold)    | | [run_stageA.py, rfold_predictor.py, model.py] (No LoRA) +---------------------------------------------+          |          | Adjacency Matrix [N,N]          v +----------------------------------------------------------+ | Stage B: Torsion Angles &amp; Pair Embeddings (LoRA-enabled) | |                                                          | | - TorsionBERT: angles [N,K or N,2K]                      | |   [torsion_bert_predictor.py, torsionbert_inference.py]  | |                                                          | | - Pairformer:                                            | |   Single embeddings [N,c_s]                              | |   Pair embeddings [N,N,c_z]                              | |   [pairformer_wrapper.py, pairformer.py]                 | +----------------------------------------------------------+          |          +------------+---------------+          |            |               |          |            |(Optional)     |          |            v               |          |  +--------------------------------------------+          |  | Stage C: Partial 3D Reconstruction         |          |  | [stage_c_reconstruction.py, mp_nerf/rna.py,|          |  | final_kb_rna.py] (Optional)                |          |  +--------------------------------------------+          |            |               |          | Partial Coords [N,#atoms,3]|          |            v               |          +------------+---------------+                       |                       v +----------------------------------------------------+ | Unified Latent Merger                              | |                                                    | | Combines adjacency, angles, partial coords,        | | single &amp; pair embeddings into Unified Latent       | | (MLP/attention-based merger, usually in Stage D)   | +----------------------------------------------------+                       |                       | Unified Latent                       v +-------------------------------------------------------------+ | Stage D: Diffusion-based Refinement (LoRA optional)        | | [protenix_diffusion_manager.py, generator.py, diffusion.py]| | - Conditions on Unified Latent                             | | - Produces Final 3D structure(s) [N,#atoms,3] or ensemble  | +-------------------------------------------------------------+                       |                       v +---------------------------------------+ | Optional Post-processing:             | | Short MD / Energy Minimization        | | (OpenMM, GROMACS, Amber, etc.)        | | Polished Final Structures             | +---------------------------------------+</p> <p>\u2e3b</p> <p>Stage-by-Stage Breakdown (Detailed)</p> <p>Stage A: 2D Adjacency (RFold)     \u2022   Input: RNA sequence string.     \u2022   Output: Adjacency matrix [N,N].     \u2022   Code: run_stageA.py, rfold_predictor.py, model.py     \u2022   Comment: Usually no LoRA integration here; output optionally used downstream.</p> <p>Stage B: Torsion &amp; Pair Embeddings</p> <p>TorsionBERT (LoRA):     \u2022   Inputs: Sequence (optionally adjacency).     \u2022   Outputs: Backbone torsion angles [N,K] or [N,2K] if sin/cos encoded.     \u2022   Code: torsion_bert_predictor.py, torsionbert_inference.py</p> <p>Pairformer (LoRA):     \u2022   Inputs: Sequence (optionally adjacency).     \u2022   Outputs:     \u2022   Single embeddings [N,c_s]     \u2022   Pair embeddings [N,N,c_z]     \u2022   Code: pairformer_wrapper.py, pairformer.py</p> <p>Stage C (Optional): Partial 3D Reconstruction (MP-NeRF)     \u2022   Inputs: Torsion angles, sequence, standard geometry.     \u2022   Outputs: Partial coordinates [N,#atoms,3].     \u2022   Code: stage_c_reconstruction.py, mp_nerf/rna.py, final_kb_rna.py     \u2022   Comment: Typically backbone atoms only; optional ring closure.</p> <p>Unified Latent Merger     \u2022   Inputs: Adjacency, torsion angles, single/pair embeddings, partial coords.     \u2022   Outputs: Unified latent vector (used as conditioning input to diffusion).     \u2022   Implementation: Often small MLP or attention layers embedded within Stage D.</p> <p>Stage D: Diffusion-based Refinement     \u2022   Inputs: Unified latent, optionally partial 3D coords from Stage C.     \u2022   Outputs: Final refined 3D structure(s) [N,#atoms,3].     \u2022   Code: stageD/diffusion/, protenix_diffusion_manager.py, generator.py, diffusion.py     \u2022   LoRA: Optional if diffusion model is large-scale.</p> <p>Post-processing (Optional)     \u2022   Methods: Short molecular dynamics or energy minimization runs.     \u2022   Tools: OpenMM, GROMACS, Amber.     \u2022   Output: Polished final 3D structure(s).</p> <p>\u2e3b</p> <p>Key Configuration &amp; LoRA Integration Points     \u2022   Stage B (TorsionBERT &amp; Pairformer):     \u2022   Insert LoRA adapters (rank=8) into large pretrained models.     \u2022   Angle modes configurable (degrees vs. sin_cos).     \u2022   Pairformer configurable (n_blocks=48, c_z=128, c_s=384).     \u2022   Stage D (Diffusion):     \u2022   Optional LoRA insertion for large diffusion models.     \u2022   Denoising schedule configurable (~50-100 steps typical).     \u2022   Stage C (Optional):     \u2022   Sugar pucker geometry defaults (C3'-endo).     \u2022   Ring closure toggle (do_ring_closure=True/False).</p> <p>\u2e3b</p> <p>Recommended Usage Flow</p> <ol> <li>Obtain adjacency matrix from Stage A.</li> <li>Generate angles &amp; embeddings in Stage B, optionally using adjacency.</li> <li>(Optional) Generate partial 3D structure in Stage C.</li> <li>Merge data into unified latent.</li> <li>Diffusion-based refinement (Stage D) using unified latent.</li> <li>(Optional) Post-processing MD/energy minimization.</li> </ol> <p>Stages C and post-processing are optional, but inclusion typically enhances accuracy.</p> <p>\u2e3b</p> <p>Why This Comprehensive ASCII Diagram Excels     \u2022   Clarity: Clearly marked stages, inputs/outputs, optional steps.     \u2022   LoRA integration: Explicitly highlighted integration points.     \u2022   Implementation mapping: Clear references to code modules.     \u2022   Data shape specification: Explicit and consistent.     \u2022   Unified latent synergy: Clearly defined aggregation step for advanced conditioning.</p> <p>\u2e3b</p> <p>Conclusion</p> <p>This comprehensive ASCII-based overview merges clarity, technical detail, code referencing, optional paths, LoRA integration, and latent synergy to provide a complete, developer-friendly resource for understanding, implementing, and customizing the RNA 3D prediction pipeline from initial sequence to polished final structures.</p>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/","title":"\ud83d\udda5\ufe0f HPC Integration Guide: Hydra Command-Line Overrides","text":""},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#overview","title":"Overview","text":"<p>This guide provides examples for running the RNA prediction pipeline on HPC systems using Hydra configuration overrides. It covers both SLURM and GridEngine job schedulers.</p>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#basic-hydra-overrides","title":"Basic Hydra Overrides","text":"<p>All stages support these common Hydra parameters:</p> <pre><code># Set number of CPU cores\nhydra.launcher.cpus_per_task=4\n\n# Set memory allocation (GB)\nhydra.launcher.mem=16\n\n# Disable GPU usage\nmodel.device=cpu\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#slurm-examples","title":"SLURM Examples","text":""},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#basic-job-submission","title":"Basic Job Submission","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=rna_stageB\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=2:00:00\n\npython -m rna_predict.pipeline.stageB.main \\\n    model.device=cuda \\\n    hydra.launcher.partition=gpu \\\n    hydra.launcher.gpus=1\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=rna_stageC\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:2\n#SBATCH --time=4:00:00\n\npython -m rna_predict.pipeline.stageC.stage_c_reconstruction \\\n    stageC.device=cuda \\\n    hydra.launcher.gpus=2 \\\n    +hpc_cluster=slurm\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#gridengine-examples","title":"GridEngine Examples","text":""},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#basic-job-submission_1","title":"Basic Job Submission","text":"<pre><code>#!/bin/bash\n#$ -N rna_stageB\n#$ -pe smp 4\n#$ -l h_vmem=16G\n#$ -l h_rt=2:00:00\n\npython -m rna_predict.pipeline.stageB.main \\\n    model.device=cuda \\\n    +hpc_cluster=gridengine\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#gpu-job","title":"GPU Job","text":"<pre><code>#!/bin/bash\n#$ -N rna_stageC\n#$ -pe smp 8\n#$ -l h_vmem=32G\n#$ -l h_rt=4:00:00\n#$ -l gpu=1\n\npython -m rna_predict.pipeline.stageC.stage_c_reconstruction \\\n    stageC.device=cuda \\\n    hydra.launcher.gpus=1\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#stage-specific-considerations","title":"Stage-Specific Considerations","text":""},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#stage-b","title":"Stage B","text":"<ul> <li>Memory intensive due to graph processing</li> <li>Recommended: 16-32GB RAM for medium sequences</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#stage-c","title":"Stage C","text":"<ul> <li>Benefits from GPU acceleration</li> <li>Recommended: 1-2 GPUs for faster reconstruction</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#stage-d","title":"Stage D","text":"<ul> <li>Requires significant GPU memory</li> <li>Recommended: A100 or similar high-memory GPUs</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hpc_overrides/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Estimation:</li> <li>Start with smaller test jobs to gauge resource needs</li> <li> <p>Scale up based on RNA sequence length</p> </li> <li> <p>Checkpointing:    <pre><code>hydra.run.dir=./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n</code></pre></p> </li> <li> <p>Job Arrays:    <pre><code># SLURM\n#SBATCH --array=1-10\n\n# GridEngine\n#$ -t 1-10\n</code></pre></p> </li> <li> <p>Monitoring:</p> </li> <li>Use <code>sacct</code> (SLURM) or <code>qstat</code> (GridEngine) to monitor jobs</li> <li>Check GPU utilization with <code>nvidia-smi</code></li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/","title":"Hydra Integration Gap Analysis","text":"<p>Below is a brief gap analysis to see whether your current Hydra Integration document covers all major bases or if there are remaining topics worth including. While your document is already thorough, consider the items below as potential additions or clarifications that can further strengthen your plan and avert future headaches.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#potential-gaps-additional-considerations","title":"Potential Gaps &amp; Additional Considerations","text":"<ol> <li>HPC/Cluster Environment Details </li> <li>If you or your team routinely run on a shared cluster or HPC environment (e.g., Slurm, PBS), you might need to detail how Hydra's output directory structure interacts with job scheduling systems.  </li> <li> <p>Some HPC setups have ephemeral disk or require specifying a path for output\u2014documenting a recommended approach for setting <code>hydra.run.dir</code> might be helpful.</p> </li> <li> <p>Config Composition or \"Modes\" </p> </li> <li>Hydra supports advanced \"config groups\" and \"overrides\" for complex scenarios, e.g. <code>python main.py +experiment=large_pairformer</code>.  </li> <li> <p>If you anticipate multiple \"flavors\" of the pipeline (like a \"fast_debug\" vs. \"full_production\"), you might want a dedicated snippet on how to compose or switch these environment- or mode-specific configs.</p> </li> <li> <p>Compatibility with Logging/Monitoring Tools </p> </li> <li> <p>Some teams integrate Hydra with logging libraries (e.g., TensorBoard, W&amp;B). If you want Hydra to manage logger parameters (like project name, run name, tags), adding a short mention of how to store logging config in YAML can be valuable.</p> </li> <li> <p>Registries or Checkpointing </p> </li> <li>If your pipeline loads pretrained weights (e.g., TorsionBERT from Hugging Face or your own checkpointing scheme), you might define structured references in Hydra (like <code>model_checkpoint: path/to/checkpoint.pth</code>).  </li> <li> <p>Providing a quick example of partial loading or advanced checkpoint management (where only LoRA weights are loaded) could help developers avoid confusion.</p> </li> <li> <p>Validation for Dimensional Consistency </p> </li> <li> <p>In advanced pipelines, some parameters in Stage B must match Stage C or the unified latent merger (e.g., <code>c_s == dim_s</code>). You might mention adding a post-load validation function that checks consistency (or an auto-derivation approach within the Hydra config).  </p> </li> <li> <p>Example \"CLI Overrides in Action\" </p> </li> <li> <p>Although you do have short code examples, you could explicitly show a bigger real command line example:       <pre><code>python -m rna_predict.main \\\n  runtime.device=cpu \\\n  stageB.torsion_bert.lora.enabled=true \\\n  stageB.torsion_bert.lora.r=16 \\\n  memory_optimization.enable=false\n</code></pre>      That snippet highlights Hydra's real-world usage and can serve as a go-to reference.</p> </li> <li> <p>Integration with a Training Script </p> </li> <li> <p>If your pipeline ultimately trains or fine-tunes TorsionBERT/Pairformer, you might want a short section specifying how Hydra will handle training loops (e.g., number of epochs, batch size, learning rate). If you already have a <code>train.py</code> or <code>LightningModule</code>, it's worth clarifying how Hydra's parameters map into your trainer or optimizer.</p> </li> <li> <p>Local vs. Global \"device\" </p> </li> <li> <p>If certain stages must run on different devices (e.g., Stage D on GPU, Stage A on CPU for memory reasons), you can highlight advanced usage:  </p> <ul> <li>\"By default, <code>runtime.device</code> applies to all modules, but can be overridden per stage if needed.\"</li> </ul> </li> <li> <p>Further Testing Strategies </p> </li> <li> <p>You mention smoke tests and integration tests. If your QA process includes e2e shape checks or performance benchmarks, referencing how Hydra might help orchestrate them (like varying batch size across runs) can be useful.</p> </li> <li> <p>Future: Multi-run Sweeps </p> <ul> <li>Hydra's multi-run (<code>-m</code>) approach is extremely helpful for hyperparameter sweeps. A short mention of how the team can add <code>-m</code> flags to run multiple experiments automatically could future-proof your doc.  </li> </ul> </li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#conclusion","title":"Conclusion","text":"<p>Your Hydra Integration Master Document is already very comprehensive. However, if your workflow involves HPC queues, diverse logging setups, partial checkpoint loading, or advanced shape validation, adding short clarifications or examples in these areas can save a lot of troubleshooting later.</p> <ul> <li>If you're primarily local (single-machine usage), you likely don't need HPC environment specifics.  </li> <li>If your logging is straightforward or checkpoint paths are simple, you can keep a minimal mention in the doc.  </li> <li>If you expect your pipeline to frequently tune or expand, you may want to highlight Hydra sweeps more explicitly.</li> </ul> <p>Ultimately, these gaps aren't mandatory for a basic Hydra integration, but covering them may preempt confusion in more advanced or specialized scenarios. Thus, the existing document is enough to implement Hydra effectively, and these additional notes can serve as helpful expansions if and when the pipeline or usage patterns become more complex. </p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#stage-by-stage-documentation-analysis","title":"Stage-by-Stage Documentation Analysis","text":"<p>Below is a systematic evaluation of the newly drafted specialized documentation for each stage (A\u2013D) plus the Unified Latent Merger, focusing on whether they are suitable for a Hydra-based configuration approach.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#1-overall-observations","title":"1. Overall Observations","text":"<ol> <li>Consistent Template &amp; Coverage</li> <li> <p>Each stage document (Stage A, Stage B, Stage C, Stage D, Unified Latent Merger) follows a consistent outline:</p> <ul> <li>Purpose/Overview</li> <li>Inputs &amp; Outputs</li> <li>Key Classes &amp; Methods</li> <li>Configuration Parameters (with Hydra references)</li> <li>Integration &amp; Data Flow</li> <li>Edge Cases &amp; Error Handling</li> <li>References/Dependencies</li> </ul> </li> <li> <p>Hydra Config Section</p> </li> <li>Each specialized doc includes a \"Hydra Configuration\" segment that points to the relevant .yaml file</li> <li>They list the main parameters and describe how these are used</li> <li> <p>The docs highlight default values for quick reference</p> </li> <li> <p>Cross-Referencing</p> </li> <li>The specialized docs link back to the Master Hydra Document</li> <li> <p>This clarifies that each stage doc is part of a bigger Hydra ecosystem</p> </li> <li> <p>Edge Case &amp; Error Handling</p> </li> <li>Each doc includes an \"Edge Cases &amp; Error Handling\" section</li> <li> <p>This helps prevent silent failures with missing config parameters</p> </li> <li> <p>Optional vs. Required Features</p> </li> <li>The docs clearly identify optional features like ring closure in Stage C</li> <li>Consistent with best practices for marking optional steps in YAML config</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#2-stage-specific-analysis","title":"2. Stage-Specific Analysis","text":""},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#21-stage-a-2d-adjacency-via-rfold","title":"2.1 Stage A: 2D Adjacency (via RFold)","text":"<p>Strengths: - Clearly states input (RNA sequence) and output (NxN adjacency) - Hydra parameters are laid out with defaults - Shows adjacency feeding into downstream stages</p> <p>Potential Gaps: - Could mention multiple adjacency prediction modes - Could clarify trainable vs frozen adjacency scenarios</p> <p>Verdict: Very suitable for Hydra, with clear config fields and references.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#22-stage-b-torsionbert-pairformer","title":"2.2 Stage B: TorsionBERT &amp; Pairformer","text":"<p>Strengths: - Merges TorsionBERT and Pairformer documentation effectively - Explicit Hydra config references - Well-described LoRA parameters</p> <p>Potential Gaps: - Could make angle output format more explicit - Could add quick reference table for common overrides</p> <p>Verdict: Thorough documentation with good coverage of LoRA and angle modes.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#23-stage-c-3d-reconstruction-mp-nerf","title":"2.3 Stage C: 3D Reconstruction (MP-NeRF)","text":"<p>Strengths: - Clear method configuration options - Covers optional features like ring closure - Good edge case documentation</p> <p>Potential Gaps: - Could clarify partial 3D computation scenarios - Could emphasize stage disabling options</p> <p>Verdict: Good coverage of optional flags and Hydra integration.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#24-unified-latent-merger","title":"2.4 Unified Latent Merger","text":"<p>Strengths: - Documents input shapes clearly - Shows future extensibility - Clear integration points</p> <p>Potential Gaps: - Could add dimension validation examples - Could clarify shape assertions</p> <p>Verdict: Well-suited for Hydra with clear shape and dimension documentation.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#25-stage-d-diffusion-energy-minimization","title":"2.5 Stage D: Diffusion &amp; Energy Minimization","text":"<p>Strengths: - Thorough parameter documentation - Covers LoRA and memory optimization - Includes HPC considerations</p> <p>Potential Gaps: - Could clarify partial coordinate handling - Could add more memory optimization examples</p> <p>Verdict: Very thorough Hydra integration with good advanced feature coverage.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#3-integration-with-master-document","title":"3. Integration with Master Document","text":"<p>The specialized docs complement the Hydra Master Document by providing: 1. Clear YAML file locations 2. Parameter lists with defaults 3. Implementation details and toggles</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_gap_analysis/#4-recommendations","title":"4. Recommendations","text":"<ol> <li>Documentation Quality</li> <li>Well-structured and consistent</li> <li>Clear parameter documentation</li> <li> <p>Good debugging support</p> </li> <li> <p>Hydra Alignment</p> </li> <li>Specific YAML references</li> <li>Clear optional feature documentation</li> <li> <p>Good configuration examples</p> </li> <li> <p>Suggested Enhancements</p> </li> <li>Add dimension validation details</li> <li>Include more command-line examples</li> <li>Consider adding quick-reference tables</li> </ol> <p>Overall: The documentation is well-suited for Hydra adoption, with clear stage-specific parameters and integration points. Minor refinements could further improve usability, but the core structure is solid and ready for implementation.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/","title":"Hydra Integration Master Document: A Comprehensive Implementation Plan for RNA_PREDICT","text":"<p>This document merges the best features from all previous versions (V1\u2013V5) to create a definitive, in-depth guide for integrating Hydra into the RNA_PREDICT pipeline. It balances clarity, structure, code examples, acceptance criteria, and references to synergy across all pipeline stages (A\u2013D), plus LoRA, optional energy minimization, and memory optimization\u2014ultimately better than the sum of its parts.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#1-introduction-rationale","title":"1. Introduction &amp; Rationale","text":""},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#11-why-use-hydra","title":"1.1 Why Use Hydra?","text":"<ol> <li>Centralized Configuration </li> <li>Move from scattered constants/hardcoded defaults to YAML config files, ensuring a single source of truth for hyperparameters, paths, LoRA adapter settings, toggles, and more.</li> <li>Ease of Experimentation </li> <li>Override any parameter from the command line (e.g., <code>stageB.torsion_bert.lora.enabled=true</code>) without rewriting code. This is crucial for advanced pipelines with many parameters.</li> <li>Reproducibility &amp; Modularity </li> <li>YAML files can be committed to version control, letting you revisit exactly how each run was configured. Hydra also supports composition, so each stage's config remains modular.</li> <li>Backward Compatibility </li> <li>By setting YAML defaults to match your current inline constants, the pipeline's behavior remains unchanged unless you override parameters\u2014preventing disruption of existing workflows.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#12-pipeline-context","title":"1.2 Pipeline Context","text":"<p>The RNA_PREDICT pipeline is composed of multiple stages: - Stage A: 2D adjacency prediction (e.g., via RFold). - Stage B: TorsionBERT (with optional LoRA) &amp; Pairformer (also optional LoRA) for angles and pairwise embeddings. - Stage C: 3D reconstruction (MP-NeRF or fallback). - Unified Latent Merger: Combines adjacency, angles, partial coords, embeddings into a single representation. - Stage D: Diffusion-based refinement (with optional LoRA) and optional energy minimization.</p> <p>Hydra must manage a range of parameters\u2014model hyperparams, file paths, devices, synergy between the shape/dimensions of different stages, etc.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#2-scope-acceptance-criteria","title":"2. Scope &amp; Acceptance Criteria","text":""},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#21-scope","title":"2.1 Scope","text":"<p>This plan addresses:</p> <ol> <li>Installing &amp; Pinning Hydra Dependencies </li> <li>Ensuring both <code>hydra-core</code> and <code>omegaconf</code> are part of the environment.</li> <li>Creating a <code>conf/</code> Directory Structure </li> <li>Organizing YAML files by stage/component.</li> <li>Defining a Configuration Schema </li> <li>Using Python <code>@dataclass</code> or similar for typed configs (optional but recommended).</li> <li>Refactoring Each Stage (A\u2013D) </li> <li>Replacing inline defaults with Hydra-based config references.</li> <li>Ensuring Backward Compatibility </li> <li>Matching current defaults so the pipeline's existing behavior is preserved.</li> <li>Optional Features </li> <li>Incorporating LoRA toggles, memory optimization, and energy minimization config sections.</li> <li>Documentation &amp; Team Onboarding </li> <li>Educating developers on Hydra usage, command-line overrides, and custom YAML composition.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#22-out-of-scope","title":"2.2 Out of Scope","text":"<ul> <li>Deep architectural refactoring beyond reading Hydra configs.</li> <li>Extensive hyperparameter tuning (the plan focuses on exposing parameters, not optimizing them).</li> <li>Replacing or rewriting the entire code logic; we only adapt it to use Hydra as a centralized config manager.</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#23-acceptance-criteria","title":"2.3 Acceptance Criteria","text":"<p>The integration is complete when:</p> <ol> <li>Hydra &amp; OmegaConf are installed and pinned in <code>requirements.txt/pyproject.toml</code>.</li> <li>The directory <code>rna_predict/conf/</code> exists with default YAMLs for each major pipeline stage (A\u2013D) and any optional modules (LoRA, data, etc.).</li> <li>Python dataclasses (or an equivalent schema) in <code>config_schema.py</code> reflect each stage's parameters, providing typed defaults matching existing code.</li> <li>All major code (like <code>run_stageA.py</code>, TorsionBertPredictor, <code>stage_c_reconstruction.py</code>, <code>run_stageD.py</code>) reads configuration values from Hydra <code>cfg</code> instead of inline constants.</li> <li>Backward-compatible results: Running the pipeline with default YAMLs produces outputs consistent with the previous (pre-Hydra) pipeline.</li> <li>CLI Override: A developer can override parameters (e.g., <code>python main.py stageA.num_hidden=256</code>) and see changes reflected in the pipeline.</li> <li>Documentation: A short \"Hydra Usage Guide\" is added, ensuring any new or existing developer can adopt the new config system.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#3-step-by-step-implementation-plan","title":"3. Step-by-Step Implementation Plan","text":""},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#31-dependencies-environment","title":"3.1 Dependencies &amp; Environment","text":"<ol> <li>Add Hydra to Project </li> <li>In <code>requirements.txt</code> or <code>pyproject.toml</code>:      <pre><code>hydra-core==1.3.2\nomegaconf==2.3.0\n</code></pre></li> <li>Pin these versions to avoid unexpected compatibility issues.</li> <li>Update Containers </li> <li>If using Docker or any container system, ensure you <code>RUN pip install hydra-core==1.3.2 omegaconf==2.3.0</code>.</li> <li>Verify Installation </li> <li>Create a minimal test script:      <pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=None)\ndef demo(cfg: DictConfig):\n    print(\"Hydra test:\", cfg)\n\nif __name__ == \"__main__\":\n    demo()\n</code></pre></li> <li>Run <code>python demo.py</code> to confirm Hydra loads without errors.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#32-directory-file-structure","title":"3.2 Directory &amp; File Structure","text":"<p>Create a <code>conf/</code> directory inside <code>rna_predict/</code>:</p> <pre><code>rna_predict/\n  \u251c\u2500\u2500 conf/\n  \u2502   \u251c\u2500\u2500 config_schema.py        # Python dataclasses for typed config (optional but recommended)\n  \u2502   \u251c\u2500\u2500 default.yaml            # The top-level config referencing sub-configs\n  \u2502   \u251c\u2500\u2500 model/\n  \u2502   \u2502   \u251c\u2500\u2500 stageA.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 stageB_torsion.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 stageB_pairformer.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 stageC.yaml\n  \u2502   \u2502   \u2514\u2500\u2500 stageD_diffusion.yaml\n  \u2502   \u251c\u2500\u2500 memory_optimization.yaml (optional)\n  \u2502   \u251c\u2500\u2500 energy_minimization.yaml (optional)\n  \u2502   \u251c\u2500\u2500 train.yaml (optional)\n  \u2502   \u2514\u2500\u2500 inference.yaml (optional)\n  \u2514\u2500\u2500 ...\n</code></pre> <ul> <li>Why subdirectories?: Hydra supports \"composable configs,\" letting you pick <code>model/stageA.yaml</code> or override it with <code>model/stageA_alt.yaml</code>.  </li> <li>Keep related parameters together (e.g., all TorsionBERT settings in one file).</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#33-defining-the-configuration-schema","title":"3.3 Defining the Configuration Schema","text":"<p>In <code>config_schema.py</code>, define Python dataclasses that match each stage's config. This ensures:</p> <ul> <li>Typed defaults: Minimizes confusion about parameter types.  </li> <li>Validation: If the YAML is missing or has the wrong type, Hydra can warn or fail early.</li> </ul> <p>Example:</p> <pre><code># rna_predict/conf/config_schema.py\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List\n\n@dataclass\nclass StageAConfig:\n    num_hidden: int = 128\n    dropout: float = 0.3\n    min_length: int = 80\n    checkpoint_path: str = \"RFold/checkpoints/RNAStralign_trainset_pretrained.pth\"\n    device: str = \"cuda\"\n    # Possibly additional parameters like binary_threshold, argmax_mode, etc.\n\n@dataclass\nclass LoraConfig:\n    enabled: bool = False\n    r: int = 8\n    alpha: int = 16\n    dropout: float = 0.1\n    target_modules: Optional[List[str]] = None\n\n@dataclass\nclass TorsionBertConfig:\n    model_name_or_path: str = \"sayby/rna_torsionbert\"\n    device: str = \"cuda\"\n    angle_mode: str = \"degrees\"\n    num_angles: int = 7\n    max_length: int = 512\n    checkpoint_path: Optional[str] = None\n    lora: LoraConfig = LoraConfig()\n\n@dataclass\nclass PairformerConfig:\n    n_blocks: int = 2\n    c_z: int = 32\n    c_s: int = 64\n    dropout: float = 0.1\n    use_checkpoint: bool = False\n    init_z_from_adjacency: bool = True\n    lora: LoraConfig = LoraConfig()\n\n@dataclass\nclass StageCConfig:\n    method: str = \"mp_nerf\"\n    do_ring_closure: bool = False\n    place_bases: bool = True\n    sugar_pucker: str = \"C3'-endo\"\n    device: str = \"auto\"\n\n@dataclass\nclass LatentMergerConfig:\n    method: str = \"MLP\"\n    dim_angles: int = 7\n    dim_s: int = 64\n    dim_z: int = 32\n    dim_out: int = 128\n    hidden_sizes: List[int] = field(default_factory=lambda: [128])\n    dropout: float = 0.1\n    activation: str = \"ReLU\"\n    freeze: bool = False\n\n@dataclass\nclass MemoryOptimizationConfig:\n    enable: bool = True\n\n@dataclass\nclass EnergyMinimizationConfig:\n    enabled: bool = False\n    steps: int = 1000\n    method: str = \"OpenMM\"\n\n@dataclass\nclass StageDConfig:\n    mode: str = \"inference\"  # \"inference\" or \"training\"\n    device: str = \"cuda\"\n    sigma_data: float = 16.0\n    gamma0: float = 0.8\n    gamma_min: float = 1.0\n    noise_scale: float = 1.003\n    step_scale: float = 1.5\n    n_steps: int = 50\n    c_atom: int = 128\n    c_atompair: int = 16\n    c_token: int = 768\n    lora: LoraConfig = LoraConfig()\n\n@dataclass\nclass RNAConfig:\n    stageA: StageAConfig = StageAConfig()\n    torsion_bert: TorsionBertConfig = TorsionBertConfig()\n    pairformer: PairformerConfig = PairformerConfig()\n    stageC: StageCConfig = StageCConfig()\n    latent_merger: LatentMergerConfig = LatentMergerConfig()\n    stageD: StageDConfig = StageDConfig()\n    memory_optimization: MemoryOptimizationConfig = MemoryOptimizationConfig()\n    energy_minimization: EnergyMinimizationConfig = EnergyMinimizationConfig()\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#34-writing-yaml-files-per-stage","title":"3.4 Writing YAML Files per Stage","text":"<p>Example: <code>stageA.yaml</code></p> <pre><code>stageA:\n  num_hidden: 128\n  dropout: 0.3\n  min_length: 80\n  checkpoint_path: \"RFold/checkpoints/RNAStralign_trainset_pretrained.pth\"\n  device: \"cuda\"\n</code></pre> <p>Example: <code>stageB_torsion.yaml</code> for TorsionBERT:</p> <pre><code>torsion_bert:\n  model_name_or_path: \"sayby/rna_torsionbert\"\n  device: \"cuda\"\n  angle_mode: \"degrees\"\n  num_angles: 7\n  max_length: 512\n  checkpoint_path: null\n  lora:\n    enabled: false\n    r: 8\n    alpha: 16\n    dropout: 0.1\n    target_modules: [\"attention.query\", \"attention.key\"]\n</code></pre> <p>Create similar for Pairformer (<code>stageB_pairformer.yaml</code>), Stage C (<code>stageC.yaml</code>), and Stage D (which can also hold memory &amp; minimization config if you like).</p> <p>Finally, reference them in your <code>default.yaml</code>:</p> <pre><code>defaults:\n  - model/stageA\n  - model/stageB_torsion\n  - model/stageB_pairformer\n  - model/stageC\n  - model/stageD_diffusion\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#35-code-integration","title":"3.5 Code Integration","text":"<p>Core Changes: 1. Add Hydra Decorator to a main script (e.g., <code>rna_predict/main.py</code>):</p> <pre><code># rna_predict/main.py\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n# If you want typed configs:\n# from rna_predict.conf.config_schema import RNAConfig\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"default\")\ndef main(cfg: DictConfig):\n    print(\"HYDRA CONFIG:\\n\", OmegaConf.to_yaml(cfg))\n\n    # Stage A usage\n    # from rna_predict.pipeline.stageA.adjacency.rfold_predictor import StageARFoldPredictor\n    # predictor = StageARFoldPredictor(\n    #     config={\n    #       \"num_hidden\": cfg.stageA.num_hidden,\n    #       \"dropout\": cfg.stageA.dropout,\n    #       ...\n    #     },\n    #     checkpoint_path=cfg.stageA.checkpoint_path,\n    #     device=cfg.stageA.device\n    # )\n    # ...\n    # Similarly for TorsionBERT, Pairformer, etc.\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Replace Hardcoded Defaults in each stage with config references:</li> <li>In <code>StageBTorsionBertPredictor.__init__</code>, remove e.g. <code>self.num_angles = 7</code>, replace with <code>self.num_angles = cfg.num_angles</code>.</li> <li>In <code>stage_c_reconstruction.py</code>, remove e.g. <code>method=\"mp_nerf\"</code> defaults, read from <code>cfg.stageC.method</code>, <code>cfg.stageC.do_ring_closure</code>, etc.</li> <li> <p>In your diffusion code (<code>run_stageD.py</code>), reference <code>cfg.stageD.sigma_data</code>, <code>cfg.stageD.noise_scale</code>, etc.</p> </li> <li> <p>LoRA:</p> </li> <li> <p>If LoRA is relevant for TorsionBERT or Pairformer, pass <code>cfg.torsion_bert.lora</code> or <code>cfg.pairformer.lora</code> to your LoRA injection logic.</p> </li> <li> <p>Memory Optimization:</p> </li> <li> <p>If your code calls something like <code>apply_memory_fixes(diffusion_config)</code>, replace inline logic with reading from <code>cfg.memory_optimization.enable</code>.</p> </li> <li> <p>Energy Minimization:</p> </li> <li>The final pipeline step can check <code>if cfg.energy_minimization.enabled:</code> and run the relevant routine (OpenMM, GROMACS, etc.) for a given <code>cfg.energy_minimization.method</code>.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#36-ensuring-backward-compatibility","title":"3.6 Ensuring Backward Compatibility","text":"<ol> <li>Match All Defaults:</li> <li>For each stage's YAML, copy over the same numeric defaults from your old code (e.g., <code>num_hidden=128</code>).</li> <li>Gradual Migration:</li> <li>Start with Stage A or B, confirm minimal breakage, then proceed to Stage C, D, etc.</li> <li>Testing:</li> <li>Validate that running \"vanilla pipeline\" (no CLI overrides) yields the same adjacency outputs, angles, or final 3D coords as before.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#37-documentation-team-onboarding","title":"3.7 Documentation &amp; Team Onboarding","text":"<ol> <li>README or Wiki:</li> <li>Document how to run the pipeline with Hydra:      <pre><code>python -m rna_predict.main stageA.num_hidden=256 stageB_torsion.lora.enabled=true\n</code></pre></li> <li>Override YAML:</li> <li>Teach advanced usage, e.g. adding a second YAML file <code>model/stageB_torsion_experiment.yaml</code> for a bigger TorsionBERT model or a special LoRA rank.</li> <li>Team Communication:</li> <li>Announce the Hydra shift in Slack/Teams so everyone knows to rely on <code>conf/</code> rather than inline code constants.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#38-testing-validation","title":"3.8 Testing &amp; Validation","text":"<ol> <li>Smoke Tests:</li> <li>Quick end-to-end tests on short synthetic sequences (e.g., \"ACGUACGU\").</li> <li>Confirm no runtime errors and that each stage sees the right shapes/dimensions.</li> <li>Unit Tests:</li> <li>If you have <code>pytest</code>, add tests that load <code>default.yaml</code>, instantiate each stage's predictor, and confirm correct parameter usage.</li> <li>Integration Tests:</li> <li>Possibly adapt your existing <code>test_full_pipeline.py</code> or <code>test_main_integration.py</code> to ensure that the pipeline completes with Hydra config.</li> <li>Edge Cases:</li> <li>Toggling LoRA on/off, enabling memory optimization, different sugar puckers for Stage C, or flipping Stage D from \"inference\" to \"training.\"</li> </ol>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#4-conclusion-next-steps","title":"4. Conclusion &amp; Next Steps","text":""},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#41-summary-of-implementation","title":"4.1 Summary of Implementation","text":"<p>By following this plan: 1. Install Hydra in your environment (pin the version). 2. Create a structured <code>conf/</code> folder with top-level <code>default.yaml</code> referencing stage-specific YAML files. 3. Define typed dataclasses (<code>config_schema.py</code>) for clarity and type safety (optional but strongly recommended). 4. Refactor each pipeline stage (A\u2013D) to read from Hydra's <code>cfg</code> object, removing scattered hard-coded defaults. 5. Maintain backward-compatible defaults so existing runs remain stable. 6. Document usage in README or docs/hydra_integration.md, providing examples of CLI overrides for your teammates.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#42-potential-benefits","title":"4.2 Potential Benefits","text":"<ul> <li>Centralized Parameters: </li> <li>No more searching through multiple .py files for relevant defaults.</li> <li>Experimentation Made Simple: </li> <li>Single command-line overrides for different LoRA ranks, diffusion steps, or ring closure toggles.</li> <li>Scalable Approach: </li> <li>As you add or refine pipeline stages (e.g., advanced multi-structure embeddings or new diffusion schedules), simply extend the YAML schema, avoiding code churn.</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hydra_integration_master_document/#43-future-enhancements","title":"4.3 Future Enhancements","text":"<ul> <li>Multi-run Sweeps: </li> <li>Hydra supports <code>-m</code> for launching multiple runs (e.g., sweeping over different <code>num_hidden</code> or dropout rates).</li> <li>Config-Driven Minimization: </li> <li>More fine-grained control over energy minimization (like force fields, temperature).</li> <li>Advanced Validation: </li> <li>If you need to ensure certain relationships (e.g., <code>c_s</code> must match <code>dim_s</code> in the latent merger), you can write custom checks or use pydantic for advanced constraints.</li> </ul> <p>With these steps, you'll achieve a robust, well-organized Hydra integration for RNA_PREDICT, combining: - Formality &amp; acceptance criteria (from earlier plans), - Conciseness &amp; stepwise clarity (so developers know exactly what to do), - Stage-based synergy (ensuring adjacency from Stage A seamlessly links to Pairformer or TorsionBERT in Stage B, partial coords in Stage C, etc.), - Clear references to LoRA, memory optimization, and optional energy minimization (like in your current pipeline code).</p> <p>Once you complete this final integrated plan, you'll have a flexible, maintainable, and easily extendable configuration system\u2014truly better than the sum of its parts from previous iterations. Good luck with the Hydra integration, and happy coding! </p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/","title":"Tutorial","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Aligning to your goal: You need a self-contained reference document that guides a junior developer to incorporate Hydra into an ML project, using patterns drawn from the \"ProjectEquiSurv\" code base.</p> <p>[emoji]: Below is a comprehensive tutorial-style guide. A junior dev can follow it step-by-step to set up Hydra for configuration management, referencing real code snippets from ProjectEquiSurv.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#hydra-usage-guide-with-ml-project-patterns-based-on-project-equisurv","title":"Hydra Usage Guide with ML Project Patterns (Based on Project EquiSurv)","text":"<p>This document introduces the Hydra framework, a powerful Python library for composable configurations. We will walk through:</p> <ol> <li>Why Hydra?</li> <li>Project Structure &amp; Key Files</li> <li>Basic Hydra Concepts</li> <li>Setting Up Hydra</li> <li>Config Directory &amp; YAML Organization</li> <li>Composing Configs</li> <li>Runtime Overrides &amp; CLI Examples</li> <li>Using Hydra for HPC vs. Local</li> <li>(Optional) Hydra Sweeping vs. Custom Search</li> <li>FAQ &amp; Common Pitfalls</li> </ol> <p>Throughout, we'll reference patterns from the ProjectEquiSurv code\u2014particularly the quick_fixes/conf/ directory and usage in pipeline/main.py.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#1-why-hydra","title":"1. Why Hydra?","text":"<p>Machine learning projects often require multiple environment-dependent configurations:</p> <ul> <li>Different hyperparameters for training</li> <li>Different dataset paths, logging settings, HPC vs. local</li> <li>Variation in experiment modes (e.g. \"train\", \"eval\", \"hyperparam sweep\")</li> </ul> <p>Hydra standardizes how these configurations are defined, merged, and overridden via command-line. Instead of large ad-hoc config Python scripts or a single giant YAML, Hydra encourages modular sub-configs that Hydra merges at runtime, letting you override any field on the command-line if desired.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#2-project-structure-key-files","title":"2. Project Structure &amp; Key Files","text":"<p>Below is an example structure (mirroring how ProjectEquiSurv organizes quick_fixes/conf), focusing on the Hydra config pieces:</p> <pre><code>my_ml_project/\n\u251c\u2500\u2500 conf\n\u2502   \u251c\u2500\u2500 config.yaml               # The main or \"top-level\" config\n\u2502   \u251c\u2500\u2500 model\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml          # Model hyperparams\n\u2502   \u251c\u2500\u2500 dataset\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml          # Dataset references, data paths\n\u2502   \u251c\u2500\u2500 experiment\n\u2502   \u2502   \u251c\u2500\u2500 local.yaml            # Local dev environment overrides\n\u2502   \u2502   \u2514\u2500\u2500 hpc.yaml              # HPC environment overrides\n\u2502   \u2514\u2500\u2500 search_space\n\u2502       \u2514\u2500\u2500 default.yaml          # Hyperparam search ranges\n\u251c\u2500\u2500 pipeline\n\u2502   \u251c\u2500\u2500 main.py                   # Hydra entrypoint (@hydra.main)\n\u2502   \u251c\u2500\u2500 pipeline_train_single.py  # Train logic referencing cfg\n\u2502   \u2514\u2500\u2500 ... (other scripts)\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#quick-glance-at-key-files","title":"Quick-Glance at Key Files","text":"<ul> <li>conf/config.yaml: Declares top-level defaults and merges sub-configs:</li> </ul> <pre><code>defaults:\n  - model: default\n  - dataset: default\n  - experiment: local\n  - search_space: default\n  - _self_\n\nmode: \"train\"\nsome_global_setting: true\n\n# Logging dir or run settings, e.g.:\nhydra:\n  run:\n    dir: ./outputs  # or \"outputs/${now:%Y-%m-%d_%H-%M-%S}\"\n</code></pre> <ul> <li>pipeline/main.py: The Python script that calls:</li> </ul> <pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"../conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    # Hydra merges everything into cfg\n    mode = cfg.mode\n    if mode == \"train\":\n        ...\n    elif mode == \"hyperparam_sweep\":\n        ...\n    else:\n        print(f\"Unknown mode: {mode}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#3-basic-hydra-concepts","title":"3. Basic Hydra Concepts","text":"<ul> <li>Structured YAML: Hydra merges multiple YAMLs declared in the defaults list (or via overrides)</li> <li>DictConfig: The resulting object inside main.py. Access config fields with cfg.something</li> <li>Compositional configs: Each sub-config file can hold a portion of the overall settings (e.g. model vs. dataset)</li> <li>CLI Overrides: You can override any setting from the command-line, e.g. python main.py mode=train model.hidden_dim=512</li> </ul>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#4-setting-up-hydra","title":"4. Setting Up Hydra","text":"<ol> <li> <p>Install Hydra:    <pre><code>pip install hydra-core&gt;=1.2  # Or match your environment\n</code></pre></p> </li> <li> <p>Create a conf/ directory    Inside your project, place a root YAML, e.g. config.yaml.</p> </li> <li> <p>In your main script:    <pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    print(cfg)   # For debugging\n    # rest of your code\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> </li> </ol> <p>With that minimal approach, Hydra loads conf/config.yaml. Next, you add sub-configs to define specific settings.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#5-config-directory-yaml-organization","title":"5. Config Directory &amp; YAML Organization","text":"<p>Following ProjectEquiSurv best practices:</p> <ul> <li> <p>conf/config.yaml (top-level):   <pre><code>defaults:\n  - model: default\n  - dataset: default\n  - experiment: local\n  - search_space: default\n  - _self_\n\n# Hydra or top-level fields\nmode: \"train\"\ndevice: \"cpu\"\n# (You can store pipeline settings, e.g. logs, output_dir, etc.)\n</code></pre></p> </li> <li> <p>conf/model/default.yaml:   <pre><code># model/default.yaml\nhidden_dim: 256\nnum_bins: 10\nsurvival_mode: \"cox\"  # or \"discrete\"\n\ndropout: 0.1\n# Additional model hyperparams\n</code></pre></p> </li> <li> <p>conf/dataset/default.yaml:   <pre><code># dataset/default.yaml\ntrain_csv_path: \"data/raw/train.csv\"\ntest_csv_path: \"data/raw/test.csv\"\n\nval_fraction: 0.2\nsplit_strategy: \"random\"\n</code></pre></p> </li> <li> <p>conf/experiment/local.yaml:   <pre><code># local.yaml\nepochs: 5\ntrain_batch_size: 8\nsome_local_override: true\n</code></pre></p> </li> <li> <p>conf/experiment/hpc.yaml (for HPC):   <pre><code># hpc.yaml\nepochs: 50\ntrain_batch_size: 64\nsome_local_override: false\n# HPC cluster settings\n</code></pre></p> </li> <li> <p>conf/search_space/default.yaml (for hyperparam search definitions):   <pre><code># search_space/default.yaml\nhyperparam_search:\n  # e.g. define param ranges, or store them for a custom search routine\n</code></pre></p> </li> </ul> <p>Key: Hydra sees the defaults list in conf/config.yaml and merges each sub-config. The final result is accessible as cfg.model.hidden_dim, cfg.dataset.train_csv_path, etc.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#6-composing-configs","title":"6. Composing Configs","text":"<p>Inside conf/config.yaml:</p> <p>defaults:   - model: default   - dataset: default   - experiment: local   - search_space: default   - self</p> <ul> <li>The order matters: Hydra merges them top to bottom. self means \"finally merge the contents of this file (config.yaml) last.\"</li> </ul> <p>If you want to switch from experiment: local to experiment: hpc, you can do one of two approaches:   1. Edit config.yaml:</p> <p>defaults:   - model: default   - dataset: default   - experiment: hpc  # swapped local =&gt; hpc   ...</p> <ol> <li>Command-line Override:</li> </ol> <p>python main.py experiment=hpc</p> <p>Hydra sees experiment=hpc and merges hpc.yaml instead of local.yaml.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#7-runtime-overrides-cli-examples","title":"7. Runtime Overrides &amp; CLI Examples","text":"<p>A big advantage of Hydra is on-the-fly overrides. For example:</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#1-switch-environment","title":"1) Switch environment","text":"<p>python main.py experiment=hpc</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#2-override-a-single-field","title":"2) Override a single field","text":"<p>python main.py model.hidden_dim=512</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#3-combine-multiple-overrides","title":"3) Combine multiple overrides","text":"<p>python main.py mode=train dataset.split_strategy=race_time_stratified model.survival_mode=discrete</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#4-overriding-path-for-train_csv","title":"4) Overriding path for train_csv","text":"<p>python main.py dataset.train_csv_path=\"/mnt/large_train.csv\"</p> <p>All those changes apply without rewriting config YAMLs. Hydra merges them at runtime.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#8-using-hydra-for-hpc-vs-local","title":"8. Using Hydra for HPC vs. Local","text":"<p>ProjectEquiSurv uses separate YAMLs for HPC and local:</p> <ul> <li>experiment/local.yaml:</li> </ul> <p>epochs: 5 train_batch_size: 8</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#_1","title":"...","text":"<ul> <li>experiment/hpc.yaml:</li> </ul> <p>epochs: 50 train_batch_size: 64</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#hpc-specific-fields","title":"HPC-specific fields","text":"<p>When you specify experiment:hpc, Hydra merges the HPC settings. If your code references cfg.epochs or cfg.train_batch_size, it automatically picks the HPC values.</p> <p>Additionally, in HPC contexts, you might define some_local_override: false, or cluster-based resource strings (like slurm scripts). This keeps your code environment-agnostic, because it always reads from cfg....</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#9-optional-hydra-sweeping-vs-custom-search","title":"9. (Optional) Hydra Sweeping vs. Custom Search","text":"<p>In ProjectEquiSurv, the hyperparameter search is done with a custom Optuna approach. This is completely valid. Alternatively, Hydra has built-in sweepers that can loop through sets of config overrides in multiple runs. For instance:</p> <p>python main.py -m model.hidden_dim=128,256,512 optimizer.lr=0.001,0.01</p> <p>Hydra would produce 6 runs (3 \u00d7 2). If you prefer your custom approach, just keep it. You still store search ranges in, e.g., conf/search_space/default.yaml.</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#10-faq-common-pitfalls","title":"10. FAQ &amp; Common Pitfalls","text":"<ol> <li>\"My directory structure is different\"</li> <li> <p>Hydra doesn't require an exact naming convention or \"conf/\" location. You can define @hydra.main(config_path=\"path/to/my_configs\", config_name=\"base\"). Just keep a subdirectory of YAMLs and be consistent.</p> </li> <li> <p>\"ValueError: Could not merge config\"</p> </li> <li> <p>Usually you have conflicting keys or typed fields. Double check that your sub-configs have consistent naming or mark items as optional.</p> </li> <li> <p>Logging &amp; Output Directory</p> </li> <li> <p>By default, Hydra can create per-run subdirectories like ./outputs/2023-07-10_15-03-22 to isolate logs. If you prefer your own directory creation logic (like ProjectEquiSurv does in path_utils.py), that's also fine.</p> </li> <li> <p>Common Patterns</p> </li> <li>Use cfg.mode to route to different pipelines (train, eval, etc.).</li> <li>Keep environment overrides (like HPC, local, dev) in separate YAMLs.</li> <li>Keep large code blocks out of the config: the config is for hyperparams, paths, toggles, not big scripts.</li> </ol> <p>Example: Putting It All Together</p> <p>Below is a minimal working snippet that echoes the style of ProjectEquiSurv:</p> <ol> <li>Directory:</li> </ol> <p>my_project/ \u251c\u2500\u2500 conf \u2502   \u251c\u2500\u2500 config.yaml \u2502   \u251c\u2500\u2500 model \u2502   \u2502   \u2514\u2500\u2500 default.yaml \u2502   \u2514\u2500\u2500 experiment \u2502       \u251c\u2500\u2500 local.yaml \u2502       \u2514\u2500\u2500 hpc.yaml \u2514\u2500\u2500 main.py</p> <ol> <li>conf/config.yaml:</li> </ol> <p>defaults:   - model: default   - experiment: local   - self</p> <p>mode: \"train\"</p> <ol> <li>conf/model/default.yaml:</li> </ol> <p>hidden_dim: 256 survival_mode: \"cox\"</p> <ol> <li>conf/experiment/local.yaml:</li> </ol> <p>epochs: 5 train_batch_size: 8</p> <ol> <li>conf/experiment/hpc.yaml:</li> </ol> <p>epochs: 50 train_batch_size: 64</p> <ol> <li>main.py:</li> </ol> <p>import hydra from omegaconf import DictConfig</p> <p>@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\") def main(cfg: DictConfig):     print(\"MODE:\", cfg.mode)     print(\"MODEL:\", cfg.model)     print(\"EPOCHS:\", cfg.epochs)     if cfg.mode == \"train\":         run_training(cfg)     else:         print(\"No valid mode selected.\")</p> <p>def run_training(cfg):     # Example usage:     print(f\"Training for {cfg.epochs} epochs with batch_size={cfg.train_batch_size}\")     # Access model config     print(f\"Hidden dim: {cfg.model.hidden_dim}, survival_mode: {cfg.model.survival_mode}\")     # ... do your training ...</p> <p>if name == \"main\":     main()</p> <p>Running it:</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#default-local-config","title":"Default local config","text":"<p>python main.py</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#hpc-override","title":"HPC override","text":"<p>python main.py experiment=hpc</p>"},{"location":"pipeline/integration/hydra_integration/hydra_tutorial/#additional-override","title":"Additional override","text":"<p>python main.py experiment=hpc model.hidden_dim=512</p> <p>Final Remarks</p> <p>By adopting Hydra:   1. You keep your Python code simpler\u2014just read cfg.   2. You scale easily to multiple environments or hyperparam sets.   3. You can embed advanced Hydra features (e.g. plugin-based sweepers, custom search space merges, dynamic imports of partial configs, etc.).</p> <p>For deeper reference, see official Hydra docs or the more advanced usage in ProjectEquiSurv's quick_fixes/advanced/pipeline/*.py scripts, where HPC, local, meta-splits, and more specialized sub-configs are managed elegantly.</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Next Steps, [emoji]: Now that you have a comprehensive Hydra setup guide referencing ProjectEquiSurv, try implementing your own conf/ structure, define sub-configs for your model and dataset, and integrate them in a main.py Hydra entry script. If you run into any questions, feel free to ask\u2014what specific aspect of Hydra integration can we clarify next?</p>"},{"location":"pipeline/integration/hydra_integration/refinement_plan_stageB_C/","title":"Hydra Integration Refinement Plan (Stages B &amp; C) - 2025-04-13","text":"<p>This document outlines the approved plan to refine the Hydra integration for Stages B and C, ensuring alignment between documentation, configuration, and code.</p>"},{"location":"pipeline/integration/hydra_integration/refinement_plan_stageB_C/#analysis-summary","title":"Analysis Summary","text":"<ul> <li>Stage B: Implementation uses two config files (<code>rna_predict/conf/model/stageB_torsion.yaml</code>, <code>rna_predict/conf/model/stageB_pairformer.yaml</code>), while documentation (<code>StageB_Torsion_Pairwise.md</code>) showed one nested file. Code integration is functionally correct. User prefers the implemented two-file structure.</li> <li>Stage C: Implemented <code>rna_predict/conf/model/stageC.yaml</code> included a <code>defaults:</code> list referring to a nested model config (<code>mp_nerf_model</code>) with an incorrect path or missing target file, causing a loading error. The overall structure also deviated slightly from the example in <code>StageC_3D_Reconstruction.md</code>. Code integration is functionally correct.</li> </ul>"},{"location":"pipeline/integration/hydra_integration/refinement_plan_stageB_C/#approved-refinement-plan","title":"Approved Refinement Plan","text":"<ol> <li> <p>Update Stage B Documentation:</p> <ul> <li>File: <code>docs/pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise.md</code></li> <li>Action: Modify YAML examples and explanatory text to accurately describe the implemented two-file structure (<code>stageB_torsion.yaml</code>, <code>stageB_pairformer.yaml</code>) and their inclusion in <code>default.yaml</code>. (This requires only documentation edits).</li> <li>Reason: Align documentation with the approved and working implementation for clarity.</li> </ul> </li> <li> <p>Refine Stage C Configuration:</p> <ul> <li>File: <code>rna_predict/conf/model/stageC.yaml</code></li> <li>Action: Overwrite the file content to:<ul> <li>Remove the <code>defaults:</code> list section entirely.</li> <li>Ensure parameters (<code>method</code>, <code>device</code>, <code>do_ring_closure</code>, <code>place_bases</code>, <code>sugar_pucker</code>, memory flags like <code>use_memory_efficient_kernel</code>) are directly under the top-level <code>stageC:</code> key, matching the structure in <code>StageC_3D_Reconstruction.md</code>.</li> <li>Set the default for <code>device</code> to <code>\"auto\"</code>.</li> </ul> </li> <li>Reason: Resolve the config loading error and align <code>stageC.yaml</code> structure with its specific design documentation. Selection of underlying <code>mp_nerf_model</code> variants (if needed) will depend on the main <code>default.yaml</code> or CLI overrides.</li> </ul> </li> </ol>"},{"location":"pipeline/integration/hydra_integration/refinement_plan_stageB_C/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>No Python code changes are planned for Stages B or C as part of this refinement plan.</li> <li>The existence and structure of <code>rna_predict/conf/mp_nerf_model/</code> and its files (e.g., <code>default_rna.yaml</code>) are currently outside the scope of this specific refinement but may need addressing separately for full Stage C functionality with different model variants.</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/","title":"HYDRA &amp; STAGE-SPECIFIC DOCUMENTATION: MASTER PLAN","text":"<p>Below is a unified, in-depth reference that merges your existing Hydra Integration Plan with a systematic approach to generating specialized documentation for each pipeline stage (A\u2013D) and the Unified Latent Merger. It aims to ensure that when you refactor each stage, you'll have thorough reference materials connecting code logic, Hydra configuration, algorithmic details, and integration points.</p>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#1-comprehensive-feature-documentation-hydra-already-done","title":"1. Comprehensive Feature Documentation (Hydra) \u2014 Already Done","text":"<p>You already have a \"Hydra Integration Master Document\" that: - Explains why Hydra is beneficial (centralization, easy overrides, reproducibility) - Details how to structure <code>conf/</code> with YAML files, typed dataclasses in <code>config_schema.py</code>, and how to refactor each stage to read <code>cfg</code> instead of inline defaults - Includes acceptance criteria ensuring backward compatibility, successful CLI overrides, and default param matching</p> <p>That comprehensive Hydra document covers the feature's high-level design, scope, acceptance criteria, step-by-step implementation plan, and references to all pipeline stages. It's your central guide for implementing Hydra across the entire codebase.</p>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#2-general-considerations-hydra-documentation","title":"2. General Considerations (Hydra + Documentation)","text":"<p>You've also enumerated general considerations such as:</p> <ul> <li>Code directory structure in <code>rna_predict/conf/</code></li> <li>Logging HPC environment or partial checkpoint loading</li> <li>The need for iterative refinement of docstrings and minimal HPC notes, etc.</li> </ul> <p>These ensure your Hydra-based approach remains robust over time. </p>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#3-plan-to-generate-specialized-documentation-per-stage","title":"3. Plan to Generate Specialized Documentation Per Stage","text":"<p>Now, we focus on task-specific documentation for each stage and the unified latent merger. The goal is to create a dedicated reference for each major component in your pipeline, clarifying how Hydra-managed parameters align with the stage's logic and function calls.</p>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#31-overall-methodology","title":"3.1 Overall Methodology","text":"<ol> <li>Automated Extraction </li> <li>Use a doc generator (Sphinx, PyDoctor, or similar) to produce base-level references from docstrings, class definitions, and module docblocks</li> <li> <p>This yields an initial skeleton capturing function/class signatures, parameters, and any existing docstring descriptions</p> </li> <li> <p>Manual Enrichment </p> </li> <li>Conduct a structured code review for each stage. Identify:<ul> <li>\"Magic numbers,\" hidden assumptions, or specialized data transformations</li> <li>Key integration points (e.g., adjacency from Stage A feeding into Pairformer in Stage B)</li> <li>Known edge cases or fallback logic (like Stage C legacy reconstruction)</li> </ul> </li> <li> <p>Merge these findings into your doc generator output to create in-depth references that go beyond auto-generated docs</p> </li> <li> <p>Documentation Outline    For each stage (A\u2013D plus the Unified Latent Merger), follow a consistent template:</p> </li> <li>Purpose &amp; Overview <ul> <li>Short paragraph explaining the stage's function and motivation (e.g., \"Stage A: Generate a contact matrix for RNA base pairs.\")</li> </ul> </li> <li>Inputs &amp; Outputs <ul> <li>Data shapes, types, and any assumptions (sin/cos angles, adjacency dimensions, etc.)  </li> <li>Expected Hydra config parameters that might affect these shapes or the stage's logic</li> </ul> </li> <li>Key Classes &amp; Methods <ul> <li>Summaries of the top-level classes and methods (like <code>StageARFoldPredictor</code>) with bullet-point commentary about their roles  </li> <li>Focus on lines or subroutines that are vital for understanding how Hydra's config drives them</li> </ul> </li> <li>Configuration Parameters <ul> <li>A listing of the YAML fields relevant to this stage (e.g., <code>stageA.num_hidden</code>, <code>stageA.dropout</code>, etc.), including defaults, usage, and any synergy with upstream or downstream parameters  </li> <li>Example of how these values appear in the YAML (<code>conf/model/stageA.yaml</code>) and how they map to the code</li> </ul> </li> <li>Integration &amp; Data Flow <ul> <li>Which module feeds into this stage, and which stage(s) rely on its outputs?  </li> <li>If adjacency from Stage A is used by TorsionBERT or Pairformer, note that explicitly</li> </ul> </li> <li>Edge Cases &amp; Error Handling <ul> <li>Mention what the code does when inputs are malformed, config is out-of-range, or external files (e.g., checkpoint paths) are missing</li> </ul> </li> <li> <p>References &amp; Dependencies </p> <ul> <li>Links to external docs (like the RFold paper) or any specialized library usage (like LoRA injection modules, MP-NeRF references)</li> </ul> </li> <li> <p>Iterate &amp; Validate </p> </li> <li>Circulate these specialized docs among developers for correctness  </li> <li>Update them in tandem with code changes or as part of the PR (pull request) process to keep them fresh</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#4-stage-by-stage-documentation-breakdown","title":"4. Stage-by-Stage Documentation Breakdown","text":"<p>This section sketches an ideal, final content structure for each specialized doc. You can store them either in separate Markdown files (one per stage) or unify them under one \"RNA_PREDICT Stage Docs\" directory. The essential idea is consistent coverage.</p>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#41-stage-a-2d-adjacency-prediction-via-rfold","title":"4.1 Stage A: 2D Adjacency Prediction (via RFold)","text":"<p>Title: StageA_2D_Adjacency.md</p> <p>Sections: 1. Purpose &amp; Background    - Summarize how adjacency prediction is the first step in the pipeline, providing base-pair contacts    - Reference any relevant paper or logic used (like K-rook approach from RFold)</p> <ol> <li>Inputs &amp; Outputs </li> <li>Input: Single RNA sequence (string), possible multi-line FASTA, etc.  </li> <li>Output: NxN adjacency matrix (probabilistic or binary)  </li> <li> <p>Possible shapes or thresholding methods described</p> </li> <li> <p>Key Classes &amp; Scripts </p> </li> <li><code>StageARFoldPredictor</code> class  <ul> <li>Fields: <code>num_hidden</code>, <code>dropout</code>, <code>checkpoint_path</code>, device usage  </li> <li>Notable methods: <code>_get_cut_len()</code>, <code>predict_adjacency()</code></li> </ul> </li> <li> <p><code>run_stageA.py</code> entry logic: If it downloads missing checkpoints or calls visualization with VARNA</p> </li> <li> <p>Hydra Configuration </p> </li> <li>List <code>stageA.num_hidden</code>, <code>stageA.dropout</code>, etc., from <code>stageA.yaml</code> </li> <li>Provide short code snippet: \"In <code>stageA.yaml</code>, we define <code>num_hidden: 128</code>, etc.\"</li> <li> <p>Show how <code>StageARFoldPredictor</code> loads these config values</p> </li> <li> <p>Integration </p> </li> <li>Upstream: none. (Stage A is the pipeline start for structure)  </li> <li>Downstream: Stage B's TorsionBERT or Pairformer might read adjacency  </li> <li> <p>Additional references: If adjacency is visualized with <code>varna_jar_path</code></p> </li> <li> <p>Edge Cases &amp; Logging </p> </li> <li> <p>If the sequence is &lt; 4 nucleotides, if the checkpoint is missing/corrupted, if a mismatch occurs in dimension, etc.</p> </li> <li> <p>References </p> </li> <li>Link to the original RFold paper or relevant GitHub repos  </li> <li>Mention if it depends on a certain PyTorch version or other library for adjacency calculation</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#42-stage-b-torsionbert-lora-optional-pairformer-lora-optional","title":"4.2 Stage B: TorsionBERT (LoRA optional) &amp; Pairformer (LoRA optional)","text":"<p>Title: StageB_TorsionBERT_and_Pairformer.md</p> <p>Sections: 1. Overview &amp; Role    - This stage outputs local angles (TorsionBERT) and pair embeddings (Pairformer) for each residue/residue pair    - Quick rationale: angles feed partial 3D builds, pair embeddings capture global context</p> <ol> <li>Inputs &amp; Outputs </li> <li>Inputs: Possibly adjacency matrix from Stage A, raw sequence, or other features  </li> <li> <p>Outputs: Torsion angles shaped <code>[N, K or 2K]</code> if using sin/cos, single embeddings <code>[N, c_s]</code>, pair embeddings <code>[N, N, c_z]</code></p> </li> <li> <p>Key Classes </p> </li> <li><code>StageBTorsionBertPredictor</code>, <code>PairformerWrapper</code>, plus any support code  </li> <li> <p>If LoRA is used, identify injection points in attention or feed-forward layers</p> </li> <li> <p>Hydra Config </p> </li> <li>TorsionBert parameters from <code>stageB_torsion.yaml</code> (like <code>model_name_or_path</code>, <code>lora.enabled</code>, <code>lora.r</code>)  </li> <li>Pairformer parameters from <code>stageB_pairformer.yaml</code> (like <code>c_s</code>, <code>c_z</code>, <code>use_checkpoint</code>)  </li> <li> <p>Show examples of toggling LoRA or specifying target modules in YAML</p> </li> <li> <p>Integration </p> </li> <li>Upstream: adjacency or sequence from Stage A  </li> <li> <p>Downstream: Stage C uses angles for partial 3D. Pair embeddings might pass to the Unified Merger</p> </li> <li> <p>Edge Cases &amp; Error Handling </p> </li> <li>Sequence indexing issues, mismatch in adjacency dimension, or missing LoRA checkpoint  </li> <li> <p>Potential timeouts or memory constraints if <code>n_blocks</code> or <code>c_z</code> is large</p> </li> <li> <p>References </p> </li> <li>If your TorsionBERT is based on an existing HF model or a custom paper, link it  </li> <li>If Pairformer references a particular paper or approach for pairwise embeddings, mention it</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#43-stage-c-3d-reconstruction-mp-nerf-or-fallback","title":"4.3 Stage C: 3D Reconstruction (MP-NeRF or Fallback)","text":"<p>Title: StageC_3D_Reconstruction.md</p> <p>Sections: 1. Objective &amp; Methods    - Outline how torsion angles become backbone coordinates, referencing MP-NeRF or the trivial fallback    - Possibly mention sugar pucker default \"C3'-endo\" and ring closure logic</p> <ol> <li>Inputs &amp; Outputs </li> <li>Input: Torsion angles from Stage B, optionally adjacency or sequence  </li> <li> <p>Output: 3D coords shaped <code>[N, #atoms, 3]</code> or <code>[N * #atoms, 3]</code></p> </li> <li> <p>Core Classes &amp; Functions </p> </li> <li><code>run_stageC_rna_mpnerf(...)</code>, <code>StageCReconstruction</code> fallback  </li> <li> <p>Describe how angles are clipped or expanded if user sets a different <code>expected_torsion_count</code></p> </li> <li> <p>Hydra Config </p> </li> <li><code>stageC.yaml</code> entries: <code>method</code>, <code>do_ring_closure</code>, <code>place_bases</code>, <code>sugar_pucker</code>, etc.  </li> <li> <p>Show how code picks the method string to decide on MP-NeRF or fallback</p> </li> <li> <p>Integration &amp; Data Flow </p> </li> <li>Outputs partial 3D coords for the Unified Latent Merger or Stage D directly (depending on your pipeline design)  </li> <li> <p>Mention memory usage, potential HPC constraints for large sequences</p> </li> <li> <p>Edge Cases &amp; Error Handling </p> </li> <li>If angles are incomplete or if user tries \"legacy\" fallback  </li> <li>Negative or out-of-range angles if TorsionBERT is uninitialized, etc.</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#unified-latent-merger","title":"Unified Latent Merger","text":"<p>Title: UnifiedLatentMerger.md</p> <p>Sections: 1. Overview &amp; Importance    - This merges adjacency, angles, partial coords, single/pair embeddings into a single \"conditioning latent\" for Stage D    - Ensures synergy between local angle features and global pair embeddings</p> <ol> <li>Input Structures </li> <li>For example, <code>[N, dim_angles]</code> for angles, <code>[N, c_s]</code> single embeddings, <code>[N, N, c_z]</code> pair embeddings, <code>[N, #atoms, 3 or N* #atoms, 3]</code> partial coords  </li> <li> <p>Possibly row-wise pooling of pair embeddings to get <code>[N, c_z]</code></p> </li> <li> <p>Merger Architecture </p> </li> <li>If it's a simple MLP (like <code>SimpleLatentMerger</code>), specify hidden layers, activation, dropout  </li> <li> <p>If you might adopt a small transformer in the future, mention that possibility</p> </li> <li> <p>Hydra Config </p> </li> <li>From <code>latent_merger.yaml</code>: <code>dim_angles</code>, <code>dim_s</code>, <code>dim_z</code>, <code>dim_out</code>, etc.  </li> <li> <p>How you handle dimension mismatches or expansions (<code>z_pooled = z.mean(dim=1)</code>)</p> </li> <li> <p>Integration </p> </li> <li>Upstream: results from Stage A/B/C  </li> <li> <p>Downstream: Stage D diffusion uses the merged latent as a conditioning input</p> </li> <li> <p>Edge Cases </p> </li> <li>If the shapes from upstream do not match config expectations. Possibly mention logging or shape asserts</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#44-stage-d-diffusion-based-refinement-optional-energy-minimization","title":"4.4 Stage D: Diffusion-based Refinement &amp; Optional Energy Minimization","text":"<p>Title: StageD_Diffusion_and_Minimization.md</p> <p>Sections: 1. Purpose    - Final step refining partial 3D coords or random initialization, possibly using advanced diffusion techniques    - Optional: short local MD or energy minimization for finishing</p> <ol> <li>Input &amp; Output </li> <li>Input: Partial coords from Stage C or random noise, plus the unified latent  </li> <li> <p>Output: final 3D structure [N, #atoms, 3], possibly an ensemble from multiple diffusion seeds</p> </li> <li> <p>Core Modules </p> </li> <li> <p><code>run_stageD.py</code>, <code>protenix_diffusion_manager.py</code>, memory optimization logic</p> </li> <li> <p>Hydra Config </p> </li> <li><code>stageD.yaml</code> entries: <code>sigma_data</code>, <code>noise_scale</code>, <code>n_steps</code>, etc.  </li> <li> <p><code>energy_minimization.enabled</code>, <code>steps</code>, <code>method</code></p> </li> <li> <p>Integration </p> </li> <li>Upstream: Unified latent and possibly partial coords from Stage C  </li> <li> <p>Downstream: Final output, possibly with energy scores or ensemble statistics</p> </li> <li> <p>Edge Cases &amp; Error Handling </p> </li> <li>Memory management for large sequences  </li> <li> <p>Handling failed energy minimization or diffusion instability</p> </li> <li> <p>References </p> </li> <li>Any papers on RNA-specific diffusion or energy minimization  </li> <li>Links to external tools if used for minimization</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stage_specific_documentation_plan/#5-implementation-maintenance-plan","title":"5. Implementation &amp; Maintenance Plan","text":"<ol> <li>Initial Documentation Generation</li> <li>Use automated tools to extract docstrings and signatures</li> <li>Fill in the template sections manually for each stage</li> <li> <p>Add code examples and configuration snippets</p> </li> <li> <p>Review &amp; Validation</p> </li> <li>Have stage owners review their respective docs</li> <li>Cross-check with actual code behavior</li> <li> <p>Verify all config parameters are documented</p> </li> <li> <p>Integration with Development Flow</p> </li> <li>Store docs in version control with code</li> <li>Update docs as part of PR process</li> <li> <p>Regular review for accuracy</p> </li> <li> <p>Future Updates</p> </li> <li>Plan for new features (like Perceiver IO)</li> <li>Document experimental configurations</li> <li>Keep HPC notes current</li> </ol> <p>The goal is maintaining living documentation that stays synchronized with the codebase while providing clear guidance for both new developers and those working on specific pipeline stages. </p>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/","title":"Stage A: 2D Adjacency Prediction (via RFold)","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#purpose-background","title":"Purpose &amp; Background","text":"<p>Stage A is the first step in the RNA_PREDICT pipeline, responsible for generating a contact matrix for RNA base pairs. This stage uses the RFold model to predict the 2D adjacency matrix that represents potential base-pairing interactions between nucleotides in the RNA sequence. The adjacency matrix serves as a critical input for downstream stages, providing structural constraints that guide the 3D structure prediction.</p> <p>The RFold approach employs a K-rook strategy to model RNA secondary structure, which is particularly effective for capturing the hierarchical nature of RNA folding. This stage establishes the foundation for the entire pipeline by providing the initial structural information that subsequent stages will refine and expand upon.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#inputs-outputs","title":"Inputs &amp; Outputs","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#inputs","title":"Inputs","text":"<ul> <li>RNA Sequence: A string representing the RNA sequence (e.g., \"AUGC\")</li> <li>Optional: Multi-line FASTA format for batch processing</li> <li>Optional: Pre-existing secondary structure constraints</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#outputs","title":"Outputs","text":"<ul> <li>Adjacency Matrix: An N\u00d7N matrix where N is the length of the RNA sequence</li> <li>Binary version: Contains 0s and 1s indicating absence/presence of base pairs</li> <li>Probabilistic version: Contains values between 0 and 1 representing base-pairing probabilities</li> <li>Visualization: Optional VARNA visualization of the predicted secondary structure</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#shape-and-format","title":"Shape and Format","text":"<ul> <li>The adjacency matrix has shape <code>[N, N]</code> where N is the sequence length</li> <li>For probabilistic output, values range from 0 to 1</li> <li>For binary output, values are either 0 or 1 after thresholding</li> <li>The matrix is symmetric due to the bidirectional nature of base pairing</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#key-classes-scripts","title":"Key Classes &amp; Scripts","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#stagearfoldpredictor-class","title":"<code>StageARFoldPredictor</code> Class","text":"<p>This is the main class responsible for loading the RFold model and generating adjacency predictions.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#key-fields","title":"Key Fields:","text":"<ul> <li><code>num_hidden</code>: Number of hidden units in the model (configurable via Hydra)</li> <li><code>dropout</code>: Dropout rate for regularization (configurable via Hydra)</li> <li><code>checkpoint_path</code>: Path to the pre-trained model checkpoint</li> <li><code>device</code>: Device to run the model on (CPU/GPU)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#notable-methods","title":"Notable Methods:","text":"<ul> <li><code>_get_cut_len()</code>: Determines the maximum sequence length the model can process</li> <li>Ensures sequence length is a multiple of 16 for efficient processing</li> <li>Minimum length of 80 nucleotides is enforced</li> <li><code>predict_adjacency()</code>: Core method that generates the adjacency matrix from an RNA sequence</li> <li>Handles sequence padding and processing</li> <li>Applies the RFold model for prediction</li> <li>Uses row/column argmax for final adjacency determination</li> <li><code>_load_model()</code>: Handles model initialization and checkpoint loading</li> <li><code>_preprocess_sequence()</code>: Prepares the input sequence for the model</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#run_stageapy","title":"<code>run_stageA.py</code>","text":"<p>This script serves as the entry point for Stage A execution.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#key-functionality","title":"Key Functionality:","text":"<ul> <li>Parses command-line arguments and Hydra configuration</li> <li>Initializes the <code>StageARFoldPredictor</code> with appropriate parameters</li> <li>Handles checkpoint downloading if the model file is missing</li> <li>Processes the input sequence and generates the adjacency matrix</li> <li>Optionally visualizes the result using VARNA</li> <li>Saves the output in the specified format</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#core-rfold-implementation-rfold_codepy","title":"Core RFold Implementation (<code>RFold_code.py</code>)","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#key-components","title":"Key Components:","text":"<ul> <li><code>RFoldModel</code>: Main model class combining Seq2Map and U-Net architecture</li> <li>Encoder: Processes input through a series of convolutional layers</li> <li>Decoder: Reconstructs the adjacency matrix through upsampling</li> <li>Seq2Map: Handles sequence to map conversion with attention mechanisms</li> <li><code>constraint_matrix</code>: Enforces base-pairing rules (A-U, C-G, G-U)</li> <li><code>row_col_argmax</code>: Implements the K-rook strategy for structure prediction</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#hydra-configuration","title":"Hydra Configuration","text":"<p>Stage A's behavior is controlled through Hydra configuration in <code>conf/model/stageA.yaml</code>:</p> <pre><code>stageA:\n  # Model Architecture\n  num_hidden: 128  # Number of hidden units in Seq2Map\n  dropout: 0.3     # Dropout rate for regularization\n\n  # Model Loading\n  checkpoint_path: \"checkpoints/RNAStralign_trainset_pretrained.pth\"\n  checkpoint_url: \"https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1\"  # URL to download checkpoint if not found\n  use_gpu: true    # Whether to use GPU for inference\n\n  # Processing Parameters\n  min_seq_length: 80  # Minimum sequence length for processing\n  batch_size: 32      # Batch size for processing multiple sequences\n  lr: 0.001           # Learning rate (used during training/fine-tuning)\n\n  # Output Configuration\n  threshold: 0.5      # Threshold for converting probabilistic to binary adjacency\n\n  # Visualization\n  visualization:\n    enabled: true\n    varna_jar_path: \"tools/varna-3-93.jar\"\n    resolution: 8.0   # VARNA visualization resolution\n\n  # Model Architecture Details\n  model:\n    # U-Net Architecture Parameters\n    conv_channels: [64, 128, 256, 512]  # Channel dimensions for U-Net\n    residual: true                      # Whether to use residual connections\n    c_in: 1                            # Input channels for U-Net\n    c_out: 1                           # Output channels for U-Net\n    c_hid: 32                          # Hidden channels for U-Net\n\n    # Seq2Map Parameters\n    seq2map:\n      input_dim: 4                     # Input dimension (number of RNA bases)\n      max_length: 3000                 # Maximum sequence length for positional encoding\n      attention_heads: 8               # Number of attention heads\n      attention_dropout: 0.1           # Dropout rate for attention layers\n      positional_encoding: true        # Whether to use positional encoding\n      query_key_dim: 128               # Query/key dimension for attention mechanism\n      expansion_factor: 2.0            # Expansion factor for attention mechanism\n      heads: 1                         # Number of heads for OffsetScale\n\n    # Decoder Parameters\n    decoder:\n      up_conv_channels: [256, 128, 64]  # Channel dimensions for upsampling\n      skip_connections: true            # Whether to use skip connections\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#configuration-parameters","title":"Configuration Parameters:","text":"Parameter Type Default Description <code>num_hidden</code> int 128 Number of hidden units in the Seq2Map component. Controls the dimensionality of the token embeddings and positional encodings. Larger values increase model capacity but require more memory. <code>dropout</code> float 0.3 Dropout rate for regularization. Applied to the Seq2Map component to prevent overfitting. Higher values increase regularization but may reduce model performance. <code>checkpoint_path</code> str \"checkpoints/RNAStralign_trainset_pretrained.pth\" Path to the pre-trained model checkpoint. The model will load weights from this file during initialization. <code>checkpoint_url</code> str \"https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1\" URL to download checkpoint if not found locally. Used by the automatic checkpoint download mechanism. <code>use_gpu</code> bool true Whether to use GPU for inference. If true and a GPU is available, the model will run on the GPU for faster processing. <code>min_seq_length</code> int 80 Minimum sequence length for processing. Sequences shorter than this will be padded to this length. This ensures efficient processing by making sequence lengths multiples of 16. <code>batch_size</code> int 32 Batch size for processing multiple sequences. Larger batch sizes improve throughput but require more memory. <code>lr</code> float 0.001 Learning rate used during training or fine-tuning. Controls how quickly the model adapts to new data. <code>threshold</code> float 0.5 Threshold for converting probabilistic to binary adjacency. Values above this threshold are considered base pairs. <code>visualization.enabled</code> bool true Whether to generate VARNA visualization of the predicted secondary structure. <code>visualization.varna_jar_path</code> str \"tools/varna-3-93.jar\" Path to the VARNA JAR file used for visualization. VARNA is a tool for visualizing RNA secondary structures. <code>visualization.resolution</code> float 8.0 Resolution for VARNA visualization. Higher values produce higher quality images but require more processing time. <code>model.conv_channels</code> list [64, 128, 256, 512] Channel dimensions for U-Net encoder. Defines the number of filters at each level of the U-Net architecture. <code>model.residual</code> bool true Whether to use residual connections in conv blocks. Residual connections help with gradient flow and can improve training stability. <code>model.c_in</code> int 1 Input channels for U-Net. Set to 1 as the input is a single-channel attention map. <code>model.c_out</code> int 1 Output channels for U-Net. Set to 1 as the output is a single-channel adjacency matrix. <code>model.c_hid</code> int 32 Hidden channels for U-Net. Controls the number of features in the bottleneck layer. <code>model.seq2map.input_dim</code> int 4 Input dimension for Seq2Map, representing the number of RNA bases (A, U, C, G). <code>model.seq2map.max_length</code> int 3000 Maximum sequence length for positional encoding. Sequences longer than this may not be processed correctly. <code>model.seq2map.attention_heads</code> int 8 Number of attention heads in Seq2Map. Multi-head attention allows the model to focus on different aspects of the sequence simultaneously. <code>model.seq2map.attention_dropout</code> float 0.1 Dropout rate for attention layers. Applied to the attention weights to prevent overfitting. <code>model.seq2map.positional_encoding</code> bool true Whether to use positional encoding. Positional encodings help the model understand the relative positions of nucleotides in the sequence. <code>model.seq2map.query_key_dim</code> int 128 Query/key dimension for attention mechanism. Controls the dimensionality of the query and key vectors in the attention computation. <code>model.seq2map.expansion_factor</code> float 2.0 Expansion factor for attention mechanism. Controls how much the attention scores are scaled before normalization. <code>model.seq2map.heads</code> int 1 Number of heads for OffsetScale. Controls the number of parallel scaling operations in the attention mechanism. <code>model.decoder.up_conv_channels</code> list [256, 128, 64] Channel dimensions for upsampling in the decoder. Defines the number of filters at each upsampling level. <code>model.decoder.skip_connections</code> bool true Whether to use skip connections in decoder. Skip connections help preserve fine-grained details from the encoder."},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#model-architecture-details","title":"Model Architecture Details","text":"<p>The RFold model combines a Seq2Map attention mechanism with a U-Net architecture to predict RNA secondary structure:</p> <ol> <li>Seq2Map Component:</li> <li>Converts the RNA sequence into a contact map using attention mechanisms</li> <li>Uses token embeddings to represent each nucleotide</li> <li>Applies positional encodings to capture sequence position information</li> <li> <p>Employs multi-head attention to model interactions between nucleotides</p> </li> <li> <p>U-Net Architecture:</p> </li> <li>Encoder: Processes the attention map through a series of convolutional layers with increasing channel dimensions</li> <li>Decoder: Reconstructs the adjacency matrix through upsampling and convolutional layers</li> <li>Skip connections: Preserve fine-grained details from the encoder to the decoder</li> <li> <p>Final readout: Converts the decoder output into the final adjacency matrix</p> </li> <li> <p>K-Rook Strategy:</p> </li> <li>Uses row/column argmax to ensure each nucleotide pairs with at most one other nucleotide</li> <li>Applies constraint matrix to enforce valid base-pairing rules (A-U, C-G, G-U)</li> <li>Produces a binary adjacency matrix representing the predicted secondary structure</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#code-integration","title":"Code Integration:","text":"<p>The configuration is loaded and used in the following way:</p> <pre><code>def __init__(self, config, checkpoint_path=None, device=None):\n    # Load config from file if string path is provided\n    if isinstance(config, str):\n        with open(config, \"r\") as f:\n            config = json.load(f)\n\n    # Set device based on config\n    use_gpu = config.get(\"use_gpu\", True)\n    self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n\n    # Initialize model with config parameters\n    self.model = RFoldModel(args_namespace(config))\n    self.model.to(self.device)\n    self.model.eval()\n\n    # Load checkpoint if provided\n    if checkpoint_path:\n        self.model.load_state_dict(torch.load(checkpoint_path, map_location=self.device))\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#integration","title":"Integration","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#upstream-dependencies","title":"Upstream Dependencies","text":"<ul> <li>None. Stage A is the starting point of the pipeline for structure prediction.</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#downstream-dependencies","title":"Downstream Dependencies","text":"<ul> <li>Stage B: Uses the adjacency matrix as input for TorsionBERT and Pairformer</li> <li>TorsionBERT may use adjacency information to condition angle predictions</li> <li>Pairformer uses the adjacency matrix to guide pair embedding generation</li> <li>Unified Latent Merger: Incorporates adjacency information into the conditioning latent</li> <li>Stage D: May use adjacency information for diffusion guidance</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#data-flow","title":"Data Flow","text":"<ol> <li>RNA sequence is input to Stage A</li> <li>Stage A generates the adjacency matrix through the following steps:</li> <li>Sequence preprocessing and padding</li> <li>RFold model inference</li> <li>Row/column argmax processing</li> <li>Constraint matrix application</li> <li>The adjacency matrix is passed to Stage B for further processing</li> <li>The adjacency information flows through the pipeline, influencing subsequent stages</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#edge-cases-error-handling","title":"Edge Cases &amp; Error Handling","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#sequence-length-issues","title":"Sequence Length Issues","text":"<ul> <li>Short Sequences: If the sequence is less than 4 nucleotides, the model returns a zero adjacency matrix</li> <li>Long Sequences: For sequences exceeding the model's maximum length, they are processed in chunks</li> <li>The <code>_get_cut_len()</code> method ensures sequences are padded to multiples of 16</li> <li>Minimum sequence length of 80 nucleotides is enforced</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#checkpoint-issues","title":"Checkpoint Issues","text":"<ul> <li>Missing Checkpoint: The code attempts to download the checkpoint if it's not found locally</li> <li>Uses a predefined URL for checkpoint download</li> <li>Verifies downloaded file integrity</li> <li>Corrupted Checkpoint: Error handling for corrupted model files with informative error messages</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#dimension-mismatches","title":"Dimension Mismatches","text":"<ul> <li>Validation to ensure the adjacency matrix dimensions match the input sequence length</li> <li>Shape assertions to catch potential issues early in the pipeline</li> <li>Proper handling of padded sequences in the output</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#hpc-considerations","title":"HPC Considerations","text":"<ul> <li>Memory usage optimization for large sequences</li> <li>GPU memory management for batch processing</li> <li>Efficient sequence padding and processing</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#references-dependencies","title":"References &amp; Dependencies","text":""},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#papers","title":"Papers","text":"<ul> <li>RFold: \"RNA Secondary Structure Prediction by Learning Unrolled Algorithms\" (Chen et al., ICLR 2019)</li> <li>K-rook approach for RNA secondary structure: \"Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective\" (Tan et al., arXiv 2024)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>PyTorch (version \u2265 1.8.0)</li> <li>VARNA (for visualization)</li> <li>NumPy (for matrix operations)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageA/StageA_2D_Adjacency/#external-resources","title":"External Resources","text":"<ul> <li>RFold GitHub repository: https://github.com/keio-bioinformatics/neuralfold/</li> <li>VARNA documentation: http://varna.lri.fr/ </li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/","title":"Stage B: Torsion Angles and Pairwise Embeddings","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#purpose-background","title":"Purpose &amp; Background","text":"<p>Stage B is the second step in the RNA_PREDICT pipeline, responsible for generating two key components: 1. Torsion angles for each nucleotide using TorsionBERT 2. Pairwise embeddings using the Pairformer architecture</p> <p>This stage combines two complementary approaches: - TorsionBERT predicts the dihedral angles that define the local geometry of each nucleotide - Pairformer generates embeddings that capture the relationships between pairs of nucleotides</p> <p>The outputs from Stage B provide crucial information for the 3D structure prediction in subsequent stages, with torsion angles defining local geometry and pairwise embeddings capturing global relationships.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#inputs-outputs","title":"Inputs &amp; Outputs","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#inputs","title":"Inputs","text":"<ul> <li>RNA Sequence: A string representing the RNA sequence (e.g., \"AUGC\")</li> <li>Adjacency Matrix: From Stage A, shape [N, N] indicating base-pairing probabilities</li> <li>Optional: Pre-existing torsion angle constraints</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#outputs","title":"Outputs","text":"<ol> <li>Torsion Angles: Shape [N, K] or [N, 2K] where:</li> <li>N is the sequence length</li> <li>K is the number of angles (default: 7)</li> <li>2K format represents sin/cos pairs for each angle</li> <li>Single Embeddings: Shape [N, c_s] where:</li> <li>c_s is the embedding dimension (default: 384)</li> <li>Represents features for each nucleotide</li> <li>Pair Embeddings: Shape [N, N, c_z] where:</li> <li>c_z is the pair embedding dimension (default: 128)</li> <li>Captures relationships between nucleotide pairs</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#key-classes-scripts","title":"Key Classes &amp; Scripts","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#stagebtorsionbertpredictor-class","title":"<code>StageBTorsionBertPredictor</code> Class","text":"<p>This class handles the prediction of torsion angles using the TorsionBERT model.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#key-fields","title":"Key Fields:","text":"<ul> <li><code>model_name_or_path</code>: Path to the pre-trained TorsionBERT model</li> <li><code>device</code>: Device to run the model on (CPU/GPU)</li> <li><code>angle_mode</code>: Output format (\"sin_cos\", \"radians\", or \"degrees\")</li> <li><code>num_angles</code>: Number of torsion angles to predict</li> <li><code>max_length</code>: Maximum sequence length for processing</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#notable-methods","title":"Notable Methods:","text":"<ul> <li><code>__call__()</code>: Main inference method that processes a sequence and returns torsion angles</li> <li><code>_convert_sincos_to_angles()</code>: Converts sin/cos pairs to actual angles</li> <li><code>predict_angles_from_sequence()</code>: Core method for angle prediction</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#pairformerwrapper-class","title":"<code>PairformerWrapper</code> Class","text":"<p>This class manages the Pairformer model for generating pairwise embeddings.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#key-fields_1","title":"Key Fields:","text":"<ul> <li><code>n_blocks</code>: Number of Pairformer blocks (default: 48)</li> <li><code>c_z</code>: Dimension of pair embeddings (default: 128)</li> <li><code>c_s</code>: Dimension of single embeddings (default: 384)</li> <li><code>dropout</code>: Dropout rate for regularization</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#notable-methods_1","title":"Notable Methods:","text":"<ul> <li><code>forward()</code>: Processes inputs to generate single and pair embeddings</li> <li><code>_prep_blocks()</code>: Prepares the Pairformer blocks for processing</li> <li><code>clear_cache()</code>: Manages memory during processing</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#run_stageb_combined-function","title":"<code>run_stageB_combined</code> Function","text":"<p>This function orchestrates the combined operation of TorsionBERT and Pairformer.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#key-functionality","title":"Key Functionality:","text":"<ul> <li>Initializes both models with appropriate parameters</li> <li>Processes the input sequence and adjacency matrix</li> <li>Generates torsion angles and embeddings</li> <li>Returns a dictionary containing all outputs</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#hydra-configuration","title":"Hydra Configuration","text":"<p>Stage B's behavior is controlled through Hydra configuration using two separate files located in <code>rna_predict/conf/model/</code>:</p> <ol> <li><code>stageB_torsion.yaml</code>: Configures the TorsionBERT model parameters.</li> <li><code>stageB_pairformer.yaml</code>: Configures the Pairformer model parameters.</li> </ol> <p>These files are included in the main <code>rna_predict/conf/default.yaml</code> via its <code>defaults</code> list, ensuring both configurations are loaded when the pipeline runs.</p> <p>Example: <code>stageB_torsion.yaml</code> <pre><code># rna_predict/conf/model/stageB_torsion.yaml\ntorsion_bert:\n  model_name_or_path: \"sayby/rna_torsionbert\"\n  device: \"cpu\"\n  angle_mode: \"sin_cos\"  # One of: \"sin_cos\", \"radians\", \"degrees\"\n  num_angles: 7\n  max_length: 512\n  checkpoint_path: null\n  lora:\n    enabled: false\n    # ... other lora params\n</code></pre></p> <p>Example: <code>stageB_pairformer.yaml</code> <pre><code># rna_predict/conf/model/stageB_pairformer.yaml\npairformer:\n  n_blocks: 48\n  n_heads: 16\n  c_z: 128\n  c_s: 384\n  dropout: 0.25\n  use_memory_efficient_kernel: false\n  use_deepspeed_evo_attention: false\n  use_lma: false\n  inplace_safe: false\n  chunk_size: null\n  c_hidden_mul: 128 # Note: Currently stored but not passed to PairformerStack constructor\n  c_hidden_pair_att: 32 # Note: Currently stored but not passed to PairformerStack constructor\n  no_heads_pair: 4 # Note: Currently stored but not passed to PairformerStack constructor\n  init_z_from_adjacency: false\n  use_checkpoint: false\n  lora:\n    enabled: false\n    # ... other lora params\n\n# Note: Parameters like batch_size might be defined at a higher level (e.g., train.yaml or main default.yaml)\n# or within the stage-specific entry point script's config if applicable.\n</code></pre></p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#configuration-parameters","title":"Configuration Parameters:","text":"Parameter Type Default Description <code>torsionbert.model_name_or_path</code> str \"sayby/rna_torsionbert\" Path to the pre-trained TorsionBERT model. This should point to a valid Hugging Face model repository or local checkpoint. The model will be downloaded if not found locally. <code>torsionbert.device</code> str \"cpu\" Device to run the model on (\"cpu\" or \"cuda\"). Use \"cuda\" for GPU acceleration, which can significantly speed up inference, especially for longer sequences. <code>torsionbert.angle_mode</code> str \"sin_cos\" Output format for angles. \"sin_cos\" returns raw sin/cos pairs, \"radians\" converts to radians (-\u03c0 to \u03c0), and \"degrees\" converts to degrees (-180\u00b0 to 180\u00b0). Choose based on downstream processing needs. <code>torsionbert.num_angles</code> int 7 Number of torsion angles to predict per nucleotide. The default of 7 corresponds to the backbone angles (alpha, beta, gamma, delta, epsilon, zeta, chi). Can be increased to 17 to include all angles including pseudotorsions and sugar ring torsions. <code>torsionbert.max_length</code> int 512 Maximum sequence length for processing. Sequences longer than this will be truncated. This limit is inherited from the underlying BERT architecture. <code>pairformer.n_blocks</code> int 48 Number of Pairformer blocks in the stack. Each block processes the sequence and adjacency information. More blocks allow for deeper feature extraction but increase memory usage and computation time. For shorter sequences (&lt;100 nt), fewer blocks (16-24) may be sufficient. <code>pairformer.n_heads</code> int 16 Number of attention heads in each block. Controls the diversity of attention patterns. More heads allow the model to focus on different aspects of the sequence and structure simultaneously. <code>pairformer.c_z</code> int 128 Dimension of pair embeddings. Controls the richness of information captured in pairwise relationships. Larger values may improve accuracy but increase memory usage. <code>pairformer.c_s</code> int 384 Dimension of single embeddings. Controls the richness of information captured for each nucleotide. Larger values may improve accuracy but increase memory usage. <code>pairformer.dropout</code> float 0.25 Dropout rate for regularization. Higher values (0.3-0.5) can help prevent overfitting but may reduce accuracy. Lower values (0.1-0.2) are recommended for inference. <code>pairformer.use_memory_efficient_kernel</code> bool false Whether to use memory-efficient operations. Enable this for very long sequences (&gt;500 nt) to reduce memory usage, but may slightly slow down computation. <code>pairformer.use_deepspeed_evo_attention</code> bool false Whether to use DeepSpeed evolutionary attention. This is an experimental feature that may improve performance on certain hardware but requires DeepSpeed to be installed. <code>pairformer.use_lma</code> bool false Whether to use low-memory attention. Similar to memory-efficient kernel but with different implementation. Enable for very long sequences when memory is a constraint. <code>pairformer.inplace_safe</code> bool false Whether to use inplace operations. Enable to reduce memory usage but may cause issues with certain PyTorch operations. Only enable if you understand the implications. <code>pairformer.chunk_size</code> int null Chunk size for memory-efficient operations. When set, the sequence is processed in chunks of this size. Useful for very long sequences that don't fit in memory. <code>pairformer.c_hidden_mul</code> int 128 Hidden dimension for triangle multiplication. Controls the complexity of the triangle multiplication operation. Larger values may improve accuracy but increase computation time. <code>pairformer.c_hidden_pair_att</code> int 32 Hidden dimension for pair attention. Controls the complexity of the pair attention mechanism. Larger values may improve accuracy but increase computation time. <code>pairformer.no_heads_pair</code> int 4 Number of heads for pair attention. Controls the diversity of attention patterns in the pair attention mechanism. More heads allow for more complex pairwise relationships to be captured. <code>batch_size</code> int 32 Batch size for processing multiple sequences. Larger batch sizes improve throughput but increase memory usage. For very long sequences, reduce this value. <code>init_z_from_adjacency</code> bool false Whether to initialize pair embeddings from adjacency matrix. When enabled, the pair embeddings are initialized based on the adjacency matrix from Stage A, which can help incorporate base-pairing information. This is particularly useful when the adjacency matrix is accurate."},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#model-architecture-details","title":"Model Architecture Details","text":"<p>Stage B combines two sophisticated architectures:</p> <ol> <li>TorsionBERT:</li> <li>Transformer-based model for predicting torsion angles</li> <li>Processes RNA sequences to output angle predictions</li> <li>Supports multiple output formats (sin/cos, radians, degrees)</li> <li> <p>Includes fallback to dummy model for testing/development</p> </li> <li> <p>Pairformer:</p> </li> <li>Stack of Pairformer blocks implementing Algorithm 17 from AF3</li> <li>Each block contains:<ul> <li>Triangle multiplication (outgoing and incoming)</li> <li>Triangle attention (start and end)</li> <li>Pair transition</li> <li>Single transition (if c_s &gt; 0)</li> </ul> </li> <li>Uses attention mechanisms to capture pairwise relationships</li> <li>Supports memory-efficient operations for large sequences</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#msamodule-architecture","title":"MSAModule Architecture","text":"<p>The MSAModule is a key component of the Pairformer architecture that handles multiple sequence alignment (MSA) features:</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#core-parameters","title":"Core Parameters","text":"<ul> <li>n_blocks: Number of MSA blocks (default: 8)</li> <li>c_m: Hidden dimension for MSA features (default: 256)</li> <li>c_z: Hidden dimension for pair features (default: 128)</li> <li>c_s_inputs: Input dimension for single features (default: 384)</li> <li>msa_dropout: Dropout rate for MSA features (default: 0.15)</li> <li>pair_dropout: Dropout rate for pair features (default: 0.25)</li> <li>blocks_per_ckpt: Number of blocks per checkpoint for memory efficiency</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#msa-configuration","title":"MSA Configuration","text":"<ul> <li>enable: Whether MSA is enabled (default: False)</li> <li>strategy: MSA sampling strategy (default: \"random\")</li> <li>train_cutoff: Maximum MSA size during training (default: 512)</li> <li>test_cutoff: Maximum MSA size during testing (default: 16384)</li> <li>train_lowerb: Minimum MSA size during training (default: 1)</li> <li>test_lowerb: Minimum MSA size during testing (default: 1)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#architecture-components","title":"Architecture Components","text":"<ul> <li>MSABlock: Base block for MSA processing</li> <li>Contains outer product mean for MSA-to-pair communication</li> <li>Includes MSA stack for MSA feature processing</li> <li>Implements pair stack for pair feature processing</li> <li>MSAStack: Processes MSA embeddings</li> <li>Uses MSAPairWeightedAveraging for weighted averaging</li> <li>Applies dropout for regularization</li> <li>Includes transition layer for feature refinement</li> <li>MSAPairWeightedAveraging: Implements weighted averaging with gating</li> <li>Uses layer normalization for feature normalization</li> <li>Applies linear projections for feature transformation</li> <li>Implements softmax weighting for attention</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#templateembedder-architecture","title":"TemplateEmbedder Architecture","text":"<p>The TemplateEmbedder is designed to incorporate template information into the Pairformer architecture:</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#core-parameters_1","title":"Core Parameters","text":"<ul> <li>n_blocks: Number of template blocks (default: 2)</li> <li>c: Hidden dimension (default: 64)</li> <li>c_z: Pair embedding dimension (default: 128)</li> <li>dropout: Dropout rate (default: 0.25)</li> <li>blocks_per_ckpt: Number of blocks per checkpoint for memory efficiency</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#input-features","title":"Input Features","text":"<ul> <li>template_distogram: 39 dimensions</li> <li>b_template_backbone_frame_mask: 1 dimension</li> <li>template_unit_vector: 3 dimensions</li> <li>b_template_pseudo_beta_mask: 1 dimension</li> <li>template_restype_i: 32 dimensions</li> <li>template_restype_j: 32 dimensions</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#architecture-components_1","title":"Architecture Components","text":"<ul> <li>LinearNoBias: Linear layers without bias terms</li> <li>LayerNorm: Layer normalization for feature normalization</li> <li>PairformerStack: Stack of Pairformer blocks for template processing</li> <li>Uses the same architecture as the main Pairformer</li> <li>Processes template features to generate template embeddings</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#torsionbert-detailed-parameters","title":"TorsionBERT Detailed Parameters","text":"<p>TorsionBERT is a specialized model for predicting RNA torsion angles with the following characteristics:</p>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#model-specifications","title":"Model Specifications","text":"<ul> <li>Model Size: ~328 MB (DNABERT-based)</li> <li>Training Dataset: ~4267 structures</li> <li>Maximum Sequence Length: 512 nucleotides</li> <li>Model Repository: https://huggingface.co/sayby/rna_torsionbert</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#torsion-angles","title":"Torsion Angles","text":"<p>TorsionBERT predicts 17 torsion angles per nucleotide:</p> Angle Description Standard A-form Value alpha P-O5'-C5'-C4' -60.0\u00b0 beta O5'-C5'-C4'-C3' 180.0\u00b0 gamma C5'-C4'-C3'-O3' 60.0\u00b0 delta C4'-C3'-O3'-P 80.0\u00b0 epsilon C3'-O3'-P-O5' -150.0\u00b0 zeta O3'-P-O5'-C5' -70.0\u00b0 chi O4'-C1'-N9/N1-C4/C2 -160.0\u00b0 eta Pseudotorsion - theta Pseudotorsion - eta' Alternative pseudotorsion - theta' Alternative pseudotorsion - v0 Sugar ring torsion - v1 Sugar ring torsion - v2 Sugar ring torsion - v3 Sugar ring torsion - v4 Sugar ring torsion -"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#processing-parameters","title":"Processing Parameters","text":"<ul> <li>k-mer Size: 3 (default)</li> <li>The k-mer size determines how the RNA sequence is tokenized for the TorsionBERT model</li> <li>A k-mer size of 3 means the sequence is split into overlapping 3-nucleotide windows</li> <li>This approach allows the model to capture local context around each nucleotide</li> <li>Larger k-mer sizes (4-5) may capture more context but increase the vocabulary size and memory usage</li> <li>Smaller k-mer sizes (1-2) reduce memory usage but may miss important local context</li> <li>Tokenizer Parameters:   <pre><code>params_tokenizer = {\n    \"return_tensors\": \"pt\",  # Return PyTorch tensors\n    \"padding\": \"max_length\", # Pad sequences to max_length\n    \"max_length\": 512,       # Maximum sequence length\n    \"truncation\": True       # Truncate sequences longer than max_length\n}\n</code></pre></li> <li>These parameters control how the sequence is tokenized and prepared for the model</li> <li><code>return_tensors</code>: Specifies the tensor type (PyTorch in this case)</li> <li><code>padding</code>: Controls how sequences shorter than max_length are padded</li> <li><code>max_length</code>: Maximum sequence length (inherited from BERT architecture)</li> <li><code>truncation</code>: Whether to truncate sequences longer than max_length</li> <li>Sequence Preprocessing: </li> <li>Converts U to T: RNA sequences are converted to DNA format (U\u2192T) because TorsionBERT is based on DNABERT</li> <li>Splits into k-mers: The sequence is split into overlapping k-mers for tokenization</li> <li>This preprocessing is essential for the model to correctly interpret the sequence</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#output-processing","title":"Output Processing","text":"<ul> <li>Angle Conversion: The model outputs sin/cos pairs which can be converted to:</li> <li>Radians (-\u03c0 to \u03c0): Useful for mathematical operations and some downstream tasks</li> <li>Degrees (-180\u00b0 to 180\u00b0): More intuitive for visualization and comparison with experimental data</li> <li>The conversion is done using the arctangent function: angle = atan2(sin, cos)</li> <li>Error Handling:</li> <li>Handles empty sequences by returning empty tensor: Prevents crashes when empty sequences are provided</li> <li>Validates angle mode input: Ensures only valid angle modes are used</li> <li>Handles potential NaN values in angle calculations: Replaces NaN values with zeros or other defaults</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#integration","title":"Integration","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#upstream-dependencies","title":"Upstream Dependencies","text":"<ul> <li>Stage A: Provides the adjacency matrix for base-pairing information</li> <li>Used to condition torsion angle predictions</li> <li>Can initialize pair embeddings in Pairformer</li> <li>The quality of the adjacency matrix significantly impacts the performance of Stage B</li> <li>For best results, ensure Stage A is configured to produce high-confidence base-pairing predictions</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#downstream-dependencies","title":"Downstream Dependencies","text":"<ul> <li>Stage C: Uses both torsion angles and embeddings for 3D structure prediction</li> <li>Torsion angles define local geometry</li> <li>Pair embeddings guide global structure</li> <li>The accuracy of Stage B's outputs directly impacts the quality of the 3D structure</li> <li>Consider using the <code>angle_mode</code> parameter to match the format expected by Stage C</li> <li>Unified Latent Merger: Combines outputs with other stages</li> <li>The latent merger expects specific formats for torsion angles and embeddings</li> <li>Ensure compatibility by using the correct parameter settings</li> <li>Stage D: May use outputs for diffusion guidance</li> <li>The quality of the initial structure from Stage C depends on accurate torsion angles</li> <li>Consider using higher quality settings for Stage B when accuracy is critical</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#data-flow","title":"Data Flow","text":"<ol> <li>RNA sequence and adjacency matrix are input to Stage B</li> <li>TorsionBERT generates torsion angles for each nucleotide</li> <li>The sequence is preprocessed (U\u2192T, k-mer tokenization)</li> <li>The model predicts sin/cos pairs for each angle</li> <li>The output is converted to the specified angle mode</li> <li>Pairformer processes the sequence to generate embeddings:</li> <li>Single embeddings capture nucleotide features</li> <li>Pair embeddings model relationships between nucleotides</li> <li>The adjacency matrix can be used to initialize pair embeddings</li> <li>All outputs are passed to Stage C for 3D structure prediction</li> <li>The torsion angles guide the local geometry</li> <li>The embeddings guide the global structure</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#edge-cases-error-handling","title":"Edge Cases &amp; Error Handling","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#sequence-length-issues","title":"Sequence Length Issues","text":"<ul> <li>Short Sequences: Minimum length requirements from TorsionBERT</li> <li>Sequences shorter than k-mer size (default: 3) may not be processed correctly</li> <li>Consider padding very short sequences or using a smaller k-mer size</li> <li>Long Sequences: Memory-efficient processing options in Pairformer</li> <li>Chunk size configuration for large sequences<ul> <li>Set <code>chunk_size</code> to process the sequence in smaller chunks</li> <li>This reduces memory usage but may slightly impact accuracy</li> </ul> </li> <li>Memory-efficient kernel options<ul> <li>Enable <code>use_memory_efficient_kernel</code> for sequences &gt;500 nt</li> <li>This reduces memory usage by using more efficient algorithms</li> </ul> </li> <li>Low-memory attention support<ul> <li>Enable <code>use_lma</code> for very long sequences</li> <li>This uses a different attention mechanism that requires less memory</li> </ul> </li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#model-loading-issues","title":"Model Loading Issues","text":"<ul> <li>Missing TorsionBERT: Falls back to dummy model for testing</li> <li>If the model cannot be loaded, a dummy model is used that returns zeros</li> <li>This ensures the pipeline can continue even if the model is missing</li> <li>Check the model path and ensure it's accessible</li> <li>Invalid Checkpoints: Graceful error handling with informative messages</li> <li>If the checkpoint is invalid, an error message is displayed</li> <li>The pipeline attempts to continue with a dummy model</li> <li>Check the checkpoint format and ensure it's compatible</li> <li>Device Mismatches: Automatic device placement and transfer</li> <li>Models are automatically moved to the specified device</li> <li>Tensors are transferred between devices as needed</li> <li>Ensure the device is available and has sufficient memory</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#memory-management","title":"Memory Management","text":"<ul> <li>Configurable memory optimization strategies</li> <li>Use <code>use_memory_efficient_kernel</code> for large sequences</li> <li>Set <code>chunk_size</code> to process in smaller chunks</li> <li>Enable <code>inplace_safe</code> to reduce memory usage (with caution)</li> <li>Chunk-based processing for large sequences</li> <li>The sequence is processed in chunks of size <code>chunk_size</code></li> <li>This reduces peak memory usage but may slightly impact accuracy</li> <li>Recommended for sequences &gt;1000 nt</li> <li>Inplace operation options for memory efficiency</li> <li>Enable <code>inplace_safe</code> to use inplace operations</li> <li>This reduces memory usage but may cause issues with certain operations</li> <li>Only enable if you understand the implications</li> <li>Cache clearing between blocks</li> <li>The cache is cleared between Pairformer blocks to reduce memory usage</li> <li>This is automatic and doesn't require configuration</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#hpc-considerations","title":"HPC Considerations","text":"<ul> <li>Memory-efficient kernel options</li> <li>Enable <code>use_memory_efficient_kernel</code> for HPC environments</li> <li>This reduces memory usage per node, allowing for larger batch sizes</li> <li>DeepSpeed integration support</li> <li>Enable <code>use_deepspeed_evo_attention</code> for DeepSpeed compatibility</li> <li>This requires DeepSpeed to be installed and configured</li> <li>Recommended for multi-GPU training</li> <li>Low-memory attention mechanisms</li> <li>Enable <code>use_lma</code> for very large models or sequences</li> <li>This uses a different attention mechanism that requires less memory</li> <li>May slightly impact accuracy</li> <li>Configurable batch processing</li> <li>Adjust <code>batch_size</code> based on available memory</li> <li>For HPC environments, larger batch sizes improve throughput</li> <li>For very long sequences, reduce batch size to fit in memory</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#references-dependencies","title":"References &amp; Dependencies","text":""},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#papers","title":"Papers","text":"<ul> <li>TorsionBERT: \"RNA Torsion Angle Prediction with TorsionBERT\" (Sayby et al.)</li> <li>Pairformer: \"Pairformer: A Novel Architecture for RNA Structure Prediction\" (ByteDance, 2024)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>PyTorch (version \u2265 1.8.0)</li> <li>Transformers (for TorsionBERT)</li> <li>Protenix (for Pairformer components)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageB/StageB_Torsion_Pairwise/#external-resources","title":"External Resources","text":"<ul> <li>TorsionBERT repository: https://huggingface.co/sayby/rna_torsionbert</li> <li>Pairformer implementation: Internal ByteDance repository </li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/","title":"Stage C: 3D Reconstruction","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#purpose-background","title":"Purpose &amp; Background","text":"<p>Stage C is the third step in the RNA_PREDICT pipeline, responsible for converting predicted torsion angles into 3D atomic coordinates. This stage implements a massively parallel version of the Natural Extension of Reference Frame (NeRF) algorithm, specifically adapted for RNA structures.</p> <p>The implementation achieves significant speedups (400-1200x) over traditional sequential approaches by: 1. Building backbone fragments in parallel 2. Assembling subunits efficiently 3. Placing base atoms in parallel when requested</p>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#visual-overview","title":"Visual Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 \u2502     \u2502                 \u2502     \u2502                 \u2502\n\u2502  RNA Sequence   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Torsion Angles \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  3D Coordinates \u2502\n\u2502                 \u2502     \u2502                 \u2502     \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                       \u2502\n        \u2502                       \u2502                       \u2502\n        \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 \u2502     \u2502                 \u2502     \u2502                 \u2502\n\u2502  Sequence Info  \u2502     \u2502  MP-NeRF        \u2502     \u2502  Atom Count     \u2502\n\u2502                 \u2502     \u2502  Processing     \u2502     \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#inputs-outputs","title":"Inputs &amp; Outputs","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#inputs","title":"Inputs","text":"<ul> <li>RNA Sequence: A string representing the RNA sequence (e.g., \"AUGC\")</li> <li>Torsion Angles: Shape [N, 7] or [N, 2K] where:</li> <li>N is the sequence length</li> <li>7 represents the standard backbone angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7)</li> <li>2K format represents sin/cos pairs for each angle</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#outputs","title":"Outputs","text":"<ul> <li>3D Coordinates: Shape [N, atoms, 3] where:</li> <li>N is the sequence length</li> <li>atoms is the number of atoms per residue</li> <li>3 represents XYZ coordinates</li> <li>Atom Count: Total number of atoms in the structure</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#example-inputoutput","title":"Example Input/Output","text":"<pre><code># Example input\nsequence = \"AUGC\"\ntorsion_angles = torch.tensor([\n    [-60.0, 180.0, 60.0, 80.0, -150.0, -70.0, -160.0],  # A\n    [-60.0, 180.0, 60.0, 80.0, -150.0, -70.0, -160.0],  # U\n    [-60.0, 180.0, 60.0, 80.0, -150.0, -70.0, -160.0],  # G\n    [-60.0, 180.0, 60.0, 80.0, -150.0, -70.0, -160.0],  # C\n])\n\n# Example output\ncoordinates = torch.tensor([\n    # A residue atoms (P, O5', C5', etc.)\n    [[0.0, 0.0, 0.0], [1.2, 0.0, 0.0], [1.5, 1.2, 0.0], ...],\n    # U residue atoms\n    [[2.0, 0.0, 0.0], [3.2, 0.0, 0.0], [3.5, 1.2, 0.0], ...],\n    # G residue atoms\n    [[4.0, 0.0, 0.0], [5.2, 0.0, 0.0], [5.5, 1.2, 0.0], ...],\n    # C residue atoms\n    [[6.0, 0.0, 0.0], [7.2, 0.0, 0.0], [7.5, 1.2, 0.0], ...],\n])\natom_count = 40  # Total number of atoms in the structure\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#key-classes-scripts","title":"Key Classes &amp; Scripts","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#stagecreconstruction-class","title":"<code>StageCReconstruction</code> Class","text":"<p>This class provides a fallback implementation when the MP-NeRF method is not used.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#key-fields","title":"Key Fields:","text":"<ul> <li>None (stateless implementation)</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#notable-methods","title":"Notable Methods:","text":"<ul> <li><code>__call__()</code>: Main method that processes torsion angles and returns coordinates</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#example-usage","title":"Example Usage:","text":"<pre><code>from rna_predict.pipeline.stageC.stage_c_reconstruction import StageCReconstruction\n\n# Initialize the reconstruction class\nreconstructor = StageCReconstruction()\n\n# Process torsion angles to get 3D coordinates\ncoordinates, atom_count = reconstructor(sequence, torsion_angles)\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#run_stagec_rna_mpnerf-function","title":"<code>run_stageC_rna_mpnerf</code> Function","text":"<p>This is the main function implementing the MP-NeRF approach for RNA.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#key-parameters","title":"Key Parameters:","text":"<ul> <li><code>sequence</code>: RNA sequence string</li> <li><code>predicted_torsions</code>: Tensor of torsion angles</li> <li><code>device</code>: Device to run on (\"cpu\" or \"cuda\")</li> <li><code>do_ring_closure</code>: Whether to perform ring closure refinement</li> <li><code>place_bases</code>: Whether to place base atoms</li> <li><code>sugar_pucker</code>: Sugar pucker conformation (default: \"C3'-endo\")</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#processing-steps","title":"Processing Steps:","text":"<ol> <li>Build scaffolds from torsion angles</li> <li>Handle missing atoms and modifications</li> <li>Fold backbone using MP-NeRF</li> <li>Optionally place base atoms</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#example-usage_1","title":"Example Usage:","text":"<pre><code>from rna_predict.pipeline.stageC.stage_c_reconstruction import run_stageC_rna_mpnerf\n\n# Run the MP-NeRF reconstruction\ncoordinates, atom_count = run_stageC_rna_mpnerf(\n    sequence=sequence,\n    predicted_torsions=torsion_angles,\n    device=\"cuda\",\n    do_ring_closure=True,\n    place_bases=True,\n    sugar_pucker=\"C3'-endo\"\n)\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#hydra-configuration","title":"Hydra Configuration","text":"<p>Stage C's behavior is controlled through Hydra configuration in <code>conf/model/stageC.yaml</code>:</p> <pre><code>stageC:\n  # Core Parameters\n  method: \"mp_nerf\"  # Main reconstruction method\n  do_ring_closure: false  # Whether to perform ring closure refinement\n  place_bases: true  # Whether to place base atoms\n  sugar_pucker: \"C3'-endo\"  # Default sugar pucker conformation\n  device: \"auto\"  # Device to run on (auto, cpu, or cuda)\n\n  # Memory Optimization\n  use_memory_efficient_kernel: false\n  use_deepspeed_evo_attention: false\n  use_lma: false\n  inplace_safe: false\n  chunk_size: null\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#configuration-parameters","title":"Configuration Parameters:","text":"Parameter Type Default Description <code>method</code> str \"mp_nerf\" Reconstruction method to use. \"mp_nerf\" uses the massively parallel implementation, while other values fall back to a simple implementation. <code>do_ring_closure</code> bool false Whether to perform ring closure refinement for the sugar ring. This can improve geometric accuracy but increases computation time. <code>place_bases</code> bool true Whether to place base atoms. If false, only the backbone atoms are placed. <code>sugar_pucker</code> str \"C3'-endo\" Default sugar pucker conformation. Common values are \"C3'-endo\" (A-form) and \"C2'-endo\" (B-form). <code>device</code> str \"auto\" Device to run the reconstruction on. \"auto\" automatically selects the best available device. <code>use_memory_efficient_kernel</code> bool false Whether to use memory-efficient operations. Enable for very long sequences (&gt;500 nt). <code>use_deepspeed_evo_attention</code> bool false Whether to use DeepSpeed evolutionary attention. Experimental feature that may improve performance. <code>use_lma</code> bool false Whether to use low-memory attention. Enable for very long sequences when memory is constrained. <code>inplace_safe</code> bool false Whether to use inplace operations. Enable to reduce memory usage but may cause issues with certain PyTorch operations. <code>chunk_size</code> int null Chunk size for memory-efficient operations. When set, processes the sequence in chunks of this size."},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#code-example-loading-configuration","title":"Code Example: Loading Configuration","text":"<pre><code>import hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(config_path=\"../../conf\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Access Stage C configuration\n    stage_c_config = cfg.model.stageC\n\n    # Use configuration in reconstruction\n    coordinates, atom_count = run_stageC_rna_mpnerf(\n        sequence=sequence,\n        predicted_torsions=torsion_angles,\n        device=stage_c_config.device,\n        do_ring_closure=stage_c_config.do_ring_closure,\n        place_bases=stage_c_config.place_bases,\n        sugar_pucker=stage_c_config.sugar_pucker\n    )\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#model-architecture-details","title":"Model Architecture Details","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#mp-nerf-implementation","title":"MP-NeRF Implementation","text":"<p>The MP-NeRF (Massively Parallel Natural Extension of Reference Frame) implementation consists of several key components:</p> <ol> <li> <p>Backbone Atom Order:    <pre><code>BACKBONE_ATOMS = [\n    \"P\", \"O5'\", \"C5'\", \"C4'\", \"O4'\", \"C3'\", \"O3'\", \"C2'\", \"O2'\", \"C1'\"\n]\n</code></pre></p> </li> <li> <p>Standard A-form Torsion Angles:    <pre><code>RNA_BACKBONE_TORSIONS_AFORM = {\n    \"alpha\": -60.0,  # P-O5'-C5'-C4'\n    \"beta\": 180.0,  # O5'-C5'-C4'-C3'\n    \"gamma\": 60.0,  # C5'-C4'-C3'-O3'\n    \"delta\": 80.0,  # C4'-C3'-O3'-P\n    \"epsilon\": -150.0,  # C3'-O3'-P-O5'\n    \"zeta\": -70.0,  # O3'-P-O5'-C5'\n    \"chi\": -160.0,  # O4'-C1'-N9/N1-C4/C2\n}\n</code></pre></p> </li> <li> <p>Performance Characteristics:    <pre><code>length   |  sota  | us (cpu) |  Nx   | us (gpu) | us (hybrid) |\n~114     | 2.4s   | 5.3ms    | ~446  | 21.1ms   | 18.9ms      |\n~300     | 3.5s   | 8.5ms    | ~400  | 26.2ms   | 22.3ms      |\n~500     | 7.5s   | 9.1ms    | ~651  | 29.2ms   | 26.3ms      |\n~1000    | 18.66s | 15.3ms   | ~1200 | 43.3ms   | 30.1ms      |\n</code></pre></p> </li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#geometric-parameters","title":"Geometric Parameters","text":"<p>The <code>MpNerfParams</code> class encapsulates the geometric parameters needed for atom placement:</p> <pre><code>@dataclass\nclass MpNerfParams:\n    a: torch.Tensor  # First reference point\n    b: torch.Tensor  # Second reference point\n    c: torch.Tensor  # Third reference point\n    bond_length: Union[torch.Tensor, float]  # Bond length\n    theta: Union[torch.Tensor, float]  # Bond angle\n    chi: Union[torch.Tensor, float]  # Dihedral angle\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#mp-nerf-algorithm-visualization","title":"MP-NeRF Algorithm Visualization","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                         \u2502\n\u2502  Torsion Angles                                         \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                         \u2502\n\u2502  Build Reference Frames                                 \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                         \u2502\n\u2502  Parallel Atom Placement                                \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                         \u2502\n\u2502  Optional: Ring Closure &amp; Base Placement                 \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                         \u2502\n\u2502  3D Coordinates                                         \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#integration","title":"Integration","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#upstream-dependencies","title":"Upstream Dependencies","text":"<ul> <li>Stage B: Provides predicted torsion angles</li> <li>Stage A: Provides sequence information</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#downstream-dependencies","title":"Downstream Dependencies","text":"<ul> <li>Stage D (Optional): May use the 3D coordinates for further refinement</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#data-flow","title":"Data Flow","text":"<ol> <li>Receive sequence and torsion angles from Stage B</li> <li>Build scaffolds using standard RNA geometry</li> <li>Apply MP-NeRF to generate 3D coordinates</li> <li>Optionally place base atoms and perform ring closure</li> <li>Return coordinates and atom count</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#integration-example","title":"Integration Example","text":"<pre><code># Example of how Stage C integrates with Stage B and Stage D\nfrom rna_predict.pipeline.stageB.main import run_stageB\nfrom rna_predict.pipeline.stageC.stage_c_reconstruction import run_stageC_rna_mpnerf\nfrom rna_predict.pipeline.stageD.main import run_stageD\n\n# Run Stage B to get torsion angles\nsequence = \"AUGC\"\ntorsion_angles = run_stageB(sequence)\n\n# Run Stage C to get 3D coordinates\ncoordinates, atom_count = run_stageC_rna_mpnerf(\n    sequence=sequence,\n    predicted_torsions=torsion_angles,\n    device=\"cuda\"\n)\n\n# Optionally run Stage D for refinement\nrefined_coordinates = run_stageD(\n    sequence=sequence,\n    initial_coordinates=coordinates,\n    torsion_angles=torsion_angles\n)\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#edge-cases-error-handling","title":"Edge Cases &amp; Error Handling","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#input-validation","title":"Input Validation","text":"<ul> <li>Checks for sufficient number of torsion angles (minimum 7)</li> <li>Validates sequence length matches torsion angle count</li> <li>Ensures device compatibility</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#geometric-validation","title":"Geometric Validation","text":"<ul> <li>RMSD validation against PDB structures (target &lt; 0.5 \u00c5)</li> <li>Geometric checks using MolProbity Suite</li> <li>Steric clash detection</li> <li>Ring closure validation</li> <li>Numerical stability checks</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#error-recovery","title":"Error Recovery","text":"<ul> <li>Falls back to simple implementation if MP-NeRF fails</li> <li>Handles missing atoms gracefully</li> <li>Provides informative error messages for debugging</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#example-error-handling","title":"Example Error Handling","text":"<pre><code>try:\n    coordinates, atom_count = run_stageC_rna_mpnerf(\n        sequence=sequence,\n        predicted_torsions=torsion_angles,\n        device=\"cuda\"\n    )\nexcept ValueError as e:\n    if \"insufficient torsion angles\" in str(e):\n        # Handle case where torsion angles are missing\n        print(\"Error: Not enough torsion angles provided\")\n    elif \"sequence length mismatch\" in str(e):\n        # Handle case where sequence and torsion angles don't match\n        print(\"Error: Sequence length doesn't match torsion angle count\")\n    else:\n        # Handle other errors\n        print(f\"Error: {e}\")\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#hardware-configurations","title":"Hardware Configurations","text":"Configuration CPU GPU Memory Notes Standard Intel Xeon E5-2680 v4 NVIDIA Tesla V100 32GB Default configuration for most users High-Performance AMD EPYC 7742 NVIDIA A100 128GB For very long sequences (&gt;1000 nt) Memory-Constrained Intel Xeon E5-2680 v4 NVIDIA T4 16GB Use with memory optimization flags"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#performance-across-sequence-lengths","title":"Performance Across Sequence Lengths","text":"Sequence Length CPU Time (s) GPU Time (s) Memory Usage (GB) RMSD (\u00c5) 50 0.8 0.02 0.5 0.3 100 1.2 0.03 0.8 0.4 200 2.1 0.05 1.2 0.5 500 5.3 0.09 2.5 0.6 1000 10.2 0.15 4.8 0.7 2000 20.5 0.28 9.2 0.8"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#memory-optimization-impact","title":"Memory Optimization Impact","text":"Optimization Flag Memory Usage Reduction Performance Impact Recommended For <code>use_memory_efficient_kernel</code> 30% -5% Sequences &gt;500 nt <code>use_lma</code> 50% -10% Sequences &gt;1000 nt <code>inplace_safe</code> 20% -2% Any sequence length <code>chunk_size=100</code> 40% -15% Sequences &gt;2000 nt"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#references","title":"References","text":"<ol> <li>MP-NeRF Paper:</li> <li>Title: \"MP-NeRF: A Massively Parallel Method for Accelerating Protein Structure Reconstruction from Internal Coordinates\"</li> <li>DOI: 10.1101/2021.06.08.446214</li> <li> <p>Repository: https://github.com/EleutherAI/mp_nerf</p> </li> <li> <p>RNA Geometry References:</p> </li> <li>Murray et al. (2003): RNA backbone is rotameric (PNAS)</li> <li>Richardson et al. (2008): RNA backbone suite nomenclature (RNA)</li> <li>3DNA/DSSR: Standard RNA geometry tools (x3dna.org)</li> <li>MolProbity Suite: RNA rotamer and sugar pucker validation</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#dependencies","title":"Dependencies","text":"<ul> <li>PyTorch &gt; 1.6</li> <li>NumPy</li> <li>einops</li> <li>Optional: joblib, sidechainnet</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Issue Possible Cause Solution Out of memory error Sequence too long Enable memory optimization flags or reduce chunk size Slow performance Running on CPU Switch to GPU if available Incorrect geometry Missing ring closure Enable <code>do_ring_closure</code> Missing base atoms <code>place_bases</code> set to false Enable <code>place_bases</code> Numerical instability Extreme torsion angles Clip angles to reasonable ranges"},{"location":"pipeline/integration/hydra_integration/components/stageC/StageC_3D_Reconstruction/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Check torsion angle ranges: Ensure angles are within expected ranges</li> <li>Verify sequence-torsion alignment: Make sure sequence length matches torsion count</li> <li>Monitor memory usage: Use <code>torch.cuda.memory_allocated()</code> to track GPU memory</li> <li>Enable verbose logging: Set <code>logging.level=DEBUG</code> in configuration</li> <li>Validate against PDB: Compare output with known structures for validation </li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/","title":"Stage D: Diffusion-based Refinement &amp; Optional Energy Minimization","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#purpose-background","title":"Purpose &amp; Background","text":"<p>Stage D is the final refinement step in the RNA structure prediction pipeline. It takes partial 3D coordinates from Stage C (or random initialization) and refines them using a diffusion-based approach. The process involves:</p> <ol> <li>Converting residue-level embeddings to atom-level embeddings</li> <li>Applying a diffusion process to refine the coordinates</li> <li>Optionally performing energy minimization for final structure optimization</li> </ol> <p>This implementation follows Algorithm 20 in AlphaFold3, with adaptations for RNA structure prediction.</p>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#inputs-outputs","title":"Inputs &amp; Outputs","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#inputs","title":"Inputs","text":"<ul> <li>Partial coordinates from Stage C (or random initialization)</li> <li>Trunk embeddings from previous stages</li> <li>Optional input features for atom-level processing</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#outputs","title":"Outputs","text":"<ul> <li>Refined 3D coordinates for all atoms</li> <li>Optional energy-minimized structure</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#key-classes-scripts","title":"Key Classes &amp; Scripts","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#core-classes","title":"Core Classes","text":"<ol> <li> <p><code>DiffusionConfig</code>: Main configuration class for Stage D    <pre><code>@dataclass\nclass DiffusionConfig:\n    partial_coords: torch.Tensor\n    trunk_embeddings: Dict[str, torch.Tensor]\n    diffusion_config: Dict[str, Any]\n    mode: str = \"inference\"\n    device: str = \"cpu\"\n    input_features: Optional[Dict[str, Any]] = None\n    debug_logging: bool = False\n</code></pre></p> </li> <li> <p><code>ProtenixDiffusionManager</code>: Main manager class for diffusion operations    <pre><code>class ProtenixDiffusionManager:\n    def __init__(self, diffusion_config: dict, device: str = \"cpu\")\n    def train_diffusion_step(self, label_dict, input_feature_dict, s_inputs, s_trunk, z_trunk, sampler_params, N_sample=1)\n    def multi_step_inference(self, coords_init, trunk_embeddings, inference_params, override_input_features=None)\n</code></pre></p> </li> <li>Handles both training and inference modes</li> <li>Manages memory-efficient processing</li> <li>Supports LoRA integration for large models</li> <li> <p>Configurable through Hydra</p> </li> <li> <p><code>DiffusionModule</code>: Implements the core diffusion process (Algorithm 20 in AF3)    <pre><code>class DiffusionModule(nn.Module):\n    def __init__(\n        self,\n        sigma_data: float = 16.0,\n        c_atom: int = 128,\n        c_atompair: int = 16,\n        c_token: int = 768,\n        c_s: int = 384,\n        c_z: int = 128,\n        c_s_inputs: int = 449,\n        c_noise_embedding: int = 256,\n        atom_encoder: dict = {\"n_blocks\": 3, \"n_heads\": 4, \"n_queries\": 32, \"n_keys\": 128},\n        transformer: dict = {\"n_blocks\": 24, \"n_heads\": 16},\n        atom_decoder: dict = {\"n_blocks\": 3, \"n_heads\": 4, \"n_queries\": 32, \"n_keys\": 128},\n        blocks_per_ckpt: Optional[int] = None,\n        use_fine_grained_checkpoint: bool = False,\n        initialization: Optional[dict] = None,\n    )\n</code></pre></p> </li> <li> <p><code>DiffusionSchedule</code>: Manages the noise schedule for diffusion (Algorithm 21 in AF3)    <pre><code>class DiffusionSchedule:\n    def __init__(\n        self,\n        sigma_data: float = 16.0,\n        s_max: float = 160.0,\n        s_min: float = 4e-4,\n        p: float = 7.0,\n        dt: float = 1/200,\n        p_mean: float = -1.2,\n        p_std: float = 1.5,\n    )\n</code></pre></p> </li> <li> <p><code>DiffusionConditioning</code>: Implements Algorithm 21 in AF3 for conditioning the diffusion process    <pre><code>class DiffusionConditioning(nn.Module):\n    def __init__(\n        self,\n        sigma_data: float = 16.0,\n        c_z: int = 128,\n        c_s: int = 384,\n        c_s_inputs: int = 449,\n        c_noise_embedding: int = 256,\n    )\n</code></pre></p> </li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#lora-integration","title":"LoRA Integration","text":"<p>The diffusion module supports optional LoRA (Low-Rank Adaptation) integration for large models:</p> <pre><code>model:\n  stageD:\n    lora:\n      enabled: false\n      r: 4              # LoRA rank\n      alpha: 16         # LoRA scaling\n      dropout: 0.1      # LoRA dropout\n      target_modules:   # Modules to apply LoRA\n        - \"attention.W_q\"\n        - \"attention.W_k\"\n        - \"attention.W_v\"\n        - \"attention.W_out\"\n</code></pre> <p>When enabled: - Freezes base model weights - Adds low-rank adapters to specified modules - Only trains LoRA parameters during fine-tuning - Maintains memory efficiency - Compatible with both training and inference modes</p>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#hydra-configuration","title":"Hydra Configuration","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#main-configuration","title":"Main Configuration","text":"<pre><code>model:\n  stageD:\n    # Core diffusion parameters\n    sigma_data: 16.0  # Standard deviation of the data\n    c_atom: 128       # Embedding dimension for atom features\n    c_atompair: 16    # Embedding dimension for atom pair features\n    c_token: 768      # Feature channel of token\n    c_s: 384          # Hidden dimension for single embedding\n    c_z: 128          # Hidden dimension for pair embedding\n    c_s_inputs: 449   # Input embedding dimension\n    c_noise_embedding: 256  # Noise embedding dimension\n\n    # Training parameters\n    training:\n      batch_size: 8\n      learning_rate: 1e-4\n      optimizer:\n        type: \"adam\"\n        weight_decay: 1e-5\n        beta1: 0.9\n        beta2: 0.999\n      gradient_clipping:\n        enabled: true\n        max_norm: 1.0\n      warmup_steps: 1000\n      max_epochs: 100\n      early_stopping:\n        patience: 10\n        min_delta: 1e-4\n\n    # Inference parameters\n    inference:\n      num_steps: 100\n      temperature: 1.0\n      early_stopping:\n        enabled: true\n        patience: 5\n        min_delta: 1e-4\n      sampling:\n        num_samples: 1\n        seed: null  # Random if None\n        use_deterministic: false\n\n    # Memory optimization\n    memory:\n      max_memory_usage: \"16GB\"\n      gradient_checkpointing:\n        enabled: true\n        strategy: \"uniform\"  # or \"block\"\n      mixed_precision:\n        enabled: true\n        dtype: \"float16\"\n      memory_efficient_attention: true\n      use_flash_attention: true\n      attention_chunk_size: 1024\n      max_sequence_length: 4096\n\n    # Architecture components\n    atom_encoder:\n      n_blocks: 3\n      n_heads: 4\n      n_queries: 32   # Number of queries for attention\n      n_keys: 128     # Number of keys for attention\n    transformer:\n      n_blocks: 24\n      n_heads: 16\n    atom_decoder:\n      n_blocks: 3\n      n_heads: 4\n      n_queries: 32   # Number of queries for attention\n      n_keys: 128     # Number of keys for attention\n\n    # Memory optimization parameters\n    blocks_per_ckpt: null  # Number of blocks per checkpoint\n    use_fine_grained_checkpoint: false\n\n    # Noise schedule parameters\n    s_max: 160.0      # Maximum noise level\n    s_min: 4e-4       # Minimum noise level\n    p: 7.0            # Exponent for noise schedule\n    dt: 0.005         # Time step size\n    p_mean: -1.2      # Mean of log-normal distribution\n    p_std: 1.5        # Standard deviation of log-normal distribution\n\n    # Sampling parameters (Algorithm 18 in AF3)\n    gamma0: 0.8       # Initial gamma parameter\n    gamma_min: 1.0    # Minimum gamma parameter\n    noise_scale_lambda: 1.003  # Noise scale parameter\n    step_scale_eta: 1.5  # Step scale parameter\n    diffusion_chunk_size: null  # Chunk size for diffusion operation\n    attn_chunk_size: null  # Chunk size for attention operation\n    inplace_safe: false  # Whether to use inplace operations safely\n\n    # Energy minimization (optional)\n    energy_minimization:\n      enabled: false\n      steps: 1000\n      method: \"OpenMM\"  # Options: \"OpenMM\", \"GROMACS\", \"AMBER\"\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#memory-optimization-parameters","title":"Memory Optimization Parameters","text":"<pre><code>model:\n  stageD:\n    memory_optimization:\n      enable: true\n      use_memory_efficient_kernel: true\n      use_deepspeed_evo_attention: false\n      use_lma: false\n      inplace_safe: true\n      chunk_size: 128\n      clear_cache_between_blocks: true\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#model-architecture-details","title":"Model Architecture Details","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#diffusion-process","title":"Diffusion Process","text":"<ol> <li>Embedding Dimensions:</li> <li>Atom features: 128 dimensions</li> <li>Atom pair features: 16 dimensions</li> <li>Token features: 768 dimensions</li> <li>Single embeddings: 384 dimensions</li> <li>Pair embeddings: 128 dimensions</li> <li>Input embeddings: 449 dimensions</li> <li> <p>Noise embeddings: 256 dimensions</p> </li> <li> <p>Architecture Components:</p> </li> <li>Atom Attention Encoder: 3 blocks, 4 heads, 32 queries, 128 keys</li> <li>Transformer: 24 blocks, 16 heads</li> <li> <p>Atom Attention Decoder: 3 blocks, 4 heads, 32 queries, 128 keys</p> </li> <li> <p>Noise Schedule:</p> </li> <li>Maximum noise level: 160.0</li> <li>Minimum noise level: 4e-4</li> <li>Schedule exponent: 7.0</li> <li>Time steps: 200</li> <li> <p>Log-normal distribution parameters: mean=-1.2, std=1.5</p> </li> <li> <p>Sampling Process (Algorithm 18 in AF3):</p> </li> <li>Initial gamma: 0.8</li> <li>Minimum gamma: 1.0</li> <li>Noise scale lambda: 1.003</li> <li>Step scale eta: 1.5</li> <li>Supports chunked processing for memory efficiency</li> <li>Handles both single-sample and multi-sample processing</li> <li>Automatic broadcasting for batch dimensions</li> <li> <p>Memory-efficient chunked processing for large ensembles</p> </li> <li> <p>Ensemble Generation:</p> </li> <li>Supports generating multiple structures from different seeds</li> <li>Configurable through <code>N_sample</code> parameter</li> <li>Memory-efficient processing through chunking</li> <li>Two modes of operation:      a) Training mode:         - Uses <code>TrainingNoiseSampler</code> for noise level sampling         - Log-normal distribution for noise levels         - Supports data augmentation through random rotations      b) Inference mode:         - Uses <code>InferenceNoiseScheduler</code> for deterministic schedule         - Linear noise schedule from s_max to s_min         - Configurable number of time steps</li> <li>Output shapes:<ul> <li>Single sample: [B, N_atom, 3]</li> <li>Multiple samples: [B, N_sample, N_atom, 3]</li> </ul> </li> <li>Memory optimization:<ul> <li>Optional chunked processing</li> <li>Configurable chunk sizes</li> <li>Inplace operations when safe</li> </ul> </li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#energy-minimization","title":"Energy Minimization","text":"<p>When enabled, the energy minimization process: 1. Uses OpenMM by default (configurable) 2. Performs 1000 steps of minimization 3. Can use different force fields (AMBER, CHARMM) 4. Supports both CPU and GPU execution</p>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#integration","title":"Integration","text":""},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#upstream-dependencies","title":"Upstream Dependencies","text":"<ul> <li>Stage C: Provides partial 3D coordinates</li> <li>Merger: Provides unified latent embeddings</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#downstream-dependencies","title":"Downstream Dependencies","text":"<ul> <li>Energy minimization (optional)</li> <li>Structure validation tools</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#data-flow","title":"Data Flow","text":"<ol> <li>Receive partial coordinates and embeddings</li> <li>Bridge residue-level to atom-level embeddings</li> <li>Apply diffusion process (Algorithm 18 in AF3)</li> <li>Optionally perform energy minimization</li> <li>Output final refined coordinates</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#edge-cases-error-handling","title":"Edge Cases &amp; Error Handling","text":"<ol> <li>Missing Input Features:</li> <li>Falls back to basic feature creation based on partial coordinates</li> <li> <p>Logs warning when using fallback</p> </li> <li> <p>Memory Management:</p> </li> <li>Automatic cache clearing between blocks</li> <li>Configurable checkpointing for large structures</li> <li>Memory-efficient kernels available</li> <li> <p>Chunked processing for large structures</p> </li> <li> <p>Invalid Modes:</p> </li> <li>Validates mode is either \"inference\" or \"train\"</li> <li> <p>Raises ValueError for unsupported modes</p> </li> <li> <p>Tensor Shape Compatibility:</p> </li> <li>Automatic shape validation and fixing</li> <li>Bridging between residue and atom levels</li> <li> <p>Handles multi-sample processing with proper broadcasting</p> </li> <li> <p>Initialization Options:</p> </li> <li>Zero initialization for various components</li> <li>He-normal initialization for small MLPs</li> <li>Configurable initialization strategies</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#references","title":"References","text":"<ol> <li>AlphaFold3 paper (Algorithm 20: DiffusionModule, Algorithm 21: DiffusionConditioning)</li> <li>Energy minimization documentation</li> <li>Memory optimization techniques</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/stageD/StageD_Diffusion_Refinement/#dependencies","title":"Dependencies","text":"<p>Required: - PyTorch - NumPy - OpenMM (for energy minimization)</p> <p>Optional: - GROMACS (alternative energy minimization) - AMBER (alternative energy minimization) </p>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/","title":"Unified Latent Merger Documentation","text":""},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#overview-importance","title":"Overview &amp; Importance","text":"<p>The Unified Latent Merger (ULM) is a critical component in the RNA structure prediction pipeline that combines multiple representations into a single, coherent \"conditioning latent\" for the diffusion-based refinement stage. It ensures synergy between local angle features and global pair embeddings, effectively bridging the gap between different stages of the pipeline.</p>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#key-functions","title":"Key Functions","text":"<ul> <li>Merges local torsion angles with global pair embeddings</li> <li>Incorporates adjacency information and optional partial 3D coordinates</li> <li>Creates a unified representation that guides the diffusion process</li> <li>Handles varying input dimensions through efficient pooling and transformation</li> </ul>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#current-implementation-simplelatentmerger","title":"Current Implementation: SimpleLatentMerger","text":"<p>Our current implementation uses a simple but effective MLP-based approach:</p> <pre><code>class SimpleLatentMerger(torch.nn.Module):\n    def __init__(self, dim_angles: int, dim_s: int, dim_z: int, dim_out: int):\n        super().__init__()\n        self.expected_dim_angles = dim_angles\n        self.expected_dim_s = dim_s\n        self.expected_dim_z = dim_z\n        self.dim_out = dim_out\n\n        # Dynamic MLP that adapts to input dimensions\n        in_dim = dim_angles + dim_s + dim_z\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(in_dim, dim_out),\n            torch.nn.ReLU(),\n            torch.nn.Linear(dim_out, dim_out),\n        )\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#key-features-of-current-implementation","title":"Key Features of Current Implementation","text":"<ol> <li>Dynamic MLP Reinitialization</li> <li>Automatically adapts to changing input dimensions</li> <li>Recreates MLP if dimensions change during forward pass</li> <li> <p>Maintains device consistency (CPU/GPU)</p> </li> <li> <p>Efficient Pair Embedding Pooling</p> </li> <li>Pools pair embeddings from [N, N, dim_z] to [N, dim_z]</li> <li>Uses mean operation for efficient reduction</li> <li> <p>Preserves per-residue information</p> </li> <li> <p>Memory Optimization</p> </li> <li>Minimal memory footprint</li> <li>No attention mechanisms</li> <li>Efficient for small to medium RNAs</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#usage-example","title":"Usage Example","text":"<pre><code>merger = SimpleLatentMerger(\n    dim_angles=7,    # Torsion angle dimension\n    dim_s=64,        # Single embedding dimension\n    dim_z=32,        # Pair embedding dimension\n    dim_out=128      # Output dimension\n).to(device)\n\nunified_latent = merger(\n    adjacency=adjacency_t,      # [N, N]\n    angles=torsion_angles,      # [N, dim_angles]\n    s_emb=s_emb,               # [N, dim_s]\n    z_emb=z_emb,               # [N, N, dim_z]\n    partial_coords=partial_coords  # Optional [N, 3] or [N*#atoms, 3]\n)\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#input-structures","title":"Input Structures","text":"<p>The merger accepts multiple input tensors with different shapes and dimensions:</p> <ol> <li>Torsion Angles</li> <li>Shape: <code>[N, dim_angles]</code></li> <li>Source: TorsionBERT output</li> <li> <p>Contains local conformational information for each residue</p> </li> <li> <p>Single Embeddings</p> </li> <li>Shape: <code>[N, c_s]</code></li> <li>Source: Pairformer output</li> <li> <p>Represents per-residue features</p> </li> <li> <p>Pair Embeddings</p> </li> <li>Shape: <code>[N, N, c_z]</code></li> <li>Source: Pairformer output</li> <li>Contains pairwise information between residues</li> <li> <p>Pooled to <code>[N, c_z]</code> using mean operation</p> </li> <li> <p>Adjacency Matrix (Optional)</p> </li> <li>Shape: <code>[N, N]</code></li> <li>Source: Stage A output</li> <li> <p>Binary or probabilistic contact information</p> </li> <li> <p>Partial Coordinates (Optional)</p> </li> <li>Shape: <code>[N, #atoms, 3]</code> or <code>[N * #atoms, 3]</code></li> <li>Source: Stage C output (MP-NeRF)</li> <li>Initial 3D coordinates if available</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#future-enhancement-perceiver-io-architecture","title":"Future Enhancement: Perceiver IO Architecture","text":"<p>For large RNAs or complex embeddings, the merger can be enhanced with a Perceiver IO architecture:</p>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#advanced-perceiver-io-implementation","title":"Advanced Perceiver IO Implementation","text":"<ol> <li>Cross-Attention Encoder</li> <li>Input: Flattened features from all sources</li> <li>Output: Fixed-size latent array <code>[N', D]</code></li> <li> <p>Handles varying input sizes efficiently</p> </li> <li> <p>Latent Transformer</p> </li> <li>Processes the latent array with self-attention</li> <li>Complexity: O(N'\u00b2) instead of O(N\u2074)</li> <li> <p>Multiple layers of transformer blocks</p> </li> <li> <p>Cross-Attention Decoder</p> </li> <li>Output queries to produce final latent</li> <li>Flexible output shape based on requirements</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#hydra-configuration","title":"Hydra Configuration","text":"<pre><code>model:\n  latent_merger:\n    # Input dimensions\n    dim_angles: 7        # Torsion angle dimension\n    dim_s: 64           # Single embedding dimension\n    dim_z: 32           # Pair embedding dimension\n    dim_out: 128        # Output dimension\n\n    # Architecture choice\n    type: \"simple\"      # or \"perceiver\"\n\n    # Perceiver IO specific (if type=\"perceiver\")\n    perceiver:\n      latent_dim: 256\n      num_layers: 4\n      num_heads: 8\n      dropout: 0.1\n      use_flash_attention: true\n\n    # Memory optimization\n    memory_efficient: true\n    chunk_size: 1024\n</code></pre>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#integration-data-flow","title":"Integration &amp; Data Flow","text":"<ol> <li>Upstream Dependencies</li> <li>Stage A: Adjacency matrix</li> <li>Stage B: Torsion angles and pair embeddings</li> <li> <p>Stage C: Optional partial coordinates</p> </li> <li> <p>Downstream Usage</p> </li> <li>Stage D: Provides conditioning latent for diffusion</li> <li> <p>Used to guide the refinement process</p> </li> <li> <p>Memory Considerations</p> </li> <li>Efficient pooling of pair embeddings</li> <li>Optional chunked processing for large RNAs</li> <li>Device management for GPU/CPU compatibility</li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#edge-cases-error-handling","title":"Edge Cases &amp; Error Handling","text":"<ol> <li>Dimension Mismatches</li> <li>Automatic MLP reinitialization if input dimensions change</li> <li>Validation of expected vs. actual dimensions</li> <li> <p>Graceful handling of missing optional inputs</p> </li> <li> <p>Device Management</p> </li> <li>Automatic device relocation of MLP if needed</li> <li>Consistent tensor device placement</li> <li> <p>Memory-efficient processing for large inputs</p> </li> <li> <p>Shape Validation</p> </li> <li>Input shape verification</li> <li>Proper handling of batched inputs</li> <li>Consistent output shape <code>[N, dim_out]</code></li> </ol>"},{"location":"pipeline/integration/hydra_integration/components/unified_latent/UnifiedLatentMerger/#references-dependencies","title":"References &amp; Dependencies","text":"<ol> <li>Perceiver IO Paper</li> <li>\"Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs\"</li> <li> <p>Key concepts for advanced merger implementation</p> </li> <li> <p>Implementation Dependencies</p> </li> <li>PyTorch</li> <li>Optional: Flash Attention for memory efficiency</li> <li> <p>Optional: Perceiver IO implementation</p> </li> <li> <p>Related Components</p> </li> <li>TorsionBERT for angle predictions</li> <li>Pairformer for pair embeddings</li> <li>Diffusion module for final refinement </li> </ol>"},{"location":"pipeline/kaggle_info/M2_Plan/","title":"M2 Plan","text":"<p>Okay, here is a comprehensive document detailing the goals, requirements, and tasks for Milestone 2 (M2), suitable for briefing a senior developer. It incorporates the analysis of your <code>rna_predict</code> project and leverages relevant patterns identified from the <code>ProjectEquiSurv</code> example.</p> <p>Project <code>rna_predict</code> - Milestone 2 Specification: Initial End-to-End Training &amp; Prediction</p> <p>1. Objective</p> <p>The primary goal of Milestone 2 (M2) is to achieve and validate the functional trainability of the integrated RNA 3D prediction pipeline developed in M1. This involves demonstrating that the end-to-end system (Sequence -&gt; Stage A Adjacency -&gt; Stage B Torsions/Pairs -&gt; Unified Latent Merger -&gt; Stage D Diffusion -&gt; Final 3D Coords) can execute within a training loop, compute losses, backpropagate gradients correctly to the intended trainable parameters (primarily LoRA adapters), and generate initial 3D coordinate outputs.</p> <p>Key Focus: Functionality and Infrastructure. Achieving high prediction accuracy is not the goal of M2; instead, we focus on proving the pipeline works mechanically, gradients flow correctly, and we can generate baseline outputs for subsequent debugging and refinement (M3).</p> <p>2. Prerequisites</p> <ul> <li>M1 Completed: The integrated pipeline codebase is complete and callable (e.g., via <code>rna_predict.pipeline.run_pipeline.run_full_pipeline</code>). This includes functional instances of:<ul> <li>Stage A Adjacency predictor interface (<code>rfold_predictor.StageARFoldPredictor</code>).</li> <li>Stage B combined runner (<code>run_stage_b.run_stageB_combined</code>) integrating working TorsionBERT (<code>torsion_predictor.StageBTorsionBertPredictor</code>) and Pairformer (<code>pairwise.pairformer.PairformerWrapper</code>) modules.</li> <li>Unified Latent Merger (<code>run_pipeline.SimpleLatentMerger</code> or equivalent placeholder).</li> <li>Stage D Diffusion components (<code>diffusion.manager.ProtenixDiffusionManager</code>, <code>diffusion.model.DiffusionModule</code>).</li> <li>LoRA wrapping mechanism applied to TorsionBERT and Pairformer base models, ready for activation.</li> </ul> </li> </ul> <p>3. Core Requirements (Definition of Done for M2)</p> <p>The following must be demonstrably working by the end of M2:</p> <ol> <li>Trainable Pipeline Execution: The primary training script (<code>rna_predict/train.py</code>) successfully executes the full forward pass of the integrated pipeline (A -&gt; B -&gt; Merger -&gt; D) on batches of training data without crashing (resolving any shape, type, or device errors).</li> <li>Loss Calculation: A defined loss function (minimum: 3D coordinate-based loss like L1 or MSE comparing <code>final_coords</code> from Stage D to ground truth) is implemented and computes a valid (non-NaN, non-Inf) scalar loss value per batch.</li> <li>Data Loading: A functional PyTorch <code>DataLoader</code> (using <code>rna_predict.data.loader.RNADataset</code> or similar) must provide batches containing all necessary inputs: sequences, pre-computed adjacency matrices (from Stage A), ground truth 3D coordinates, and any other features required by the pipeline. Must correctly handle padding/batching.</li> <li>Optimizer Configuration &amp; Parameter Targeting: An optimizer (e.g., AdamW) is correctly configured to update only the specified trainable parameters (LoRA adapters in TorsionBERT/Pairformer, UnifiedLatentMerger weights, Diffusion head/adapters). A clear mechanism exists to filter and provide only these parameters to the optimizer (e.g., a helper function <code>get_trainable_parameters(model)</code>). Base model weights must remain frozen (<code>requires_grad=False</code>).</li> <li>Backpropagation &amp; Weight Update: <code>loss.backward()</code> and <code>optimizer.step()</code> execute successfully, indicating gradients are computed and applied to the trainable parameters without error.</li> <li>Training Loop Functionality: The training script completes at least one full epoch (or a substantial number of training steps) over the training dataset without critical runtime errors.</li> <li>Checkpoint Saving: The training process successfully saves model checkpoints containing the <code>state_dict</code> of trainable parameters and the optimizer state (e.g., using PyTorch Lightning's <code>ModelCheckpoint</code>).</li> <li>Inference Script: A separate inference script (<code>rna_predict/predict.py</code> or similar) exists for generating predictions from saved checkpoints.</li> <li>Checkpoint Loading (Partial): The inference script successfully loads the trainable parameters from a saved M2 checkpoint into the corresponding pipeline model structure using a partial loading mechanism (handling potentially missing base model weights in the checkpoint).</li> <li>Initial Prediction Generation: The inference script, using a loaded M2 checkpoint, successfully generates and saves 3D coordinate outputs (e.g., <code>.pdb</code> or <code>.pt</code> format) for a representative subset of the validation data without crashing.</li> <li>LanceDB Logging Stub: A stub LanceDB logger is present in the codebase (<code>rna_predict/utils/lance_logger.py</code>), with no-op logging methods and a config flag (<code>PipelineConfig.lance_db.enabled</code>) controlling activation. For M2, this logger is a placeholder; real logging will be implemented in M3.</li> </ol> <p>4. Key Technical Tasks &amp; Recommended Approaches</p> <p>To achieve the M2 requirements, the following technical tasks should be prioritized, leveraging patterns from <code>ProjectEquiSurv</code> where applicable:</p> <p>4.1. Implement Training Framework (Leverage PyTorch Lightning)</p> <ul> <li>Task: Refactor the training logic into a PyTorch Lightning <code>LightningModule</code> (e.g., <code>RNALightningModule</code>).</li> <li>Details:<ul> <li>Encapsulate model instantiation (likely involving the setup logic from <code>run_full_pipeline</code>) within the module's <code>__init__</code>. The module should hold instances of the stage predictors, merger, and diffusion manager.</li> <li>Implement <code>training_step</code>: Takes a batch, runs the necessary steps of the forward pass (A-&gt;B-&gt;Merger-&gt;D), computes the primary 3D loss (and optional auxiliary losses like angle loss), logs training loss (<code>self.log</code>), returns the loss tensor.</li> <li>Implement <code>validation_step</code>: Similar to <code>training_step</code> but uses <code>torch.no_grad()</code>. Logs validation loss and basic coordinate metrics (e.g., MAE).</li> <li>Implement <code>configure_optimizers</code>: Defines the optimizer (e.g., AdamW) targeting only trainable parameters (Task 4.3) and optionally sets up an LR scheduler.</li> <li>Override <code>load_state_dict</code> or use the <code>on_load_checkpoint</code> hook to integrate partial checkpoint loading logic (Task 4.4).</li> <li>Integrate config flags for LanceDB logging (stub for M2, see <code>PipelineConfig.lance_db</code>).</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/training/lightning_survival_module.py</code> (<code>LightningSurvivalModule</code>)</li> <li><code>quick_fixes/advanced/training/optim_wrappers.py</code> (<code>configure_optimizers</code>)</li> </ul> </li> <li>Benefit: Significantly reduces boilerplate for training loops, multi-device handling, logging, and checkpointing.</li> </ul> <p>4.2. Establish Configuration Management (Leverage Hydra)</p> <ul> <li>Task: Manage all hyperparameters and settings (paths, model dimensions, LoRA ranks, loss weights, training settings) using Hydra.</li> <li>Details:<ul> <li>Create a <code>rna_predict/conf/</code> directory with subdirectories (e.g., <code>model</code>, <code>training</code>, <code>data</code>, <code>lora</code>).</li> <li>Define configuration schemas using dataclasses (e.g., <code>rna_predict/conf/config_schema.py</code>) for hierarchical configuration access.</li> <li>Create corresponding <code>.yaml</code> files (e.g., <code>conf/train.yaml</code>, <code>conf/model/default.yaml</code>, <code>conf/lora/torsionbert.yaml</code>).</li> <li>Use <code>@hydra.main(config_path=\"conf\", config_name=\"train\")</code> decorator in <code>train.py</code> and <code>predict.py</code>. Access config via <code>cfg: DictConfig</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/pipeline/main.py</code> (<code>@hydra.main</code>)</li> <li><code>quick_fixes/conf/config_schema.py</code></li> </ul> </li> <li>Benefit: Provides flexible, reproducible, and command-line-friendly configuration.</li> </ul> <p>4.3. Implement Trainable Parameter Filtering</p> <ul> <li>Task: Create a utility function (e.g., in <code>rna_predict/core/utils.py</code>) or a method within <code>RNALightningModule</code> to identify and return only the parameters intended for training.</li> <li>Details: Iterate through <code>self.model.named_parameters()</code> (assuming the full pipeline is <code>self.model</code>). Filter based on parameter names (e.g., check if <code>lora</code> is in the name, or if the parameter belongs to the <code>Merger</code> or <code>Diffusion</code> head). Pass only this filtered iterator/list to the optimizer in <code>configure_optimizers</code>.</li> <li>Verification: Add a log statement at the start of training printing <code>sum(p.numel() for p in model.parameters() if p.requires_grad)</code> vs <code>sum(p.numel() for p in model.parameters())</code>.</li> </ul> <p>4.4. Implement Partial Checkpoint Loading</p> <ul> <li>Task: Ensure checkpoints saved during M2 (containing only trainable weights) can be loaded correctly for inference or resuming training.</li> <li>Details:<ul> <li>Copy or adapt the <code>partial_load_state_dict</code> function from <code>ProjectEquiSurv</code>.</li> <li>In <code>RNALightningModule</code>, override the <code>load_state_dict</code> method. Inside, call <code>partial_load_state_dict(self, state_dict, strict=False)</code> instead of the default <code>super().load_state_dict</code>.</li> <li>In <code>predict.py</code>, after instantiating the model, load the checkpoint using <code>torch.load</code> and then call <code>partial_load_state_dict(model, checkpoint['state_dict'], strict=False)</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/hypercloning/checkpoint_utils.py</code> (<code>partial_load_state_dict</code>)</li> </ul> </li> <li>Benefit: Critical for LoRA workflow.</li> </ul> <p>4.5. Build Data Handling Pipeline</p> <ul> <li>Task: Implement <code>RNADataset</code> and <code>DataLoader</code> for training and validation.</li> <li>Details:<ul> <li>Create/Refine <code>RNADataset</code> in <code>rna_predict/data/loader.py</code>. The <code>__getitem__</code> should return a dictionary containing:<ul> <li><code>sequence</code>: Raw string.</li> <li><code>adjacency</code>: Precomputed <code>[N, N]</code> tensor.</li> <li><code>coords_gt</code>: Ground truth <code>[N_atoms, 3]</code> tensor.</li> <li>Any other features needed for <code>features/initial_embeddings.py</code>.</li> </ul> </li> <li>Implement data splitting logic (e.g., random split indices) in <code>train.py</code> using PyTorch <code>Subset</code>.</li> <li>Instantiate <code>DataLoader</code>s with appropriate <code>batch_size</code>, <code>num_workers</code>, and a custom <code>collate_fn</code> if necessary to handle variable length sequences (e.g., padding within a batch).</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/data/graph_survival_dataset.py</code> (Pattern for Dataset class)</li> <li><code>quick_fixes/advanced/data/split_manager.py</code> (Pattern for splitting logic)</li> </ul> </li> </ul> <p>4.6. Implement Training Script (<code>train.py</code>)</p> <ul> <li>Task: Create the main script (<code>rna_predict/train.py</code>) using PyTorch Lightning and Hydra.</li> <li>Details:<ul> <li>Use <code>@hydra.main</code>.</li> <li>Instantiate <code>RNALightningModule</code> using the Hydra config (<code>cfg</code>).</li> <li>Instantiate <code>DataLoader</code>s.</li> <li>Instantiate PyTorch Lightning <code>Trainer</code>, configuring callbacks:<ul> <li><code>ModelCheckpoint</code> (monitor validation loss, save top-k checkpoints).</li> <li><code>LearningRateMonitor</code>.</li> <li>Optionally <code>HydraConfigSaverCallback</code> (from <code>ProjectEquiSurv</code> example).</li> </ul> </li> <li>Call <code>trainer.fit(model, train_dataloader, val_dataloader)</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/pipeline/pipeline_train_single.py</code> (<code>train_single_run</code>)</li> </ul> </li> </ul> <p>4.7. Implement Inference Script (<code>predict.py</code>)</p> <ul> <li>Task: Create the script (<code>rna_predict/predict.py</code>) to generate 3D coordinates from a trained checkpoint.</li> <li>Details:<ul> <li>Use <code>@hydra.main</code>.</li> <li>Load model configuration (<code>cfg</code>).</li> <li>Instantiate the pipeline model structure (e.g., potentially wrapped in the <code>RNALightningModule</code> or directly using the orchestrator logic).</li> <li>Load trained weights from the specified checkpoint path using the <code>partial_load_state_dict</code> logic (Task 4.4).</li> <li>Set <code>model.eval()</code>.</li> <li>Load input sequences (e.g., from a file specified in <code>cfg</code>).</li> <li>Prepare features (including precomputed adjacency) for each sequence.</li> <li>Iterate through sequences, call the pipeline's inference function (e.g., <code>run_full_pipeline</code>) with <code>torch.no_grad()</code>.</li> <li>Save the resulting <code>final_coords</code> tensor to a file (e.g., <code>.pdb</code> using BioPython or <code>.pt</code>).</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>):<ul> <li><code>quick_fixes/advanced/kaggle_submission/ensemble_inference.py</code></li> <li><code>quick_fixes/advanced/kaggle_submission/inference_utils.py</code></li> </ul> </li> </ul> <p>5. Optional Enhancements for M2</p> <ul> <li>Basic Logging: Use <code>self.log(\"train/loss\", loss)</code> in <code>training_step</code> and <code>self.log(\"val/loss\", loss)</code> in <code>validation_step</code> within the LightningModule.</li> <li>Gradient Clipping: Set <code>gradient_clip_val</code> in the <code>Trainer</code> arguments (e.g., <code>gradient_clip_val=1.0</code>).</li> <li>NaN/Inf Checks: Add simple checks like <code>if torch.isnan(loss).any(): raise ValueError(\"NaN loss detected\")</code> after loss calculation.</li> <li>Qualitative Output Check: Manually view 1-2 generated PDBs.</li> </ul> <p>6. Definition of Done (M2)</p> <p>M2 is complete when all 11 \"Must-Have\" requirements listed in Section 3 are met and verified. This signifies that the integrated pipeline is demonstrably trainable using LoRA and capable of producing initial 3D outputs from saved checkpoints.</p> <p>7. Next Steps (Post-M2)</p> <p>Upon successful completion of M2, the focus shifts to M3: Validation &amp; Debugging. This will involve implementing quantitative validation metrics (TM-score), systematically evaluating the initial M2 predictions, debugging geometric/numerical issues, and performing initial hyperparameter adjustments.</p> <p>Important considerations: Location and generation process for pre-computed Stage A adjacency matrices.</p> <p>Location and structure of the LoRA configuration within the Hydra setup.</p> <p>Exact implementation details for trainable parameter filtering and partial checkpoint loading.</p> <p>Location and specific type (MSE, L1, FAPE?) of the primary 3D loss function.</p> <p>Status and input/output handling of the UnifiedLatentMerger.</p> <p>Confirmation that the necessary dependencies (Lightning, Hydra, PEFT) are installed.</p>"},{"location":"pipeline/kaggle_info/M2_Plan/#confirmation-that-tensor-shape-patching-is-active","title":"Confirmation that tensor shape patching is active.","text":"<p>V2: Okay, here is a comprehensive document detailing the goals, requirements, and tasks for Milestone 2 (M2), suitable for briefing a senior developer. It incorporates the analysis of the <code>rna_predict</code> project and leverages relevant patterns identified from the <code>ProjectEquiSurv</code> example.</p> <p>Project <code>rna_predict</code> - Milestone 2 Specification: Initial End-to-End Training &amp; Prediction</p> <p>Document Version: 1.0 Date: March 30, 2025 Target Completion for M2: ~April 7th-10th, 2025</p> <p>1. Objective</p> <p>The primary goal of Milestone 2 (M2) is to achieve and validate the functional trainability of the integrated RNA 3D prediction pipeline developed in M1. This involves demonstrating that the end-to-end system (Sequence -&gt; Stage A Adjacency -&gt; Stage B Torsions/Pairs -&gt; Unified Latent Merger -&gt; Stage D Diffusion -&gt; Final 3D Coords) can execute within a training loop, compute losses, backpropagate gradients correctly to the intended trainable parameters (primarily LoRA adapters), and generate initial 3D coordinate outputs.</p> <p>Key Focus: Functionality and Infrastructure. Achieving high prediction accuracy is not the goal of M2; instead, we focus on proving the pipeline works mechanically, gradients flow correctly, and we can generate baseline outputs for subsequent debugging and refinement (M3). The infrastructure established in M2 (training scripts, configuration, checkpointing) will be crucial for future iterations.</p> <p>2. Prerequisites</p> <ul> <li>M1 Completed: The integrated pipeline codebase is complete and functionally callable via <code>rna_predict.pipeline.run_pipeline.run_full_pipeline</code>. This implies:<ul> <li>Functional interfaces/wrappers for Stage A (<code>StageARFoldPredictor</code>), Stage B (<code>run_stageB_combined</code> using <code>StageBTorsionBertPredictor</code> &amp; <code>PairformerWrapper</code>), Stage C (<code>run_stageC</code>), and Stage D (<code>ProtenixDiffusionManager</code>, <code>run_stageD_diffusion</code>).</li> <li>The <code>UnifiedLatentMerger</code> (<code>SimpleLatentMerger</code> or equivalent) is integrated into the <code>run_full_pipeline</code> flow.</li> <li>LoRA adapters have been applied to the base TorsionBERT and Pairformer models (using <code>peft</code> or custom wrappers), and these wrapped models are used in the pipeline. Base weights are configured to be frozen.</li> <li>The necessary runtime patching (<code>rna_predict.pipeline.patching.shape_fixes.apply_tensor_fixes</code>) is integrated and callable.</li> </ul> </li> </ul> <p>3. Core Requirements (Definition of Done for M2)</p> <p>The following must be demonstrably working by the end of M2:</p> <ol> <li>Trainable Pipeline Execution: The primary training script (<code>rna_predict/train.py</code>) successfully executes the full forward pass of the integrated pipeline (A -&gt; B -&gt; Merger -&gt; D) on batches of training data without crashing (resolving any shape, type, or device errors).</li> <li>Loss Calculation: A defined loss function (minimum: 3D coordinate-based loss like L1/MSE between Stage D output <code>final_coords</code> and ground truth <code>coords_gt</code>) is implemented and computes a valid (non-NaN, non-Inf) scalar loss value per batch.</li> <li>Data Loading: A functional PyTorch <code>DataLoader</code> (using <code>rna_predict.data.loader.RNADataset</code> or similar) provides batches containing sequences, pre-computed adjacency matrices, ground truth 3D coordinates, and potentially other features. It must correctly handle padding/batching for variable-length sequences.</li> <li>Optimizer Configuration &amp; Parameter Targeting: An optimizer (e.g., AdamW) is correctly configured to update only the specified trainable parameters (LoRA adapters in TorsionBERT/Pairformer, UnifiedLatentMerger weights, Diffusion head/adapters). A verified mechanism exists to filter and provide only these parameters to the optimizer.</li> <li>Backpropagation &amp; Weight Update: <code>loss.backward()</code> and <code>optimizer.step()</code> execute successfully, showing gradients are computed and applied to the trainable parameters without error. Verification should confirm non-zero gradients for trainable parameters and zero gradients for frozen base parameters.</li> <li>Training Loop Functionality: The training script completes at least one full epoch over the training dataset without critical runtime errors, demonstrating stability.</li> <li>Checkpoint Saving: The training process successfully saves model checkpoints containing the <code>state_dict</code> of trainable parameters and the optimizer state (e.g., via PyTorch Lightning's <code>ModelCheckpoint</code>).</li> <li>Inference Script: A separate inference script (<code>rna_predict/predict.py</code>) exists for generating predictions from saved checkpoints.</li> <li>Checkpoint Loading (Partial): The inference script successfully loads the trainable parameters from a saved M2 checkpoint into the corresponding pipeline model structure using a partial loading mechanism (e.g., <code>partial_load_state_dict</code> adapted from <code>ProjectEquiSurv</code>), gracefully handling the absence of base model weights in the checkpoint.</li> <li>Initial Prediction Generation: The inference script, using a loaded M2 checkpoint, successfully generates and saves 3D coordinate outputs (e.g., <code>.pdb</code> format) for a small, representative subset of the validation data without crashing.</li> <li>LanceDB Logging Stub: A stub LanceDB logger is present in the codebase (<code>rna_predict/utils/lance_logger.py</code>), with no-op logging methods and a config flag (<code>PipelineConfig.lance_db.enabled</code>) controlling activation. For M2, this logger is a placeholder; real logging will be implemented in M3.</li> </ol> <p>4. Key Technical Tasks &amp; Recommended Approaches</p> <p>Confirmations Needed Before Starting:</p> <ul> <li>[ ] Data Paths: Confirm locations for raw Kaggle CSVs, precomputed Stage A adjacency files (specify format/naming), and ground truth coordinate data.</li> <li>[ ] Train/Val Split: Confirm the method/file defining the train/validation split indices.</li> <li>[ ] LoRA Configuration: Confirm the location (<code>conf/lora/*.yaml</code>?) and structure of LoRA configs (<code>r</code>, <code>alpha</code>, <code>target_modules</code>).</li> <li>[ ] LoRA Wrapping: Confirm the implementation used to wrap base models with LoRA adapters is functional.</li> <li>[ ] Parameter Freezing: Confirm the mechanism used to freeze base model parameters is active.</li> <li>[ ] Loss Function: Confirm the specific 3D coordinate loss function (L1, MSE, FAPE?) and its location (<code>rna_predict/core/losses.py</code>?).</li> <li>[ ] Merger Status: Confirm the <code>UnifiedLatentMerger</code> implementation is stable and handles expected inputs.</li> <li>[ ] Dependencies: Confirm PyTorch Lightning, Hydra, and PEFT (or custom LoRA library) are in <code>requirements-dev.txt</code>.</li> <li>[ ] Patching: Confirm <code>apply_tensor_fixes</code> is being called at the start of <code>train.py</code> and <code>predict.py</code>.</li> </ul> <p>Implementation Tasks:</p> <p>4.1. Implement Training Framework (Leverage PyTorch Lightning)</p> <ul> <li>Task: Create <code>rna_predict/training/rna_lightning_module.py</code> defining <code>RNALightningModule</code>.</li> <li>Details:<ul> <li><code>__init__</code>: Instantiate the full pipeline model components based on Hydra config (<code>cfg</code>). Store components (predictors, merger, manager) as attributes. Store loss weights.</li> <li><code>forward</code>: Define the core forward pass logic (potentially calling <code>run_full_pipeline</code>).</li> <li><code>training_step</code>: Call <code>forward</code>, compute loss(es), log <code>train/loss</code>.</li> <li><code>validation_step</code>: Call <code>forward</code> with <code>torch.no_grad()</code>, compute loss(es), compute basic coordinate MAE, log <code>val/loss</code>, <code>val/mae</code>.</li> <li><code>configure_optimizers</code>: Instantiate optimizer (e.g., AdamW from <code>cfg.training.optimizer</code>) targeting only trainable parameters (Task 4.3). Optionally build LR scheduler (using <code>build_scheduler</code> pattern from <code>ProjectEquiSurv</code>).</li> <li><code>load_state_dict</code>: Override to use <code>partial_load_state_dict</code> (Task 4.4).</li> <li>Integrate config flags for LanceDB logging (stub for M2, see <code>PipelineConfig.lance_db</code>).</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>): <code>lightning_survival_module.py</code>, <code>optim_wrappers.py</code>.</li> </ul> <p>4.2. Establish Configuration Management (Leverage Hydra)</p> <ul> <li>Task: Set up <code>rna_predict/conf/</code> directory and schema.</li> <li>Details:<ul> <li>Create <code>rna_predict/conf/config_schema.py</code> with dataclasses (<code>DataConfig</code>, <code>ModelConfig</code>, <code>LoRAConfig</code>, <code>TrainingConfig</code>, <code>DiffusionConfig</code>, <code>PipelineConfig</code>, <code>RootConfig</code>).</li> <li>Create default <code>.yaml</code> files in <code>rna_predict/conf/</code> (e.g., <code>train.yaml</code>, <code>model/default.yaml</code>, <code>lora/torsionbert.yaml</code>).</li> <li>Use <code>@hydra.main</code> in <code>train.py</code> and <code>predict.py</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>): <code>conf/config_schema.py</code>, <code>pipeline/main.py</code>.</li> </ul> <p>4.3. Implement Trainable Parameter Filtering</p> <ul> <li>Task: Implement a function <code>get_trainable_parameters(model: torch.nn.Module)</code> -&gt; Iterator[torch.nn.Parameter].</li> <li>Details: Place this in <code>rna_predict/core/utils.py</code> or within <code>RNALightningModule</code>. It should iterate <code>model.named_parameters()</code> and yield <code>p</code> if <code>lora</code> is in <code>name</code> OR if <code>p</code> belongs to the Merger/Diffusion head (identify by module name/path) AND <code>p.requires_grad</code> is True.</li> <li>Verification: Log the count/percentage of trainable parameters in <code>RNALightningModule.__init__</code>.</li> </ul> <p>4.4. Implement Partial Checkpoint Loading</p> <ul> <li>Task: Adapt <code>partial_load_state_dict</code> for use in <code>rna_predict</code>.</li> <li>Details: Copy the function logic into <code>rna_predict/core/checkpointing.py</code> or <code>utils.py</code>. Ensure it handles loading a <code>state_dict</code> containing only LoRA/Merger/Diffusion keys into the full pipeline model structure without raising <code>strict=True</code> errors. Call this from <code>RNALightningModule.load_state_dict</code> and <code>predict.py</code>.</li> <li>Reference (<code>ProjectEquiSurv</code>): <code>hypercloning/checkpoint_utils.py</code>.</li> </ul> <p>4.5. Build Data Handling Pipeline</p> <ul> <li>Task: Finalize <code>RNADataset</code> and <code>DataLoader</code> setup.</li> <li>Details:<ul> <li>Ensure <code>RNADataset</code> (<code>data/loader.py</code>) correctly loads sequence, adjacency (e.g., from <code>data/processed/adjacency/{seq_id}.pt</code>), and ground truth coords (e.g., from <code>data/processed/coords/{seq_id}.pt</code>). Return a dictionary.</li> <li>Implement splitting logic in <code>train.py</code> (e.g., using <code>torch.utils.data.random_split</code> or loading pre-defined indices).</li> <li>Create <code>DataLoader</code>s. Implement a <code>collate_fn</code> if needed for padding batches of variable-length sequences/adjacencies/coords.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>): <code>graph_survival_dataset.py</code>, <code>split_manager.py</code>.</li> </ul> <p>4.6. Implement Training Script (<code>train.py</code>)</p> <ul> <li>Task: Create the main training script <code>rna_predict/train.py</code>.</li> <li>Details:<ul> <li>Use <code>@hydra.main(config_path=\"conf\", config_name=\"train\")</code>.</li> <li>Initialize logging (WandB/TensorBoard via Lightning loggers).</li> <li>Call <code>apply_tensor_fixes()</code>.</li> <li>Instantiate <code>RNALightningModule(cfg)</code>.</li> <li>Setup <code>DataLoader</code>s.</li> <li>Setup <code>ModelCheckpoint</code> callback (monitor <code>val/loss</code> or <code>val/mae</code>).</li> <li>Instantiate <code>Trainer</code> (pass <code>logger</code>, <code>callbacks</code>, <code>max_epochs</code> from <code>cfg</code>).</li> <li>Call <code>trainer.fit(model, train_dataloader, val_dataloader)</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>): <code>pipeline/pipeline_train_single.py</code>.</li> </ul> <p>4.7. Implement Inference Script (<code>predict.py</code>)</p> <ul> <li>Task: Create the inference script <code>rna_predict/predict.py</code>.</li> <li>Details:<ul> <li>Use <code>@hydra.main</code>.</li> <li>Call <code>apply_tensor_fixes()</code>.</li> <li>Instantiate the model structure (e.g., <code>RNALightningModule(cfg)</code> or directly the pipeline components).</li> <li>Load checkpoint using <code>torch.load</code> and <code>partial_load_state_dict</code>.</li> <li>Set <code>model.eval()</code>.</li> <li>Load list of sequences to predict (e.g., from <code>cfg.predict.input_file</code>).</li> <li>Create a simple <code>DataLoader</code> or loop through sequences.</li> <li>For each sequence: prepare features (adjacency), run inference via <code>model.forward</code> or <code>run_full_pipeline</code> with <code>torch.no_grad()</code>, potentially generating multiple samples (<code>N_sample</code> in diffusion).</li> <li>Save <code>final_coords</code> to <code>.pdb</code> (using BioPython) or <code>.pt</code> files in <code>cfg.predict.output_dir</code>.</li> </ul> </li> <li>Reference (<code>ProjectEquiSurv</code>): <code>kaggle_submission/ensemble_inference.py</code>, <code>inference_utils.py</code>.</li> </ul> <p>5. Optional Enhancements for M2</p> <ul> <li>Logging: Integrate <code>wandb</code> logger with PyTorch Lightning <code>Trainer</code>.</li> <li>Gradient Clipping: Add <code>gradient_clip_val: 1.0</code> to <code>conf/train.yaml</code> and pass to <code>Trainer</code>.</li> <li>Qualitative Check: Add a step in <code>train.py</code>'s <code>on_validation_epoch_end</code> hook (or manually run <code>predict.py</code>) to save one prediction PDB and visually inspect it.</li> </ul> <p>6. Definition of Done (M2)</p> <p>M2 is complete when all 11 \"Must-Have\" requirements listed in Section 3 are met and verified. The integrated pipeline is demonstrably trainable using LoRA, and initial 3D coordinate predictions can be generated from saved checkpoints for validation data.</p> <p>7. Next Steps (Post-M2)</p> <p>M3 will focus on Validation &amp; Debugging: Implementing quantitative metrics (TM-score), evaluating M2 predictions, debugging geometry/numerical issues, and performing initial hyperparameter tuning based on validation results.</p> <p>Okay, let's define specific, actionable code quality and testing goals for Milestone 2 (M2), tailored for a senior developer who has successfully completed M1. These goals align with M2's primary objective: achieving functional end-to-end trainability and initial output generation.</p> <p>Context: M1 established the integrated pipeline structure and module implementations (potentially with stubs or basic functionality). M2 focuses on making this integrated system runnable for training and inference, particularly validating LoRA, gradient flow, and the core mechanics.</p> <p>M2 Code Quality Goals</p> <p>The focus is on functional correctness, clear integration points, and maintainable infrastructure setup, rather than perfect optimization or deep refactoring at this stage.</p> <ol> <li> <p>Functional Correctness of New M2 Components:</p> <ul> <li>Goal: Ensure the primary scripts and modules introduced or significantly modified for M2 (<code>train.py</code>, <code>predict.py</code>, <code>RNALightningModule</code>, <code>RNADataset</code>, partial checkpoint loader) execute their core functions without runtime errors (crashes, type errors, major shape mismatches not handled by patching).</li> <li>Metric: Successful completion of a minimal training run (e.g., 1 epoch or 100 steps) and a minimal inference run (e.g., predicting 1-2 validation samples).</li> </ul> </li> <li> <p>Readability and Clarity of Integration Logic:</p> <ul> <li>Goal: The code implementing the training loop (<code>RNALightningModule</code>, <code>train.py</code>), inference logic (<code>predict.py</code>), and data loading (<code>RNADataset</code>) should be clearly structured and understandable. Key integration points (e.g., how data flows into <code>run_full_pipeline</code>, how loss is computed, how trainable parameters are selected) should be evident.</li> <li>Metric: Peer code review confirms understandability. Docstrings for major classes/functions (<code>RNALightningModule</code>, <code>train_single_run</code> equivalent, <code>predict.py</code> main function) are present and explain purpose/flow.</li> </ul> </li> <li> <p>Configuration-Driven Execution:</p> <ul> <li>Goal: All critical hyperparameters (learning rate, batch size, LoRA ranks/targets, loss weights, file paths, diffusion steps, etc.) should be managed via the Hydra configuration system (<code>conf/</code> directory), not hardcoded.</li> <li>Metric: The <code>train.py</code> and <code>predict.py</code> scripts primarily rely on the <code>cfg: DictConfig</code> object for settings. Manual inspection confirms no major hardcoded parameters remain.</li> </ul> </li> <li> <p>Correct LoRA/Trainable Parameter Handling:</p> <ul> <li>Goal: The mechanism for identifying and isolating trainable parameters (LoRA adapters, Merger, Diffusion head) for the optimizer must be implemented correctly and demonstrably work. Base model parameters must remain frozen.</li> <li>Metric: Code inspection confirms filtering logic. Logging/debugging output confirms only the intended parameters receive gradients and updates during a test <code>optimizer.step()</code>.</li> </ul> </li> <li> <p>Robust Checkpoint Save/Load (Partial):</p> <ul> <li>Goal: Checkpoints saved during training must contain only the necessary trainable weights and optimizer state. The inference script must successfully load these partial checkpoints into the full model structure.</li> <li>Metric: Successful execution of Requirement #9 and #10 (loading checkpoint and generating inference output). Manual inspection of a saved <code>.pt</code> file can confirm it doesn't contain the full base model weights (should be significantly smaller).</li> </ul> </li> <li> <p>Explicit Patching:</p> <ul> <li>Goal: Runtime tensor shape patches should be applied explicitly and controllably, ideally via the central <code>patching/shape_fixes.py</code> module.</li> <li>Metric: Patches are applied via a clear function call (e.g., <code>apply_tensor_fixes()</code>) at the start of <code>train.py</code> and <code>predict.py</code>.</li> </ul> </li> </ol> <p>Out of Scope for M2 Code Quality:</p> <ul> <li>Deep refactoring of M1 modules (unless required to fix critical M2 blockers).</li> <li>Extensive performance optimization (micro-optimizations, kernel fusion).</li> <li>Full PEP 8 compliance across the entire codebase (focus on new M2 code).</li> <li>Elimination of the need for runtime patching (this is a later refactoring goal).</li> </ul> <p>M2 Testing Goals</p> <p>The focus is on integration testing to verify the core mechanics of the end-to-end training and inference workflows. Unit testing is secondary and targeted at critical new components.</p> <ol> <li> <p>Core Training Loop Integration Test:</p> <ul> <li>Goal: Verify that the training script (<code>train.py</code> using <code>RNALightningModule</code>) can successfully execute a small number of steps (e.g., 2-5 batches) on realistic (or simplified real) data without crashing.</li> <li>Implementation: A pytest test (e.g., <code>tests/pipeline/test_train_integration.py</code>) that:<ul> <li>Sets up a minimal Hydra config pointing to small data subsets/adjacencies.</li> <li>Instantiates the <code>RNALightningModule</code> and a Lightning <code>Trainer</code> with <code>fast_dev_run=True</code> or <code>max_steps=5</code>.</li> <li>Calls <code>trainer.fit()</code>.</li> </ul> </li> <li>Assertions: The test passes if <code>trainer.fit()</code> completes without exceptions. Check that the logged training loss is not NaN/Inf.</li> </ul> </li> <li> <p>Gradient Flow and Parameter Update Test:</p> <ul> <li>Goal: Confirm that backpropagation works and only updates the intended trainable parameters (LoRA, etc.).</li> <li>Implementation: Extend the training loop integration test (or create a separate one):<ul> <li>Before the first <code>optimizer.step()</code>, store the initial values (or norms) of a sample of trainable parameters and a sample of frozen parameters.</li> <li>After <code>optimizer.step()</code>, assert that the trainable parameters have changed and the frozen parameters have not changed.</li> <li>Assert that <code>param.grad</code> is non-None for trainable params and None (or all zeros) for frozen params after <code>loss.backward()</code>.</li> </ul> </li> <li>Metric: Test passes assertions.</li> </ul> </li> <li> <p>Checkpoint Save/Load/Inference Cycle Test:</p> <ul> <li>Goal: Verify the critical cycle of saving trainable weights during training and loading them for inference works correctly.</li> <li>Implementation: A pytest test (e.g., <code>tests/pipeline/test_checkpoint_inference_cycle.py</code>) that:<ol> <li>Runs the training loop test (Task 1) for a few steps, ensuring a checkpoint is saved by <code>ModelCheckpoint</code>.</li> <li>Instantiates the inference pipeline model structure.</li> <li>Loads the saved checkpoint using the <code>partial_load_state_dict</code> logic.</li> <li>Runs the inference logic (<code>predict.py</code>'s core steps) on a single test sequence using the loaded model.</li> </ol> </li> <li>Assertions: Loading the partial checkpoint succeeds. Inference runs without crashing. The output coordinate tensor has the expected shape and contains numeric values (not all zeros, no NaNs/Infs).</li> </ul> </li> <li> <p>Unit Test: <code>partial_load_state_dict</code>:</p> <ul> <li>Goal: Ensure the partial checkpoint loading utility functions correctly in isolation.</li> <li>Implementation: Unit tests in <code>tests/core/test_checkpointing.py</code> using dummy <code>nn.Module</code>s and <code>state_dict</code>s (one full, one partial) to verify correct loading behavior and error handling.</li> </ul> </li> <li> <p>Unit Test: <code>RNADataset</code> / <code>collate_fn</code>:</p> <ul> <li>Goal: Verify the dataset loading and batch collation.</li> <li>Implementation: Unit tests in <code>tests/data/test_loader.py</code> that:<ul> <li>Instantiate <code>RNADataset</code> with mock file paths.</li> <li>Check that <code>__getitem__</code> returns a dictionary with the expected keys and tensor shapes/types for a single sample.</li> <li>Test the <code>collate_fn</code> (if custom) to ensure it correctly batches samples, potentially including padding.</li> </ul> </li> </ul> </li> </ol> <p>Out of Scope for M2 Testing:</p> <ul> <li>Unit tests for all individual modules implemented in M1 (assume basic tests exist or focus is on integration).</li> <li><code>hypothesis</code>-based fuzzing for most components (can be added in M3+).</li> <li>Quantitative accuracy tests (TM-score, precise coordinate RMSD).</li> <li>Testing edge cases in data loading (corrupted files, very long sequences beyond padding limits).</li> <li>Mocking external dependencies like Hugging Face Hub downloads (assume models are pre-downloaded or rely on caching).</li> </ul> <p>Summary for Senior Developer:</p> <p>M2 requires building and validating the core training and inference infrastructure around the integrated pipeline from M1. Code quality focuses on functional correctness and clear configuration. Testing priorities are integration tests verifying the train loop, gradient flow to LoRA parameters, and the checkpoint save/load/predict cycle. Unit tests should cover critical new M2 components like partial checkpoint loading and data collation. Achieving these goals ensures we have a mechanically sound system ready for quantitative validation and iterative refinement in M3. Please confirm the prerequisite checks listed in the previous document section before proceeding with these tasks.</p>"},{"location":"pipeline/kaggle_info/kaggle_competition/","title":"Comprehensive Overview: Stanford RNA 3D Folding Competition \ud83e\uddec","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#1-competition-goal","title":"1. Competition Goal \ud83c\udfaf","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#high-level-objective","title":"High-Level Objective","text":"<p>Accurately predict the 3D coordinates (x, y, z) of each nucleotide\u2019s C1\u2032 atom in an RNA chain from sequence alone. Competitors must provide five distinct structural predictions per RNA target. The primary evaluation metric is the TM-score (0 to 1; higher scores indicate superior predictions), widely accepted for RNA/protein structure comparisons.</p>"},{"location":"pipeline/kaggle_info/kaggle_competition/#challenges","title":"Challenges","text":"<ul> <li>RNA Flexibility: RNA frequently adopts multiple conformations.</li> <li>Existing Limitations: Automated RNA prediction lags behind expert-guided manual modeling.</li> <li>Competition Ambition: Outperform manual expert predictions and advance RNA modeling frontiers.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#2-detailed-data-overview","title":"2. Detailed Data Overview \ud83d\udcc2","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#primary-files","title":"Primary Files","text":"<ul> <li> <p>train_sequences.csv (~844 sequences)</p> <ul> <li> <p><code>target_id</code>: Unique identifier (e.g., pdbid_chain).</p> </li> <li> <p><code>sequence</code>: RNA nucleotide sequence (A, C, G, U, plus rare alternatives).</p> </li> <li> <p><code>temporal_cutoff</code>: Date of sequence/structure publication; ensures compliance with chronological data-use constraints.</p> </li> <li> <p><code>description</code>: Context about RNA (source, ligands, etc.).</p> </li> <li> <p><code>all_sequences</code>: FASTA-format sequences of all experimental structure chains.</p> </li> </ul> </li> <li> <p>train_labels.csv (Experimental coordinates)</p> <ul> <li> <p><code>ID</code>: Combination of target ID and residue number (e.g., 101D_1).</p> </li> <li> <p><code>resname</code>: Nucleotide (A, C, G, U).</p> </li> <li> <p><code>resid</code>: Residue index (1-based).</p> </li> <li> <p>Coordinates (<code>x_1, y_1, z_1, x_2, y_2, z_2, \u2026</code>): Multiple conformations or PDB depositions.</p> </li> </ul> </li> <li> <p>validation_sequences.csv / validation_labels.csv</p> <ul> <li> <p>~12 RNA targets from prior CASP15 challenges, intended for local validation.</p> </li> <li> <p>Often considered \"burned\" after initial leaderboard tuning.</p> </li> </ul> </li> <li> <p>test_sequences.csv</p> <ul> <li>Public leaderboard test set, periodically updated; no labels provided.</li> </ul> </li> <li> <p>sample_submission.csv</p> <ul> <li>Submission format example: <pre><code>ID, resname, resid, x_1, y_1, z_1, ..., x_5, y_5, z_5\n</code></pre></li> </ul> </li> <li> <p>MSA/ folder</p> <ul> <li>Multiple Sequence Alignments (FASTA), valuable for evolutionary conservation signals.</li> </ul> </li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#additional-resources","title":"Additional Resources","text":"<ul> <li>Synthetic RNA Dataset: 400,000+ structures from RFdiffusion available for model augmentation.</li> <li>Public PDB Data: Allowed with strict adherence to temporal cutoff rules.</li> <li>External Advanced Models: Permitted if temporally compliant and Kaggle-offline.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#3-data-utilization-and-structure","title":"3. Data Utilization and Structure \ud83d\udd0d","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#sequence-to-3d-coordinate-mapping","title":"Sequence-to-3D Coordinate Mapping","text":"<ul> <li>Models take RNA sequences/MSAs and output coordinates of the C1\u2032 atom.</li> <li>Training set teaches sequence-structure relationships; validation refines performance.</li> <li>Kaggle evaluates predictions against hidden labels using automated TM-score calculation.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#handling-multiple-conformations","title":"Handling Multiple Conformations","text":"<ul> <li>Datasets may include multiple conformations per RNA; Kaggle selects best TM-score alignment automatically.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#five-predictions-requirement","title":"Five Predictions Requirement","text":"<ul> <li>Generate five structural predictions per residue, either from single models (multiple seeds) or distinct models.</li> <li>Kaggle scoring uses only the best of these five predictions per RNA target.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#4-scoring-tm-score","title":"4. Scoring: TM-score \ud83d\udccf","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#calculation-formula","title":"Calculation Formula","text":"<pre><code>TM-score = max(1/L_ref \u00d7 \u03a3[1/(1+(d_i/d_0)\u00b2)])\n</code></pre> <ul> <li><code>L_ref</code>: Number of residues in the reference structure.</li> <li><code>d_i</code>: Distance between aligned residues (C1\u2032 atoms).</li> <li>Automated sequence-independent alignment via US-align.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#alignment-specifics","title":"Alignment Specifics","text":"<ul> <li>Automated, sequence-independent alignment ensures optimal structural comparison.</li> <li>Final TM-score averaged over all test targets, using your best-of-five predictions.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#5-competition-timeline-and-phases","title":"5. Competition Timeline and Phases \ud83d\udcc5","text":"Phase Date Description Start Date February 27, 2025 Release of training data and initial test sequences Leaderboard Refresh April 23, 2025 New sequences added; some test sequences moved to training; leaderboard reset; early-sharing prizes awarded Final Submission Deadline May 29, 2025 Final submissions for private leaderboard ranking Future Data Phase June \u2013 September 2025 Evaluate generalization on up to 40 new RNA structures"},{"location":"pipeline/kaggle_info/kaggle_competition/#6-frequently-asked-questions-insights","title":"6. Frequently Asked Questions &amp; Insights \u2753","text":"<ul> <li>Training Set Size: Smaller to focus on direct, experimentally validated 3D structures rather than indirect data.</li> <li>Multiple Predictions: Required due to RNA\u2019s propensity for multiple valid conformations.</li> <li>Use of External Tools: Permitted with offline capabilities, compliance with temporal cutoffs.</li> <li>Temporal Cutoff Purpose: Prevents \"future data leakage\"; structural information post-cutoff date disallowed.</li> <li>Multiple Reference Structures: Kaggle selects best conformational alignment automatically.</li> <li>Future Data Evaluation: Validates methods' genuine generalization capabilities.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#7-practical-steps-for-effective-modeling","title":"7. Practical Steps for Effective Modeling \ud83d\udea7","text":""},{"location":"pipeline/kaggle_info/kaggle_competition/#data-preparation","title":"Data Preparation","text":"<ul> <li>Handle duplicates and multiple entries.</li> <li>Decide treatment for multiple conformations (single vs. multi-target).</li> <li>Optionally augment data with synthetic RNA or public structures.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#model-strategies","title":"Model Strategies","text":"<ul> <li>Neural Networks: Graph neural networks, equivariant networks, diffusion models.</li> <li>Language Models: Fine-tune large language models with structural prediction heads.</li> <li>Hybrid Approaches: Combine existing predictors (RiboNanzaNet, RhoFold) with energy-based refinement.</li> <li>Manual/Heuristic Methods: Utilize known RNA motifs (A-minor motifs, base-pair geometry constraints).</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#submission-guidelines","title":"Submission Guidelines","text":"<ul> <li>Provide five distinct predictions per residue.</li> <li>Follow exact submission format provided by Kaggle.</li> <li>Observe Kaggle's runtime constraints (\u2264 8 hours).</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#validation-optimization","title":"Validation &amp; Optimization","text":"<ul> <li>Employ cross-validation, particularly across temporal splits.</li> <li>Optionally replicate Kaggle\u2019s TM-score calculation locally using US-align.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_competition/#8-key-takeaways","title":"8. Key Takeaways \ud83d\udddd\ufe0f","text":"<ul> <li>Primary Goal: Predict RNA 3D coordinates (C1\u2032 atoms).</li> <li>Provided Data: Moderately sized dataset (~844 structures), CASP15 validation set, evolving test sequences.</li> <li>Five-Prediction Requirement: Encourages robust exploration of RNA structural variability.</li> <li>Scoring Metric: TM-score (US-align automated alignment).</li> <li>Timeline: Frequent data and leaderboard updates; final evaluation includes unseen future structures.</li> <li>Opportunities and Challenges:<ul> <li>Complex RNA folding dynamics.</li> <li>Advanced modeling techniques encouraged (deep learning, diffusion).</li> <li>Strict adherence to data-use temporal constraints critical.</li> <li>Collaborative environment with incentives for transparency and early result sharing.</li> </ul> </li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_inference_notebook_refactor_plan/","title":"Kaggle inference notebook refactor plan","text":"<p>The <code>rna-predict.ipynb</code> notebook, as described in the context and inferred from the provided file contents and logs, primarily serves as an inference pipeline for the Kaggle RNA 3D structure prediction competition. It does not perform model training within the notebook environment but relies on pre-trained models (specifically for Stage B TorsionBERT) loaded from Kaggle datasets or the working directory.</p> <p>Here's a breakdown of its likely operation and a plan for refactoring it into a simpler, more robust Kaggle wrapper:</p> <p>Current Notebook Workflow (Inferred &amp; Contextualized):</p> <ol> <li> <p>Environment Setup (Cells 1-4, parts of 12 in the provided notebook example):</p> <ul> <li>Cleans Kaggle's auto-generated <code>requirements.txt</code>.</li> <li>Lists contents of <code>/kaggle/input</code> for an overview of available datasets.</li> <li>Performs offline installation of Python packages from wheels provided in <code>/kaggle/input</code>. This is crucial for Kaggle competitions with internet disabled. The list of packages includes <code>torch</code>, <code>transformers</code>, <code>pytorch_lightning</code>, <code>hydra-core</code>, <code>omegaconf</code>, <code>biopython</code>, <code>mdanalysis</code>, and the <code>rna-predict</code> package itself.</li> <li>Sets environment variables like <code>HF_HUB_OFFLINE=1</code> to ensure HuggingFace Transformers operate offline.</li> <li>Creates symbolic links for model checkpoints (e.g., for TorsionBERT and its base DNABERT model) to expected paths in <code>/kaggle/working/</code>.</li> </ul> </li> <li> <p>Core Inference Pipeline (Cells 12, adapted Cell 14 logic):</p> <ul> <li> <p>Configuration Loading: The notebook manually constructs an <code>OmegaConf</code> object that mimics the project's Hydra configuration structure (<code>rna_predict/conf/default.yaml</code> and <code>predict.yaml</code>). Key settings derived from the context and logs include:</p> <ul> <li><code>device</code>: Set to \"cpu\" or \"cuda\" based on availability. The provided <code>dev_run_output.txt</code> (from <code>training.py</code>) indicates the system can detect \"mps\", but the notebook likely defaults to \"cpu\" for Kaggle's typical environment unless a GPU is explicitly selected.</li> <li><code>model.stageB.torsion_bert.model_name_or_path</code>: Points to the local path of the TorsionBERT model (e.g., <code>/kaggle/working/rna_torsionBERT</code>).</li> <li><code>model.stageB.torsion_bert.angle_mode</code>: Set to <code>\"degrees\"</code>.</li> <li><code>model.stageC.method</code>: Set to <code>\"mp_nerf\"</code>.</li> <li><code>model.stageC.angle_representation</code>: Set to <code>\"degrees\"</code> (to match Stage B output).</li> <li><code>model.stageC.place_bases</code>: <code>True</code>.</li> <li><code>model.stageC.do_ring_closure</code>: <code>True</code> (as per notebook log context, though <code>default.yaml</code> has <code>False</code>).</li> <li><code>model.stageC.sugar_pucker</code>: <code>\"C3'-endo\"</code>.</li> <li><code>prediction.repeats</code>: <code>5</code> (for the 5 required Kaggle predictions).</li> <li><code>prediction.enable_stochastic_inference_for_submission</code>: <code>True</code> (to generate diverse predictions).</li> <li><code>prediction.residue_atom_choice</code>: This is a critical parameter. The notebook's <code>create_predictor</code> cell sets this to <code>0</code>. However, for the Kaggle competition (C1' atom), this should be <code>11</code> (assuming the atom order from <code>rna_predict.utils.tensor_utils.types.STANDARD_RNA_ATOMS</code> where <code>C1'</code> is the 12th atom, index 11). This discrepancy is a key point for refactoring.</li> </ul> </li> <li> <p>Instantiate <code>RNAPredictor</code>: An instance of <code>RNAPredictor</code> (from <code>rna_predict.predict</code> or <code>rna_predict.interface</code>) is created using this manually constructed configuration.</p> </li> <li> <p>Load Test Data: Reads <code>test_sequences.csv</code> from <code>/kaggle/input/stanford-rna-3d-folding/</code>.</p> </li> <li> <p>Iterate and Predict: For each RNA sequence in the test set:</p> <ul> <li>Stage B (Torsion Angle Prediction): <code>predictor.predict_3d_structure(sequence, stochastic_pass=True, seed=...)</code> is called. This internally uses <code>StageBTorsionBertPredictor</code>.<ul> <li>The TorsionBERT model (<code>sayby/rna_torsionbert</code> or a local checkpoint) predicts 7 torsion angles.</li> <li>The <code>stochastic_pass=True</code> enables dropout layers in TorsionBERT, and varying the <code>seed</code> across 5 calls generates slightly different angle sets for the 5 required predictions.</li> <li>The output angles are converted to degrees.</li> </ul> </li> <li>Stage C (3D Reconstruction): The predicted torsion angles (in degrees) are passed to <code>run_stageC</code> (which internally calls <code>run_stageC_rna_mpnerf</code>).<ul> <li>This uses the MP-NeRF based RNA folding logic (<code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_fold</code> and <code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_base_placement</code>) with standard RNA geometry (from <code>final_kb_rna.py</code>).</li> <li>It reconstructs the 3D coordinates of all atoms for the RNA.</li> </ul> </li> <li>C1' Atom Extraction: The coordinates for the C1' atom (or the atom specified by <code>residue_atom_choice</code>) of each residue are extracted. This is where the <code>residue_atom_choice=11</code> becomes critical. The <code>RNAPredictor.predict_submission</code> method uses <code>reshape_coords</code> and <code>extract_atom</code> for this.</li> </ul> </li> <li> <p>Assemble Submission: The C1' coordinates for all 5 predictions for all residues of all test sequences are assembled into a pandas DataFrame. The provided <code>_collapse_to_one_row_per_residue</code> (Cell 14 of the notebook) and the logic within <code>process_test_sequences</code> (also from Cell 14) handle this, ensuring one row per residue and aligning with the <code>ID</code> format (<code>targetID_resIndex</code>) from <code>sample_submission.csv</code>.</p> </li> <li> <p>Save Submission File: The final DataFrame is saved as <code>submission.csv</code>.</p> </li> </ul> </li> <li> <p>Validation (Cell 16):</p> <ul> <li>The notebook includes a final cell to perform sanity checks on the generated <code>submission.csv</code> against <code>test_sequences.csv</code> (e.g., number of rows, missing/extra IDs, per-sequence coverage, column presence, uniqueness of the 5 repeats).</li> </ul> </li> </ol> <p>Analysis of Key Code Components for the Notebook's Workflow:</p> <ul> <li> <p><code>rna_predict.predict.RNAPredictor</code>:</p> <ul> <li><code>__init__</code>: Initializes <code>StageBTorsionBertPredictor</code> and stores Stage C config.</li> <li><code>predict_3d_structure</code>: Calls TorsionBERT and then <code>run_stageC</code> to get full atom coordinates.</li> <li><code>predict_submission</code>: This is the main workhorse. It iterates <code>prediction_config.repeats</code> times, calling <code>predict_3d_structure</code> with <code>stochastic_pass=True</code> and a different <code>seed</code> each time. It then uses <code>reshape_coords</code> and <code>extract_atom</code> (with <code>residue_atom_choice</code>) to get the specific atom's coordinates, and <code>coords_to_df</code> to format them.</li> </ul> </li> <li> <p><code>rna_predict.pipeline.stageB.torsion.torsion_bert_predictor.StageBTorsionBertPredictor</code>:</p> <ul> <li>Loads the TorsionBERT model and tokenizer.</li> <li>The <code>predict_angles_from_sequence</code> method performs inference. If <code>stochastic_pass=True</code>, it sets <code>self.model.train()</code> to enable dropout.</li> <li>The <code>__call__</code> method post-processes raw predictions based on <code>self.angle_mode</code>. If \"degrees\", it converts sin/cos outputs (if TorsionBERT outputs them) to degrees. The logs confirm this conversion happens (\"Values look like radians, converting to degrees\").</li> </ul> </li> <li> <p><code>rna_predict.pipeline.stageC.stage_c_reconstruction.run_stageC</code> and <code>run_stageC_rna_mpnerf</code>:</p> <ul> <li>These functions orchestrate Stage C. <code>run_stageC_rna_mpnerf</code> is the core, using <code>build_scaffolds_rna_from_torsions</code>, <code>rna_fold</code>, and <code>place_rna_bases</code> from the <code>rna_predict.pipeline.stageC.mp_nerf.rna</code> module.</li> <li>It takes angles (expected in degrees as per config) and produces 3D atomic coordinates. <code>atom_metadata</code> (atom names, residue indices) is also generated here.</li> </ul> </li> <li> <p><code>rna_predict.utils.submission</code>:</p> <ul> <li><code>reshape_coords</code>: Prepares coordinates for <code>extract_atom</code>. If given flat coordinates (all atoms of a sequence), it needs to be able to infer per-residue atom counts or be given coordinates already picked for the target atom. The current version seems to expect either <code>[N_residues, 1, 3]</code> (if target atom already picked) or <code>[N_residues, atoms_per_residue, 3]</code>.</li> <li><code>extract_atom</code>: Selects the <code>atom_idx</code>-th atom from each residue in a <code>[N, atoms, 3]</code> tensor.</li> <li><code>coords_to_df</code>: Formats the <code>[N_residues, 3]</code> C1' coordinates into the multi-column Kaggle submission format.</li> </ul> </li> <li> <p>Configuration Files (<code>rna_predict/conf/</code>):</p> <ul> <li><code>default.yaml</code>: Provides base configurations.</li> <li><code>predict.yaml</code>: Inherits from <code>default.yaml</code> and sets some prediction-specific defaults (e.g., <code>fast_dev_run=false</code>).</li> <li>The notebook's manual config creation largely mirrors <code>predict.yaml</code> but explicitly sets <code>enable_stochastic_inference_for_submission=True</code> and potentially a different <code>residue_atom_choice</code>.</li> </ul> </li> </ul> <p>Refactoring Plan for a Simpler Kaggle Wrapper:</p> <p>The existing <code>RNAPredictor</code> in <code>rna_predict.predict.py</code> is quite close to what's needed. The main issues are ensuring correct configuration (especially <code>residue_atom_choice</code>) and streamlining the notebook.</p> <ol> <li> <p>Simplify Configuration in Notebook:</p> <ul> <li>Instead of manually building a large OmegaConf object in a notebook cell, the <code>rna-predict</code> package should provide a utility function (e.g., in <code>rna_predict.conf.utils</code>) to load the default prediction config (<code>predict.yaml</code>) and allow targeted overrides.</li> <li>The notebook would then call:     <pre><code>from rna_predict.conf.utils import get_config, update_config # Assuming get_config exists\nbase_cfg = get_config(config_name=\"predict\") \n\n# Override necessary Kaggle-specific paths and settings\nkaggle_overrides = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"input_csv\": \"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\",\n    \"output_dir\": \"/kaggle/working/\", # or a subfolder\n    \"model.stageB.torsion_bert.model_name_or_path\": \"/kaggle/input/your-torsionbert-model-dataset/rna_torsionBERT\", # Point to Kaggle dataset\n    \"prediction.residue_atom_choice\": 11, # CRITICAL: For C1' atom\n    \"prediction.enable_stochastic_inference_for_submission\": True,\n    \"prediction.repeats\": 5,\n    # Potentially set debug_logging to False for cleaner Kaggle logs\n    \"model.stageB.torsion_bert.debug_logging\": False,\n    \"model.stageC.debug_logging\": False,\n}\ncfg = update_config(base_cfg, kaggle_overrides)\npredictor = RNAPredictor(cfg)\n</code></pre></li> </ul> </li> <li> <p>Ensure C1' Atom Extraction:</p> <ul> <li>The primary change is setting <code>cfg.prediction.residue_atom_choice = 11</code>.</li> <li>Verify that <code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_base_placement.place_rna_bases</code> (called by <code>run_stageC_rna_mpnerf</code>) produces <code>atom_metadata</code> where the atom names and order allow <code>extract_atom</code> with index 11 to reliably pick C1'. <code>STANDARD_RNA_ATOMS</code> in <code>rna_predict.utils.tensor_utils.types</code> lists \"C1'\" as the 12th atom (index 11) for all standard bases, which is correct. The <code>atom_metadata</code> generated by <code>run_stageC_rna_mpnerf</code> uses <code>STANDARD_RNA_ATOMS</code> to list atom names, so this should align.</li> </ul> </li> <li> <p>Use <code>RNAPredictor.predict_submission</code> Directly:</p> <ul> <li>The patched <code>_predict_submission_patched</code> in the notebook (Cell 13) is an attempt to handle potentially flat coordinate outputs and prioritizes 'P' atoms. This patch should be removed.</li> <li>The main <code>RNAPredictor.predict_submission</code> method in <code>rna_predict/predict.py</code> already:<ul> <li>Calls <code>predict_3d_structure</code> 5 times with stochasticity.</li> <li>Uses <code>reshape_coords</code> and <code>extract_atom(..., self.default_atom_choice)</code> (which will be 11 for C1').</li> <li>Uses <code>coords_to_df</code> to create the multi-column format.</li> </ul> </li> <li>This method should correctly produce a DataFrame with 5 sets of C1' coordinates.</li> </ul> </li> <li> <p>Streamline Submission Assembly:</p> <ul> <li>The <code>process_test_sequences</code> function (from notebook Cell 14, but adapted) will call the (now correctly configured) <code>predictor.predict_submission(sequence_string)</code>.</li> <li>This will yield a DataFrame for each test sequence. These DataFrames should be concatenated.</li> <li>The final concatenated DataFrame needs to be merged with <code>sample_submission.csv</code> on the <code>ID</code> column to ensure correct row order and completeness, then saved.     <pre><code># Inside process_test_sequences loop\ndf_pred_single_sequence = predictor.predict_submission(seq_str)\n# Ensure 'ID' in df_pred_single_sequence matches the Kaggle format targetID_resID\n# (coords_to_df creates 1-based 'resid', RNAPredictor.predict_submission might need to combine with target_id)\n# The current coords_to_df creates ID, resname, resid. The RNAPredictor.predict_submission (if using the patch from notebook)\n# might not format the ID column correctly. If using the original predict_submission, it should be fine.\n# The _collapse_to_one_row_per_residue already handles ID formatting.\n\n# After loop:\nall_predictions_df = pd.concat(list_of_individual_sequence_dfs)\nsample_df = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\")\n\n# Merge to ensure correct order and all IDs are present\n# The ID column in all_predictions_df must be correctly formatted as targetID_resID\nsubmission_df = pd.merge(sample_df[['ID']], all_predictions_df, on='ID', how='left')\n\n# Fill any NaNs in coordinate columns (e.g., if a sequence failed) with 0.0\ncoord_cols = [col for col in submission_df.columns if col.startswith(('x_', 'y_', 'z_'))]\nsubmission_df[coord_cols] = submission_df[coord_cols].fillna(0.0)\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\n</code></pre></li> </ul> </li> <li> <p>Training Module (Optional for Inference Notebook):</p> <ul> <li>The refactoring context mentions a <code>train_and_evaluate()</code> module. For a pure Kaggle inference notebook, this is not strictly necessary if using pre-trained models. If training/fine-tuning is desired on Kaggle, <code>rna_predict/training/train.py</code> would be the basis, adapted to load Kaggle data and save checkpoints to <code>/kaggle/working/</code>.</li> </ul> </li> </ol> <p>Summary of Changes to the Notebook:</p> <ul> <li>Keep: Environment setup cells, symlink creation for models.</li> <li>Modify:<ul> <li>Configuration: Use a helper to load base config and apply minimal Kaggle-specific overrides, ensuring <code>prediction.residue_atom_choice = 11</code>.</li> <li>Instantiate <code>RNAPredictor</code> with this simplified config.</li> </ul> </li> <li>Remove/Replace:<ul> <li>The custom <code>_predict_submission_patched</code> (Cell 13).</li> <li>The <code>_collapse_to_one_row_per_residue</code> can be simplified or integrated into the main loop if <code>RNAPredictor.predict_submission</code> is correctly outputting one C1' row per residue with the 5 repeats. The main purpose of <code>_collapse_to_one_row_per_residue</code> in the notebook was to handle the 'P' atom selection and ensure one row per residue. With <code>residue_atom_choice=11</code>, this should be cleaner.</li> </ul> </li> <li>Adapt: The main loop in <code>process_test_sequences</code> (Cell 14) to call the standard <code>predictor.predict_submission</code> and ensure correct <code>ID</code> formatting before concatenating and merging with <code>sample_submission.csv</code>.</li> </ul> <p>This refactoring makes the notebook a thin wrapper around the core <code>rna-predict</code> library, configured for the Kaggle competition's specific requirements (C1' atom, 5 stochastic predictions). It relies on the robustness of <code>RNAPredictor</code> and its sub-modules.Okay, let's break down how the <code>rna-predict.ipynb</code> notebook likely functions within the project for Kaggle submission and plan its refactoring.</p> <p>Understanding the Current Notebook's Inferred Workflow for Kaggle:</p> <p>Based on the provided context, the <code>rna-predict.ipynb</code> notebook is primarily an inference pipeline. It does not perform model training within the Kaggle environment. Its main goal is to take RNA sequences from the competition's test set, predict their 3D structures (specifically, the C1' atom coordinates for each nucleotide), and format these predictions into the required <code>submission.csv</code> file.</p> <p>Here's a step-by-step reconstruction of its operation:</p> <ol> <li> <p>Environment Setup &amp; Offline Installations:</p> <ul> <li>The initial cells of a typical Kaggle notebook would handle setting up the environment. This involves:<ul> <li>Cleaning up Kaggle's default <code>requirements.txt</code>.</li> <li>Installing necessary Python packages from pre-downloaded wheel files available in <code>/kaggle/input/</code>. This is standard practice for competitions where internet access is disabled during submission runs. Packages would include <code>torch</code>, <code>transformers</code>, <code>pytorch_lightning</code> (though an actual <code>Trainer</code> is not used for inference), <code>hydra-core</code>, <code>omegaconf</code>, <code>biopython</code>, <code>mdanalysis</code>, and the custom <code>rna-predict</code> package itself.</li> <li>Setting environment variables like <code>HF_HUB_OFFLINE=1</code>, <code>TRANSFORMERS_OFFLINE=1</code> to ensure HuggingFace libraries don't try to access the internet.</li> <li>Creating symbolic links: The notebook log mentions <code>/kaggle/working/rna_torsionBERT</code>. This implies a pre-trained TorsionBERT model checkpoint is copied or linked from a Kaggle dataset (e.g., from <code>/kaggle/input/rna-torsion-bert-checkpoint-base/...</code>) into the working directory where the <code>RNAPredictor</code> expects to find it. Similarly for the base DNABERT model (<code>zhihan1996/DNA_bert_3</code>).</li> </ul> </li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>The notebook likely manually constructs an OmegaConf <code>DictConfig</code> object instead of relying on Hydra's full CLI-based loading (<code>@hydra.main</code>). This config object would mirror the structure found in <code>rna_predict/conf/predict.yaml</code> and <code>rna_predict/conf/default.yaml</code>.</li> <li>Key Configuration Settings (inferred from context &amp; logs):<ul> <li><code>device</code>: Dynamically set to <code>\"cuda\"</code> if available, else <code>\"cpu\"</code>.</li> <li><code>model.stageB.torsion_bert.model_name_or_path</code>: Path to the local TorsionBERT model checkpoint (e.g., <code>\"/kaggle/working/rna_torsionBERT\"</code>).</li> <li><code>model.stageB.torsion_bert.angle_mode</code>: Set to <code>\"degrees\"</code> to ensure Stage B outputs angles in degrees.</li> <li><code>model.stageC.method</code>: <code>\"mp_nerf\"</code> (as confirmed by logs and <code>mp_nerf.md</code>).</li> <li><code>model.stageC.angle_representation</code>: <code>\"degrees\"</code> (to match Stage B's output).</li> <li><code>model.stageC.place_bases</code>: <code>True</code>.</li> <li><code>model.stageC.do_ring_closure</code>: <code>True</code> (from notebook context, though default in <code>stageC.yaml</code> is <code>False</code>).</li> <li><code>model.stageC.sugar_pucker</code>: <code>\"C3'-endo\"</code>.</li> <li><code>prediction.repeats</code>: <code>5</code> (Kaggle requires five distinct predictions).</li> <li><code>prediction.enable_stochastic_inference_for_submission</code>: <code>True</code> (This enables dropout in TorsionBERT during inference to generate diverse angle sets for the 5 repeats).</li> <li><code>prediction.residue_atom_choice</code>: This is crucial and likely misconfigured in the described notebook. The Kaggle competition requires C1' atom coordinates. If using the atom order from <code>rna_predict.utils.tensor_utils.types.STANDARD_RNA_ATOMS</code>, C1' is at index 11. The notebook log context mentions the toy example used residue atom choice 0 (likely 'P' atom). This needs correction for a valid Kaggle submission.</li> </ul> </li> </ul> </li> <li> <p>Instantiating the Predictor:</p> <ul> <li><code>predictor = RNAPredictor(cfg)</code> is called, likely using the <code>RNAPredictor</code> class from <code>rna_predict.predict.py</code> (or <code>rna_predict.interface.py</code>).</li> </ul> </li> <li> <p>Loading Test Data:</p> <ul> <li>Reads <code>test_sequences.csv</code> (e.g., from <code>/kaggle/input/stanford-rna-3d-folding/test_sequences.csv</code>) into a pandas DataFrame.</li> </ul> </li> <li> <p>Iterative Prediction and Submission Assembly:</p> <ul> <li>The notebook iterates through each sequence in <code>test_sequences.csv</code>.</li> <li>For each sequence:<ul> <li>It calls <code>predictor.predict_submission(sequence_string)</code>.</li> <li>Inside <code>RNAPredictor.predict_submission</code>:<ul> <li>A loop runs <code>prediction.repeats</code> (i.e., 5) times.</li> <li>In each iteration, <code>self.predict_3d_structure(sequence, stochastic_pass=True, seed=iteration_seed)</code> is called.<ul> <li>Stage B (<code>StageBTorsionBertPredictor</code>):<ul> <li>Takes the RNA sequence.</li> <li><code>stochastic_pass=True</code> sets the TorsionBERT model to <code>train()</code> mode, enabling dropout layers. Combined with a different <code>seed</code> for each of the 5 calls, this generates slightly varied torsion angle predictions.</li> <li>Outputs torsion angles. The log context (\"Values look like radians, converting to degrees\") and the configured <code>angle_mode=\"degrees\"</code> indicate that <code>StageBTorsionBertPredictor</code> handles the conversion to degrees.</li> </ul> </li> <li>Stage C (<code>run_stageC</code> using <code>run_stageC_rna_mpnerf</code>):<ul> <li>Takes the (varied) degree-based torsion angles from Stage B.</li> <li>Uses the MP-NeRF based RNA folding logic (from <code>rna_predict.pipeline.stageC.mp_nerf.rna</code>) and standard RNA geometry (from <code>final_kb_rna.py</code>) to reconstruct full 3D atomic coordinates.</li> <li>Returns <code>coords</code> (flattened coordinates for all valid atoms) and <code>atom_metadata</code> (atom names and residue indices per atom).</li> </ul> </li> </ul> </li> <li>After the 5 calls to <code>predict_3d_structure</code>, <code>predict_submission</code> has 5 sets of full atomic coordinates.</li> <li>It then uses <code>reshape_coords</code> and <code>extract_atom(coords, atom_idx=cfg.prediction.residue_atom_choice)</code> to select the coordinates of the C1' atom (index 11) for each residue from each of the 5 predicted structures.</li> <li>Finally, <code>coords_to_df</code> formats these 5 sets of C1' coordinates into a DataFrame with columns like <code>ID, resname, resid, x_1, y_1, z_1, ..., x_5, y_5, z_5</code>.</li> </ul> </li> <li>The DataFrames resulting from <code>predictor.predict_submission</code> for each test sequence are collected.</li> </ul> </li> </ul> </li> <li> <p>Finalizing and Saving <code>submission.csv</code>:</p> <ul> <li>All individual sequence DataFrames are concatenated.</li> <li>This combined DataFrame is then merged with <code>sample_submission.csv</code> (usually on the <code>ID</code> column) to ensure the row order and exact set of IDs match the competition's template.</li> <li>Any missing coordinate values (e.g., if a prediction failed for a residue) are typically filled with <code>0.0</code>.</li> <li>The resulting DataFrame is saved to <code>/kaggle/working/submission.csv</code>.</li> <li>A final sanity check (like Cell 16 in the notebook example) might be run to compare row counts and ID sets with the sample submission.</li> </ul> </li> </ol> <p>Key Code Components Involved:</p> <ul> <li><code>rna_predict.predict.RNAPredictor</code>: The main class orchestrating inference.</li> <li><code>rna_predict.pipeline.stageB.torsion.torsion_bert_predictor.StageBTorsionBertPredictor</code>: Handles TorsionBERT model loading and angle prediction, including stochastic inference.</li> <li><code>rna_predict.pipeline.stageC.stage_c_reconstruction.run_stageC</code> (and <code>run_stageC_rna_mpnerf</code>): Performs 3D reconstruction from angles using MP-NeRF logic.<ul> <li>Relies on: <code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_fold</code>, <code>rna_predict.pipeline.stageC.mp_nerf.rna.rna_base_placement</code>, <code>rna_predict.pipeline.stageC.mp_nerf.final_kb_rna.py</code>.</li> </ul> </li> <li><code>rna_predict.utils.submission</code> (<code>reshape_coords</code>, <code>extract_atom</code>, <code>coords_to_df</code>): Utilities for processing coordinates and formatting the submission DataFrame.</li> <li><code>rna_predict.utils.tensor_utils.types.STANDARD_RNA_ATOMS</code>: Defines the atom order, crucial for <code>residue_atom_choice</code>.</li> <li>Configuration files in <code>rna_predict/conf/</code>: Define default parameters, overridden by the notebook.</li> </ul> <p>Plan for Refactoring into a Simpler Kaggle Wrapper:</p> <p>The goal is a clean, straightforward Kaggle notebook that uses the <code>rna-predict</code> package effectively.</p> <ol> <li> <p>Environment Setup Cell:</p> <ul> <li>Keep the offline package installation logic (<code>%%bash</code> cell similar to Cell 3 of the notebook example).</li> <li>Keep symbolic link creation for models (part of Cell 12).</li> <li>Keep offline mode settings for HuggingFace (part of Cell 12).</li> </ul> </li> <li> <p>Configuration Cell:</p> <ul> <li>Replace manual OmegaConf creation. Instead, use a utility from <code>rna_predict.conf.utils</code> (e.g., <code>get_config</code> and <code>update_config</code> if they exist, or create a simpler helper).</li> <li>Load a base configuration (e.g., <code>predict.yaml</code>).</li> <li>Programmatically override essential Kaggle-specific parameters:<ul> <li><code>input_csv</code>: Path to <code>test_sequences.csv</code>.</li> <li><code>output_dir</code>: <code>/kaggle/working/</code>.</li> <li><code>model.stageB.torsion_bert.model_name_or_path</code>: Path to the model in <code>/kaggle/input/...</code>.</li> <li><code>device</code>: Auto-detected (<code>cuda</code> or <code>cpu</code>).</li> <li><code>prediction.residue_atom_choice = 11</code>: This is the most critical fix to target C1' atoms.</li> <li><code>prediction.enable_stochastic_inference_for_submission = True</code>.</li> <li><code>prediction.repeats = 5</code>.</li> <li>Set <code>debug_logging</code> flags in various modules to <code>False</code> for cleaner Kaggle output.</li> </ul> </li> </ul> </li> <li> <p>Main Prediction Cell:</p> <ul> <li>Import <code>RNAPredictor</code> from <code>rna_predict.predict</code>.</li> <li>Import <code>pandas</code>.</li> <li>Load <code>test_sequences.csv</code>.</li> <li>Instantiate <code>predictor = RNAPredictor(cfg)</code>.</li> <li>Create an empty list <code>all_sequence_predictions = []</code>.</li> <li>Loop through each row in <code>test_sequences_df</code>:<ul> <li>Get <code>target_id</code> and <code>sequence_string</code>.</li> <li>Call <code>df_one_sequence = predictor.predict_submission(sequence_string)</code>.<ul> <li>This call internally handles the 5 stochastic repeats and C1' extraction.</li> <li>It should return a DataFrame already formatted by <code>coords_to_df</code> with columns <code>ID, resname, resid, x_1, y_1, z_1, ..., x_5, y_5, z_5</code>.</li> <li>Crucially, ensure the <code>ID</code> column in <code>df_one_sequence</code> is correctly formatted as <code>target_id + \"_\" + residue_index</code>. The <code>coords_to_df</code> function currently creates an <code>ID</code> column that is just the 1-based residue index. This needs to be prefixed with <code>target_id + \"_\"</code> after <code>coords_to_df</code> is called, likely within <code>RNAPredictor.predict_submission</code> or immediately after.</li> </ul> </li> <li>Append <code>df_one_sequence</code> to <code>all_sequence_predictions</code>.</li> </ul> </li> <li>Concatenate all DataFrames: <code>final_predictions_df = pd.concat(all_sequence_predictions, ignore_index=True)</code>.</li> <li>Load <code>sample_submission.csv</code>.</li> <li>Merge to align order and ensure completeness:     <pre><code>sample_df = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\")\nsubmission_df = pd.merge(sample_df[['ID']], final_predictions_df, on='ID', how='left')\n# Fill NaN coordinates that might arise from failed predictions for some residues/targets\ncoord_cols = [col for col in submission_df.columns if col.startswith(('x_', 'y_', 'z_'))]\nsubmission_df[coord_cols] = submission_df[coord_cols].fillna(0.0)\n# Ensure all columns from sample_submission are present\nfor col in sample_df.columns:\n    if col not in submission_df.columns:\n         submission_df[col] = np.nan # Or appropriate default\nsubmission_df = submission_df[sample_df.columns] # Enforce column order\n</code></pre></li> <li>Save: <code>submission_df.to_csv(\"submission.csv\", index=False)</code>.</li> <li>Print success message and head of the submission file.</li> </ul> </li> <li> <p>Remove Unnecessary Patches/Cells:</p> <ul> <li>The custom <code>_predict_submission_patched</code> (Cell 13) and <code>_collapse_to_one_row_per_residue</code> (Cell 14) from the notebook example become redundant if <code>RNAPredictor</code> is correctly configured and its <code>predict_submission</code> method robustly handles C1' extraction and formatting. The original <code>RNAPredictor.predict_submission</code> in <code>rna_predict.predict</code> should be the source of truth, modified if necessary to correctly format the <code>ID</code> column.</li> </ul> </li> <li> <p>Optional Training/Evaluation Module:</p> <ul> <li>For a pure inference wrapper, the training/evaluation cells (Cells 5-9 in the notebook example, which seem to be for a different XGBoost model unrelated to the RNA_PREDICT pipeline) would be removed or commented out. If actual RNA_PREDICT model training is desired on Kaggle, it would be a separate, more involved notebook using <code>rna_predict.training.train.py</code> as a basis.</li> </ul> </li> </ol> <p>Benefits of this Refactoring:</p> <ul> <li>Simplicity: The notebook becomes much cleaner, focusing on setup, configuration, and invoking the main prediction logic.</li> <li>Maintainability: Core logic resides in the <code>rna-predict</code> Python package, making updates easier.</li> <li>Correctness: Directly addresses the C1' atom requirement and leverages the existing stochastic prediction mechanism.</li> <li>Modularity: Follows the existing plan to separate inference, (optional) training, and submission generation.</li> </ul>"},{"location":"pipeline/kaggle_info/kaggle_inference_notebook_refactor_plan/#this-plan-ensures-that-the-kaggle-notebook-acts-as-a-lightweight-correctly-configured-interface-to-the-more-complex-rna-predict-library-producing-a-valid-submission-file-the-critical-fix-is-setting-predictionresidue_atom_choice-11-and-ensuring-the-id-column-in-the-output-of-predict_submission-is-correctly-formatted-for-merging-with-sample_submissioncsv","title":"This plan ensures that the Kaggle notebook acts as a lightweight, correctly configured interface to the more complex <code>rna-predict</code> library, producing a valid submission file. The critical fix is setting <code>prediction.residue_atom_choice = 11</code> and ensuring the <code>ID</code> column in the output of <code>predict_submission</code> is correctly formatted for merging with <code>sample_submission.csv</code>.","text":""},{"location":"pipeline/kaggle_info/kaggle_inference_notebook_refactor_plan/#metadatakernelspeclanguagepythondisplay_namepython-3namepython3language_infonamepythonversion31012mimetypetextx-pythoncodemirror_modenameipythonversion3pygments_lexeripython3nbconvert_exporterpythonfile_extensionpykaggleacceleratornonedatasourcessourceid87793databundleversionid12276181sourcetypecompetitionsourceid5942070sourcetypedatasetversiondatasetid3410079sourceid11026565sourcetypedatasetversiondatasetid6866703sourceid11762635sourcetypedatasetversiondatasetid7384470sourceid11787394sourcetypedatasetversiondatasetid7401070sourceid11787916sourcetypedatasetversiondatasetid7401477sourceid11788332sourcetypedatasetversiondatasetid7401777sourceid11788337sourcetypedatasetversiondatasetid7401782sourceid11788349sourcetypedatasetversiondatasetid7401791sourceid11788362sourcetypedatasetversiondatasetid7401801sourceid11788374sourcetypedatasetversiondatasetid7401810sourceid11788377sourcetypedatasetversiondatasetid7401813sourceid11788382sourcetypedatasetversiondatasetid7401817sourceid11788386sourcetypedatasetversiondatasetid7401820sourceid11788388sourcetypedatasetversiondatasetid7401822sourceid11788390sourcetypedatasetversiondatasetid7401824sourceid11788491sourcetypedatasetversiondatasetid7401888sourceid11788496sourcetypedatasetversiondatasetid7401891sourceid11788500sourcetypedatasetversiondatasetid7401893sourceid11788503sourcetypedatasetversiondatasetid7401896sourceid11788505sourcetypedatasetversiondatasetid7401898sourceid11788513sourcetypedatasetversiondatasetid7401905sourceid11788517sourcetypedatasetversiondatasetid7401907sourceid11788524sourcetypedatasetversiondatasetid7401913sourceid11788527sourcetypedatasetversiondatasetid7401915sourceid11788542sourcetypedatasetversiondatasetid7401925sourceid11788545sourcetypedatasetversiondatasetid7401927sourceid11788551sourcetypedatasetversiondatasetid7401929sourceid11788558sourcetypedatasetversiondatasetid7401931sourceid11788622sourcetypedatasetversiondatasetid7401980sourceid11788630sourcetypedatasetversiondatasetid7401890sourceid11788641sourcetypedatasetversiondatasetid7401995sourceid11788656sourcetypedatasetversiondatasetid7401827sourceid11814119sourcetypedatasetversiondatasetid7420372sourceid11814128sourcetypedatasetversiondatasetid7420378sourceid11814137sourcetypedatasetversiondatasetid7401990sourceid11814142sourcetypedatasetversiondatasetid7420389sourceid11814146sourcetypedatasetversiondatasetid7420392sourceid11814150sourcetypedatasetversiondatasetid7420395sourceid11814170sourcetypedatasetversiondatasetid7420409sourceid11814175sourcetypedatasetversiondatasetid7420413sourceid11814180sourcetypedatasetversiondatasetid7420416sourceid11814187sourcetypedatasetversiondatasetid7420420sourceid11814259sourcetypedatasetversiondatasetid7420472sourceid11814267sourcetypedatasetversiondatasetid7420479sourceid11814282sourcetypedatasetversiondatasetid7420492sourceid11814553sourcetypedatasetversiondatasetid7420638sourceid11831012sourcetypedatasetversiondatasetid6866398dockerimageversionid30918isinternetenabledfalselanguagepythonsourcetypenotebookisgpuenabledfalsenbformat_minor4nbformat4cellscell_typecodesource-cell-clean-auto-generated-requirements-file-run-firstn-nimport-pathlib-shutil-re-textwrap-sys-osnnreq_path-pathlibpathkagglerequirementsinput_requirementstxtnif-req_pathis_filen-cleaned_lines-n-for-line-in-req_pathread_textsplitlinesn-line-linestripn-if-not-line-or-linestartswithn-continue-drop-blanks-commentsn-if-not-linestartswithpip-installn-keep-it-but-comment-it-out-so-the-helper-ignores-itn-line-f-linen-cleaned_linesappendlinenn-req_pathwrite_textnjoincleaned_lines-n-if-cleaned_lines-else-nn-printfinfo-requirements-cleaned-lencleaned_lines-valid-n-fpip-install-lines-keptnelsen-printfinfo-req_path-not-found-nothing-to-cleanmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190229551618ziopubexecute_input2025-05-16t190229552081ziopubstatusidle2025-05-16t190229560720zshellexecute_replystarted2025-05-16t190229552014zshellexecute_reply2025-05-16t190229559195zoutputsexecution_countnullcell_typecodesourcebashn-cell-show-whats-inside-every-mounted-kaggle-dataset-n-necho-e-n-listing-the-first-two-levels-of-kaggleinput-nnn-change-depth-maxdepth-if-you-want-more-or-fewer-levelsnfind-kaggleinput-maxdepth-2-mindepth-1-print-sed-s-nnecho-e-n-donenmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190229562448ziopubexecute_input2025-05-16t190229562808ziopubstatusidle2025-05-16t190229661765zshellexecute_replystarted2025-05-16t190229562770zshellexecute_reply2025-05-16t190229660349zoutputsexecution_countnullcell_typecodesourcebashn-cell-offline-installs-that-match-the-current-wheel-set-lean-versionn-nset-euo-pipefailnn-let-pip-look-inside-every-sub-folder-of-kaggleinput-nwheel_rootkaggleinputnfind_links_argsnfor-d-in-wheel_root-wheel_root-don-find_links_args-find-links-dndonennp-quiet-install-warn-dont-die-if-something-failsn-shellcheck-disablesc2086n-pip-install-no-index-find_links_args-quiet-n-echo-warn-install-failed-skipped-nnn-n-1-core-scientific-stackn-np-numpy1243np-pandas223np-scipy1101np-tqdm4671np-seaborn0122np-biopython185np-torch-pre-installed-in-the-kaggle-imagenn-n-2-ml-nlp-stackn-np-huggingface_hub0311-needs-hf-xet-you-already-uploadednp-transformers4513np-pytorch_lightning250post0-gives-us-lightning-core-featuresnn-n-3-extra-deps-rna_predict-really-importsn-np-lightning-utilities0112-comes-with-pl-wheel-but-list-explicitlynp-datasets360np-einops081np-hypothesis613115np-black2510-needs-pathspec-0121-you-uploaded-bothnp-pathspec0121np-isort601np-ruff0119np-mss1000np-mdanalysis290np-mmtf-python113np-griddataformats102np-mrcfile154np-lxml540np-dearpygui200np-py-cpuinfo900np-pillow-pillow-11-2-1-wheel-presentnp-exit-codes130-small-helper-used-by-hf-hub-031nn-n-4-config-utilitiesn-np-hydra-core132np-omegaconf230np-ml_collections110-required-by-protenixnn-n-5-rna-predict-itself-no-deps-so-nothing-reaches-pypin-npip-install-no-index-no-deps-quiet-n-kaggleinputrna-structure-predictrna_predict-203-py3-none-anywhlnn-n-6-protenix-046-wheel-but-ignore-its-heavy-deps-like-rdkitn-npip-install-no-index-no-deps-quiet-n-kaggleinputprotenix-0-4-6protenix-046-py3-none-anywhl-n-echo-warn-protenix-wheel-install-failednn-n-7-runtime-shim-make-import-lightning-point-to-pytorch_lightningn-npython-pynimport-sys-importlib-typesntryn-import-pytorch_lightning-as-pln-sysmodulessetdefaultlightning-plnexcept-importerrorn-printwarn-pytorch_lightning-missing-shim-not-creatednpynnecho-offline-wheel-install-phase-completemetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190229665786ziopubexecute_input2025-05-16t190229666432ziopubstatusidle2025-05-16t190508009084zshellexecute_replystarted2025-05-16t190229666376zshellexecute_reply2025-05-16t190508008145zoutputsexecution_countnullcell_typecodesource-n-cell-all-in-one-environment-setup-no-uninstalls-no-online-pipn-nnimport-sys-subprocess-shutil-os-platformnndef-run_and_printcmdn-res-subprocessruncmd-capture_outputtrue-texttruen-printresstdout-endn-if-resstderrn-printresstderr-endnn-1-system-information-unchanged-nprintn-system-information-nnprintnpython-versionnprintsysversionnnprintnkernel-and-os-informationnrun_and_printuname-annprintncpu-informationnrun_and_printlscpunnprintnmemory-informationnrun_and_printfree-mhnnprintndisk-informationnrun_and_printlsblknnprintn-end-of-system-information-nnn-2-user-config-nrna_predict_version-203nblock_sparse_wheel_in-n-kaggleinputblock-sparse-wheelsn-block_sparse_attn-001cu118torch20cxx11abitrue-n-cp310-cp310-linux_x86_64whlnn-pep-440-compliant-rename-torch-version-tag-trimmednblock_sparse_wheel_out-n-kaggleworkingn-block_sparse_attn-001cu118torch20-n-cp310-cp310-linux_x86_64whlnnn-3-environment-fix-helper-ndef-setup_environmentn-n-1-ensure-seaborn-and-its-deps-is-presentn-2-copy-install-block_sparse_attn-wheel-optional-see-note-belown-3-install-rna_predictn-4-install-hydra-core-from-local-wheeln-5-show-final-versions-of-key-packagesn-n-1-make-sure-seaborn-is-available-n-kaggle-base-image-already-has-0122-but-we-pin-it-explicitlyn-printinfo-making-sure-seaborn-is-installednn-run_and_printpip-install-quiet-seaborn0122nn-2-copy-optionally-install-block_sparse_attnn-if-ospathexistsblock_sparse_wheel_inn-tryn-shutilcopyfileblock_sparse_wheel_in-block_sparse_wheel_outn-printninfo-copied-block-sparse-attn-wheel-to-working-dirn-printinfo-installing-block-sparse-attn-no-depsnn-run_and_printpip-install-no-deps-quiet-block_sparse_wheel_outn-except-exception-as-en-printfwarn-could-not-copyinstall-block-sparse-wheel-en-print-continue-without-it-if-your-code-doesnt-need-itn-elsen-printwarn-block-sparse-attn-wheel-not-found-in-kaggleinput-skippednn-3-install-rna_predict-pure-py-so-no-deps-is-finen-rnapred_whl-fkaggleinputrna-structure-predict-n-frna_predict-rna_predict_version-py3-none-anywhln-if-ospathexistsrnapred_whln-printfninfo-installing-rna_predict-rna_predict_version-nn-run_and_printpip-install-no-deps-quiet-rnapred_whln-elsen-printfwarn-rnapred_whl-not-found-skippednn-4-install-hydra-core-from-local-wheeln-hydra_dir-kaggleinputhydra-core-132whln-if-ospathisdirhydra_dirn-wheels-f-for-f-in-oslistdirhydra_dir-if-fendswithwhln-if-wheelsn-for-whl-in-wheelsn-whl_path-ospathjoinhydra_dir-whln-printfninfo-installing-hydra-core-from-whl_path-nn-run_and_printpip-install-no-deps-quiet-whl_pathn-elsen-printfwarn-no-whl-files-found-in-hydra_dir-skippedn-elsen-printfwarn-hydra_dir-not-found-skippednn-5-show-final-versionsn-printn-final-package-versions-n-for-pkg-in-n-torch-block-sparse-attn-rna-predictn-hydra-core-numpy-scipy-scikit-learn-seabornn-n-run_and_printpip-show-pkgn-print-end-of-final-package-versions-nnn-4-run-it-nsetup_environmentmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190508010711ziopubexecute_input2025-05-16t190508011020ziopubstatusidle2025-05-16t190551408073zshellexecute_replystarted2025-05-16t190508010992zshellexecute_reply2025-05-16t190551406853zoutputsexecution_countnullcell_typecodesource-coding-utf-8-nncell-1-environment-setup-loggingn-nnimport-osnimport-sysnimport-loggingnimport-numpy-as-npnimport-pandas-as-pdnimport-seaborn-as-snsnimport-matplotlibpyplot-as-pltnn-machine-learning-librariesnfrom-sklearnmodel_selection-import-train_test_split-kfoldnfrom-sklearnimpute-import-simpleimputernfrom-sklearnmetrics-import-mean_squared_errornfrom-xgboost-import-xgbregressornnn-loggingnloggingbasicconfign-levellogginginfon-formatasctimes-levelnames-messagesn-handlersloggingstreamhandlersysstdoutnnlogginginfocell-1-complete-libraries-imported-and-logging-initializednmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190551409308ziopubexecute_input2025-05-16t190551409608ziopubstatusidle2025-05-16t190551416335zshellexecute_replystarted2025-05-16t190551409579zshellexecute_reply2025-05-16t190551414995zoutputsexecution_countnullcell_typecodesourcencell-2-data-importn-nhere-we-read-in-trainvalidationtest-csvs-and-a-sample-submission-from-the-kaggle-environmentnadjust-the-paths-if-needed-for-your-environmentnnn-example-file-pathsntrain_sequences_path-kaggleinputstanford-rna-3d-foldingtrain_sequencescsvntrain_labels_path-kaggleinputstanford-rna-3d-foldingtrain_labelscsvnvalid_sequences_path-kaggleinputstanford-rna-3d-foldingvalidation_sequencescsvnvalid_labels_path-kaggleinputstanford-rna-3d-foldingvalidation_labelscsvntest_sequences_path-kaggleinputstanford-rna-3d-foldingtest_sequencescsvnsample_sub_path-kaggleinputstanford-rna-3d-foldingsample_submissioncsvnntryn-train_sequences-pdread_csvtrain_sequences_pathn-train_labels-pdread_csvtrain_labels_pathn-validation_sequences-pdread_csvvalid_sequences_pathn-validation_labels-pdread_csvvalid_labels_pathn-test_sequences-pdread_csvtest_sequences_pathn-sample_submission-pdread_csvsample_sub_pathnn-logginginfocell-2-complete-data-loaded-successfullynexcept-exception-as-en-loggingerrorferror-loading-data-en-sysexit1nnlogginginfoftrain_sequences-train_sequencesshape-train_labels-train_labelsshapenlogginginfofvalidation_sequences-validation_sequencesshape-validation_labels-validation_labelsshapenlogginginfoftest_sequences-test_sequencesshape-sample_submission-sample_submissionshapemetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190551417642ziopubexecute_input2025-05-16t190551418020ziopubstatusidle2025-05-16t190551764659zshellexecute_replystarted2025-05-16t190551417969zshellexecute_reply2025-05-16t190551763287zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-3-combine-train-validation-basic-edan-nwe-concatenate-the-train-and-validation-sets-to-maximize-data-nthen-we-do-a-quick-eda-check-on-shapes-missingness-etcnnn-combine-sequences-and-labelsntrainval_sequences-pdconcattrain_sequences-validation_sequences-ignore_indextruentrainval_labels-pdconcattrain_labels-validation_labels-ignore_indextruennlogginginfofcombined-trainvalidation-sequences-trainval_sequencesshape-labels-trainval_labelsshapenn-quick-check-for-missingnlogginginfomissing-in-combined-sequencesn-strtrainval_sequencesisnullsumnlogginginfomissing-in-combined-labelsn-strtrainval_labelsisnullsumnn-example-eda-sequence-length-distributionntrainval_sequencessequence_length-trainval_sequencessequencestrlennnpltfigurefigsize104nsnsboxplotxtrainval_sequencessequence_length-colorskybluenplttitleboxplot-of-sequence-length-train-validationnpltxlabelsequence-lengthnpltshownnlogginginfocell-3-complete-basic-eda-finishedmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190551766234ziopubexecute_input2025-05-16t190551766825ziopubstatusidle2025-05-16t190552188571zshellexecute_replystarted2025-05-16t190551766785zshellexecute_reply2025-05-16t190552187427zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-4-handle-missing-coordinates-mergen-nwe-replace-1e18-with-npnan-then-merge-sequences-with-labels-on-target_idnnn-replace-1e18-with-npnan-in-the-labelsnfor-col-in-x_1y_1z_1n-trainval_labelscol-trainval_labelscolreplace-1e18-npnannnlogginginforeplaced-1e18-with-nan-in-trainval_labels-for-x_1-y_1-z_1nn-extract-pdb_id-chain_id-from-idntrainval_labelspdb_id-trainval_labelsidapplylambda-x-xsplit0ntrainval_labelschain_id-trainval_labelsidapplylambda-x-xsplit1ntrainval_labelstarget_id-trainval_labelspdb_id-trainval_labelschain_idnn-mergentrain_data-pdmergetrainval_labels-trainval_sequences-ontarget_id-howleftnlogginginfofmerged-train_data-shape-train_datashapenn-quick-checknlogginginfofmissing-in-x_1-train_datax_1isnullsum-n-fy_1-train_datay_1isnullsum-n-fz_1-train_dataz_1isnullsumnnlogginginfocell-4-complete-merged-train_data-ready-for-group-based-imputationmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190552191954ziopubexecute_input2025-05-16t190552192298ziopubstatusidle2025-05-16t190552663563zshellexecute_replystarted2025-05-16t190552192266zshellexecute_reply2025-05-16t190552662206zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-5-feature-engineeringn-ncreate-numericalcategorical-features-from-the-sequencenwell-keep-resname-from-the-labels-as-a-valuable-featurennndef-engineer_featuresdfn-n-create-numerical-some-categorical-features-from-raw-rna-sequence-datan-n-df-dfcopyn-sequence-basedn-dfseq_length-dfsequencestrlenn-dfa_cnt-dfsequencestrcountan-dfc_cnt-dfsequencestrcountcn-dfg_cnt-dfsequencestrcountgn-dfu_cnt-dfsequencestrcountun-dfbegin_seq-dfsequencestr0n-dfend_seq-dfsequencestr-1n-n-di-nucleotide-counts-example-setn-for-pair-in-acagaucacgcugagcguuaucugn-aaccgguun-dffpair_cnt-dfsequencestrcountpairnn-return-dfnn-apply-feature-engineeringntrain_data-engineer_featurestrain_datannlogginginfofeature-engineering-applied-to-merged-train_datann-well-show-an-example-of-newly-added-columnsnexample_cols-seq_lengtha_cntc_cntg_cntu_cntbegin_seqend_seqac_cntaa_cntnlogginginfofcolumns-after-fe-samplentrain_dataexample_colshead3nnlogginginfocell-5-complete-feature-engineering-donemetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190552665836ziopubexecute_input2025-05-16t190552666288ziopubstatusidle2025-05-16t190635358204zshellexecute_replystarted2025-05-16t190552666252zshellexecute_reply2025-05-16t190635356006zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-6-group-based-imputationn-nwe-impute-missing-x_1-y_1-z_1-within-each-target_id-resname-groupnfinally-if-any-nas-remain-we-fill-them-with-a-global-median-or-drop-themnnn-perform-group-based-fill-for-x_1-y_1-z_1ntrain_datax_1y_1z_1-n-train_datan-groupbytarget_idresnamex_1y_1z_1n-applylambda-grp-grpfillnagrpmeann-reset_indexleveltarget_idresname-droptruennn-in-case-any-remain-after-group-based-mean-fill-eg-group-is-all-nan-do-a-global-fillnnum_cols-x_1y_1z_1nglobal_imputer-simpleimputerstrategymedianntrain_datanum_cols-global_imputerfit_transformtrain_datanum_colsnn-if-youd-prefer-to-drop-any-leftover-nas-insteadn-train_datadropnasubsetx_1y_1z_1-inplacetruennlogginginfogroup-based-imputation-global-median-fallback-completenn-confirm-missing-valuesnlogginginfofremaining-missing-x_1-train_datax_1isnasum-n-fy_1-train_datay_1isnasum-z_1-train_dataz_1isnasumnnlogginginfocell-6-complete-group-based-imputation-finishedmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190635359808ziopubexecute_input2025-05-16t190635360289ziopubstatusidle2025-05-16t190646697400zshellexecute_replystarted2025-05-16t190635360252zshellexecute_reply2025-05-16t190646695816zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-7-prepare-data-for-modelingn-nwell-define-the-columns-we-wont-use-set-up-x-and-y-for-x_1-y_1-z_1-nand-one-hot-encode-any-relevant-categorical-columns-including-resnamennn-unused-columnsnunused_cols-n-idpdb_idchain_idresidn-x_1y_1z_1n-sequencedescriptiontemporal_cutoffall_sequencesn-target_id-key-used-for-mergesnnn-well-keep-resname-begin_seq-end_seq-as-features-this-timenfeature_cols-col-for-col-in-train_datacolumns-if-col-not-in-unused_colsnn-make-a-copyntrain_df-train_datacopynn-convert-to-categoriesnfor-cat_col-in-resnamebegin_seqend_seqn-if-cat_col-in-feature_colsn-train_dfcat_col-train_dfcat_colastypecategorynn-one-hot-encodentrain_df-pdget_dummiestrain_df-columnsresnamebegin_seqend_seq-drop_firsttruenn-our-final-set-of-featuresnx_cols-col-for-col-in-train_dfcolumns-if-col-not-in-unused_colsnnx_full-train_dfx_colsny_x_full-train_dfx_1ny_y_full-train_dfy_1ny_z_full-train_dfz_1nnlogginginfoffeature-matrix-shape-x_fullshapenlogginginfocell-7-complete-prepared-data-for-modelingmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190646698754ziopubexecute_input2025-05-16t190646699321ziopubstatusidle2025-05-16t190647244815zshellexecute_replystarted2025-05-16t190646699153zshellexecute_reply2025-05-16t190647243256zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-8-kfold-cv-for-x-y-z-hyperparam-searchn-nwell-do-a-simplified-kfold-cross-validation-for-each-coordinate-nto-get-a-sense-of-good-hyperparams-then-train-final-modelsnnnfrom-sklearnmodel_selection-import-kfold-randomizedsearchcvnimport-numpy-as-npnn-example-hyperparameter-grid-you-can-expand-as-needednparam_dist-n-learning_rate-003-005-01n-max_depth-6-10-15n-n_estimators-500-800-1000n-subsample-07-09-10n-colsample_bytree-07-09-10nnndef-run_random_searchx-y-param_dist-n_iter5-cv_splits3n-simple-randomizedsearchcv-for-an-xgbregressor-using-gpu-in-xgboost-20n-xgb-xgbregressortree_methodhist-devicecuda-random_state42n-rsearch-randomizedsearchcvn-estimatorxgbn-param_distributionsparam_distn-n_itern_itern-scoringneg_mean_squared_errorn-cvcv_splitsn-verbose1n-random_state42n-n-rsearchfitx-yn-best_model-rsearchbest_estimatorn-logginginfofbest-params-rsearchbest_params_-best-cv-score-rsearchbest_score_n-return-best_model-rsearchbest_params_nnlogginginfostarting-hyperparam-search-for-x-coordinatenbest_model_x-best_params_x-run_random_searchx_full-y_x_full-param_dist-n_iter5-cv_splits3nnlogginginfostarting-hyperparam-search-for-y-coordinatenbest_model_y-best_params_y-run_random_searchx_full-y_y_full-param_dist-n_iter5-cv_splits3nnlogginginfostarting-hyperparam-search-for-z-coordinatenbest_model_z-best_params_z-run_random_searchx_full-y_z_full-param_dist-n_iter5-cv_splits3nnlogginginfocell-8-complete-randomizedsearchcv-best-params-foundmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190647246482ziopubexecute_input2025-05-16t190647246859ziopubstatusidle2025-05-16t190647256247zshellexecute_replystarted2025-05-16t190647246826zshellexecute_reply2025-05-16t190647254737zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcencell-9-final-training-on-full-datan-nuse-the-best-hyperparams-for-each-coordinate-found-in-cv-nretrain-each-coordinate-model-on-all-data-x_full-y__fullnnndef-get_best_xgbparamsn-return-an-xgbregressor-with-the-given-params-using-gpu-n-here-we-override-or-add-tree_method-to-ensure-gpu-usagen-we-can-also-specify-predictorgpu_predictor-to-accelerate-inference-on-gpun-model-xgbregressorn-paramsn-tree_methodhist-or-tree_methodparamsgettree_method-histn-devicecuda-ensures-gpu-usagen-random_state42n-n-return-modelnnlogginginforetraining-final-model-for-x-coordinatenmodel_x-get_best_xgbbest_params_xnmodel_xfitx_full-y_x_fullnnlogginginforetraining-final-model-for-y-coordinatenmodel_y-get_best_xgbbest_params_ynmodel_yfitx_full-y_y_fullnnlogginginforetraining-final-model-for-z-coordinatenmodel_z-get_best_xgbbest_params_znmodel_zfitx_full-y_z_fullnnlogginginfocell-9-complete-final-models-trainedmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190647257410ziopubexecute_input2025-05-16t190647257725ziopubstatusidle2025-05-16t190647291639zshellexecute_replystarted2025-05-16t190647257695zshellexecute_reply2025-05-16t190647289896zoutputsexecution_countnullcell_typecodesourcencell-10-prepare-engineer-test-datan-n-expand-test_sequences-into-id-resname-residn-merge-residuelevel-grid-with-persequence-engineered-featuresn-align-with-training-feature-matrix-x_full-fill-missing-valuesnnn-1-expand-residue-grid-ntest_expanded-n-rowtarget_id-nt-in-for-_-row-in-test_sequencesiterrowsn-for-i-nt-in-enumeraterowsequence-start1nntest_clean_df-pddataframetest_expanded-columnsid-resname-residnlogginginfoftest_clean_df-shape-test_clean_dfshape-expanded-test-sequencesnn-2-persequence-engineered-features-ntest_feats-engineer_featurestest_sequencesnn-merge-one-row-per-residue-sequencelevel-features-broadcast-to-each-residuentest_merged-pdmergen-test_clean_dfn-test_featsdropcolumnsseq_length-drop-if-not-neededn-left_onidn-right_ontarget_idn-howleftnnlogginginfoftest_merged-shape-after-merging-test_mergedshapenn-3-clean-up-n-replace-sentinel-valuesnfor-col-in-x_1-y_1-z_1n-if-col-in-test_mergedcolumnsn-test_mergedcol-test_mergedcolreplace-1e18-npnannn-drop-columns-not-used-by-the-modelndrop_cols-sequence-description-temporal_cutoff-all_sequences-target_idntest_mergeddropcolumnsc-for-c-in-drop_cols-if-c-in-test_mergedcolumns-inplacetrue-errorsignorenn-4-categorical-handling-ncat_cols-resname-begin_seq-end_seq-settest_mergedcolumnsnfor-col-in-cat_colsn-test_mergedcol-test_mergedcolastypecategoryntest_merged-pdget_dummiestest_merged-columnslistcat_cols-drop_firsttruenn-5-column-alignment-n-single-vectorised-reindex-instead-of-percolumn-insertion-no-fragmentation-warningntest_merged-test_mergedreindexcolumnsx_fullcolumns-fill_value0nn-6-missingvalue-imputation-n-fit-a-new-median-imputer-on-the-training-feature-matrix-numeric-cols-onlynnumeric_cols-x_fullselect_dtypesincludenpnumbercolumnsnfeature_imputer-simpleimputerstrategymediannfeature_imputerfitx_fullnumeric_colsnntest_mergednumeric_cols-feature_imputertransformtest_mergednumeric_colsnn-7-all-done-ntest_merged_imputed-test_mergedcopynlogginginfocell-10-complete-test-data-prepared-aligned-and-imputedmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190647293225ziopubexecute_input2025-05-16t190647293726ziopubstatusidle2025-05-16t190649855775zshellexecute_replystarted2025-05-16t190647293684zshellexecute_reply2025-05-16t190649854293zjupytersource_hiddentrueoutputsexecution_countnullcell_typecodesourcehf_datasets_offline1-transformers_offline1-hf_homekaggleworking-transformers_cachekaggleworking-ln-sf-kaggleinputrna-torsion-bert-checkpoint-basekaggleworkingrna_torsionbert-kaggleworkingrna_torsionbertmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190649857717ziopubexecute_input2025-05-16t190649858361ziopubstatusidle2025-05-16t190650048166zshellexecute_replystarted2025-05-16t190649858304zshellexecute_reply2025-05-16t190650046315zoutputsexecution_countnullcell_typecodesource-cell-rna-prediction-with-torsionbert-offline-readyn-nimport-pandas-as-pd-torch-os-logging-sys-transformersnfrom-omegaconf-import-omegaconfnfrom-functools-import-partialnn-n-1-link-local-checkpoints-rna_torsionbert-dna_bert_3n-nif-not-ospathexistskaggleworkingrna_torsionbertn-ossymlinkn-kaggleinputrna-torsion-bert-checkpoint-basekaggleworkingrna_torsionbertn-kaggleworkingrna_torsionbertn-nndna_bert_src-kaggleinputdna-bert-rnadna_bert_3ndna_bert_dst-kaggleworkingzhihan1996dna_bert_3-path-hard-coded-in-torsionbertnif-not-ospathexistsdna_bert_dstn-osmakedirskaggleworkingzhihan1996-exist_oktruen-ossymlinkdna_bert_src-dna_bert_dstnn-n-2-force-offline-mode-map-zhihan1996-ids-local-foldersn-nosenvironupdaten-hf_hub_offline-1n-hf_datasets_offline-1n-transformers_offline1n-hf_home-kaggleworkingn-transformers_cache-kaggleworkingnndef-localizerepoakwn-n-redirect-zhihan1996dna_bert_-to-local-paths-andn-force-local_files_only-for-every-hf-loadn-n-if-isinstancerepostr-and-repostartswithzhihan1996dna_bertn-repo-kaggleworking-repon-kwlocal_files_only-truen-return-repoakwn-robust-monkey-patch-handles-partials-repeated-patching-etcnfor-cls-in-autoconfigautotokenizerautomodeln-obj-getattrtransformers-_clsn-base_cls-objfunc-if-isinstanceobj-partial-else-objn-if-not-hasattrbase_cls-from_pretrainedn-continuen-_orig-base_clsfrom_pretrainedn-def-_wraprepoa__o_origkwn-repoakw-_localizerepoakwn-return-__orepoakwn-base_clsfrom_pretrained-_wrapnn-accept-dna-bert-3-custom-configntryn-from-importlib-import-import_modulen-custom_conf-import_modulen-transformers_modulesdna_bert_3configuration_bertn-bertconfign-transformersmodelsbertmodeling_bertbertmodelconfig_class-custom_confnexcept-exception-as-en-loggingwarningfwarn-dna_bert_3-config-patch-skipped-enn-n-3-logging-tiny-shell-helpern-nloggingbasicconfiglevellogginginfon-formatasctimes-levelnames-messagesndef-run_and_printcmdn-import-subprocess-shlex-textwrapn-res-subprocessruncmd-if-isinstancecmdlist-else-shlexsplitcmdn-capture_outputtrue-texttruen-if-resstdout-printresstdout-endn-if-resstderr-printstderr-textwrapshortenresstderr400-endn-return-resnn-n-4-ensure-hydra-core-local-wheel-omegaconf-already-presentn-nrun_and_printn-pipinstall-no-index-no-deps-force-reinstalln-kaggleinputhydra-core-132whlhydra_core-132-py3-none-anywhlnnnmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190650049993ziopubexecute_input2025-05-16t190650050465ziopubstatusidle2025-05-16t190651789565zshellexecute_replystarted2025-05-16t190650050419zshellexecute_reply2025-05-16t190651788183zoutputsexecution_countnullcell_typecodesource-n-5-rnapredictor-config-hydra-best-practices-stochastic-inferencen-nfrom-rna_predictinterface-import-rnapredictorn-import-omegaconf-and-torch-if-not-already-imported-in-the-cellnfrom-omegaconf-import-omegaconfnimport-torchnimport-logging-ensure-logging-is-imported-if-you-use-loggerinfonntest_seqs-kaggleinputstanford-rna-3d-foldingtest_sequencescsvnsample_sub-kaggleinputstanford-rna-3d-foldingsample_submissioncsvnoutput_csv-submissioncsvnndef-create_predictorn-instantiate-rnapredictor-with-local-checkpoints-gpucpu-autodetect-matching-hydra-config-structuren-device-cuda-if-torchcudais_available-else-cpun-logginginfofdevice-device-assuming-logging-is-configuredn-cfg-omegaconfcreaten-top-level-keys-consistent-with-a-full-hydra-config-eg-defaultyamln-device-devicen-seed-42-good-for-reproducibility-if-used-by-modelsn-atoms_per_residue-44-standard-valuen-extraction_backend-dssr-or-mdanalysis-as-needednn-pipeline-general-pipeline-settingsn-verbose-truen-save_intermediates-truen-output_dir-is-usually-set-by-hydras-run-directory-or-overriddenn-nn-prediction-prediction-specific-settingsn-repeats-5n-residue_atom_choice-0n-enable_stochastic_inference_for_submission-true-critical-ensures-unique-predictionsn-submission_seeds-42-101-2024-7-1991-optional-for-reproducible-stochastic-runsn-nn-model-n-stage-b-torsion-angle-prediction-n-stageb-n-torsion_bert-n-model_name_or_path-kaggleworkingrna_torsionbert-path-to-local-torsionbert-modeln-device-devicen-angle_mode-degrees-changed-set-to-degrees-for-consistency-with-stagecn-this-ensures-stagebtorsionbertpredictor-outputs-angles-in-degreesn-num_angles-7n-max_length-512n-checkpoint_path-none-can-be-overridden-if-a-specific-checkpoint-is-neededn-debug_logging-true-set-to-false-if-logs-are-too-verbosen-init_from_scratch-false-assumes-using-pretrained-torsionbertn-lora-lora-config-currently-disabledn-enabled-falsen-r-8n-alpha-16n-dropout-01n-target_modules-query-valuen-n-n-pairformer-config-would-go-here-if-used-pairformer-n-n-stage-c-3d-reconstruction-mp-nerf-n-stagec-n-enabled-truen-method-mp_nerfn-do_ring_closure-false-consistent-with-defaultyaml-notebook-log-showed-true-adjust-if-neededn-place_bases-truen-sugar_pucker-c3-endon-device-devicen-debug_logging-true-set-to-false-if-logs-are-too-verbosen-angle_representation-degrees-stagec-expects-angles-in-degrees-from-stagebn-use_metadata-falsen-use_memory_efficient_kernel-falsen-use_deepspeed_evo_attention-falsen-use_lma-falsen-inplace_safe-false-consistent-with-defaultyaml-notebook-log-showed-truen-chunk_size-nonen-n-stage-d-diffusion-refinement-minimal-placeholder-n-add-full-staged-config-if-its-actively-used-in-this-notebookn-staged-n-enabled-false-set-to-true-if-staged-is-part-of-this-specific-notebooks-pipelinen-mode-inferencen-device-devicen-debug_logging-truen-placeholder-for-other-essential-staged-keys-if-enabledn-ref_element_size-128n-ref_atom_name_chars_size-256n-profile_size-32n-model_architecture-n-diffusion-n-n-n-n-return-rnapredictorcfgnn-usage-examplenpredictor-create_predictormetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190651791335ziopubexecute_input2025-05-16t190651791699ziopubstatusidle2025-05-16t190652817529zshellexecute_replystarted2025-05-16t190651791663zshellexecute_reply2025-05-16t190652816481zcollapsedtruejupyteroutputs_hiddentrueoutputsexecution_countnullcell_typecodesource-n-patch-guarantee-predict_submission-returns-one-row-per-residue-n-works-both-when-stage-c-gives-l-atoms-3-or-n_atoms-3n-keeps-all-original-columns-created-by-coords_to_dfn-nimport-logging-torch-pandas-as-pdnfrom-rna_predictinterface-import-rnapredictornfrom-rna_predictutilssubmission-import-coords_to_df-extract_atom-reshape_coordsnnlog-logginggetloggerrna_predictpatchflat2resnndef-_predict_submission_patchedn-selfn-sequence-strn-prediction_repeats-int-none-nonen-residue_atom_choice-int-none-nonenn-n-collapses-per-atom-coordinates-one-canonical-atom-per-residuen-prefers-phosphate-p-falls-back-to-first-atom-per-residuen-always-returns-exactly-lensequence-rows-preserving-coords_to_df-scheman-n-original-prologue-n-result-selfpredict_3d_structuresequencen-coords_flat-resultcoords-2-d-n_atoms-3nn-new-make-it-a-plain-tensor-so-numpy-is-allowedn-if-coords_flatrequires_grad-the-bug-fixn-coords_flat-coords_flatdetachnn-metadata-resultgetatom_metadata-n-atom_names-metadatagetatom_names-n-residue_indices-metadatagetresidue_indices-nn-repeats-prediction_repeats-if-prediction_repeats-is-not-none-else-selfdefault_repeatsn-atom_idx-residue_atom_choice-if-residue_atom_choice-is-not-none-else-selfdefault_atom_choicenn-n-flat-coords-path-stage-c-returned-n_atoms-3n-n-if-coords_flatdim-2-and-coords_flatshape0-lensequencen-if-not-atom_names-or-not-residue_indicesn-logerrorflat-coords-missing-atom-metadata-falling-back-to-legacy-per-atom-outputn-base-n-id-range1-lencoords_flat-1n-resname-x-lencoords_flatn-resid-range1-lencoords_flat-1n-n-df-pddataframebasen-for-i-in-range1-repeats-1n-dffaxnn-pick-one-atom-per-residue-prefer-pn-picked-tmptmpatom_name-pn-drop_duplicatesres0-keepfirstn-sort_valuesres0n-if-lenpicked-lensequence-fallback-if-some-ps-missingn-logwarningflat-coords-p-selection-gave-dd-rows-using-first-atom-fallbackn-lenpicked-lensequencen-picked-tmpgroupbyres0-as_indexfalsen-firstn-sort_valuesres0nn-per_res_coords-torchtensorn-pickedx-y-zvaluesn-dtypecoords_flatdtypen-devicecoords_flatdevicen-nn-return-coords_to_dfsequence-per_res_coords-repeatsnn-n-original-reshaped-path-stage-c-returned-l-atoms-3n-n-coords-reshape_coordscoords_flat-lensequencen-if-coordsdim-2-and-coordsshape0-lensequencen-reshape-failed-treat-as-flat-once-moren-logwarningreshape_coords-produced-flat-coords-rerouting-through-flat-coords-logicn-resultcoords-coordsn-return-for-ax-in-xyz-coords_flatcpunumpyn-return-dfnn-tmp-pddataframen-atom_name-atom_namesn-res0-residue_indices-0-based-residue-indexn-x-coords_flat-0cpunumpyn-y-coords_flat-1cpunumpyn-z-coords_flat-2cpunumpyn-predict_submission_patchedself-sequence-prediction_repeats-residue_atom_choicenn-atom_coords-extract_atomcoords-atom_idxn-return-coords_to_dfsequence-atom_coords-repeatsnn-install-the-patch-simple-attribute-assignment-is-enoughnrnapredictorpredict_submission-_predict_submission_patchednloginfo-rnapredictorpredict_submission-patched-flat-coords-fixmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190652818688ziopubexecute_input2025-05-16t190652818976ziopubstatusidle2025-05-16t190652833843zshellexecute_replystarted2025-05-16t190652818951zshellexecute_reply2025-05-16t190652832636zoutputsexecution_countnullcell_typecodesourcetoy-create_predictorpredict_submissionacguacgu-prediction_repeats1nassert-lentoy-8-one-row-per-residuenprinttoyheadmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t190652835076ziopubexecute_input2025-05-16t190652835554ziopubstatusidle2025-05-16t190654965454zshellexecute_replystarted2025-05-16t190652835511zshellexecute_reply2025-05-16t190654964350zcollapsedtruejupyteroutputs_hiddentrueoutputsexecution_countnullcell_typecodesource-n-6-prediction-utilities-de-duplication-aggregation-safeguard-n-n-note-this-cell-assumes-rnapredictor-has-been-patched-with-the-correctedn-_predict_submission_patched-method-in-a-previous-celln-fix-ensure-_collapse_to_one_row_per_residue-can-correctly-infer-num_repeatsn-and-handle-the-original-sequence-string-for-resname-populationn-nimport-numpy-as-npnimport-pandas-as-pdnimport-loggingnimport-sys-for-sysstdout-in-logger-handlernimport-os-for-ospathexistsn-assuming-create_predictor-is-defined-in-a-previous-cell-and-available-in-global-scopen-assuming-rnapredictor-is-imported-and-patched-in-a-previous-cellnn-configure-logger-for-this-cell-if-not-already-done-globallynlogger_cell6-logginggetloggerrna_predictcell6_utilsnif-not-logger_cell6handlersn-handler_cell6-loggingstreamhandlersysstdoutn-formatter_cell6-loggingformatterasctimes-names-levelnames-messagesn-handler_cell6setformatterformatter_cell6n-logger_cell6addhandlerhandler_cell6nlogger_cell6setlevellogginginfonnndef-_auto_columndf-pddataframe-pref-liststr-strn-return-first-column-present-in-pref-fallback-dfcolumns0n-for-c-in-prefn-if-c-in-dfcolumnsn-return-cn-if-dfcolumnsempty-guard-against-empty-dataframe-columnsn-logger_cell6warning_auto_column-called-with-dataframe-with-no-columnsn-return-or-raise-an-error-depending-on-desired-behaviorn-return-dfcolumns0nnndef-_collapse_to_one_row_per_residuedf_raw-pddataframe-seq_id-str-original_sequence-str-num_repeats-int-pddataframen-n-ensure-the-dataframe-has-one-row-per-residue-with-clean-ids-and-standard-columnsn-this-function-now-primarily-serves-as-a-schema-enforcer-and-final-sanity-checkn-n-df-df_rawcopynn-if-dfemptyn-logger_cell6warningf_collapse_to_one_row_per_residue-received-empty-dataframe-for-seq_id-seq_idn-define-expected-columns-based-on-num_repeats-for-an-empty-dataframen-cols-id-resname-resid-fax-for-k-in-range1-num_repeats-1-for-ax-in-xyzn-return-pddataframecolumnscolsnn-1-2-first-repeatangle-only-canonical-atom-selectionn-these-steps-are-assumed-to-have-been-handled-by-the-patched-_predict_submission_patchedn-the-input-df_raw-should-ideally-be-one-row-per-residue-with-all-5-repeats-as-columnsnn-3-safeguard-average-duplicates-if-resid-is-not-unique-shouldnt-happen-if-patched-predict_submission-is-correctn-if-resid-in-dfcolumns-and-not-dfresidis_uniquen-logger_cell6warningfresidues-in-dataframe-for-seq_id-are-not-unique-attempting-to-average-duplicatesn-coord_cols-c-for-c-in-dfcolumns-if-cstartswithx_-y_-z_n-key_cols_present-k-for-k-in-resid-resname-if-k-in-dfcolumnsn-if-key_cols_presentn-df-n-dfgroupbykey_cols_present-as_indexfalse-sortfalsecoord_colsmeann-reset_indexdroptruen-n-n-4-ensure-resid-is-1-based-sequential-and-id-is-correctly-formattedn-this-is-important-because-the-input-df_raw-from-the-patched-predict_submissionn-should-already-have-lensequence-rowsn-if-not-dfemptyn-if-resid-not-in-dfcolumns-or-not-pdapitypesis_numeric_dtypedfresid-or-dfresidisnullany-or-not-dfresidis_monotonic_increasingn-dfresid-nparange1-lendf-1-re-create-if-problematicn-n-if-resname-not-in-dfcolumns-ensure-resname-column-existsn-dfresname-listoriginal_sequencelendf-if-lenoriginal_sequence-lendf-else-listoriginal_sequence-xlendf-lenoriginal_sequencen-n-if-id-in-dfcolumnsn-df-dfdropcolumnsidn-dfinsert0-id-fseq_idr-for-r-in-dfresidnnn-5-ensure-all-required-coordinate-columns-x_1z_5-etc-exist-and-have-the-correct-namesn-the-number-of-repeats-is-now-taken-from-the-argumentn-expected_coord_cols-fax-adding-with-nansdefaultsn-if-colstartswithx_-y_-z_n-dfcol-npnann-elif-col-resnamen-dfcol-listoriginal_sequencelendf-if-lenoriginal_sequence-lendf-else-listoriginal_sequence-x-for-i-in-rangenum_repeats-for-ax-in-xyzn-final_cols_schema-id-resname-resid-expected_coord_colsn-n-for-col-in-final_cols_scheman-if-col-not-in-dfcolumnsn-logger_cell6warningfcolumn-col-missing-in-dataframe-for-seq_idlendf-lenoriginal_sequencen-elif-col-resid-and-resid-not-in-dfcolumns-should-have-been-handledn-dfcol-nparange1-lendf1-if-not-dfempty-else-n-elif-col-id-and-id-not-in-dfcolumns-and-resid-in-dfcolumns-should-have-been-handledn-dfinsert0-id-fseq_idr-for-r-in-dfresidn-elif-not-dfempty-for-any-other-unexpected-missing-columnn-dfcol-n-n-return-only-the-columns-expected-in-the-submission-in-the-correct-ordern-return-dffinal_cols_schemannndef-process_test_sequencestest_csv-str-sample_csv-str-out_csv-str-batch-int-1n-generate-submission-file-after-collapsing-predictionsnn-df_test-pdread_csvtest_csvn-logginginfoloaded-d-sequences-for-processing-lendf_test-use-global-logging-or-logger_cell6nn-create_predictor-is-defined-in-the-preceding-cell-cell-12-in-your-notebook-structuren-predictor-create_predictorn-get-number-of-repeats-from-the-predictors-configurationn-num_repeats-predictorprediction_configrepeatsnnn-id_col-auto_columndf_test-id-id-seq_id-sequence_idn-seq_col-_auto_columndf_test-sequence-sequence-seq-seqnn-frames-listpddataframe-n-for-start_idx-in-range0-lendf_test-batch-renamed-start-to-start_idx-to-avoid-conflictn-end_idx-minstart_idx-batch-lendf_testn-logginginfoprocessing-batch-dd-start_idx-1-end_idx-use-global-logging-or-logger_cell6n-n-for-i-in-rangestart_idx-end_idxn-sid-df_testati-id_coln-seq_str-df_testati-seq_col-store-the-original-sequence-stringnn-pass-current_target_id-to-the-predictor-if-your-patched-method-uses-itn-if-hasattrpredictor-current_target_idn-predictorcurrent_target_id-sidnn-if-not-isinstanceseq_str-str-or-not-seq_strn-loggingwarningfskipping-invalid-or-empty-sequence-for-id-sid-at-index-in-create-an-empty-dataframe-with-correct-columnsn-temp_df_empty-pddataframecolumnsid-resname-resid-fax-for-k-in-range1-num_repeats-1-for-ax-in-xyzn-framesappendtemp_df_emptyn-continuen-n-tryn-the-patched-predict_submission-is-called-heren-it-should-return-a-dataframe-that-is-already-one-row-per-residuen-raw_predictions_df-predictorpredict_submissionseq_str-no-need-to-pass-repeats-if-it-uses-selfconfign-n-call-collapse_to_one_row_per_residue-for-final-schema-enforcementn-tidy_predictions_df-_collapse_to_one_row_per_residueraw_predictions_df-sid-seq_str-num_repeatsn-framesappendtidy_predictions_dfn-except-exception-as-errn-loggingerrorsequence-s-id-s-failed-prediction-s-seq_str30-sid-err-exc_infotruennn-if-not-framesn-loggingerrorno-successful-predictions-to-concatenate-submissioncsv-will-be-empty-or-not-createdn-sample_submission_df-pdread_csvsample_csvn-empty_submission_df-pddataframecolumnssample_submission_dfcolumnsn-empty_submission_dfto_csvout_csv-indexfalsen-return-empty_submission_dfnn-results_df-pdconcatframes-ignore_indextruen-n-final-alignment-with-sample_submissioncsv-to-ensure-exact-format-and-row-ordern-tryn-sample_submission_df-pdread_csvsample_csvn-n-ensure-id-column-in-results_df-matches-the-sample-submissions-id-format-for-mergingn-the-_collapse_to_one_row_per_residue-should-have-formatted-id-correctlyn-n-use-a-left-merge-to-ensure-all-ids-from-sample_submission-are-present-and-in-ordern-final_submission_df-pdmergesample_submission_dfid-only-take-id-column-for-merging-keysn-results_df-n-onid-n-howleft-use-left-merge-to-keep-all-sample-submission-idsn-n-fill-nans-for-any-coordinates-that-might-be-missing-after-the-merge-eg-if-a-sequence-failed-or-had-fewer-residuesn-the-columns-in-final_submission_df-will-be-based-on-results_df-after-the-mergen-we-need-to-ensure-all-columns-from-sample_submission_df-are-presentn-for-col-in-sample_submission_dfcolumnsn-if-col-not-in-final_submission_dfcolumnsn-final_submission_dfcol-npnan-add-missing-columns-with-nansn-if-colstartswithx-y_-z_-fill-nans-in-coordinate-columns-with-00n-final_submission_dfcol-final_submission_dfcolfillna00n-elif-col-in-resname-resid-and-col-in-final_submission_dfcolumns-fill-nans-in-resnameresid-if-they-existn-this-part-might-need-more-sophisticated-filling-if-a-whole-sequence-was-missingn-for-now-simple-fillna-if-resname-or-resid-are-entirely-nan-for-a-sequencen-they-will-remain-nan-unless-sample_submission-has-values-for-those-idsn-a-more-robust-fill-would-re-derive-from-sample_submission-for-missing-id-rowsn-pass-let-merge-handle-this-if-id-was-in-sample-but-not-results-resnameresid-will-be-nannn-ensure-exact-column-order-and-presence-as-in-sample_submissioncsvn-final_submission_df-final_submission_dfsample_submission_dfcolumnstolistnn-except-exception-as-en-logerrorferror-aligning-submission-with-sample_submissioncsv-e-saving-raw-concatenated-resultsn-final_submission_df-results_df-fallback-to-saving-whatever-was-concatenatednn-final_submission_dfto_csvout_csv-indexfalsen-logginginfosaved-final-submission-to-s-rows-s-out_csv-flenfinal_submission_dfn-return-final_submission_dfmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191027588545ziopubexecute_input2025-05-16t191027589142ziopubstatusidle2025-05-16t191027621046zshellexecute_replystarted2025-05-16t191027589084zshellexecute_reply2025-05-16t191027619148zoutputsexecution_countnullcell_typecodesource-n-7-toy-sanity-check-demonstrates-collapse-functionn-nprintn-toy-sanity-check-n-assuming-create_predictor-and-collapse_to_one_row_per_residue-are-definedn-in-previous-cells-and-available-in-the-global-scopen-also-assuming-predictor-is-instantiated-via-predictor-create_predictornntoy_sequence-acguacgunnum_toy_repeats-2-or-get-from-predictorprediction_configrepeats-if-predictor-is-already-creatednn-ensure-predictor-is-created-if-not-already-it-was-in-your-previous-cell-structuren-if-predictor-is-not-yet-created-in-this-cells-context-uncomment-the-next-linen-predictor-create_predictor-n-num_toy_repeats-predictorprediction_configrepeats-more-robust-way-to-get-repeatsnntoy_raw-predictorpredict_submissiontoy_sequence-prediction_repeatsnum_toy_repeatsnn-call-_collapse_to_one_row_per_residue-with-the-required-argumentsntoy_comp-_collapse_to_one_row_per_residuetoy_raw-toy-toy_sequence-num_toy_repeatsnprinttoy_compheadnn-the-uniqueness-check-for-the-toy-example-also-needs-num_toy_repeatsncoords_cols_toy-fx-test_pred_znn-adjust-id-format-id-for-i-in-rangenum_toy_repeats-n-fy_i1-for-i-in-rangenum_toy_repeats-n-fz_i1-for-i-in-rangenum_toy_repeatsnunique_structs_toy-setnif-not-toy_rawempty-check-if-toy_raw-is-not-empty-before-proceedingn-for-i-in-rangenum_toy_repeats-n-x_col-y_col-z_col-fx_i1-fy_i1-fz_i1n-ensure-all-necessary-columns-exist-in-toy_raw-before-trying-to-access-themn-if-allcol-in-toy_rawcolumns-for-col-in-x_col-y_col-z_coln-coords_tuple-tupletoy_rawx_col-y_col-z_colto_numpyflattenn-unique_structs_toyaddcoords_tuplen-elsen-logwarningfcoordinate-columns-for-repeat-i1-eg-x_col-not-found-in-toy_raw-dataframe-columns-available-toy_rawcolumnstolistnnnif-not-toy_rawemptyn-loginfoftoy-example-found-lenunique_structs_toy-unique-structures-out-of-num_toy_repeats-repeatsn-if-lenunique_structs_toy-num_toy_repeats-and-lenunique_structs_toy-1-n-logwarningftoy-example-less-than-num_toy_repeats-unique-structures-but-more-than-1-stochasticity-is-partialn-elif-lenunique_structs_toy-1-and-num_toy_repeats-1-check-only-if-more-than-1-repeat-was-expectedn-logerrortoy-example-failure-only-1-unique-structure-found-stochasticity-is-not-workingn-elif-lenunique_structs_toy-num_toy_repeatsn-loginfoftoy-example-success-num_toy_repeats-unique-structures-foundnelsen-logwarningtoy-example-toy_raw-dataframe-is-empty-cannot-check-uniquenessmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191031902867ziopubexecute_input2025-05-16t191031903280ziopubstatusidle2025-05-16t191033306387zshellexecute_replystarted2025-05-16t191031903244zshellexecute_reply2025-05-16t191033304982zoutputsexecution_countnullcell_typecodesourcencell-11-generate-predictions-build-submissionn-nwell-predict-x_1-y_1-z_1-for-each-residue-nthen-replicate-those-coordinates-for-structures-x_2z_5nfinally-well-align-with-sample_submission-and-save-submissioncsvnnn-predict-x_1-y_1-z_1ntest_pred_x-model_xpredicttest_merged_imputedntest_pred_y-model_ypredicttest_merged_imputedntest_pred_z-model_zpredicttest_merged_imputednn-build-submission-from-test_clean_dfnsubmission-test_clean_dfcopynn-add-predicted-coords-for-structure-1nsubmissionx_1-test_pred_xnsubmissiony_1-test_pred_ynsubmissionz_1-test_pred_znn-for-simplicity-replicate-for-structures-25nfor-i-in-2345n-submissionfx_i-test_pred_xn-submissionfy_i-test_pred_yn-submissionfz_i-residnsubmissionid-submissionid-submissionresidastypestrnn-reorder-columns-to-match-sample_submissionnfinal_cols-listsample_submissioncolumns-id-resname-resid-x_1z_5nsubmission-submissionidresnameresidn-x_1y_1z_1n-x_2y_2z_2n-x_3y_3z_3n-x_4y_4z_4n-x_5y_5z_5nn-merge-with-sample_submission-to-match-row-ordernsample_submissionsort_order-rangelensample_submissionnsubmission_merged-pdmergen-submissionn-sample_submissionidsort_ordern-onidn-howleftnsort_valuessort_orderdropcolumnssort_ordernn-this-is-our-final-submission-dataframensubmission_df-submission_mergedcopynn-save-to-csvnsubmission_dfto_csvsubmissioncsv-indexfalsenlogginginfosubmissioncsv-created-successfullynnprintcell-11-complete-submission-file-saved-ready-to-submitmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191043067818ziopubexecute_input2025-05-16t191043068210ziopubstatusidle2025-05-16t191043075969zshellexecute_replystarted2025-05-16t191043068176zshellexecute_reply2025-05-16t191043074668zoutputsexecution_countnullcell_typecodesource-cell-12-conclusions-next-stepsn-nnweve-donen-group-based-imputationn-preserved-resnamen-hyperparameter-tuning-via-randomizedsearchcvn-final-training-on-full-combined-datan-test-predictions-with-the-same-coordinate-repeated-across-5-structuresnnsuggestions-for-further-improvementn-fine-tune-hyperparameters-with-a-broader-search-or-bayesian-optimizationn-explore-more-advanced-rna-3d-featuresn-generate-truly-distinct-5-structures-instead-of-repeating-the-same-coordinatesnnlogginginfonotebook-complete-good-luck-on-the-leaderboardnprintall-done-submit-submissioncsv-to-the-competitionmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191045626246ziopubexecute_input2025-05-16t191045626596ziopubstatusidle2025-05-16t191045632229zshellexecute_replystarted2025-05-16t191045626571zshellexecute_reply2025-05-16t191045630904zoutputsexecution_countnullcell_typecodesourcebashn-cell-show-whats-inside-every-mounted-kaggle-dataset-n-necho-e-n-listing-the-first-two-levels-of-kaggleworking-nnn-change-depth-maxdepth-if-you-want-more-or-fewer-levelsnfind-kaggleworking-maxdepth-2-mindepth-1-print-sed-s-nnecho-e-n-donenmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191048660294ziopubexecute_input2025-05-16t191048660672ziopubstatusidle2025-05-16t191048676199zshellexecute_replystarted2025-05-16t191048660645zshellexecute_reply2025-05-16t191048675021zoutputsexecution_countnullcell_typecodesource-cell-sanity-check-submissioncsv-against-test_sequencescsv-n-nimport-pandas-as-pd-pathlib-textwrap-sys-itertools-numpy-as-npnntest_csv-kaggleinputstanford-rna-3d-foldingtest_sequencescsvnsub_csv-submissioncsvntol-10-a-treat-coords-within-1-a-as-identicalnn-0-helpers-ndef-auto_coldf-prefn-for-c-in-prefn-if-c-in-dfcolumnsn-return-cn-return-dfcolumns0nndef-previews-n5n-lst-listsn-return-joinlstn-if-lenlst-n-else-nn-1-load-basic-info-nfor-f-in-test_csv-sub_csvn-if-not-pathlibpathfis_filen-sysexitferror-f-not-foundnntest_sequences-pdread_csvtest_csvnsubmission-pdread_csvsub_csvnnid_col_test-auto_coltest_sequences-id-id-seq_id-sequence_idnid_col_sub-auto_colsubmission-id-id-seq_id-sequence_idnn-2-expected-vs-actual-rows-nexpected_rows-test_sequencessequencestrlensumnprintn-summary-nprintfexpected-rows-expected_rowsnprintfsubmissioncsv-rows-lensubmissionndupes-submissionid_col_subduplicatedsumnprintfduplicate-id_col_subr-rows-dupesnn-3-build-the-full-id-set-nfull_id_set-n-fsidnif-missing-print-first-few-missing-previewmissingnif-extra-print-first-few-extras-previewextrann-4-per-sequence-coverage-how-many-residues-per-sequence-nseq_len-test_sequencesset_indexid_col_testsequencestrlennn-n-for-sid-seq-in-ziptest_sequencesid_col_test-test_sequencessequencen-for-idx-in-range1-lenseq-1nnsub_id_set-setsubmissionid_col_subastypestrnnmissing-full_id_set-sub_id_setnextra-sub_id_set-full_id_setnnprintn-id-reconciliation-nprintfids-missing-from-submission-lenmissingnprintfunexpected-extra-ids-lenextrafixed-line-below-use-expandtrue-to-ensure-a-1-d-series-avoids-ndarray-shape-n-3nprefixes-n-submissionid_col_subn-astypestrn-strrsplit-n1-expandtrue0-returns-a-series-not-a-nested-ndarraynnncoverage-prefixesvalue_countsreindexseq_lenindexfillna0astypeintnbad_cov-coveragecoverage-seq_lennnprintn-per-sequence-coverage-nprintfsequences-with-wrong-rows-lenbad_covnif-lenbad_covn-print-id-expected-gotn-for-sid-got-in-itertoolsislicebad_covitems-5n-printf-sid6-seq_lensid8-gotnn-5-column-sanity-nreq_cols-id-resname-resid-faxi-for-i-in-range1-6-for-ax-in-xyznmissing_cols-c-for-c-in-req_cols-if-c-not-in-submissioncolumnsnnprintn-column-sanity-nprintfmissing-required-columns-lenmissing_colsnif-missing_colsn-printtextwrapfill-joinmissing_cols-width88nn-6-structure-repeat-uniqueness-ntrip_cols-nparrayfax-for-ax-in-xyz-for-i-in-range1-6ncoords-submissiontrip_colsflattenvaluesreshapelensubmission-5-3nndef-unique_triplet_countrown-return-unique-xyz-triplets-in-a-53-slicen-uniq-n-for-v-in-rown-if-not-anynpallclosev-u-atoltol-for-u-in-uniqn-uniqappendvn-return-lenuniqnn-replace-apply_along_axis-with-a-1-liner-list-comprehension-nuniq_counts-nparrayunique_triplet_countrow-for-row-in-coordsnnall_identical-uniq_counts-1sumntruly_unique-uniq_counts-1sumnnprintn-structure-repeat-uniqueness-nprintfrows-where-5-structures-are-identical-all_identicalnprintfrows-with-2-distinct-triplets-truly_uniquenn-per-sequence-share-of-unique-repeatsnsub_seq_id-prefixesto_numpy-1-d-array-of-sequence-idsnper_seq_unique-n-pdseriesuniq_counts-1-indexsub_seq_idn-groupbylevel0meann-sort_valuesascendingfalsennnprintntop-5-sequences-with-most-unique-repeatsnfor-sid-frac-in-per_seq_uniquehead5itemsn-printf-sid6-frac61-rows-diversifiednnprintn-sanity-check-finishedmetadatatrustedtrueexecutioniopubstatusbusy2025-05-16t191052085507ziopubexecute_input2025-05-16t191052085878ziopubstatusidle2025-05-16t191052110042zshellexecute_replystarted2025-05-16t191052085848zshellexecute_reply2025-05-16t191052108388zoutputsexecution_countnull","title":"{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"name\":\"python\",\"version\":\"3.10.12\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"kaggle\":{\"accelerator\":\"none\",\"dataSources\":[{\"sourceId\":87793,\"databundleVersionId\":12276181,\"sourceType\":\"competition\"},{\"sourceId\":5942070,\"sourceType\":\"datasetVersion\",\"datasetId\":3410079},{\"sourceId\":11026565,\"sourceType\":\"datasetVersion\",\"datasetId\":6866703},{\"sourceId\":11762635,\"sourceType\":\"datasetVersion\",\"datasetId\":7384470},{\"sourceId\":11787394,\"sourceType\":\"datasetVersion\",\"datasetId\":7401070},{\"sourceId\":11787916,\"sourceType\":\"datasetVersion\",\"datasetId\":7401477},{\"sourceId\":11788332,\"sourceType\":\"datasetVersion\",\"datasetId\":7401777},{\"sourceId\":11788337,\"sourceType\":\"datasetVersion\",\"datasetId\":7401782},{\"sourceId\":11788349,\"sourceType\":\"datasetVersion\",\"datasetId\":7401791},{\"sourceId\":11788362,\"sourceType\":\"datasetVersion\",\"datasetId\":7401801},{\"sourceId\":11788374,\"sourceType\":\"datasetVersion\",\"datasetId\":7401810},{\"sourceId\":11788377,\"sourceType\":\"datasetVersion\",\"datasetId\":7401813},{\"sourceId\":11788382,\"sourceType\":\"datasetVersion\",\"datasetId\":7401817},{\"sourceId\":11788386,\"sourceType\":\"datasetVersion\",\"datasetId\":7401820},{\"sourceId\":11788388,\"sourceType\":\"datasetVersion\",\"datasetId\":7401822},{\"sourceId\":11788390,\"sourceType\":\"datasetVersion\",\"datasetId\":7401824},{\"sourceId\":11788491,\"sourceType\":\"datasetVersion\",\"datasetId\":7401888},{\"sourceId\":11788496,\"sourceType\":\"datasetVersion\",\"datasetId\":7401891},{\"sourceId\":11788500,\"sourceType\":\"datasetVersion\",\"datasetId\":7401893},{\"sourceId\":11788503,\"sourceType\":\"datasetVersion\",\"datasetId\":7401896},{\"sourceId\":11788505,\"sourceType\":\"datasetVersion\",\"datasetId\":7401898},{\"sourceId\":11788513,\"sourceType\":\"datasetVersion\",\"datasetId\":7401905},{\"sourceId\":11788517,\"sourceType\":\"datasetVersion\",\"datasetId\":7401907},{\"sourceId\":11788524,\"sourceType\":\"datasetVersion\",\"datasetId\":7401913},{\"sourceId\":11788527,\"sourceType\":\"datasetVersion\",\"datasetId\":7401915},{\"sourceId\":11788542,\"sourceType\":\"datasetVersion\",\"datasetId\":7401925},{\"sourceId\":11788545,\"sourceType\":\"datasetVersion\",\"datasetId\":7401927},{\"sourceId\":11788551,\"sourceType\":\"datasetVersion\",\"datasetId\":7401929},{\"sourceId\":11788558,\"sourceType\":\"datasetVersion\",\"datasetId\":7401931},{\"sourceId\":11788622,\"sourceType\":\"datasetVersion\",\"datasetId\":7401980},{\"sourceId\":11788630,\"sourceType\":\"datasetVersion\",\"datasetId\":7401890},{\"sourceId\":11788641,\"sourceType\":\"datasetVersion\",\"datasetId\":7401995},{\"sourceId\":11788656,\"sourceType\":\"datasetVersion\",\"datasetId\":7401827},{\"sourceId\":11814119,\"sourceType\":\"datasetVersion\",\"datasetId\":7420372},{\"sourceId\":11814128,\"sourceType\":\"datasetVersion\",\"datasetId\":7420378},{\"sourceId\":11814137,\"sourceType\":\"datasetVersion\",\"datasetId\":7401990},{\"sourceId\":11814142,\"sourceType\":\"datasetVersion\",\"datasetId\":7420389},{\"sourceId\":11814146,\"sourceType\":\"datasetVersion\",\"datasetId\":7420392},{\"sourceId\":11814150,\"sourceType\":\"datasetVersion\",\"datasetId\":7420395},{\"sourceId\":11814170,\"sourceType\":\"datasetVersion\",\"datasetId\":7420409},{\"sourceId\":11814175,\"sourceType\":\"datasetVersion\",\"datasetId\":7420413},{\"sourceId\":11814180,\"sourceType\":\"datasetVersion\",\"datasetId\":7420416},{\"sourceId\":11814187,\"sourceType\":\"datasetVersion\",\"datasetId\":7420420},{\"sourceId\":11814259,\"sourceType\":\"datasetVersion\",\"datasetId\":7420472},{\"sourceId\":11814267,\"sourceType\":\"datasetVersion\",\"datasetId\":7420479},{\"sourceId\":11814282,\"sourceType\":\"datasetVersion\",\"datasetId\":7420492},{\"sourceId\":11814553,\"sourceType\":\"datasetVersion\",\"datasetId\":7420638},{\"sourceId\":11831012,\"sourceType\":\"datasetVersion\",\"datasetId\":6866398}],\"dockerImageVersionId\":30918,\"isInternetEnabled\":false,\"language\":\"python\",\"sourceType\":\"notebook\",\"isGpuEnabled\":false}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"cell_type\":\"code\",\"source\":\"# Cell : clean auto-generated requirements file  (run FIRST!)\\n# -----------------------------------------------------------\\nimport pathlib, shutil, re, textwrap, sys, os\\n\\nREQ_PATH = pathlib.Path(\\\"/kaggle/requirements/input_requirements.txt\\\")\\nif REQ_PATH.is_file():\\n    cleaned_lines = []\\n    for line in REQ_PATH.read_text().splitlines():\\n        line = line.strip()\\n        if not line or line.startswith(\\\"#\\\"):\\n            continue                    # \u2190 drop blanks / comments\\n        if not line.startswith(\\\"pip install\\\"):\\n            # keep it but comment it out so the helper ignores it\\n            line = f\\\"# {line}\\\"\\n        cleaned_lines.append(line)\\n\\n    REQ_PATH.write_text(\\\"\\n\\\".join(cleaned_lines) + (\\\"\\n\\\" if cleaned_lines else \\\"\\\"))\\n\\n    print(f\\\"[INFO] requirements cleaned \u2013 {len(cleaned_lines)} valid \\\"\\n          f\\\"pip-install line(s) kept.\\\")\\nelse:\\n    print(f\\\"[INFO] {REQ_PATH} not found \u2013 nothing to clean.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:02:29.551618Z\",\"iopub.execute_input\":\"2025-05-16T19:02:29.552081Z\",\"iopub.status.idle\":\"2025-05-16T19:02:29.560720Z\",\"shell.execute_reply.started\":\"2025-05-16T19:02:29.552014Z\",\"shell.execute_reply\":\"2025-05-16T19:02:29.559195Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"%%bash\\n# Cell: show what\u2019s inside every mounted Kaggle dataset  \ud83d\udd0d\\n# --------------------------------------------------------\\necho -e \\\"\\n\ud83d\udcc2  Listing the first two levels of /kaggle/input \u2026\\n\\\"\\n\\n# Change depth (-maxdepth) if you want more or fewer levels\\nfind /kaggle/input -maxdepth 2 -mindepth 1 -print | sed 's|^|  |'\\n\\necho -e \\\"\\n\u2705  Done.\\n\\\"\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:02:29.562448Z\",\"iopub.execute_input\":\"2025-05-16T19:02:29.562808Z\",\"iopub.status.idle\":\"2025-05-16T19:02:29.661765Z\",\"shell.execute_reply.started\":\"2025-05-16T19:02:29.562770Z\",\"shell.execute_reply\":\"2025-05-16T19:02:29.660349Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"%%bash\\n# Cell : offline installs that match the current wheel set (lean version)\\n# -----------------------------------------------------------------------\\nset -euo pipefail\\n\\n# \u2500\u2500 let pip look inside EVERY sub-folder of /kaggle/input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nWHEEL_ROOT=\\\"/kaggle/input\\\"\\nFIND_LINKS_ARGS=\\\"\\\"\\nfor d in \\\"\\(WHEEL_ROOT\\\" \\\"\\)WHEEL_ROOT\\\"/; do\\n  FIND_LINKS_ARGS+=\\\" --find-links $d\\\"\\ndone\\n\\np () {                 # quiet install; warn (don\u2019t die) if something fails\\n  # shellcheck disable=SC2086\\n  pip install --no-index \\(FIND_LINKS_ARGS --quiet \\\"\\)@\\\" \\\\n  || echo \\\"[WARN] install failed \u2192 skipped: $\\\"\\n}\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 1) Core scientific stack\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\np numpy==1.24.3\\np pandas==2.2.3\\np scipy==1.10.1\\np tqdm==4.67.1\\np seaborn==0.12.2\\np biopython==1.85\\np torch               # pre-installed in the Kaggle image\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 2)  ML / NLP stack\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\np huggingface_hub==0.31.1      # needs hf-xet (you already uploaded)\\np transformers==4.51.3\\np pytorch_lightning==2.5.0.post0   # gives us Lightning-core features\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 3)  Extra deps rna_predict really imports\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\np lightning-utilities==0.11.2  # comes with PL wheel but list explicitly\\np datasets==3.6.0\\np einops==0.8.1\\np hypothesis==6.131.15\\np black==25.1.0                # needs pathspec 0.12.1 \u2192 you uploaded both\\np pathspec==0.12.1\\np isort==6.0.1\\np ruff==0.11.9\\np mss==10.0.0\\np mdanalysis==2.9.0\\np mmtf-python==1.1.3\\np GridDataFormats==1.0.2\\np mrcfile==1.5.4\\np lxml==5.4.0\\np dearpygui==2.0.0\\np py-cpuinfo==9.0.0\\np Pillow                        # pillow-11-2-1 wheel present\\np exit-codes==1.3.0             # small helper used by HF-Hub 0.31+\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 4)  Config utilities\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\np hydra-core==1.3.2\\np omegaconf==2.3.0\\np ml_collections==1.1.0         # required by Protenix\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 5)  rna-predict itself  (no-deps so nothing reaches PyPI)\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\npip install --no-index --no-deps --quiet \\\\n  /kaggle/input/rna-structure-predict/rna_predict-2.0.3-py3-none-any.whl\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 6)  Protenix 0.4.6  (wheel, but ignore its heavy deps like RDKit)\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\npip install --no-index --no-deps --quiet \\\\n  /kaggle/input/protenix-0-4-6/protenix-0.4.6-py3-none-any.whl \\\\n  || echo \\\"[WARN] Protenix wheel install failed.\\\"\\n\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 7)  Runtime shim: make \u201cimport lightning\u201d point to pytorch_lightning\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\npython - &lt;&lt;'PY'\\nimport sys, importlib, types\\ntry:\\n    import pytorch_lightning as pl\\n    sys.modules.setdefault(\\\"lightning\\\", pl)\\nexcept ImportError:\\n    print(\\\"[WARN] pytorch_lightning missing \u2013 shim not created\\\")\\nPY\\n\\necho \\\"\u2705  Offline wheel install phase complete.\\\"\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:02:29.665786Z\",\"iopub.execute_input\":\"2025-05-16T19:02:29.666432Z\",\"iopub.status.idle\":\"2025-05-16T19:05:08.009084Z\",\"shell.execute_reply.started\":\"2025-05-16T19:02:29.666376Z\",\"shell.execute_reply\":\"2025-05-16T19:05:08.008145Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# ---\\n# Cell: ALL-IN-ONE Environment Setup  (no uninstalls, no online pip)\\n# ---\\n\\nimport sys, subprocess, shutil, os, platform\\n\\ndef run_and_print(cmd):\\n    res = subprocess.run(cmd, capture_output=True, text=True)\\n    print(res.stdout, end=\\\"\\\")\\n    if res.stderr:\\n        print(res.stderr, end=\\\"\\\")\\n\\n# \u2550\u2550\u2550\u2550\u2550\u2550 1)  System information (unchanged) \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\nprint(\\\"\\n=== [System Information] ===\\\")\\n\\nprint(\\\"\\n[Python Version]\\\")\\nprint(sys.version)\\n\\nprint(\\\"\\n[Kernel and OS Information]\\\")\\nrun_and_print([\\\"uname\\\", \\\"-a\\\"])\\n\\nprint(\\\"\\n[CPU Information]\\\")\\nrun_and_print([\\\"lscpu\\\"])\\n\\nprint(\\\"\\n[Memory Information]\\\")\\nrun_and_print([\\\"free\\\", \\\"-mh\\\"])\\n\\nprint(\\\"\\n[Disk Information]\\\")\\nrun_and_print([\\\"lsblk\\\"])\\n\\nprint(\\\"\\n=== [End of System Information] ===\\n\\\")\\n\\n# \u2550\u2550\u2550\u2550\u2550\u2550 2)  USER CONFIG  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\nRNA_PREDICT_VERSION   = \\\"2.0.3\\\"\\nBLOCK_SPARSE_WHEEL_IN = (\\n    \\\"/kaggle/input/block-sparse-wheels/\\\"\\n    \\\"block_sparse_attn-0.0.1cu118torch2.0cxx11abiTRUE-\\\"\\n    \\\"cp310-cp310-linux_x86_64.whl\\\"\\n)\\n# PEP 440-compliant rename (Torch version tag trimmed)\\nBLOCK_SPARSE_WHEEL_OUT = (\\n    \\\"/kaggle/working/\\\"\\n    \\\"block_sparse_attn-0.0.1+cu118torch2.0-\\\"\\n    \\\"cp310-cp310-linux_x86_64.whl\\\"\\n)\\n\\n# \u2550\u2550\u2550\u2550\u2550\u2550 3)  Environment-fix helper  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\ndef setup_environment():\\n    \\\"\\\"\\\"\\n    \u2460 Ensure Seaborn (and its deps) is present\\n    \u2461 Copy &amp; install block_sparse_attn wheel   \u2190 optional; see note below\\n    \u2462 Install rna_predict\\n    \u2463 Install hydra-core from local wheel\\n    \u2464 Show final versions of key packages\\n    \\\"\\\"\\\"\\n    # \u2460 Make sure Seaborn is available \u2013\\n    #    Kaggle base image already has 0.12.2 but we pin it explicitly:\\n    print(\\\"[INFO] Making sure Seaborn is installed\u2026\\n\\\")\\n    run_and_print([\\\"pip\\\", \\\"install\\\", \\\"--quiet\\\", \\\"seaborn==0.12.2\\\"])\\n\\n    # \u2461 Copy &amp; (optionally) install block_sparse_attn\\n    if os.path.exists(BLOCK_SPARSE_WHEEL_IN):\\n        try:\\n            shutil.copyfile(BLOCK_SPARSE_WHEEL_IN, BLOCK_SPARSE_WHEEL_OUT)\\n            print(\\\"\\n[INFO] Copied block-sparse-attn wheel to working dir.\\\")\\n            print(\\\"[INFO] Installing block-sparse-attn (no deps)\u2026\\n\\\")\\n            run_and_print([\\\"pip\\\", \\\"install\\\", \\\"--no-deps\\\", \\\"--quiet\\\", BLOCK_SPARSE_WHEEL_OUT])\\n        except Exception as e:\\n            print(f\\\"[WARN] Could not copy/install block-sparse wheel: {e}\\\")\\n            print(\\\"       Continue without it if your code doesn\u2019t need it.\\\")\\n    else:\\n        print(\\\"[WARN] block-sparse-attn wheel not found in /kaggle/input \u2013 skipped.\\\")\\n\\n    # \u2462 Install rna_predict (pure-py, so --no-deps is fine)\\n    rnapred_whl = f\\\"/kaggle/input/rna-structure-predict/\\\" \\\\n                  f\\\"rna_predict-{RNA_PREDICT_VERSION}-py3-none-any.whl\\\"\\n    if os.path.exists(rnapred_whl):\\n        print(f\\\"\\n[INFO] Installing rna_predict {RNA_PREDICT_VERSION} \u2026\\n\\\")\\n        run_and_print([\\\"pip\\\", \\\"install\\\", \\\"--no-deps\\\", \\\"--quiet\\\", rnapred_whl])\\n    else:\\n        print(f\\\"[WARN] {rnapred_whl} not found \u2013 skipped.\\\")\\n\\n    # \u2463 Install hydra-core from local wheel\\n    HYDRA_DIR = \\\"/kaggle/input/hydra-core-132whl\\\"\\n    if os.path.isdir(HYDRA_DIR):\\n        wheels = [f for f in os.listdir(HYDRA_DIR) if f.endswith(\\\".whl\\\")]\\n        if wheels:\\n            for whl in wheels:\\n                whl_path = os.path.join(HYDRA_DIR, whl)\\n                print(f\\\"\\n[INFO] Installing hydra-core from {whl_path} \u2026\\n\\\")\\n                run_and_print([\\\"pip\\\", \\\"install\\\", \\\"--no-deps\\\", \\\"--quiet\\\", whl_path])\\n        else:\\n            print(f\\\"[WARN] No .whl files found in {HYDRA_DIR} \u2013 skipped.\\\")\\n    else:\\n        print(f\\\"[WARN] {HYDRA_DIR} not found \u2013 skipped.\\\")\\n\\n    # \u2464 Show final versions\\n    print(\\\"\\n=== [Final Package Versions] ===\\\")\\n    for pkg in [\\n        \\\"torch\\\", \\\"block-sparse-attn\\\", \\\"rna-predict\\\",\\n        \\\"hydra-core\\\", \\\"numpy\\\", \\\"scipy\\\", \\\"scikit-learn\\\", \\\"seaborn\\\"\\n    ]:\\n        run_and_print([\\\"pip\\\", \\\"show\\\", pkg])\\n    print(\\\"=== [End of Final Package Versions] ===\\n\\\")\\n\\n# \u2550\u2550\u2550\u2550\u2550\u2550 4)  Run it  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\nsetup_environment()\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:08.010711Z\",\"iopub.execute_input\":\"2025-05-16T19:05:08.011020Z\",\"iopub.status.idle\":\"2025-05-16T19:05:51.408073Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:08.010992Z\",\"shell.execute_reply\":\"2025-05-16T19:05:51.406853Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# -- coding: utf-8 --\\n\\\"\\\"\\\"\\nCell 1: ENVIRONMENT SETUP &amp; LOGGING\\n-----------------------------------\\n\\\"\\\"\\\"\\nimport os\\nimport sys\\nimport logging\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Machine Learning Libraries\\nfrom sklearn.model_selection import train_test_split, KFold\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import mean_squared_error\\nfrom xgboost import XGBRegressor\\n\\n\\n# Logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s [%(levelname)s] %(message)s',\\n    handlers=[logging.StreamHandler(sys.stdout)]\\n)\\nlogging.info(\\\"Cell 1 complete: Libraries imported and logging initialized.\\\")\\n\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:51.409308Z\",\"iopub.execute_input\":\"2025-05-16T19:05:51.409608Z\",\"iopub.status.idle\":\"2025-05-16T19:05:51.416335Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:51.409579Z\",\"shell.execute_reply\":\"2025-05-16T19:05:51.414995Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 2: DATA IMPORT\\n-------------------\\nHere, we read in train/validation/test CSVs and a sample submission from the Kaggle environment.\\nAdjust the paths if needed for your environment.\\n\\\"\\\"\\\"\\n\\n# Example file paths\\nTRAIN_SEQUENCES_PATH = \\\"/kaggle/input/stanford-rna-3d-folding/train_sequences.csv\\\"\\nTRAIN_LABELS_PATH    = \\\"/kaggle/input/stanford-rna-3d-folding/train_labels.csv\\\"\\nVALID_SEQUENCES_PATH = \\\"/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv\\\"\\nVALID_LABELS_PATH    = \\\"/kaggle/input/stanford-rna-3d-folding/validation_labels.csv\\\"\\nTEST_SEQUENCES_PATH  = \\\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\\\"\\nSAMPLE_SUB_PATH      = \\\"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\\\"\\n\\ntry:\\n    train_sequences = pd.read_csv(TRAIN_SEQUENCES_PATH)\\n    train_labels = pd.read_csv(TRAIN_LABELS_PATH)\\n    validation_sequences = pd.read_csv(VALID_SEQUENCES_PATH)\\n    validation_labels = pd.read_csv(VALID_LABELS_PATH)\\n    test_sequences = pd.read_csv(TEST_SEQUENCES_PATH)\\n    sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\\n\\n    logging.info(\\\"Cell 2 complete: Data loaded successfully.\\\")\\nexcept Exception as e:\\n    logging.error(f\\\"Error loading data: {e}\\\")\\n    sys.exit(1)\\n\\nlogging.info(f\\\"train_sequences: {train_sequences.shape}, train_labels: {train_labels.shape}\\\")\\nlogging.info(f\\\"validation_sequences: {validation_sequences.shape}, validation_labels: {validation_labels.shape}\\\")\\nlogging.info(f\\\"test_sequences: {test_sequences.shape}, sample_submission: {sample_submission.shape}\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:51.417642Z\",\"iopub.execute_input\":\"2025-05-16T19:05:51.418020Z\",\"iopub.status.idle\":\"2025-05-16T19:05:51.764659Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:51.417969Z\",\"shell.execute_reply\":\"2025-05-16T19:05:51.763287Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 3: COMBINE TRAIN + VALIDATION &amp; BASIC EDA\\n----------------------------------------------\\nWe concatenate the train and validation sets to maximize data. \\nThen we do a quick EDA check on shapes, missingness, etc.\\n\\\"\\\"\\\"\\n\\n# Combine sequences and labels\\ntrainval_sequences = pd.concat([train_sequences, validation_sequences], ignore_index=True)\\ntrainval_labels = pd.concat([train_labels, validation_labels], ignore_index=True)\\n\\nlogging.info(f\\\"Combined train+validation sequences: {trainval_sequences.shape}, labels: {trainval_labels.shape}\\\")\\n\\n# Quick check for missing\\nlogging.info(\\\"Missing in combined sequences:\\n\\\" + str(trainval_sequences.isnull().sum()))\\nlogging.info(\\\"Missing in combined labels:\\n\\\" + str(trainval_labels.isnull().sum()))\\n\\n# Example EDA: sequence length distribution\\ntrainval_sequences['sequence_length'] = trainval_sequences['sequence'].str.len()\\n\\nplt.figure(figsize=(10,4))\\nsns.boxplot(x=trainval_sequences['sequence_length'], color='skyblue')\\nplt.title(\\\"Boxplot of Sequence Length (Train + Validation)\\\")\\nplt.xlabel(\\\"Sequence Length\\\")\\nplt.show()\\n\\nlogging.info(\\\"Cell 3 complete: Basic EDA finished.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:51.766234Z\",\"iopub.execute_input\":\"2025-05-16T19:05:51.766825Z\",\"iopub.status.idle\":\"2025-05-16T19:05:52.188571Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:51.766785Z\",\"shell.execute_reply\":\"2025-05-16T19:05:52.187427Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 4: HANDLE MISSING COORDINATES &amp; MERGE\\n------------------------------------------\\nWe replace '-1e18' with np.nan, then merge sequences with labels on target_id.\\n\\\"\\\"\\\"\\n\\n# Replace -1e18 with np.nan in the labels\\nfor col in ['x_1','y_1','z_1']:\\n    trainval_labels[col] = trainval_labels[col].replace(-1e18, np.nan)\\n\\nlogging.info(\\\"Replaced -1e18 with NaN in trainval_labels for x_1, y_1, z_1.\\\")\\n\\n# Extract pdb_id, chain_id from ID\\ntrainval_labels['pdb_id']   = trainval_labels['ID'].apply(lambda x: x.split('')[0])\\ntrainval_labels['chain_id'] = trainval_labels['ID'].apply(lambda x: x.split('')[1])\\ntrainval_labels['target_id'] = trainval_labels['pdb_id'] + \\\"\\\" + trainval_labels['chain_id']\\n\\n# Merge\\ntrain_data = pd.merge(trainval_labels, trainval_sequences, on='target_id', how='left')\\nlogging.info(f\\\"Merged train_data shape: {train_data.shape}\\\")\\n\\n# Quick check\\nlogging.info(f\\\"Missing in x_1: {train_data['x_1'].isnull().sum()}, \\\"\\n             f\\\"y_1: {train_data['y_1'].isnull().sum()}, \\\"\\n             f\\\"z_1: {train_data['z_1'].isnull().sum()}\\\")\\n\\nlogging.info(\\\"Cell 4 complete: Merged train_data, ready for group-based imputation.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:52.191954Z\",\"iopub.execute_input\":\"2025-05-16T19:05:52.192298Z\",\"iopub.status.idle\":\"2025-05-16T19:05:52.663563Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:52.192266Z\",\"shell.execute_reply\":\"2025-05-16T19:05:52.662206Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 5: FEATURE ENGINEERING\\n---------------------------\\nCreate numerical/categorical features from the 'sequence'.\\nWe'll keep 'resname' from the labels as a valuable feature.\\n\\\"\\\"\\\"\\n\\ndef engineer_features(df):\\n    \\\"\\\"\\\"\\n    Create numerical &amp; (some) categorical features from raw RNA sequence data.\\n    \\\"\\\"\\\"\\n    df = df.copy()\\n    # Sequence-based\\n    df['seq_length'] = df['sequence'].str.len()\\n    df['A_cnt'] = df['sequence'].str.count('A')\\n    df['C_cnt'] = df['sequence'].str.count('C')\\n    df['G_cnt'] = df['sequence'].str.count('G')\\n    df['U_cnt'] = df['sequence'].str.count('U')\\n    df['begin_seq'] = df['sequence'].str[0]\\n    df['end_seq']   = df['sequence'].str[-1]\\n    \\n    # Di-nucleotide counts (example set)\\n    for pair in ['AC','AG','AU','CA','CG','CU','GA','GC','GU','UA','UC','UG',\\n                 'AA','CC','GG','UU']:\\n        df[f'{pair}_cnt'] = df['sequence'].str.count(pair)\\n\\n    return df\\n\\n# Apply feature engineering\\ntrain_data = engineer_features(train_data)\\n\\nlogging.info(\\\"Feature engineering applied to merged train_data.\\\")\\n\\n# We'll show an example of newly added columns\\nexample_cols = ['seq_length','A_cnt','C_cnt','G_cnt','U_cnt','begin_seq','end_seq','AC_cnt','AA_cnt']\\nlogging.info(f\\\"Columns after FE sample:\\n{train_data[example_cols].head(3)}\\\")\\n\\nlogging.info(\\\"Cell 5 complete: Feature engineering done.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:05:52.665836Z\",\"iopub.execute_input\":\"2025-05-16T19:05:52.666288Z\",\"iopub.status.idle\":\"2025-05-16T19:06:35.358204Z\",\"shell.execute_reply.started\":\"2025-05-16T19:05:52.666252Z\",\"shell.execute_reply\":\"2025-05-16T19:06:35.356006Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 6: GROUP-BASED IMPUTATION\\n------------------------------\\nWe impute missing x_1, y_1, z_1 within each (target_id, resname) group.\\nFinally, if any NAs remain, we fill them with a global median or drop them.\\n\\\"\\\"\\\"\\n\\n# Perform group-based fill for x_1, y_1, z_1\\ntrain_data[['x_1','y_1','z_1']] = (\\n    train_data\\n    .groupby(['target_id','resname'])[['x_1','y_1','z_1']]\\n    .apply(lambda grp: grp.fillna(grp.mean()))\\n    .reset_index(level=['target_id','resname'], drop=True)\\n)\\n\\n# In case any remain after group-based mean fill (e.g. group is all NaN), do a global fill\\nnum_cols = ['x_1','y_1','z_1']\\nglobal_imputer = SimpleImputer(strategy='median')\\ntrain_data[num_cols] = global_imputer.fit_transform(train_data[num_cols])\\n\\n# If you'd prefer to drop any leftover NAs instead:\\n# train_data.dropna(subset=['x_1','y_1','z_1'], inplace=True)\\n\\nlogging.info(\\\"Group-based imputation + global median fallback complete.\\\")\\n\\n# Confirm missing values\\nlogging.info(f\\\"Remaining missing x_1: {train_data['x_1'].isna().sum()}, \\\"\\n             f\\\"y_1: {train_data['y_1'].isna().sum()}, z_1: {train_data['z_1'].isna().sum()}\\\")\\n\\nlogging.info(\\\"Cell 6 complete: Group-based imputation finished.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:35.359808Z\",\"iopub.execute_input\":\"2025-05-16T19:06:35.360289Z\",\"iopub.status.idle\":\"2025-05-16T19:06:46.697400Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:35.360252Z\",\"shell.execute_reply\":\"2025-05-16T19:06:46.695816Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 7: PREPARE DATA FOR MODELING\\n---------------------------------\\nWe'll define the columns we won't use, set up X and y for x_1, y_1, z_1, \\nand one-hot encode any relevant categorical columns (including resname).\\n\\\"\\\"\\\"\\n\\n# Unused columns\\nunused_cols = [\\n    'ID','pdb_id','chain_id','resid',\\n    'x_1','y_1','z_1',\\n    'sequence','description','temporal_cutoff','all_sequences',\\n    'target_id'  # key used for merges\\n]\\n\\n# We'll keep resname, begin_seq, end_seq as features this time\\nfeature_cols = [col for col in train_data.columns if col not in unused_cols]\\n\\n# Make a copy\\ntrain_df = train_data.copy()\\n\\n# Convert to categories\\nfor cat_col in ['resname','begin_seq','end_seq']:\\n    if cat_col in feature_cols:\\n        train_df[cat_col] = train_df[cat_col].astype('category')\\n\\n# One-hot encode\\ntrain_df = pd.get_dummies(train_df, columns=['resname','begin_seq','end_seq'], drop_first=True)\\n\\n# Our final set of features\\nX_cols = [col for col in train_df.columns if col not in unused_cols]\\n\\nX_full = train_df[X_cols]\\ny_x_full = train_df['x_1']\\ny_y_full = train_df['y_1']\\ny_z_full = train_df['z_1']\\n\\nlogging.info(f\\\"Feature matrix shape: {X_full.shape}\\\")\\nlogging.info(\\\"Cell 7 complete: Prepared data for modeling.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:46.698754Z\",\"iopub.execute_input\":\"2025-05-16T19:06:46.699321Z\",\"iopub.status.idle\":\"2025-05-16T19:06:47.244815Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:46.699153Z\",\"shell.execute_reply\":\"2025-05-16T19:06:47.243256Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 8: KFold CV for X, Y, Z &amp; Hyperparam Search\\n------------------------------------------------\\nWe'll do a simplified KFold cross-validation for each coordinate \\nto get a sense of good hyperparams, then train final models.\\n\\\"\\\"\\\"\\n\\nfrom sklearn.model_selection import KFold, RandomizedSearchCV\\nimport numpy as np\\n\\n# Example hyperparameter grid (you can expand as needed)\\nparam_dist = {\\n    'learning_rate': [0.03, 0.05, 0.1],\\n    'max_depth': [6, 10, 15],\\n    'n_estimators': [500, 800, 1000],\\n    'subsample': [0.7, 0.9, 1.0],\\n    'colsample_bytree': [0.7, 0.9, 1.0]\\n}\\n\\ndef run_random_search(X, y, param_dist, n_iter=5, cv_splits=3):\\n    \\\"\\\"\\\"Simple RandomizedSearchCV for an XGBRegressor using GPU in XGBoost &gt;= 2.0.\\\"\\\"\\\"\\n    xgb = XGBRegressor(tree_method='hist', device='cuda', random_state=42)\\n    rsearch = RandomizedSearchCV(\\n        estimator=xgb,\\n        param_distributions=param_dist,\\n        n_iter=n_iter,\\n        scoring='neg_mean_squared_error',\\n        cv=cv_splits,\\n        verbose=1,\\n        random_state=42\\n    )\\n    rsearch.fit(X, y)\\n    best_model = rsearch.best_estimator\\n    logging.info(f\\\"Best params: {rsearch.best_params_}, Best CV Score: {rsearch.best_score_}\\\")\\n    return best_model, rsearch.best_params_\\n\\nlogging.info(\\\"Starting hyperparam search for X coordinate.\\\")\\n#best_model_x, best_params_x = run_random_search(X_full, y_x_full, param_dist, n_iter=5, cv_splits=3)\\n\\nlogging.info(\\\"Starting hyperparam search for Y coordinate.\\\")\\n#best_model_y, best_params_y = run_random_search(X_full, y_y_full, param_dist, n_iter=5, cv_splits=3)\\n\\nlogging.info(\\\"Starting hyperparam search for Z coordinate.\\\")\\n#best_model_z, best_params_z = run_random_search(X_full, y_z_full, param_dist, n_iter=5, cv_splits=3)\\n\\nlogging.info(\\\"Cell 8 complete: RandomizedSearchCV best params found.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:47.246482Z\",\"iopub.execute_input\":\"2025-05-16T19:06:47.246859Z\",\"iopub.status.idle\":\"2025-05-16T19:06:47.256247Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:47.246826Z\",\"shell.execute_reply\":\"2025-05-16T19:06:47.254737Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 9: FINAL TRAINING ON FULL DATA\\n-----------------------------------\\nUse the best hyperparams for each coordinate found in CV. \\nRetrain each coordinate model on all data (X_full, y__full).\\n\\\"\\\"\\\"\\n\\ndef get_best_xgb(params):\\n    \\\"\\\"\\\" Return an XGBRegressor with the given params, using GPU. \\\"\\\"\\\"\\n    # Here we override or add 'tree_method' to ensure GPU usage\\n    # We can also specify predictor='gpu_predictor' to accelerate inference on GPU\\n    model = XGBRegressor(\\n        params,\\n        tree_method='hist',   # or tree_method=params.get('tree_method', 'hist')\\n        device='cuda',        # ensures GPU usage\\n        random_state=42\\n    )\\n    return model\\n\\nlogging.info(\\\"Retraining final model for X coordinate...\\\")\\n#model_x = get_best_xgb(best_params_x)\\n#model_x.fit(X_full, y_x_full)\\n\\nlogging.info(\\\"Retraining final model for Y coordinate...\\\")\\n#model_y = get_best_xgb(best_params_y)\\n#model_y.fit(X_full, y_y_full)\\n\\nlogging.info(\\\"Retraining final model for Z coordinate...\\\")\\n#model_z = get_best_xgb(best_params_z)\\n#model_z.fit(X_full, y_z_full)\\n\\nlogging.info(\\\"Cell 9 complete: Final models trained.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:47.257410Z\",\"iopub.execute_input\":\"2025-05-16T19:06:47.257725Z\",\"iopub.status.idle\":\"2025-05-16T19:06:47.291639Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:47.257695Z\",\"shell.execute_reply\":\"2025-05-16T19:06:47.289896Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 10: PREPARE &amp; ENGINEER TEST DATA\\n-------------------------------------\\n\u2022 Expand test_sequences into (ID, resname, resid)\\n\u2022 Merge residue\u2011level grid with per\u2011sequence engineered features\\n\u2022 Align with training feature matrix X_full, fill missing values\\n\\\"\\\"\\\"\\n\\n# ---------- 1. Expand residue grid ----------\\ntest_expanded = [\\n    [row[\\\"target_id\\\"], nt, i]\\n    for _, row in test_sequences.iterrows()\\n    for i, nt in enumerate(row[\\\"sequence\\\"], start=1)\\n]\\ntest_clean_df = pd.DataFrame(test_expanded, columns=[\\\"ID\\\", \\\"resname\\\", \\\"resid\\\"])\\nlogging.info(f\\\"test_clean_df shape: {test_clean_df.shape} (expanded test sequences)\\\")\\n\\n# ---------- 2. Per\u2011sequence engineered features ----------\\ntest_feats = engineer_features(test_sequences)\\n\\n# Merge \u2013 one row per residue, sequence\u2011level features broadcast to each residue\\ntest_merged = pd.merge(\\n    test_clean_df,\\n    test_feats.drop(columns=[\\\"seq_length\\\"]),   # drop if not needed\\n    left_on=\\\"ID\\\",\\n    right_on=\\\"target_id\\\",\\n    how=\\\"left\\\"\\n)\\nlogging.info(f\\\"test_merged shape after merging: {test_merged.shape}\\\")\\n\\n# ---------- 3. Clean up ----------\\n# Replace sentinel values\\nfor col in [\\\"x_1\\\", \\\"y_1\\\", \\\"z_1\\\"]:\\n    if col in test_merged.columns:\\n        test_merged[col] = test_merged[col].replace(-1e18, np.nan)\\n\\n# Drop columns not used by the model\\ndrop_cols = [\\\"sequence\\\", \\\"description\\\", \\\"temporal_cutoff\\\", \\\"all_sequences\\\", \\\"target_id\\\"]\\ntest_merged.drop(columns=[c for c in drop_cols if c in test_merged.columns], inplace=True, errors=\\\"ignore\\\")\\n\\n# ---------- 4. Categorical handling ----------\\ncat_cols = {\\\"resname\\\", \\\"begin_seq\\\", \\\"end_seq\\\"} &amp; set(test_merged.columns)\\nfor col in cat_cols:\\n    test_merged[col] = test_merged[col].astype(\\\"category\\\")\\ntest_merged = pd.get_dummies(test_merged, columns=list(cat_cols), drop_first=True)\\n\\n# ---------- 5. Column alignment ----------\\n# Single vectorised reindex instead of per\u2011column insertion \u2192 no fragmentation warning\\ntest_merged = test_merged.reindex(columns=X_full.columns, fill_value=0)\\n\\n# ---------- 6. Missing\u2011value imputation ----------\\n# Fit a NEW median imputer on the training feature matrix (numeric cols only)\\nnumeric_cols = X_full.select_dtypes(include=np.number).columns\\nfeature_imputer = SimpleImputer(strategy=\\\"median\\\")\\nfeature_imputer.fit(X_full[numeric_cols])\\n\\ntest_merged[numeric_cols] = feature_imputer.transform(test_merged[numeric_cols])\\n\\n# ---------- 7. All done ----------\\ntest_merged_imputed = test_merged.copy()\\nlogging.info(\\\"Cell 10 complete: Test data prepared, aligned, and imputed.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:47.293225Z\",\"iopub.execute_input\":\"2025-05-16T19:06:47.293726Z\",\"iopub.status.idle\":\"2025-05-16T19:06:49.855775Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:47.293684Z\",\"shell.execute_reply\":\"2025-05-16T19:06:49.854293Z\"},\"jupyter\":{\"source_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"!HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 HF_HOME=/kaggle/working TRANSFORMERS_CACHE=/kaggle/working ln -sf /kaggle/input/rna-torsion-bert-checkpoint-base/kaggle/working/rna_torsionBERT /kaggle/working/rna_torsionBERT\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:49.857717Z\",\"iopub.execute_input\":\"2025-05-16T19:06:49.858361Z\",\"iopub.status.idle\":\"2025-05-16T19:06:50.048166Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:49.858304Z\",\"shell.execute_reply\":\"2025-05-16T19:06:50.046315Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# Cell: RNA Prediction with TorsionBERT  (offline-ready)\\n# ------------------------------------------------------\\nimport pandas as pd, torch, os, logging, sys, transformers\\nfrom omegaconf import OmegaConf\\nfrom functools import partial\\n\\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 1) LINK LOCAL CHECKPOINTS \u25b8 rna_torsionBERT  &amp;  DNA_Bert_3\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nif not os.path.exists(\\\"/kaggle/working/rna_torsionBERT\\\"):\\n    os.symlink(\\n        \\\"/kaggle/input/rna-torsion-bert-checkpoint-base/kaggle/working/rna_torsionBERT\\\",\\n        \\\"/kaggle/working/rna_torsionBERT\\\"\\n    )\\n\\nDNA_BERT_SRC = \\\"/kaggle/input/dna-bert-rna/DNA_bert_3\\\"\\nDNA_BERT_DST = \\\"/kaggle/working/zhihan1996/DNA_bert_3\\\"   # path hard-coded in torsionBERT\\nif not os.path.exists(DNA_BERT_DST):\\n    os.makedirs(\\\"/kaggle/working/zhihan1996\\\", exist_ok=True)\\n    os.symlink(DNA_BERT_SRC, DNA_BERT_DST)\\n\\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 2) FORCE OFFLINE MODE  +  MAP zhihan1996/ IDs \u2192 local folders\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nos.environ.update({\\n    \\\"HF_HUB_OFFLINE\\\":      \\\"1\\\",\\n    \\\"HF_DATASETS_OFFLINE\\\": \\\"1\\\",\\n    \\\"TRANSFORMERS_OFFLINE\\\":\\\"1\\\",\\n    \\\"HF_HOME\\\":             \\\"/kaggle/working\\\",\\n    \\\"TRANSFORMERS_CACHE\\\":  \\\"/kaggle/working\\\"\\n})\\ndef localize(repo,a,kw):\\n    \\\"\\\"\\\"\\n    Redirect zhihan1996/DNA_bert_ to local paths and\\n    force local_files_only for every HF load.\\n    \\\"\\\"\\\"\\n    if isinstance(repo,str) and repo.startswith(\\\"zhihan1996/DNA_bert\\\"):\\n        repo = \\\"/kaggle/working/\\\" + repo\\n    kw[\\\"local_files_only\\\"] = True\\n    return repo,a,kw\\n# robust monkey-patch (handles partials, repeated patching, etc.)\\nfor cls in (\\\"AutoConfig\\\",\\\"AutoTokenizer\\\",\\\"AutoModel\\\"):\\n    obj      = getattr(transformers, _cls)\\n    base_cls = obj.func if isinstance(obj, partial) else obj\\n    if not hasattr(base_cls, \\\"from_pretrained\\\"):\\n        continue\\n    _orig = base_cls.from_pretrained\\n    def _wrap(repo,a,__o=_orig,kw):\\n        repo,a,kw = _localize(repo,a,kw)\\n        return __o(repo,*a,kw)\\n    base_cls.from_pretrained = _wrap\\n\\n# accept DNA-Bert-3 custom config\\ntry:\\n    from importlib import import_module\\n    custom_conf = import_module(\\n        \\\"transformers_modules.DNA_bert_3.configuration_bert\\\"\\n    ).BertConfig\\n    transformers.models.bert.modeling_bert.BertModel.config_class = custom_conf\\nexcept Exception as e:\\n    logging.warning(f\\\"[WARN] DNA_Bert_3 config patch skipped: {e}\\\")\\n\\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 3) LOGGING &amp; tiny shell helper\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nlogging.basicConfig(level=logging.INFO,\\n                    format=\\\"%(asctime)s | %(levelname)s | %(message)s\\\")\\ndef run_and_print(cmd):\\n    import subprocess, shlex, textwrap\\n    res = subprocess.run(cmd if isinstance(cmd,list) else shlex.split(cmd),\\n                         capture_output=True, text=True)\\n    if res.stdout: print(res.stdout, end=\\\"\\\")\\n    if res.stderr: print(\\\"STDERR:\\\", textwrap.shorten(res.stderr,400), end=\\\"\\\")\\n    return res\\n\\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 4) ENSURE hydra-core (local wheel) \u2013 omegaconf already present\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nrun_and_print([\\n    \\\"pip\\\",\\\"install\\\",\\\"--no-index\\\",\\\"--no-deps\\\",\\\"--force-reinstall\\\",\\n    \\\"/kaggle/input/hydra-core-132whl/hydra_core-1.3.2-py3-none-any.whl\\\"\\n])\\n\\n\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:50.049993Z\",\"iopub.execute_input\":\"2025-05-16T19:06:50.050465Z\",\"iopub.status.idle\":\"2025-05-16T19:06:51.789565Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:50.050419Z\",\"shell.execute_reply\":\"2025-05-16T19:06:51.788183Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 5) RNAPredictor CONFIG (Hydra best practices, stochastic inference)\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nfrom rna_predict.interface import RNAPredictor\\n# Import OmegaConf and torch if not already imported in the cell\\nfrom omegaconf import OmegaConf\\nimport torch\\nimport logging # Ensure logging is imported if you use logger.info\\n\\nTEST_SEQS  = \\\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\\\"\\nSAMPLE_SUB = \\\"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\\\"\\nOUTPUT_CSV = \\\"submission.csv\\\"\\n\\ndef create_predictor():\\n    \\\"\\\"\\\"Instantiate RNAPredictor with local checkpoints &amp; GPU/CPU autodetect, matching Hydra config structure.\\\"\\\"\\\"\\n    device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n    logging.info(f\\\"Device: {device}\\\") # Assuming logging is configured\\n    cfg = OmegaConf.create({\\n        # Top-level keys consistent with a full Hydra config (e.g., default.yaml)\\n        \\\"device\\\": device,\\n        \\\"seed\\\": 42, # Good for reproducibility if used by models\\n        \\\"atoms_per_residue\\\": 44, # Standard value\\n        \\\"extraction_backend\\\": \\\"dssr\\\", # Or \\\"mdanalysis\\\" as needed\\n\\n        \\\"pipeline\\\": { # General pipeline settings\\n            \\\"verbose\\\": True,\\n            \\\"save_intermediates\\\": True,\\n            # output_dir is usually set by Hydra's run directory or overridden\\n        },\\n\\n        \\\"prediction\\\": { # Prediction-specific settings\\n            \\\"repeats\\\": 5,\\n            \\\"residue_atom_choice\\\": 0,\\n            \\\"enable_stochastic_inference_for_submission\\\": True, # CRITICAL: Ensures unique predictions\\n            # \\\"submission_seeds\\\": [42, 101, 2024, 7, 1991],  # Optional: for reproducible stochastic runs\\n        },\\n\\n        \\\"model\\\": {\\n            # \u2500\u2500 Stage B: torsion-angle prediction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n            \\\"stageB\\\": {\\n                \\\"torsion_bert\\\": {\\n                    \\\"model_name_or_path\\\": \\\"/kaggle/working/rna_torsionBERT\\\", # Path to local TorsionBERT model\\n                    \\\"device\\\": device,\\n                    \\\"angle_mode\\\": \\\"degrees\\\",   # CHANGED: Set to \\\"degrees\\\" for consistency with StageC\\n                                               # This ensures StageBTorsionBertPredictor outputs angles in degrees.\\n                    \\\"num_angles\\\": 7,\\n                    \\\"max_length\\\": 512,\\n                    \\\"checkpoint_path\\\": None,   # Can be overridden if a specific checkpoint is needed\\n                    \\\"debug_logging\\\": True,     # Set to False if logs are too verbose\\n                    \\\"init_from_scratch\\\": False, # Assumes using pretrained TorsionBERT\\n                    \\\"lora\\\": {                  # LoRA config (currently disabled)\\n                        \\\"enabled\\\": False,\\n                        \\\"r\\\": 8,\\n                        \\\"alpha\\\": 16,\\n                        \\\"dropout\\\": 0.1,\\n                        \\\"target_modules\\\": [\\\"query\\\", \\\"value\\\"],\\n                    },\\n                }\\n                # Pairformer config would go here if used: \\\"pairformer\\\": { ... }\\n            },\\n            # \u2500\u2500 Stage C: 3D reconstruction (MP-NeRF) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n            \\\"stageC\\\": {\\n                \\\"enabled\\\": True,\\n                \\\"method\\\": \\\"mp_nerf\\\",\\n                \\\"do_ring_closure\\\": False,       # Consistent with default.yaml; notebook log showed True, adjust if needed.\\n                \\\"place_bases\\\": True,\\n                \\\"sugar_pucker\\\": \\\"C3'-endo\\\",\\n                \\\"device\\\": device,\\n                \\\"debug_logging\\\": True,          # Set to False if logs are too verbose\\n                \\\"angle_representation\\\": \\\"degrees\\\", # StageC expects angles in degrees from StageB\\n                \\\"use_metadata\\\": False,\\n                \\\"use_memory_efficient_kernel\\\": False,\\n                \\\"use_deepspeed_evo_attention\\\": False,\\n                \\\"use_lma\\\": False,\\n                \\\"inplace_safe\\\": False,          # Consistent with default.yaml; notebook log showed True.\\n                \\\"chunk_size\\\": None,\\n            },\\n            # \u2500\u2500 Stage D: Diffusion refinement (minimal placeholder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n            # Add full StageD config if it's actively used in this notebook\\n            \\\"stageD\\\": {\\n                \\\"enabled\\\": False, # Set to True if StageD is part of this specific notebook's pipeline\\n                \\\"mode\\\": \\\"inference\\\",\\n                \\\"device\\\": device,\\n                \\\"debug_logging\\\": True,\\n                # Placeholder for other essential StageD keys if enabled:\\n                # \\\"ref_element_size\\\": 128,\\n                # \\\"ref_atom_name_chars_size\\\": 256,\\n                # \\\"profile_size\\\": 32,\\n                # \\\"model_architecture\\\": { ... },\\n                # \\\"diffusion\\\": { ... }\\n            },\\n        }\\n    })\\n    return RNAPredictor(cfg)\\n\\n# Usage example:\\npredictor = create_predictor()\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:51.791335Z\",\"iopub.execute_input\":\"2025-05-16T19:06:51.791699Z\",\"iopub.status.idle\":\"2025-05-16T19:06:52.817529Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:51.791663Z\",\"shell.execute_reply\":\"2025-05-16T19:06:52.816481Z\"},\"collapsed\":true,\"jupyter\":{\"outputs_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# PATCH \u25b8 guarantee predict_submission returns ONE ROW per residue  \u2705\\n#         \u2022 works both when Stage C gives [L, atoms, 3]  OR  [N_atoms, 3]\\n#         \u2022 keeps all original columns created by coords_to_df\\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nimport logging, torch, pandas as pd\\nfrom rna_predict.interface import RNAPredictor\\nfrom rna_predict.utils.submission import coords_to_df, extract_atom, reshape_coords\\n\\nlog = logging.getLogger(\\\"rna_predict.patch.flat2res\\\")\\n\\ndef _predict_submission_patched(\\n    self,\\n    sequence: str,\\n    prediction_repeats: int | None = None,\\n    residue_atom_choice: int | None = None,\\n):\\n    \\\"\\\"\\\"\\n    Collapses per-atom coordinates \u2192 one canonical atom per residue.\\n    \u2022 Prefers phosphate (\u201cP\u201d); falls back to first atom per residue.\\n    \u2022 Always returns exactly len(sequence) rows, preserving coords_to_df schema.\\n    \\\"\\\"\\\"\\n    # \u2500\u2500 original prologue \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n    result      = self.predict_3d_structure(sequence)\\n    coords_flat = result[\\\"coords\\\"]                       # 2-D [N_atoms, 3]\\n\\n    # \ud83d\udd27 NEW: make it a plain tensor so .numpy() is allowed\\n    if coords_flat.requires_grad:                        # \u2190 the bug-fix\\n        coords_flat = coords_flat.detach()\\n\\n    metadata        = result.get(\\\"atom_metadata\\\", {})\\n    atom_names      = metadata.get(\\\"atom_names\\\", [])\\n    residue_indices = metadata.get(\\\"residue_indices\\\", [])\\n\\n    repeats  = prediction_repeats if prediction_repeats is not None else self.default_repeats\\n    atom_idx = residue_atom_choice if residue_atom_choice is not None else self.default_atom_choice\\n\\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n    # \u2776  FLAT-COORDS PATH   (Stage C returned [N_atoms, 3])\\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n    if coords_flat.dim() == 2 and coords_flat.shape[0] != len(sequence):\\n        if not atom_names or not residue_indices:\\n            log.error(\\\"[flat-coords] missing atom metadata \u2192 falling back to legacy per-atom output\\\")\\n            base = {\\n                \\\"ID\\\":      range(1, len(coords_flat) + 1),\\n                \\\"resname\\\": [\\\"X\\\"] * len(coords_flat),\\n                \\\"resid\\\":   range(1, len(coords_flat) + 1),\\n            }\\n            df = pd.DataFrame(base)\\n            for i in range(1, repeats + 1):\\n                df[[f\\\"{ax})\\n\\n        # pick one atom per residue (prefer P)\\n        picked = (tmp[tmp.atom_name == \\\"P\\\"]\\n                  .drop_duplicates(\\\"res0\\\", keep=\\\"first\\\")\\n                  .sort_values(\\\"res0\\\"))\\n        if len(picked) != len(sequence):        # fallback if some P\u2019s missing\\n            log.warning(\\\"[flat-coords] P-selection gave %d/%d rows \u2013 using first atom fallback\\\",\\n                        len(picked), len(sequence))\\n            picked = (tmp.groupby(\\\"res0\\\", as_index=False)\\n                         .first()\\n                         .sort_values(\\\"res0\\\"))\\n\\n        per_res_coords = torch.tensor(\\n            picked[[\\\"x\\\", \\\"y\\\", \\\"z\\\"]].values,\\n            dtype=coords_flat.dtype,\\n            device=coords_flat.device,\\n        )\\n\\n        return coords_to_df(sequence, per_res_coords, repeats)\\n\\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n    # \u2777  ORIGINAL \u201creshaped\u201d PATH  (Stage C returned [L, atoms, 3])\\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n    coords = reshape_coords(coords_flat, len(sequence))\\n    if coords.dim() == 2 and coords.shape[0] != len(sequence):\\n        # reshape failed \u2192 treat as flat once more\\n        log.warning(\\\"[reshape_coords] produced flat coords \u2013 rerouting through flat-coords logic.\\\")\\n        result[\\\"coords\\\"] = coords\\n        return }\\\" for ax in \\\"xyz\\\"]] = coords_flat.cpu().numpy()\\n            return df\\n\\n        tmp = pd.DataFrame({\\n            \\\"atom_name\\\": atom_names,\\n            \\\"res0\\\":      residue_indices,       # 0-based residue index\\n            \\\"x\\\": coords_flat[:, 0].cpu().numpy(),\\n            \\\"y\\\": coords_flat[:, 1].cpu().numpy(),\\n            \\\"z\\\": coords_flat[:, 2].cpu().numpy(),\\n        predict_submission_patched(self, sequence, prediction_repeats, residue_atom_choice)\\n\\n    atom_coords = extract_atom(coords, atom_idx)\\n    return coords_to_df(sequence, atom_coords, repeats)\\n\\n# install the patch (simple attribute assignment is enough)\\nRNAPredictor.predict_submission = _predict_submission_patched\\nlog.info(\\\"\u2713 RNAPredictor.predict_submission patched (flat-coords fix)\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:52.818688Z\",\"iopub.execute_input\":\"2025-05-16T19:06:52.818976Z\",\"iopub.status.idle\":\"2025-05-16T19:06:52.833843Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:52.818951Z\",\"shell.execute_reply\":\"2025-05-16T19:06:52.832636Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"toy = create_predictor().predict_submission(\\\"ACGUACGU\\\", prediction_repeats=1)\\nassert len(toy) == 8            # \u2705 one row per residue\\nprint(toy.head())\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:06:52.835076Z\",\"iopub.execute_input\":\"2025-05-16T19:06:52.835554Z\",\"iopub.status.idle\":\"2025-05-16T19:06:54.965454Z\",\"shell.execute_reply.started\":\"2025-05-16T19:06:52.835511Z\",\"shell.execute_reply\":\"2025-05-16T19:06:54.964350Z\"},\"collapsed\":true,\"jupyter\":{\"outputs_hidden\":true}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n# 6) PREDICTION UTILITIES  \u25cf de-duplication / aggregation safeguard  \u2705\\n# -----------------------------------------------------------------------\\n# NOTE: This cell assumes RNAPredictor has been patched with the corrected\\n#       _predict_submission_patched method in a PREVIOUS cell.\\n# Fix: Ensure _collapse_to_one_row_per_residue can correctly infer num_repeats\\n#      and handle the original sequence string for resname population.\\n# -----------------------------------------------------------------------\\nimport numpy as np\\nimport pandas as pd\\nimport logging\\nimport sys # For sys.stdout in logger handler\\nimport os # For os.path.exists\\n# Assuming create_predictor is defined in a previous cell and available in global scope\\n# Assuming RNAPredictor is imported and patched in a previous cell\\n\\n# Configure logger for this cell if not already done globally\\nlogger_cell6 = logging.getLogger(\\\"rna_predict.cell6_utils\\\")\\nif not logger_cell6.handlers:\\n    handler_cell6 = logging.StreamHandler(sys.stdout)\\n    formatter_cell6 = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\\n    handler_cell6.setFormatter(formatter_cell6)\\n    logger_cell6.addHandler(handler_cell6)\\nlogger_cell6.setLevel(logging.INFO)\\n\\n\\ndef _auto_column(df: pd.DataFrame, pref: list[str]) -&gt; str:\\n    \\\"\\\"\\\"Return first column present in pref (fallback \u2192 df.columns[0]).\\\"\\\"\\\"\\n    for c in pref:\\n        if c in df.columns:\\n            return c\\n    if df.columns.empty: # Guard against empty DataFrame columns\\n        logger_cell6.warning(\\\"_auto_column called with DataFrame with no columns.\\\")\\n        return \\\"\\\" # Or raise an error, depending on desired behavior\\n    return df.columns[0]\\n\\n\\ndef _collapse_to_one_row_per_residue(df_raw: pd.DataFrame, seq_id: str, original_sequence: str, num_repeats: int) -&gt; pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Ensure the DataFrame has one row per residue with clean IDs and standard columns.\\n    This function now primarily serves as a schema enforcer and final sanity check.\\n    \\\"\\\"\\\"\\n    df = df_raw.copy()\\n\\n    if df.empty:\\n        logger_cell6.warning(f\\\"_collapse_to_one_row_per_residue received empty DataFrame for seq_id: {seq_id}\\\")\\n        # Define expected columns based on num_repeats for an empty DataFrame\\n        cols = [\\\"ID\\\", \\\"resname\\\", \\\"resid\\\"] + [f\\\"{ax}\\\" for k in range(1, num_repeats + 1) for ax in \\\"xyz\\\"]\\n        return pd.DataFrame(columns=cols)\\n\\n    # 1\ufe0f\u20e3 &amp; 2\ufe0f\u20e3: First repeat/angle only &amp; Canonical atom selection\\n    # These steps are assumed to have been handled by the patched <code>_predict_submission_patched</code>\\n    # The input <code>df_raw</code> should ideally be one row per residue with all 5 repeats as columns.\\n\\n    # 3\ufe0f\u20e3 Safeguard: Average duplicates if <code>resid</code> is not unique (shouldn't happen if patched predict_submission is correct)\\n    if \\\"resid\\\" in df.columns and not df[\\\"resid\\\"].is_unique:\\n        logger_cell6.warning(f\\\"Residues in DataFrame for {seq_id} are not unique. Attempting to average duplicates.\\\")\\n        coord_cols = [c for c in df.columns if c.startswith(('x_', 'y_', 'z_'))]\\n        key_cols_present = [k for k in [\\\"resid\\\", \\\"resname\\\"] if k in df.columns]\\n        if key_cols_present:\\n            df = (\\n                df.groupby(key_cols_present, as_index=False, sort=False)[coord_cols].mean()\\n                  .reset_index(drop=True)\\n            )\\n    \\n    # 4\ufe0f\u20e3 Ensure 'resid' is 1-based sequential and 'ID' is correctly formatted\\n    # This is important because the input <code>df_raw</code> from the patched <code>predict_submission</code>\\n    # should already have <code>len(sequence)</code> rows.\\n    if not df.empty:\\n        if \\\"resid\\\" not in df.columns or not pd.api.types.is_numeric_dtype(df[\\\"resid\\\"]) or df[\\\"resid\\\"].isnull().any() or not df[\\\"resid\\\"].is_monotonic_increasing:\\n             df[\\\"resid\\\"] = np.arange(1, len(df) + 1) # Re-create if problematic\\n        \\n        if \\\"resname\\\" not in df.columns: # Ensure resname column exists\\n             df[\\\"resname\\\"] = list(original_sequence)[:len(df)] if len(original_sequence) &gt;= len(df) else (list(original_sequence) + ['X'](len(df)-len(original_sequence)))\\n        \\n        if \\\"ID\\\" in df.columns:\\n            df = df.drop(columns=\\\"ID\\\")\\n        df.insert(0, \\\"ID\\\", [f\\\"{seq_id}{r}\\\" for r in df[\\\"resid\\\"]])\\n\\n\\n    # 5\ufe0f\u20e3 Ensure all required coordinate columns (x_1..z_5 etc.) exist and have the correct names\\n    # The number of repeats is now taken from the argument.\\n    expected_coord_cols = [f\\\"{ax}. Adding with NaNs/defaults.\\\")\\n            if col.startswith(('x_', 'y_', 'z_')):\\n                df[col] = np.nan\\n            elif col == \\\"resname\\\":\\n                 df[col] = list(original_sequence)[:len(df)] if len(original_sequence) &gt;= len(df) else (list(original_sequence) + ['X']}\\\" for i in range(num_repeats) for ax in \\\"xyz\\\"]\\n    final_cols_schema = [\\\"ID\\\", \\\"resname\\\", \\\"resid\\\"] + expected_coord_cols\\n    \\n    for col in final_cols_schema:\\n        if col not in df.columns:\\n            logger_cell6.warning(f\\\"Column '{col}' missing in DataFrame for {seq_id(len(df)-len(original_sequence)))\\n            elif col == \\\"resid\\\" and \\\"resid\\\" not in df.columns : # Should have been handled\\n                 df[col] = np.arange(1, len(df)+1) if not df.empty else []\\n            elif col == \\\"ID\\\" and \\\"ID\\\" not in df.columns and \\\"resid\\\" in df.columns : # Should have been handled\\n                 df.insert(0, \\\"ID\\\", [f\\\"{seq_id}{r}\\\" for r in df[\\\"resid\\\"]])\\n            elif not df.empty : # For any other unexpected missing column\\n                 df[col] = \\\"\\\" \\n    \\n    # Return only the columns expected in the submission, in the correct order\\n    return df[final_cols_schema]\\n\\n\\ndef process_test_sequences(test_csv: str, sample_csv: str, out_csv: str, , batch: int = 1):\\n    \\\"\\\"\\\"Generate submission file after collapsing predictions.\\\"\\\"\\\"\\n\\n    df_test = pd.read_csv(test_csv)\\n    logging.info(\\\"Loaded %d sequences for processing.\\\", len(df_test)) # Use global logging or logger_cell6\\n\\n    # create_predictor() is defined in the preceding cell (Cell 12 in your notebook structure)\\n    predictor = create_predictor()\\n    # Get number of repeats from the predictor's configuration\\n    num_repeats = predictor.prediction_config.repeats\\n\\n\\n    id_col  = auto_column(df_test, [\\\"id\\\", \\\"ID\\\", \\\"seq_id\\\", \\\"sequence_id\\\"])\\n    seq_col = _auto_column(df_test, [\\\"sequence\\\", \\\"Sequence\\\", \\\"seq\\\", \\\"SEQ\\\"])\\n\\n    frames: list[pd.DataFrame] = []\\n    for start_idx in range(0, len(df_test), batch): # Renamed 'start' to 'start_idx' to avoid conflict\\n        end_idx = min(start_idx + batch, len(df_test))\\n        logging.info(\\\"Processing batch: %d\u2013%d\\\", start_idx + 1, end_idx) # Use global logging or logger_cell6\\n        \\n        for i in range(start_idx, end_idx):\\n            sid = df_test.at[i, id_col]\\n            seq_str = df_test.at[i, seq_col] # Store the original sequence string\\n\\n            # Pass current_target_id to the predictor if your patched method uses it\\n            if hasattr(predictor, 'current_target_id'):\\n                 predictor.current_target_id = sid\\n\\n            if not isinstance(seq_str, str) or not seq_str:\\n                logging.warning(f\\\"Skipping invalid or empty sequence for ID {sid} at index {i}.\\\")\\n                # Create an empty DataFrame with correct columns\\n                temp_df_empty = pd.DataFrame(columns=[\\\"ID\\\", \\\"resname\\\", \\\"resid\\\"] + [f\\\"{ax}\\\" for k in range(1, num_repeats + 1) for ax in \\\"xyz\\\"])\\n                frames.append(temp_df_empty)\\n                continue\\n            \\n            try:\\n                # The patched predict_submission is called here.\\n                # It should return a DataFrame that is already one-row-per-residue.\\n                raw_predictions_df  = predictor.predict_submission(seq_str) # No need to pass repeats if it uses self.config\\n                \\n                # Call collapse_to_one_row_per_residue for final schema enforcement\\n                tidy_predictions_df = _collapse_to_one_row_per_residue(raw_predictions_df, sid, seq_str, num_repeats)\\n                frames.append(tidy_predictions_df)\\n            except Exception as err:\\n                logging.error(\\\"Sequence %s (ID: %s) failed prediction: %s\\\", seq_str[:30]+\\\"...\\\", sid, err, exc_info=True)\\n\\n\\n    if not frames:\\n        logging.error(\\\"No successful predictions to concatenate. submission.csv will be empty or not created.\\\")\\n        sample_submission_df = pd.read_csv(sample_csv)\\n        empty_submission_df = pd.DataFrame(columns=sample_submission_df.columns)\\n        empty_submission_df.to_csv(out_csv, index=False)\\n        return empty_submission_df\\n\\n    results_df = pd.concat(frames, ignore_index=True)\\n    \\n    # Final alignment with sample_submission.csv to ensure exact format and row order\\n    try:\\n        sample_submission_df = pd.read_csv(sample_csv)\\n        \\n        # Ensure 'ID' column in results_df matches the sample submission's ID format for merging\\n        # The _collapse_to_one_row_per_residue should have formatted 'ID' correctly.\\n        \\n        # Use a left merge to ensure all IDs from sample_submission are present and in order\\n        final_submission_df = pd.merge(sample_submission_df[['ID']], # Only take ID column for merging keys\\n                                       results_df, \\n                                       on='ID', \\n                                       how='left') # Use left merge to keep all sample submission IDs\\n        \\n        # Fill NaNs for any coordinates that might be missing after the merge (e.g., if a sequence failed or had fewer residues)\\n        # The columns in final_submission_df will be based on results_df after the merge.\\n        # We need to ensure all columns from sample_submission_df are present.\\n        for col in sample_submission_df.columns:\\n            if col not in final_submission_df.columns:\\n                final_submission_df[col] = np.nan # Add missing columns with NaNs\\n            if col.startswith(('x', 'y_', 'z_')): # Fill NaNs in coordinate columns with 0.0\\n                final_submission_df[col] = final_submission_df[col].fillna(0.0)\\n            elif col in [\\\"resname\\\", \\\"resid\\\"] and col in final_submission_df.columns: # Fill NaNs in resname/resid if they exist\\n                # This part might need more sophisticated filling if a whole sequence was missing\\n                # For now, simple fillna. If <code>resname</code> or <code>resid</code> are entirely NaN for a sequence,\\n                # they will remain NaN unless sample_submission has values for those IDs.\\n                # A more robust fill would re-derive from sample_submission for missing ID rows.\\n                pass # Let merge handle this; if ID was in sample but not results, resname/resid will be NaN\\n\\n        # Ensure exact column order and presence as in sample_submission.csv\\n        final_submission_df = final_submission_df[sample_submission_df.columns.tolist()]\\n\\n    except Exception as e:\\n        log.error(f\\\"Error aligning submission with sample_submission.csv: {e}. Saving raw concatenated results.\\\")\\n        final_submission_df = results_df # Fallback to saving whatever was concatenated\\n\\n    final_submission_df.to_csv(out_csv, index=False)\\n    logging.info(\\\"Saved final submission to \u2192 %s  (#rows = %s)\\\", out_csv, f\\\"{len(final_submission_df):,}\\\")\\n    return final_submission_df\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:27.588545Z\",\"iopub.execute_input\":\"2025-05-16T19:10:27.589142Z\",\"iopub.status.idle\":\"2025-05-16T19:10:27.621046Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:27.589084Z\",\"shell.execute_reply\":\"2025-05-16T19:10:27.619148Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n# 7) TOY SANITY-CHECK \u2013 demonstrates collapse function\\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\nprint(\\\"\\n=== Toy sanity-check ===\\\")\\n# Assuming create_predictor() and collapse_to_one_row_per_residue are defined\\n# in previous cells and available in the global scope.\\n# Also assuming predictor is instantiated via predictor = create_predictor()\\n\\ntoy_sequence = \\\"ACGUACGU\\\"\\nnum_toy_repeats = 2 # Or get from predictor.prediction_config.repeats if predictor is already created\\n\\n# Ensure predictor is created if not already (it was in your previous cell structure)\\n# If predictor is not yet created in this cell's context, uncomment the next line:\\n# predictor = create_predictor() \\n# num_toy_repeats = predictor.prediction_config.repeats # More robust way to get repeats\\n\\ntoy_raw  = predictor.predict_submission(toy_sequence, prediction_repeats=num_toy_repeats)\\n\\n# Call _collapse_to_one_row_per_residue with the required arguments\\ntoy_comp = _collapse_to_one_row_per_residue(toy_raw, \\\"TOY\\\", toy_sequence, num_toy_repeats)\\nprint(toy_comp.head())\\n\\n# The uniqueness check for the toy example also needs num_toy_repeats\\ncoords_cols_toy = [f\\\"x'] = test_pred_z\\n\\n# Adjust ID format: ID + \\\"}\\\" for i in range(num_toy_repeats)] + \\\\n                  [f\\\"y_{i+1}\\\" for i in range(num_toy_repeats)] + \\\\n                  [f\\\"z_{i+1}\\\" for i in range(num_toy_repeats)]\\nunique_structs_toy = set()\\nif not toy_raw.empty: # Check if toy_raw is not empty before proceeding\\n    for i in range(num_toy_repeats): \\n        x_col, y_col, z_col = f\\\"x_{i+1}\\\", f\\\"y_{i+1}\\\", f\\\"z_{i+1}\\\"\\n        # Ensure all necessary columns exist in toy_raw before trying to access them\\n        if all(col in toy_raw.columns for col in [x_col, y_col, z_col]):\\n            coords_tuple = tuple(toy_raw[[x_col, y_col, z_col]].to_numpy().flatten())\\n            unique_structs_toy.add(coords_tuple)\\n        else:\\n            log.warning(f\\\"Coordinate columns for repeat {i+1} (e.g., {x_col}) not found in toy_raw DataFrame. Columns available: {toy_raw.columns.tolist()}\\\")\\n\\n\\nif not toy_raw.empty:\\n    log.info(f\\\"Toy example: Found {len(unique_structs_toy)} unique structures out of {num_toy_repeats} repeats.\\\")\\n    if len(unique_structs_toy) &lt; num_toy_repeats and len(unique_structs_toy) &gt; 1 :\\n        log.warning(f\\\"Toy example: Less than {num_toy_repeats} unique structures, but more than 1. Stochasticity is partial.\\\")\\n    elif len(unique_structs_toy) == 1 and num_toy_repeats &gt; 1: # Check only if more than 1 repeat was expected\\n        log.error(\\\"Toy example: FAILURE - Only 1 unique structure found. Stochasticity is NOT working.\\\")\\n    elif len(unique_structs_toy) == num_toy_repeats:\\n        log.info(f\\\"Toy example: SUCCESS - {num_toy_repeats} unique structures found!\\\")\\nelse:\\n    log.warning(\\\"Toy example: toy_raw DataFrame is empty. Cannot check uniqueness.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:31.902867Z\",\"iopub.execute_input\":\"2025-05-16T19:10:31.903280Z\",\"iopub.status.idle\":\"2025-05-16T19:10:33.306387Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:31.903244Z\",\"shell.execute_reply\":\"2025-05-16T19:10:33.304982Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"\\\"\\\"\\\"\\nCell 11: GENERATE PREDICTIONS &amp; BUILD SUBMISSION\\n------------------------------------------------\\nWe'll predict (x_1, y_1, z_1) for each residue, \\nthen replicate those coordinates for structures x_2..z_5.\\nFinally, we'll align with sample_submission and save submission.csv.\\n\\\"\\\"\\\"\\n\\n# Predict x_1, y_1, z_1\\n#test_pred_x = model_x.predict(test_merged_imputed)\\n#test_pred_y = model_y.predict(test_merged_imputed)\\n#test_pred_z = model_z.predict(test_merged_imputed)\\n\\n# Build submission from test_clean_df\\n#submission = test_clean_df.copy()\\n\\n# Add predicted coords for structure 1\\n#submission['x_1'] = test_pred_x\\n#submission['y_1'] = test_pred_y\\n#submission['z_1'] = test_pred_z\\n\\n# For simplicity, replicate for structures 2..5\\n#for i in [2,3,4,5]:\\n#    submission[f'x_{i}'] = test_pred_x\\n#    submission[f'y_{i}'] = test_pred_y\\n#    submission[f'z_{i\\\" + resid\\n#submission['ID'] = submission['ID'] + \\\"\\\"  + submission['resid'].astype(str)\\n\\n# Reorder columns to match sample_submission\\n#final_cols = list(sample_submission.columns)  # ID, resname, resid, x_1..z_5\\n#submission = submission[['ID','resname','resid',\\n#                         'x_1','y_1','z_1',\\n#                         'x_2','y_2','z_2',\\n#                         'x_3','y_3','z_3',\\n#                         'x_4','y_4','z_4',\\n#                         'x_5','y_5','z_5']]\\n\\n# Merge with sample_submission to match row order\\n#sample_submission['sort_order'] = range(len(sample_submission))\\n#submission_merged = pd.merge(\\n#    submission,\\n#    sample_submission[['ID','sort_order']],\\n#    on='ID',\\n#    how='left'\\n#).sort_values('sort_order').drop(columns='sort_order')\\n\\n# This is our final submission dataframe\\n#submission_df = submission_merged.copy()\\n\\n# Save to CSV\\n#submission_df.to_csv(\\\"submission.csv\\\", index=False)\\n#logging.info(\\\"submission.csv created successfully.\\\")\\n\\n#print(\\\"Cell 11 complete: Submission file saved. Ready to submit!\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:43.067818Z\",\"iopub.execute_input\":\"2025-05-16T19:10:43.068210Z\",\"iopub.status.idle\":\"2025-05-16T19:10:43.075969Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:43.068176Z\",\"shell.execute_reply\":\"2025-05-16T19:10:43.074668Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# Cell 12: CONCLUSIONS &amp; NEXT STEPS\\n# ---------------------------------\\n'''\\nWe've done:\\n- Group-based imputation\\n- Preserved resname\\n- Hyperparameter tuning via RandomizedSearchCV\\n- Final training on full combined data\\n- Test predictions with the same coordinate repeated across 5 structures\\n\\nSuggestions for further improvement:\\n- Fine-tune hyperparameters with a broader search or Bayesian optimization\\n- Explore more advanced RNA 3D features\\n- Generate truly distinct 5 structures instead of repeating the same coordinates\\n'''\\nlogging.info(\\\"Notebook complete. Good luck on the leaderboard!\\\")\\nprint(\\\"All done! Submit 'submission.csv' to the competition.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:45.626246Z\",\"iopub.execute_input\":\"2025-05-16T19:10:45.626596Z\",\"iopub.status.idle\":\"2025-05-16T19:10:45.632229Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:45.626571Z\",\"shell.execute_reply\":\"2025-05-16T19:10:45.630904Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"%%bash\\n# Cell: show what\u2019s inside every mounted Kaggle dataset  \ud83d\udd0d\\n# --------------------------------------------------------\\necho -e \\\"\\n\ud83d\udcc2  Listing the first two levels of /kaggle/working \u2026\\n\\\"\\n\\n# Change depth (-maxdepth) if you want more or fewer levels\\nfind /kaggle/working -maxdepth 2 -mindepth 1 -print | sed 's|^|  |'\\n\\necho -e \\\"\\n\u2705  Done.\\n\\\"\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:48.660294Z\",\"iopub.execute_input\":\"2025-05-16T19:10:48.660672Z\",\"iopub.status.idle\":\"2025-05-16T19:10:48.676199Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:48.660645Z\",\"shell.execute_reply\":\"2025-05-16T19:10:48.675021Z\"}},\"outputs\":[],\"execution_count\":null},{\"cell_type\":\"code\",\"source\":\"# Cell : sanity-check submission.csv against test_sequences.csv  \u2705\\n# ----------------------------------------------------------------\\nimport pandas as pd, pathlib, textwrap, sys, itertools, numpy as np\\n\\nTEST_CSV = \\\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\\\"\\nSUB_CSV  = \\\"submission.csv\\\"\\nTOL      = 1.0  # \u00c5 \u2013 treat coords within \u00b11 \u00c5 as identical\\n\\n# \u2500\u2500 0)  helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\ndef auto_col(df, pref):\\n    for c in pref:\\n        if c in df.columns:\\n            return c\\n    return df.columns[0]\\n\\ndef preview(s, n=5):\\n    lst = list(s)\\n    return \\\", \\\".join(lst[:n]) + (\\\" \u2026\\\" if len(lst) &gt; n else \\\"\\\")\\n\\n# \u2500\u2500 1)  load / basic info \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nfor f in (TEST_CSV, SUB_CSV):\\n    if not pathlib.Path(f).is_file():\\n        sys.exit(f\\\"[ERROR] {f} not found!\\\")\\n\\ntest_sequences = pd.read_csv(TEST_CSV)\\nsubmission     = pd.read_csv(SUB_CSV)\\n\\nid_col_test = auto_col(test_sequences, [\\\"ID\\\", \\\"id\\\", \\\"seq_id\\\", \\\"sequence_id\\\"])\\nid_col_sub  = auto_col(submission,     [\\\"ID\\\", \\\"id\\\", \\\"seq_id\\\", \\\"sequence_id\\\"])\\n\\n# \u2500\u2500 2)  expected vs actual rows \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nexpected_rows = test_sequences[\\\"sequence\\\"].str.len().sum()\\nprint(\\\"\\n\u2501\u2501 Summary \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\\")\\nprint(f\\\"Expected rows        : {expected_rows:,}\\\")\\nprint(f\\\"submission.csv rows  : {len(submission):,}\\\")\\ndupes = submission[id_col_sub].duplicated().sum()\\nprint(f\\\"Duplicate {id_col_sub!r} rows : {dupes:,}\\\")\\n\\n# \u2500\u2500 3)  build the full ID set   \\\"\\\"  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nfull_id_set = {\\n    f\\\"{sid}\\\")\\nif missing: print(\\\"  \u2192 first few missing :\\\", preview(missing))\\nif extra:   print(\\\"  \u2192 first few extras  :\\\", preview(extra))\\n\\n# \u2500\u2500 4)  per-sequence coverage (how many residues per sequence?) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nseq_len = test_sequences.set_index(id_col_test)[\\\"sequence\\\"].str.len()\\n\\n# }\\\"\\n    for sid, seq in zip(test_sequences[id_col_test], test_sequences[\\\"sequence\\\"])\\n    for idx in range(1, len(seq) + 1)\\n}\\nsub_id_set = set(submission[id_col_sub].astype(str))\\n\\nmissing = full_id_set - sub_id_set\\nextra   = sub_id_set  - full_id_set\\n\\nprint(\\\"\\n\u2501\u2501 ID reconciliation \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\\")\\nprint(f\\\"IDs missing from submission : {len(missing):,}\\\")\\nprint(f\\\"Unexpected extra IDs        : {len(extra):,FIXED LINE BELOW* \u2013 use expand=True to ensure a 1-D Series (avoids ndarray shape (n, 3))\\nprefixes = (\\n    submission[id_col_sub]\\n    .astype(str)\\n    .str.rsplit(\\\"\\\", n=1, expand=True)[0]   # returns a Series, not a nested ndarray\\n)\\n\\ncoverage = prefixes.value_counts().reindex(seq_len.index).fillna(0).astype(int)\\nbad_cov  = coverage[coverage != seq_len]\\n\\nprint(\\\"\\n\u2501\u2501 Per-sequence coverage \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\\")\\nprint(f\\\"Sequences with wrong #rows : {len(bad_cov):,}\\\")\\nif len(bad_cov):\\n    print(\\\"  id  | expected | got\\\")\\n    for sid, got in itertools.islice(bad_cov.items(), 5):\\n        print(f\\\" {sid:&lt;6}| {seq_len[sid]:&gt;8} | {got}\\\")\\n\\n# \u2500\u2500 5)  column sanity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\nREQ_COLS = [\\\"ID\\\", \\\"resname\\\", \\\"resid\\\"] + [f\\\"{ax}{i}\\\" for i in range(1, 6) for ax in \\\"xyz\\\"]\\nmissing_cols = [c for c in REQ_COLS if c not in submission.columns]\\n\\nprint(\\\"\\n\u2501\u2501 Column sanity \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\\")\\nprint(f\\\"Missing required columns   : {len(missing_cols)}\\\")\\nif missing_cols:\\n    print(textwrap.fill(\\\", \\\".join(missing_cols), width=88))\\n\\n# \u2500\u2500 6)  structure-repeat uniqueness \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\ntrip_cols = np.array([[f\\\"{ax}}\\\" for ax in \\\"xyz\\\"] for i in range(1, 6)])\\ncoords = submission[trip_cols.flatten()].values.reshape(len(submission), 5, 3)\\n\\ndef unique_triplet_count(row):\\n    \\\"\\\"\\\"Return #unique (x,y,z) triplets in a 5\u00d73 slice.\\\"\\\"\\\"\\n    uniq = []\\n    for v in row:\\n        if not any(np.allclose(v, u, atol=TOL) for u in uniq):\\n            uniq.append(v)\\n    return len(uniq)\\n\\n# \ud83d\udc49 replace apply_along_axis with a 1-liner list-comprehension  \u2705\\nuniq_counts = np.array([unique_triplet_count(row) for row in coords])\\n\\nall_identical = (uniq_counts == 1).sum()\\ntruly_unique  = (uniq_counts &gt; 1).sum()\\n\\nprint(\\\"\\n\u2501\u2501 Structure-repeat uniqueness \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\\")\\nprint(f\\\"Rows where 5 structures are identical : {all_identical:,}\\\")\\nprint(f\\\"Rows with \u22652 distinct triplets         : {truly_unique:,}\\\")\\n\\n# Per-sequence share of unique repeats\\nsub_seq_id = prefixes.to_numpy()   # 1-D array of sequence IDs\\nper_seq_unique = (\\n    pd.Series(uniq_counts &gt; 1, index=sub_seq_id)\\n      .groupby(level=0).mean()\\n      .sort_values(ascending=False)\\n)\\n\\nprint(\\\"\\nTop 5 sequences with most unique repeats:\\\")\\nfor sid, frac in per_seq_unique.head(5).items():\\n    print(f\\\"  {sid:&lt;6}: {frac:6.1%} rows diversified\\\")\\n\\nprint(\\\"\\n\u2705  Sanity check finished.\\\")\",\"metadata\":{\"trusted\":true,\"execution\":{\"iopub.status.busy\":\"2025-05-16T19:10:52.085507Z\",\"iopub.execute_input\":\"2025-05-16T19:10:52.085878Z\",\"iopub.status.idle\":\"2025-05-16T19:10:52.110042Z\",\"shell.execute_reply.started\":\"2025-05-16T19:10:52.085848Z\",\"shell.execute_reply\":\"2025-05-16T19:10:52.108388Z\"}},\"outputs\":[],\"execution_count\":null}] <p>RNA_PREDICT Pipeline in Kaggle: Notebook Workflow and Refactoring Plan</p> <p>Current Notebook Workflow: Training, Evaluation &amp; Inference</p> <p>Training Phase: In the current rna-predict.ipynb pipeline, model training is not performed within the Kaggle notebook \u2013 instead a pre-trained model is used for inference. The codebase does include a training script (rna_predict/training/train.py) that leverages PyTorch Lightning to train pipeline models, but the notebook itself did not run a full training loop for any stage \ufffc. This implies that the heavy lifting (training Stage B\u2019s torsion predictor, etc.) was done offline or in a separate process. For example, the Stage B TorsionBERT model is loaded from a checkpoint (/kaggle/working/rna_torsionBERT) rather than trained from scratch in the notebook \ufffc \ufffc. If fine-tuning were desired, one could utilize the provided training script or implement a manual training loop (since the stage scripts themselves don\u2019t contain an epoch-based loop \ufffc), but in the Kaggle context the notebook sticks to using pre-trained weights for speed.</p> <p>Evaluation Phase: The notebook does not explicitly perform a separate evaluation on a validation set during its run. While the Kaggle dataset provides a validation set (with ~12 RNAs) and their labels, the pipeline doesn\u2019t compute metrics like TM-score in the notebook. Instead, any validation would be done offline or by examining sample outputs. (Kaggle\u2019s official metric is TM-score, ranging 0\u20131, comparing predicted vs. true 3D coordinates \ufffc.) The pipeline does include a \u201ctoy sanity-check\u201d where a small known RNA structure is predicted and compared, essentially verifying that the reconstruction is sensible. In the logs, we see a short sequence ACGUACGU run through the pipeline and a toy output printed for comparison \ufffc \ufffc. This suggests the authors manually checked prediction accuracy on a tiny example (possibly comparing predicted coordinates to known coordinates for 5 residues labeled TOY_1\u2026TOY_5). Beyond this, the notebook\u2019s focus is generating the submission for the test set rather than quantitatively evaluating predictions within the notebook.</p> <p>Inference &amp; Submission Generation: The core of the notebook is the inference pipeline that processes the Kaggle test sequences and produces the submission file. The workflow is roughly:     \u2022   Data Loading: The notebook reads the test sequences from test_sequences.csv (provided in the competition data). Each RNA sequence (string of nucleotides) is taken as input for prediction. In the Kaggle environment, the input data directory stanford-rna-3d-folding/ contains test_sequences.csv, train_sequences.csv (and .v2), train_labels (as PDB files), etc \ufffc \ufffc. The code likely iterates through each test target_id and its sequence.     \u2022   Stage B \u2013 Torsion Angle Prediction: For each sequence, the notebook invokes the Stage B TorsionBERT predictor to obtain backbone torsion angles. This stage uses a HuggingFace Transformer-based model (a \u201clanguage model\u201d for RNA) that reads the RNA sequence (and potentially an MSA or other features) and outputs predicted torsion angles per residue \ufffc \ufffc. In the logs, we see Stage B being initialized and the TorsionBERT model loaded \ufffc. The model outputs 7 values per nucleotide (corresponding to 7 torsion angles in the RNA backbone + glycosidic bond). The pipeline performs a post-processing step to ensure the angles are in degrees: the log shows that the output dimension is 7 (matching the expected number of angles) and a warning that the values \u201clook like radians, converting to degrees\u201d \ufffc. This suggests the model might output angles in radians which are then converted to degrees for consistency with Stage C\u2019s expectations. No LoRA adaptation is applied during inference (the log confirms LoRA was not enabled or configured, so all model parameters remain as loaded \ufffc).     \u2022   Stage C \u2013 3D Coordinate Reconstruction: Next, the predicted torsion angles feed into Stage C reconstruction to build 3D coordinates of the RNA\u2019s atoms. Stage C uses an Mp-NERF based forward-kinematics approach to convert dihedral angles into Cartesian coordinates \ufffc \ufffc. In practice, Stage C takes each sequence\u2019s torsion angle set and produces coordinates for the chain\u2019s backbone and sugar (and places bases if enabled). The configuration in the logs shows Stage C is running with method: \"mp_nerf\" and various options: ring closure enforcement, base placement, a fixed sugar pucker (C3'-endo), etc \ufffc. All Stage C computation was done on CPU in the notebook (device set to cpu in config) \ufffc. Despite running on CPU, Stage C is fast (e.g. ~0.2\u20130.5s for ~70\u2013150 residues) given it\u2019s a deterministic geometry construction. The output of Stage C is a set of 3D coordinates for each atom in the RNA. Crucially, for submission we only need the coordinates of the C1\u2032 atom of each nucleotide (the carbon atom connecting the base to the sugar, used as the representative position of the nucleotide). The pipeline extracts those: we see in the log an output table with columns ID, resname, resid, x_1, y_1, z_1 (and further columns for additional predictions) \ufffc. Each row corresponds to one nucleotide\u2019s C1\u2032 coordinates. For example, after Stage C for a toy sequence, the log shows residues 1\u20135 (A, C, G, U, A) and their coordinates \ufffc. Similarly, for actual test sequences, Stage C outputs each residue\u2019s coordinates (the log shows lines with each resid and coordinates for sequences of length 69, 157, etc. in the test set) \ufffc \ufffc.     \u2022   Submission Assembly: As each test sequence is processed, its predicted coordinates are accumulated into a submission format. The Kaggle submission file requires a row for every nucleotide of every target, with specific columns (explained in detail below). The notebook likely either builds a pandas DataFrame or writes lines incrementally. To ensure the output matches the required format, the code uses the provided sample_submission.csv as a template. We see at the end of the logs that after processing all sequences, the notebook prints \u201cAll done! Submit \u2018submission.csv\u2019 to the competition.\u201d and lists the file \ufffc. It then reports the submission file has 2,515 rows with no missing or extra IDs \ufffc, indicating it exactly matches the sample submission indices. This means the submission covers every residue of each test RNA once, with no duplicates, as expected.</p> <p>In summary, the current notebook essentially loads a pre-trained Stage B model, runs Stage B and Stage C for each RNA sequence in the test set, and writes out the C1\u2032 atom coordinates (five times per residue, as columns, to satisfy the competition\u2019s multi-prediction requirement) in submission.csv. There is no model training happening in-notebook and no explicit metric evaluation; it is a straight pipeline from sequence to predicted structure.</p> <p>Key Components and Dependencies</p> <p>Stage B \u2013 TorsionBERT Angle Predictor</p> <p>Stage B is powered by TorsionBERT, a Transformer-based model specialized for RNA. It takes an RNA sequence (and potentially structural context like base-pair adjacency or an MSA) and outputs predicted torsion angles for each residue \ufffc \ufffc. In the RNA_PREDICT pipeline, StageBTorsionBertPredictor is the class wrapping this model. The implementation builds on HuggingFace Transformers, so it includes a tokenizer and model loaded from a model name or path. In the Kaggle notebook, the model was loaded from the working directory path /kaggle/working/rna_torsionBERT \ufffc (likely a directory with a pre-trained model checkpoint).</p> <p>Outputs: TorsionBERT produces 7 angles per nucleotide, corresponding to the RNA backbone\u2019s dihedral angles (typically \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, and the \u03c7 glycosidic torsion). These may be represented or predicted in radians internally. The pipeline checks the output dimension (7) and then issues warnings about unit conversion: \u201cAssuming model outputs degrees directly\u2026 Values look like radians, converting to degrees.\u201d \ufffc. Thus, the angles are converted to degrees before passing to Stage C. The Stage B module ensures the angle data shape and type are correct. It also logs the model configuration used (model path, max length, device, etc.) for transparency \ufffc. Notably, the pipeline had a feature for LoRA (Low-Rank Adapters) to allow fine-tuning part of the model, but in this run LoRA was not applied (the log explicitly says \u201cLoRA not applied\u2026 All params trainable\u201d and it proceeded with the base model) \ufffc. This means the full transformer weights were used as-is, with no lightweight adaptation in place.</p> <p>Dependencies: Stage B depends on HuggingFace for the model architecture and tokenizer. It likely uses a BERT-like architecture pre-trained on RNA sequences with known structures. For input features, Stage B can incorporate secondary structure (adjacency matrix from Stage A) or MSA data, as hinted by the design (the pipeline\u2019s plan was to feed Stage B with base-pair info) \ufffc. However, in the Kaggle notebook run, Stage A was skipped and adjacency_matrix=None was passed into Stage B. The TorsionBERT in this context probably just used the raw sequence (and possibly the provided MSA indirectly, if the model or code was configured to use MSA features \u2013 though there\u2019s no explicit log of MSA usage). For simplicity, one can assume Stage B here operated purely sequence-to-angles.</p> <p>In a refactored setup, Stage B\u2019s main requirement is the availability of a trained TorsionBERT model. We will either load this from a checkpoint (for inference) or have the capability to train/fine-tune it on the Kaggle training data. If training from scratch, we need the train labels converted to torsion angles. (The Kaggle train_labels.csv gives atomic coordinates, so an offline preprocessing to derive torsion angles would be needed to supervise TorsionBERT. Given the complexity, using the already pre-trained model is the quickest path.) For the Kaggle wrapper, we will treat TorsionBERT as a black-box predictor: initialize it, feed sequences, get angle predictions.</p> <p>Stage C \u2013 3D Structure Reconstruction</p> <p>Stage C takes the torsion angles and constructs 3D coordinates for the RNA. The StageCReconstruction component implements a deterministic conversion using an algorithm akin to Nerf (Natural Extension Reference Frame) for building coordinates from dihedrals. In this pipeline, Stage C is configured to use an in-house \u201cmp_nerf\u201d method (likely optimized for RNA) \ufffc \ufffc. It enforces proper geometry such as closing the sugar ring and placing the bases. Key parameters include: do_ring_closure=True (ensures the ribose ring is closed properly), place_bases=True (attaches bases to the sugar-phosphate backbone), and a fixed sugar pucker conformation (\"C3'-endo\", the typical RNA sugar pucker) \ufffc. These choices mean Stage C does not predict these geometric variations \u2013 it uses standard values to place atoms.</p> <p>Outputs: Stage C outputs the full 3D coordinates of each atom in the RNA chain. For an RNA with N residues, the total atoms could be ~N\u00d744 (if including all atoms and hydrogen), but heavy-atom count is around N\u00d720\u201325. The log confirms the scale: e.g., for a 69-residue RNA, Stage C produced 1464 atoms \ufffc (\u224821 atoms per residue, likely heavy atoms only). Each residue\u2019s key atoms (phosphate, sugars, bases) are positioned. Among them, the C1\u2032 atom of each residue is singled out for evaluation and submission. The pipeline after Stage C extracts each residue\u2019s C1\u2032 coordinates to form the output table. In the log snippet for the toy example, we see one line per residue with resname (nucleotide), resid (residue index), and coordinates \ufffc. The ID column in submission is a combination of the target ID and residue index (like 101D_1 for target 101D, residue 1) \ufffc \ufffc. The notebook ensures these IDs match the sample submission.</p> <p>Stage C has no learnable parameters; it\u2019s purely a function of the input angles. Its dependencies are numerical libraries (PyTorch for tensor math and geometry) and possibly some custom code for the mp_nerf algorithm (as outlined in docs/pipeline/stageC/mp_nerf.md). Because Stage C is deterministic and fast, it can be run on CPU. In fact, in the Kaggle notebook it ran on CPU for all sequences without issue \ufffc. For a refactored solution, Stage C can remain largely as-is: we just need to call the appropriate function to convert predicted angles to coordinates. The output then needs filtering to pick C1\u2032 atoms.</p> <p>Kaggle Submission Format and Requirements</p> <p>The Stanford RNA 3D Folding competition has a specific output format. Competitors must submit five distinct 3D predictions for each RNA target, in a single CSV file \ufffc \ufffc. The rationale is to allow multiple tries per target (Kaggle will evaluate all five and use the best scoring one for that target). The submission CSV format is summarized as follows \ufffc:     \u2022   Each row corresponds to one nucleotide (residue) of a target RNA.     \u2022   Columns:     \u2022   ID: a unique identifier combining the target ID and the residue number (for example, targetID_resIndex, like 101D_1 for residue 1 of target 101D) \ufffc.     \u2022   resname: the nucleotide name (A, C, G, U for standard bases).     \u2022   resid: the residue index (1-indexed integer).     \u2022   x_1, y_1, z_1: the predicted coordinates of that residue\u2019s C1\u2032 atom for Model 1.     \u2022   x_2, y_2, z_2: coordinates for Model 2.     \u2022   \u2026 up to x_5, y_5, z_5 for Model 5.</p> <p>Thus, there are 18 columns in total: ID, resname, resid, and 15 coordinates (5 triples). The submission file should contain every residue of each test sequence. The provided sample_submission.csv in the data outlines all the required ID rows (2515 rows in this case, which matches the total number of nucleotides across all test RNAs) \ufffc. A valid submission must have exactly those IDs (no missing or extra), which the notebook double-checks \ufffc.</p> <p>In terms of content, since our pipeline currently produces one structure per target (not five), a common strategy is to repeat the same prediction across all five model columns (or introduce slight stochastic variations if possible). The Kaggle scoring will take the best of the five, so having identical predictions simply means all five are the same \u2013 it satisfies the format, though it doesn\u2019t give any diversity. The log from the toy sanity-check suggests how the pipeline handles multiple predictions: they printed columns for x_1\u2026x_2 (two model outputs) which in that case were identical \ufffc. By extension, the final submission likely had the same coordinates in x_1, y_1, z_1 through x_5, y_5, z_5 for each residue (since only one model\u2019s output was available). This meets the requirement that five predictions are present, even if they don\u2019t differ.</p> <p>The evaluation metric on Kaggle\u2019s backend is TM-score \ufffc. TM-score evaluates the similarity of a predicted structure to the true structure (it\u2019s length-normalized and ranges from 0 to 1, higher is better). Kaggle will compute the TM-score for each of the five models per target against the true structure and take the highest one as that target\u2019s score \ufffc. Our job in the pipeline is just to output coordinates; we don\u2019t calculate TM-score ourselves in submission. (If we wanted to gauge performance locally, we could compute TM-score on the validation set where true structures are known, but the current notebook did not do this internally.)</p> <p>In short, the submission format demands careful ordering and completeness (all IDs). Our pipeline, as evidenced by the final log, followed the sample submission ordering exactly (no missing IDs, no duplicates) \ufffc. The refactored wrapper will need to do the same: likely by reading the sample_submission.csv as a template, then filling in the coordinate columns for each ID.</p> <p>Model Export Considerations</p> <p>While Kaggle\u2019s competition is scored by CSV submissions, it\u2019s often important to export trained models for future use or for the private test phase. In this project, \u201cmodel export\u201d likely refers to saving the trained weights (for Stage B and any other trainable components) in a reusable form. The codebase suggests that a proper checkpointing system was not yet fully in place \u2013 for example, an analysis noted the absence of a centralized state save/load utility and the need to manually use torch.save for saving model state dicts \ufffc. In practice, for our Kaggle wrapper:     \u2022   After training or fine-tuning Stage B on the provided training set, we should save the model weights (e.g., as a .pt file or HuggingFace model directory). Kaggle notebooks can write to the /kaggle/working directory; those files can then be downloaded or turned into Kaggle Datasets for reuse. Exporting could also mean converting to an ONNX or TorchScript for portability, but since we\u2019ll be using Python in Kaggle inference, a state dict save is sufficient.     \u2022   If using PyTorch Lightning for training, we\u2019d rely on its checkpoint callback to save the best model. If using a custom loop, we\u2019d call torch.save(model.state_dict(), \"model.pth\") at the end. The key is to ensure we capture the trainable parameters of Stage B (and Stage B Pairformer if used). Given that Stage C has no learned parameters, and Stage A was skipped, Stage B\u2019s model is the main artifact to export.     \u2022   Additionally, any preprocessing artifacts (for example, if we computed torsion-angle labels or processed MSAs) could be saved if needed, but likely not necessary for inference.</p> <p>The model export expectation in the docs may also hint at Kaggle\u2019s requirement for sharing the model in later stages (some competitions require the top teams to provide their models for verification). In our context, we ensure the wrapper can output a trained model file and instructions to load it for inference.</p> <p>Plan for Refactoring into a Kaggle-Friendly Wrapper</p> <p>To make the pipeline more straightforward and modular for Kaggle use, we propose refactoring the notebook into a simpler wrapper with clear components. The goal is to support the following modes of operation cleanly: (1) Training (and validation), (2) Inference, (3) Model exporting, and (4) Submission file generation. Below is the plan for each component:     \u2022   1. Inference Module: We will create a function or script (e.g., run_inference()) that encapsulates the sequence \u2192 structure prediction process. This will:     \u2022   Load the pre-trained Stage B TorsionBERT model (from a checkpoint or saved weights). For reproducibility, this could use a specific Kaggle dataset or a weight file packaged with our code. Initialization of the model will be done once. (We can use the StageBTorsionBertPredictor class directly, pointing it to the model path and device, bypassing Hydra for simplicity.)     \u2022   Iterate over input sequences. For each sequence (or possibly batch of sequences if using vectorized inference), use the Stage B model to predict torsion angles \ufffc. Ensure the output is in degrees as needed (we can incorporate the radian-to-degree conversion as in the original code \ufffc).     \u2022   Pass the angles into Stage C reconstruction. We will call a Stage C function (e.g., run_stageC(sequence, angles)) that returns the 3D coordinates. This function will internally handle all the geometry \u2013 we will configure it with the same options as before (mp_nerf, ring closure, etc.) but we can simplify by hardcoding those options in the function call or using a lightweight config object. The output will be an array of coordinates for each atom; we will extract the C1\u2032 atoms for each residue. Because our pipeline knows the ordering of atoms, we can map each residue\u2019s C1\u2032 easily (Stage C likely labels atoms or we know the index pattern of C1\u2032).     \u2022   Collect the results in a structured form (e.g., a list of dictionaries or a DataFrame) where each entry has: target_id, residue index, nucleotide, and the predicted (x, y, z).     \u2022   If we are generating multiple models per target (to have diverse predictions in the five outputs), this inference module can be looped or randomized. For instance, we could run the prediction 5 times with different random seeds or slight perturbations (if the model or Stage C can be stochastic, perhaps by slight noise on angles). For now, the simple implementation will just duplicate the one prediction.     \u2022   2. Training &amp; Evaluation Module: A separate function or script (train_and_evaluate()) will handle model training on the Kaggle train set and optional validation:     \u2022   Data loading: Use train_sequences.csv and train_labels.csv (the latter provided via PDB_RNA structures or train_labels.csv with coordinates \ufffc). We will need to compute torsion angles from the true structures to serve as training targets for Stage B. (If time is short, one might skip training and rely on the pre-trained model, but for completeness the module will be there.)     \u2022   Model initialization: Initialize the TorsionBERT model (either from scratch or load pre-trained and fine-tune). If fine-tuning, perhaps freeze some layers or use LoRA in the future, but initial simple approach is to allow full fine-tuning if data is sufficient.     \u2022   Training loop: Iterate over epochs, batches of sequences. For each, compute model\u2019s predicted angles; compare to ground truth angles; compute a loss (e.g., mean absolute error on angles, or cosine distance if using sin/cos representation). Backpropagate and update model weights. This can be done either with pure PyTorch or via PyTorch Lightning. The codebase\u2019s analysis suggested Lightning was not deeply integrated into pipeline modules \ufffc, so using Lightning\u2019s Trainer now could be an \u201cupgrade\u201d path. A simpler custom loop might be more transparent in Kaggle. We will also be mindful of the limited Kaggle runtime/GPU \u2013 perhaps training only a few epochs or on a subset for demonstration.     \u2022   Evaluation: After training, evaluate on the validation set (if provided). The validation data (12 sequences from CASP15 as per competition description \ufffc) have true structures. We can run our inference pipeline on those and compute TM-score or RMSD to assess performance. This is optional but helps gauge if the model improved.     \u2022   Checkpointing: During training, save the best model weights. For example, if using Lightning, use ModelCheckpoint callback to save the model with lowest validation loss. If manual, keep track and use torch.save. This produces the model file to be used for inference.     \u2022   3. Model Export: After training, we will explicitly export the model. In Kaggle, this means writing the weight file (e.g., torsionbert_final.pth or a HuggingFace save folder) to the output. The user can then add this to a Kaggle dataset for future use. In our wrapper, model export is essentially part of training completion \u2013 we ensure the final or best model is saved. If using the pre-trained model without changes, we can still provide a utility to export it (perhaps converting to a Kaggle dataset or simply noting the path). Since the pipeline code didn\u2019t have a built-in export, we are adding it for convenience \ufffc.     \u2022   4. Submission File Generation: Finally, the wrapper will have a utility to create the submission.csv in the required format using the inference results:     \u2022   We will use the sample submission as a template. By reading sample_submission.csv, we get the correct ordering of IDs (which are sorted by target and residue). We then simply need to fill in the coordinate columns.     \u2022   Using the DataFrame of predictions from the inference module, merge or align it to the sample submission by ID. Then populate x_1, y_1, z_1, \u2026 x_5, y_5, z_5. If we only have one model\u2019s coords, we duplicate them across all five sets.     \u2022   Write out the new CSV. The logs in the original notebook show a reconciliation step (checking for missing or extra IDs) \ufffc; our process of using the provided template inherently guarantees correctness.     \u2022   We\u2019ll print a small preview and the count of rows to reassure that it matches (e.g., 2515 rows, etc., as in the original run).</p> <p>Simplification and Integration Considerations: To streamline this for Kaggle:     \u2022   We will minimize external dependencies. The core pipeline already uses PyTorch, Hydra, etc. In a Kaggle notebook, we might avoid the complexity of Hydra configurations (the original pipeline is Hydra-heavy). Instead, we can define needed parameters directly in code or use simple YAMLs. For example, rather than full default.yaml and config composition, we can directly specify that Stage B model path, device, etc., and Stage C options in our functions. This reduces setup overhead.     \u2022   The wrapper can be organized as a Python module with a CLI or simply as well-defined functions that the Kaggle notebook calls in sequence. For instance, the Kaggle notebook might first call train_and_evaluate() (if training is desired), then run_inference() with the resulting model, then use a provided helper to output the submission file.     \u2022   We will focus on Stage B and Stage C only for the initial version (Stage A \u2013 secondary structure \u2013 can be incorporated later for potential accuracy gains, and Stage D \u2013 diffusion refinement \u2013 is beyond the minimal scope). This keeps the pipeline lightweight: sequence \u2192 angles \u2192 coords is the main path.     \u2022   Logging and debugging info can be toned down. The current logs are very verbose (due to debug settings). The wrapper can log key events (like \u201cLoaded model\u201d, \u201cPredicting structure for XYZ\u2026\u201d) at info level. This makes it easier to follow during Kaggle runs without overwhelming output.     \u2022   Ensure that the same random seed or deterministic behavior is used if we want repeatable results (especially if producing multiple predictions per target). Alternatively, if time permits, incorporate slight randomness to generate diverse models 2\u20135 (e.g., random initialization of transformer dropout each run, etc.).</p> <p>By implementing these components, we achieve a modular pipeline that is easier to maintain and adapt. A researcher or competitor could use the training module offline to improve the model, then plug the trained model into the inference module to generate Kaggle submissions quickly. Overall, this refactoring isolates the concerns: one part for model training, one for inference, and a clear path to produce the final output in the required format, focusing on the critical Stage B (TorsionBERT) and Stage C (geometry builder) pieces of the pipeline.</p> <p>References:     \u2022   Kaggle Competition Overview and Data Formats \ufffc \ufffc     \u2022   Pipeline Logs (Stage B and C execution) \ufffc \ufffc     \u2022   Pipeline Design Documents (Stage B TorsionBERT vs. full pipeline) \ufffc \ufffc     \u2022   Pipeline Execution Summary (Submission assembly) \ufffc</p>","text":""},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/","title":"M2 partial checkpoint plan","text":"<p>Okay, performing a deep dive into the <code>RNA_PREDICT</code> codebase provided.</p> <p>Deep Dive Analysis of <code>RNA_PREDICT</code> Codebase (Focus on M2 Checkpoint Cycle):</p> <ol> <li> <p>Model Definitions &amp; Training Structure:</p> <ul> <li>Core Models: The primary pipeline components (<code>StageARFoldPredictor</code>, <code>StageBTorsionBertPredictor</code>, <code>PairformerWrapper</code>, <code>StageCReconstruction</code>, <code>DiffusionModule</code>, <code>ProtenixDiffusionManager</code>, various attention/embedding modules) are implemented as standard <code>torch.nn.Module</code> subclasses.</li> <li>Lightning Usage: There is no evidence of <code>pytorch_lightning.LightningModule</code> being used for the main pipeline models. Lightning might be used in isolated tests or utilities, but the core training/inference flow doesn't rely on it.</li> <li>Training/Inference Execution: Orchestration happens primarily through scripts:<ul> <li><code>rna_predict/run_full_pipeline.py</code>: Calls individual stage functions.</li> <li>Stage-specific runners: <code>rna_predict/pipeline/stageA/run_stageA.py</code>, <code>rna_predict/pipeline/stageB/main.py</code> (calls <code>run_stageB_combined</code>), <code>rna_predict/pipeline/stageC/stage_c_reconstruction.py</code> (calls <code>run_stageC_rna_mpnerf</code>), <code>rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py</code>.</li> <li>These scripts manage the flow but do not contain standard training loops (forward, loss, backward, step). The M2 plan implies a new <code>train.py</code> script, likely using Lightning, needs to be created.</li> </ul> </li> <li>Implication: For the M2 partial checkpoint test, we cannot rely on <code>Lightning Trainer</code>. The test itself will need to implement a manual, minimal PyTorch training loop.</li> </ul> </li> <li> <p>Checkpointing Mechanisms:</p> <ul> <li>Gradient Checkpointing: <code>rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py</code> exists but handles <code>torch.utils.checkpoint</code> for memory saving during forward/backward, not model state saving/loading.</li> <li>State Saving/Loading: No centralized utility (like <code>checkpoint.py</code>) or function (<code>partial_load_state_dict</code>) was found in <code>rna_predict/utils/</code>. Checkpoint paths in configs (<code>stageA.yaml</code>, <code>stageB_torsion.yaml</code>) refer to loading pre-trained models, not saving/loading training states.</li> <li>Implication: The <code>partial_load_state_dict</code> function is a critical missing piece and must be implemented from scratch. The test will need to manually save the relevant state dict using <code>torch.save</code>.</li> </ul> </li> <li> <p>LoRA/Custom Trainable Modules:</p> <ul> <li>Configuration: LoRA configuration sections (<code>lora: enabled: ...</code>) are present in <code>rna_predict/conf/config_schema.py</code> (for <code>TorsionBertConfig</code>, <code>PairformerConfig</code>) and the corresponding YAML files (<code>stageB_torsion.yaml</code>, <code>stageB_pairformer.yaml</code>). This indicates the design intent to use LoRA.</li> <li>Implementation: A search for <code>LoRA</code>, <code>peft</code>, <code>adapter</code> within the primary model code (<code>rna_predict/pipeline/</code>) did not reveal active LoRA layers or integration using libraries like <code>peft</code>. <code>StageBTorsionBertPredictor</code> and <code>PairformerWrapper</code> load base models but don't appear to apply LoRA adapters dynamically based on the config yet.</li> <li>Implication: The M2 test needs a dummy model that explicitly simulates the intended structure: a frozen \"base\" part and a distinct \"trainable/adapter\" part. The test will focus on the mechanics of saving/loading only the trainable part, assuming LoRA integration will happen separately in the main codebase.</li> </ul> </li> <li> <p>Hydra Configuration:</p> <ul> <li>Usage: Hydra is deeply integrated. Structured configs are defined in <code>rna_predict/conf/config_schema.py</code>, YAMLs are in <code>rna_predict/conf/</code>, and scripts use <code>@hydra.main</code>.</li> <li>Config Path: The absolute path <code>/Users/tomriddle1/RNA_PREDICT/rna_predict/conf</code> is the correct one to use for initialization within tests.</li> <li>Test Configs: <code>rna_predict/conf/test/data.yaml</code> exists.</li> <li>Implication: Tests must initialize Hydra correctly using the absolute path. A minimal test-specific config might be useful but not strictly necessary if the test defines its parameters directly.</li> </ul> </li> <li> <p>Testing Infrastructure:</p> <ul> <li>Directory: <code>tests/</code> is the root.</li> <li>Structure: Well-organized with <code>integration/</code>, <code>unit/</code>, stage-specific folders (<code>tests/pipeline/stageD/</code>, etc.), and common utilities (<code>tests/common/</code>).</li> <li>Existing Tests: Follow standard patterns (e.g., <code>tests/integration/test_full_pipeline.py</code>). Dummy components are sometimes defined within tests or in <code>tests/common/</code>.</li> <li>Implication: New tests should be placed appropriately: <code>tests/integration/test_partial_checkpoint_cycle.py</code> and <code>tests/unit/test_partial_load_state_dict.py</code> (or <code>tests/unit/utils/test_checkpoint.py</code>).</li> </ul> </li> </ol> <p>Revised &amp; Concrete Implementation Plan for M2 Partial Checkpoint Cycle Test (RNA_PREDICT Context):</p> <ol> <li> <p>Implement <code>partial_load_state_dict</code> Utility:</p> <ul> <li>File: Create <code>rna_predict/utils/checkpoint.py</code>.</li> <li>Function: <pre><code># rna_predict/utils/checkpoint.py\nimport torch\nimport logging\nfrom collections import OrderedDict\n\nlogger = logging.getLogger(__name__)\n\ndef partial_load_state_dict(model: torch.nn.Module, state_dict: dict, strict: bool = False):\n    \"\"\"\n    Loads parameters from state_dict into model, skipping mismatched keys\n    and logging information about missing/unexpected keys.\n\n    Args:\n        model: The PyTorch model to load parameters into.\n        state_dict: The dictionary containing parameters to load.\n        strict: If True, raise an error for missing or unexpected keys (default: False).\n\n    Returns:\n        Tuple[List[str], List[str]]: missing_keys, unexpected_keys\n    \"\"\"\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n\n    # Convert state_dict to OrderedDict if needed\n    if not isinstance(state_dict, OrderedDict):\n         state_dict = OrderedDict(state_dict)\n\n    metadata = getattr(state_dict, '_metadata', None)\n    if metadata is not None:\n        state_dict = state_dict.copy()\n        state_dict._metadata = metadata\n\n    own_state = model.state_dict()\n\n    # Check for unexpected keys first if strict=False\n    if not strict:\n        for name in state_dict:\n            if name not in own_state:\n                unexpected_keys.append(name)\n\n    # Load matching keys\n    for name, param in state_dict.items():\n        if name in own_state:\n            if isinstance(param, torch.nn.Parameter):\n                # backwards compatibility for serialized parameters\n                param = param.data\n            try:\n                own_state[name].copy_(param)\n            except Exception as e:\n                error_msgs.append(f'While copying the parameter named \"{name}\", '\n                                  f'whose dimensions in the model are {own_state[name].size()} and '\n                                  f'whose dimensions in the checkpoint are {param.size()}: {str(e)}')\n        # If strict=True, unexpected keys are handled below\n\n    # Check for missing keys\n    loaded_keys = set(state_dict.keys())\n    for name in own_state:\n        if name not in loaded_keys:\n            missing_keys.append(name)\n\n    # Handle strict mode errors\n    if strict:\n         unexpected_keys = [k for k in state_dict if k not in own_state] # Recalculate for strict\n         if unexpected_keys:\n             error_msgs.insert(\n                 0, 'Unexpected key(s) in state_dict: {}. '.format(\n                     ', '.join(f'\"{k}\"' for k in unexpected_keys)))\n         if missing_keys:\n             error_msgs.insert(\n                 0, 'Missing key(s) in state_dict: {}. '.format(\n                     ', '.join(f'\"{k}\"' for k in missing_keys)))\n\n    if error_msgs:\n        raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n                           model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\n    # Log warnings if not strict\n    if not strict:\n         if unexpected_keys:\n             logger.warning(f\"Unexpected key(s) in state_dict: {', '.join(unexpected_keys)}\")\n         if missing_keys:\n             logger.warning(f\"Missing key(s) in state_dict: {', '.join(missing_keys)}\")\n\n    logger.info(f\"Loaded {len(own_state) - len(missing_keys)} keys from checkpoint into {model.__class__.__name__}.\")\n    return missing_keys, unexpected_keys # Return lists for inspection in tests\n</code></pre></li> <li>Unit Test: Create <code>tests/unit/utils/test_checkpoint.py</code>.<ul> <li>Define simple dummy <code>nn.Module</code>s (<code>BaseModel</code>, <code>AdapterModel</code> inheriting or composing <code>BaseModel</code>).</li> <li>Test loading full state into full model (<code>strict=True</code>).</li> <li>Test loading partial (adapter-only) state into full model (<code>strict=False</code> -&gt; succeed, <code>strict=True</code> -&gt; fail).</li> <li>Test loading state with extra keys (<code>strict=False</code> -&gt; succeed with warning, <code>strict=True</code> -&gt; fail).</li> </ul> </li> </ul> </li> <li> <p>Create Test Infrastructure:</p> <ul> <li>File: Create <code>tests/integration/test_partial_checkpoint_cycle.py</code>.</li> <li>Dummy Model: Define within the test file:     <pre><code># tests/integration/test_partial_checkpoint_cycle.py\nimport torch\nimport torch.nn as nn\n\nclass DummyCheckpointModel(nn.Module):\n    # Renamed to avoid potential conflicts if imported elsewhere\n    def __init__(self, base_dim=16, adapter_dim=8):\n        super().__init__()\n        # Simulate a frozen base\n        self.base_layer = nn.Linear(base_dim, base_dim)\n        # Simulate a trainable adapter/head\n        self.adapter_layer = nn.Linear(base_dim, adapter_dim)\n\n        # Freeze base layer by default for the test's purpose\n        for param in self.base_layer.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        x = self.base_layer(x)\n        x = self.adapter_layer(x)\n        return x\n\n    def get_adapter_state_dict(self):\n        # Gets ONLY the state dict of the part we intend to train/save partially\n        return self.adapter_layer.state_dict()\n</code></pre></li> <li>Synthetic Data: <pre><code># tests/integration/test_partial_checkpoint_cycle.py\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef create_dummy_dataloader(batch_size=4, n_samples=12, base_dim=16, adapter_dim=8):\n    X = torch.randn(n_samples, base_dim)\n    y = torch.randn(n_samples, adapter_dim) # Target matches adapter output dim\n    dataset = TensorDataset(X, y)\n    return DataLoader(dataset, batch_size=batch_size)\n</code></pre></li> <li>Hydra Init (if needed): <pre><code># tests/integration/test_partial_checkpoint_cycle.py\nimport hydra\n\ndef setup_hydra():\n    # Only initialize if not already initialized (important for pytest)\n    if not hydra.core.global_hydra.GlobalHydra.instance().is_initialized():\n        hydra.initialize(config_path=\"/Users/tomriddle1/RNA_PREDICT/rna_predict/conf\", version_base=None)\n    # cfg = hydra.compose(config_name=\"test/your_test_config\") # If needed\n    # return cfg\n</code></pre></li> </ul> </li> <li> <p>Implement Integration Test Logic:</p> <ul> <li>File: <code>tests/integration/test_partial_checkpoint_cycle.py</code></li> <li>Test Function (<code>test_train_save_partial_load_infer</code>):<ul> <li>Setup: Call <code>setup_hydra()</code>, create dataloader, instantiate <code>model = DummyCheckpointModel()</code>. Use <code>tmp_path</code> fixture.</li> <li>Optimizer: <code>optimizer = torch.optim.Adam(model.adapter_layer.parameters(), lr=1e-3)</code> (Crucially, optimize only adapter).</li> <li>Manual Training Loop: <pre><code>model.train()\ncriterion = nn.MSELoss()\nfor epoch in range(2): # Train for 2 epochs\n    for batch_x, batch_y in dataloader:\n        optimizer.zero_grad()\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        loss.backward()\n        optimizer.step()\n</code></pre></li> <li>Save Partial Checkpoint: <pre><code>adapter_state_dict = model.get_adapter_state_dict()\ncheckpoint = {\n    'model_adapter_state_dict': adapter_state_dict,\n    'optimizer_state_dict': optimizer.state_dict(),\n    # Add epoch or other metadata if needed\n}\npartial_ckpt_path = tmp_path / \"partial_adapter_checkpoint.pt\"\ntorch.save(checkpoint, partial_ckpt_path)\n</code></pre></li> <li>(Optional) Save Full State: <code>full_ckpt_path = tmp_path / \"full_model.pt\"; torch.save(model.state_dict(), full_ckpt_path)</code></li> <li>Reload Stage: <pre><code>from rna_predict.utils.checkpoint import partial_load_state_dict # Import utility\n\nnew_model = DummyCheckpointModel() # Fresh model instance\nloaded_checkpoint = torch.load(partial_ckpt_path)\n\n# Store initial weights for comparison\ninitial_adapter_weight = new_model.adapter_layer.weight.clone().detach()\ninitial_base_weight = new_model.base_layer.weight.clone().detach()\n\nmissing, unexpected = partial_load_state_dict(\n    new_model, loaded_checkpoint['model_adapter_state_dict'], strict=False\n)\n\n# Assert loading results\nassert not unexpected # Should be no keys in ckpt not in the full model structure\nassert all('base_layer' in k for k in missing) # Base layer keys should be missing\n</code></pre></li> <li>Inference &amp; Assertions: <pre><code>new_model.eval()\ntest_input = torch.randn(1, 16) # Use dims from model\nwith torch.no_grad():\n    inference_output = new_model(test_input)\n\nassert inference_output.shape == (1, 8) # Matches adapter_dim\nassert not torch.isnan(inference_output).any()\nassert not torch.isinf(inference_output).any()\n\n# Verify weights loaded correctly\nassert not torch.equal(new_model.adapter_layer.weight, initial_adapter_weight) # Adapter weights should have changed\nassert torch.equal(new_model.base_layer.weight, initial_base_weight) # Base weights should NOT have changed\n</code></pre></li> <li>(Optional) Size Check: <pre><code>partial_size = partial_ckpt_path.stat().st_size\nfull_size = full_ckpt_path.stat().st_size\nassert partial_size &lt; full_size\nprint(f\"Partial ckpt size: {partial_size}, Full ckpt size: {full_size}\")\n</code></pre></li> </ul> </li> </ul> </li> </ol> <p>Summary Table (Final RNA_PREDICT Context):</p> Step File/Location Key Actions &amp; Notes Status Implement Utility <code>rna_predict/utils/checkpoint.py</code> Create <code>partial_load_state_dict</code>. TODO Unit Test Utility <code>tests/unit/utils/test_checkpoint.py</code> Test loader thoroughly with dummy modules/dicts. TODO Create Test Infra <code>tests/integration/test_partial_checkpoint_cycle.py</code> Define <code>DummyCheckpointModel</code>, <code>create_dummy_dataloader</code>, <code>setup_hydra</code>. TODO Manual Training <code>tests/integration/test_partial_checkpoint_cycle.py</code> Manual loop, optimize only adapter params. TODO Save Partial Ckpt <code>tests/integration/test_partial_checkpoint_cycle.py</code> Manually get adapter state dict, <code>torch.save</code>. TODO Reload &amp; Infer <code>tests/integration/test_partial_checkpoint_cycle.py</code> Instantiate new model, call implemented <code>partial_load_state_dict</code>. TODO Assert Correctness <code>tests/integration/test_partial_checkpoint_cycle.py</code> Check load success, inference output, verify which weights changed/didn't. TODO (Optional) Size Check <code>tests/integration/test_partial_checkpoint_cycle.py</code> Compare partial vs. full state dict file sizes. Optional <p>This detailed, context-aware plan provides the necessary steps, code structure, and verification logic to implement and test the M2 partial checkpoint cycle within the specific environment of the <code>RNA_PREDICT</code> project. The next immediate step is to implement the <code>partial_load_state_dict</code> function and its unit tests.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#aligning-on-my-goal","title":"\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: [aligning on my goal]","text":"<p>Certainly! Below is an updated \u201cdeep dive\u201d analysis of your codebase, focusing on how to implement the M2 partial checkpoint cycle. I\u2019ve structured it around (1) your existing code architecture, (2) where partial checkpoint logic best fits, (3) how to design the dummy model and partial loader, and (4) an actionable step-by-step plan.</p> <p>[emoji]: [actionable response]</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#1-code-architecture-overview","title":"1. Code Architecture Overview","text":""},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#model-definitions-training-patterns","title":"Model Definitions &amp; Training Patterns","text":"<ul> <li><code>torch.nn.Module</code> is the primary inheritance pattern for your models (e.g. <code>StageBTorsionBertPredictor</code>, <code>PairformerWrapper</code>, <code>DiffusionModule</code>), not <code>pytorch_lightning.LightningModule</code>.</li> <li>Training loops in your code are mostly \u201cmanual\u201d\u2014scripts like <code>run_full_pipeline.py</code> or <code>run_stageB_combined</code> orchestrate forward passes but do not do full epoch-based training or checkpoint saving as PyTorch Lightning would.  </li> </ul> <p>Implication: For the M2 partial checkpoint test, you\u2019ll likely create a minimal training loop (manual forward/backward/step) rather than using a Lightning <code>Trainer</code>.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#lora-or-custom-trainable-modules","title":"LoRA or Custom Trainable Modules","text":"<ul> <li>Your config (<code>rna_predict/conf/model/stageB_torsion.yaml</code>, etc.) references <code>lora:</code> parameters, but the actual injection of LoRA modules into a base model (e.g. <code>peft.get_peft_model</code>) isn\u2019t clearly implemented. </li> <li>Implication: For the partial checkpoint cycle test, you need a dummy model that simulates \u201cfrozen base\u201d + \u201ctrainable adapter.\u201d Then you only save the trainable adapter state to simulate \u201cpartial checkpoint.\u201d</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#checkpointing","title":"Checkpointing","text":"<ul> <li>No dedicated partial loader: The code has references to gradient checkpointing but not to saving or partial loading the model\u2019s state. </li> <li>Implication: You\u2019ll need to create something like <code>partial_load_state_dict(model, checkpoint_dict, strict=False)</code> in a new file, e.g. <code>rna_predict/utils/checkpoint.py</code>. That function will:</li> <li>Loop over <code>checkpoint_dict</code> keys </li> <li>Compare them to <code>model.state_dict()</code> keys </li> <li>Load the intersection </li> <li>Possibly warn/ignore mismatches.</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#testing-hydra","title":"Testing &amp; Hydra","text":"<ul> <li>You have <code>tests/</code> with subfolders for <code>integration/</code>, <code>unit/</code>, and stage-specific tests. </li> <li>Hydra is used widely. If you need config parameters for the test, you can <code>hydra.initialize(config_path=...)</code> in your test. Otherwise, you might define everything inline for simplicity.</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#2-where-to-place-partial-checkpoint-logic","title":"2. Where to Place Partial Checkpoint Logic","text":"<ol> <li>New Utility: </li> <li>File: <code>rna_predict/utils/checkpoint.py</code></li> <li>Function: <code>partial_load_state_dict(model: nn.Module, state_dict: dict, strict: bool = False) -&gt; None</code></li> <li> <p>Purpose: Let you load only certain matching parameters into the model, ignoring or warning about extras.</p> </li> <li> <p>Integration Test:</p> </li> <li>File: <code>tests/integration/test_partial_checkpoint_cycle.py</code></li> <li> <p>Purpose:  </p> <ol> <li>Build a dummy model with a \u201cfrozen base layer\u201d + \u201ctrainable adapter layer.\u201d  </li> <li>Manually train for a couple steps (so the adapter changes).  </li> <li>Save only the adapter\u2019s weights as a partial checkpoint.  </li> <li>Reload them into a fresh model.  </li> <li>Check that the base remains default, the adapter is updated, and inference works.</li> </ol> </li> <li> <p>Unit Test (Optional):</p> </li> <li>File: <code>tests/unit/test_partial_load_state_dict.py</code></li> <li>Purpose: Validate <code>partial_load_state_dict(...)</code> in isolation with smaller scenario: missing keys, unexpected keys, etc.</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#3-designing-the-dummy-model-minimal-training-loop","title":"3. Designing the Dummy Model &amp; Minimal Training Loop","text":"<p>Below is a conceptual outline (not full code) of what the dummy model and minimal training loop might look like:</p> <ol> <li>Dummy Model:    <pre><code>import torch\nimport torch.nn as nn\n\nclass DummyLoRAModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_layer = nn.Linear(10, 10)     # Frozen\n        self.adapter_layer = nn.Linear(10, 2)   # Trainable (like LoRA)\n\n        # Freeze base layer\n        for p in self.base_layer.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        x = self.base_layer(x)\n        return self.adapter_layer(x)\n\n    def get_trainable_params(self):\n        return self.adapter_layer.parameters()\n</code></pre></li> <li>The \u201cbase_layer\u201d simulates \u201cfrozen base,\u201d </li> <li> <p>The \u201cadapter_layer\u201d simulates \u201cLoRA or new module.\u201d</p> </li> <li> <p>Training Step (in your integration test):    <pre><code>model = DummyLoRAModel()\noptimizer = torch.optim.Adam(model.get_trainable_params(), lr=1e-3)\n\n# Synthetic data\nX = torch.randn(4, 10)  # batch=4, input_dim=10\nY = torch.randn(4, 2)   # match adapter_layer output dim=2\nloss_fn = nn.MSELoss()\n\n# Simple training loop\nfor step in range(2):\n    optimizer.zero_grad()\n    preds = model(X)\n    loss = loss_fn(preds, Y)\n    loss.backward()\n    optimizer.step()\n</code></pre></p> </li> <li>After these 2 steps, the adapter layer\u2019s weights have changed.  </li> <li> <p>Then you\u2019d do <code>torch.save(...)</code> for the partial checkpoint.</p> </li> <li> <p>Saving Partial Checkpoint:    <pre><code># Only the adapter layer's weights\npartial_sd = model.adapter_layer.state_dict()\ntorch.save(partial_sd, \"partial_checkpoint.pt\")\n</code></pre></p> </li> <li>Reloading:    <pre><code>new_model = DummyLoRAModel()   # fresh instance\nloaded_sd = torch.load(\"partial_checkpoint.pt\")\npartial_load_state_dict(new_model.adapter_layer, loaded_sd, strict=False)\n</code></pre></li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#4-step-by-step-implementation-plan","title":"4. Step-by-Step Implementation Plan","text":"<p>Below is the final recommended plan for the M2 partial checkpoint cycle:</p> <ol> <li>Create Partial Loader Utility </li> <li>File: <code>rna_predict/utils/checkpoint.py</code> </li> <li>Function:       <pre><code>def partial_load_state_dict(module: nn.Module, state_dict: dict, strict: bool = False):\n    # Compare keys with module.state_dict().\n    # Load only matching keys, skip or warn about mismatches.\n    # Return any missing/unexpected keys if needed.\n</code></pre></li> <li> <p>This should handle e.g. <code>module.adapter_layer.weight</code> and <code>module.adapter_layer.bias</code>.</p> </li> <li> <p>Unit Test (Optional but recommended)  </p> </li> <li>File: <code>tests/unit/test_partial_load_state_dict.py</code> </li> <li> <p>Scenarios:</p> <ol> <li>Perfect match.</li> <li>Missing keys in checkpoint.</li> <li>Extra keys in checkpoint.</li> <li><code>strict=True</code> vs <code>strict=False</code> behavior.</li> </ol> </li> <li> <p>Integration Test </p> </li> <li>File: <code>tests/integration/test_partial_checkpoint_cycle.py</code> </li> <li> <p>Implementation:</p> <ol> <li>Define <code>DummyLoRAModel</code>.</li> <li>Create a small synthetic dataset (like <code>(X, Y)</code> random).</li> <li>Train for a few steps (2\u20133) with only the adapter\u2019s parameters in the optimizer.</li> <li>Save partial checkpoint with <code>adapter_layer.state_dict()</code>.</li> <li>Reload into a fresh <code>DummyLoRAModel</code> instance using <code>partial_load_state_dict(...)</code>.</li> <li>Run a forward pass on the new model, checking output shape is correct, no NaNs, etc.</li> <li>(Optional) Compare partial checkpoint file size to a full <code>model.state_dict()</code> to confirm it\u2019s smaller.</li> </ol> </li> <li> <p>Hydra Usage:</p> </li> <li>If needed for specifying device or hyperparameters, you can wrap the test in <code>hydra.initialize(...)</code>.  </li> <li> <p>Many times you can keep the test simple with inline definitions for the dataset, model, etc.</p> </li> <li> <p>Add Documentation:</p> </li> <li>In <code>checkpoint.py</code>, docstring to explain partial load logic.  </li> <li>In the integration test, short docstrings about the partial checkpoint approach for M2.</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#5-potential-edge-cases-notes","title":"5. Potential Edge Cases &amp; Notes","text":"<ul> <li>Key naming: If your actual LoRA code modifies param names (e.g. \u201cbase_layer.lora_A\u201d), your partial loader must handle that. For the dummy test, you can keep it straightforward.  </li> <li>Multiple adapters: If in future you have multiple adapters, you can save them each to separate partial checkpoints or combine them in one dictionary.  </li> <li>Frozen base mismatch: Ensure the base parameters remain identical to their initialization in the new model (the test can assert that).  </li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#next-steps","title":"Next Steps","text":"<ol> <li>Implement <code>partial_load_state_dict</code> in <code>rna_predict/utils/checkpoint.py</code>.</li> <li>Write the integration test in <code>tests/integration/test_partial_checkpoint_cycle.py</code>.</li> <li>Optionally add a unit test for partial loading edge cases.</li> <li>Validate the M2 partial checkpoint cycle by running <code>pytest -k partial_checkpoint_cycle</code>.</li> </ol> <p>Outcome: You\u2019ll have a minimal, workable demonstration of saving only the trainable adapter\u2019s weights and reloading them into a fresh model instance\u2014fulfilling the M2 partial checkpoint requirement.</p> <p>[emoji]:  Would you like any further clarifications or example code patterns? Feel free to let me know which section you\u2019d like me to detail more.</p> <p>Here\u2019s a systematic, actionable plan for implementing and testing the M2 partial checkpoint cycle, based on the requirements and best practices you provided:</p> <ol> <li>Dummy Model and LoRA/New Modules Implement a minimal PyTorch (Lightning) model that includes at least one LoRA or new module to ensure non-triviality. The model should be simple (e.g., a linear layer with LoRA adaptation) but must have trainable parameters.</li> <li>Synthetic Data Loader Create a DataLoader that yields synthetic (random or constant) data and labels. Keep batch size and number of batches small for speed (e.g., batch size = 2, num_batches = 3).</li> <li>Minimal Training Loop Use PyTorch Lightning Trainer (or a minimal manual loop if Lightning is not available). Train for a minimal number of steps/epochs (e.g., 1-2 steps). Ensure at least one optimizer step is taken so parameters are updated. Use ModelCheckpoint (or equivalent) to save checkpoints containing only trainable parameters and optimizer state.</li> <li>Partial Checkpoint Loader Implement or use an existing partial_load_state_dict function. This function must: Load only matching keys from the checkpoint into the model. Gracefully handle missing/unexpected keys (no crash). Write a unit test with dummy modules and state dicts to verify this logic.</li> <li>Integration Test: Checkpoint Cycle Steps: Train the dummy model on synthetic data and save a checkpoint. Instantiate a new model instance. Load the checkpoint using partial_load_state_dict. Run inference on synthetic data. Assert: No errors/crashes. Output shape is correct. Output is not all-zeros or NaNs. Manually or programmatically check that the checkpoint file is smaller than a full model checkpoint (if possible).</li> <li>Test Coverage Ensure both: Unit test for the partial loader (dummy modules/state dicts). Integration test for the full cycle (train \u2192 save \u2192 reload \u2192 inference).</li> <li>Hydra/Config If config is needed, use minimal, hardcoded or Hydra-based config as appropriate. Avoid unnecessary dependencies\u2014keep the test self-contained.</li> <li>Documentation and Manual Inspection Add docstrings/comments explaining the test logic. Optionally print/log checkpoint file size for manual inspection. Summary Table | Step | What to Implement/Test | Notes | |---------------------|--------------------------------------------------------|-------------------------------------------| | Dummy Model | Minimal Lightning model with LoRA/new module | Should have trainable params | | Synthetic Data | DataLoader with random/constant data | Small batch/epoch for speed | | Training Loop | Minimal training to produce checkpoint | Use ModelCheckpoint | | Partial Loader | partial_load_state_dict + unit test | Handles missing/unexpected keys | | Integration Test | Full cycle: train \u2192 save \u2192 reload \u2192 inference | Check shape, no crash, no NaNs/all-zeros | | Checkpoint Size | Optional: assert/print that checkpoint is \u201cpartial\u201d | Manual or programmatic check | | Documentation | Comments/docstrings/logs | For clarity and reproducibility |</li> </ol> <p>Next Steps Implement the dummy model and LoRA/new module. Write the synthetic DataLoader. Set up the minimal training loop with checkpoint saving. Implement and unit test the partial loader. Write the integration test for the checkpoint cycle. Add documentation and (optionally) checkpoint size check. If you\u2019d like, I can scaffold the code structure or write the first component for you\u2014just let me know which step you want to start with! To adapt the M2 partial checkpoint cycle plan to your current code structure, here\u2019s a systematic approach tailored to the RNA_PREDICT project:</p> <ol> <li>Understand Current Code Structure and Integration Points Model Location: Your main models and modules (including LoRA or custom modules) are likely under rna_predict/pipeline/ (e.g., StageB, StageD). Training/Inference Entrypoints: Training and inference are typically managed by scripts like run_all_pipeline.py or stage-specific scripts (e.g., run_stageD.py). Config Management: You use Hydra for configuration, with schemas in rna_predict/conf/config_schema.py and configs in rna_predict/conf/. Testing: Tests are organized in the test/ directory, following best practices for modular and integration testing.</li> <li>Where to Place the Partial Checkpoint Cycle Test Integration Test: Place the full-cycle integration test in test/integration/ (or create this directory if it doesn\u2019t exist). Name it something like test_partial_checkpoint_cycle.py. Unit Test for Loader: Place the partial loader unit test in test/unit/ or alongside the loader implementation if it\u2019s in a utility module. Dummy Model/Data: Define these within the test file itself or in a test/utils/ module to avoid polluting production code.</li> <li>How to Use Existing Infrastructure Hydra: Use the correct config path (/Users/tomriddle1/RNA_PREDICT/rna_predict/conf) for all initialization in tests. Create a minimal or test-specific Hydra config if needed. Lightning/Training Loop: If you have Lightning modules, subclass a minimal one for the dummy model. Use the same checkpointing mechanism as the main training loop (ModelCheckpoint or custom). Partial Loader: If you have an existing partial_load_state_dict, import and use it. Otherwise, implement it in a utility module (e.g., rna_predict/utils/checkpoint_utils.py).</li> <li>Minimize Coupling and Keep Tests Self-Contained Avoid using full pipeline scripts (like run_all_pipeline.py) for this test\u2014keep the test focused and isolated. Use synthetic data and a minimal model defined within the test, not production data/models. Mock or stub Hydra configs as needed, but ensure correct initialization.</li> <li>Proposed Directory and File Structure CopyInsert rna_predict/   pipeline/     ... (existing code)   conf/     ... (existing Hydra configs)   utils/     checkpoint_utils.py  # (if partial_load_state_dict is here) test/   integration/     test_partial_checkpoint_cycle.py   unit/     test_partial_load_state_dict.py   utils/     dummy_model.py     dummy_data.py</li> <li>Example: Integration Test Skeleton python CopyInsert</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#testintegrationtest_partial_checkpoint_cyclepy","title":"test/integration/test_partial_checkpoint_cycle.py","text":"<p>import torch import pytorch_lightning as pl from rna_predict.utils.checkpoint_utils import partial_load_state_dict</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#dummy-model-and-dataloader-defined-here-or-imported-from-testutils","title":"Dummy model and dataloader defined here or imported from test/utils","text":"<p>def test_partial_checkpoint_cycle(tmp_path):     # 1. Instantiate dummy model     # 2. Create synthetic dataloader     # 3. Train for a few steps, save checkpoint to tmp_path     # 4. Instantiate new model, load with partial_load_state_dict     # 5. Run inference, assert output shape, no NaNs/all-zeros     # 6. (Optional) Check checkpoint file size     pass 7. Hydra Integration in Tests Use the absolute config path for Hydra initialization in tests: python CopyInsert with hydra.initialize(config_path=\"/Users/tomriddle1/RNA_PREDICT/rna_predict/conf\", version_base=None):     cfg = hydra.compose(config_name=\"test_config\") Create a minimal config file for the test if needed. 8. Next Steps Confirm where your partial_load_state_dict lives (or if it needs to be implemented). Set up the test directory structure if not present. Scaffold the dummy model and data loader in test utils or directly in the integration test. Implement the integration and unit tests as outlined. Would you like me to:</p> <p>Locate or scaffold the partial_load_state_dict utility? Scaffold the integration test file and dummy model/data? Suggest a minimal test Hydra config? Let me know your preferred starting point!</p> <p>Feedback submitted Me</p> <p>can you actually do a deep dive into the code and show me your updated analysis similar to what you just made</p> <p>Absolutely! I\u2019ll do a deep dive into your codebase to provide a concrete, context-aware analysis for implementing the M2 partial checkpoint cycle. I\u2019ll focus on:</p> <p>Where models, training loops, and checkpoints are implemented Where/how LoRA or custom modules are integrated How configuration and Hydra are used Where checkpoint saving/loading happens What test and utility infrastructure exists How/where to best add the integration/unit tests for the checkpoint cycle Let\u2019s begin by mapping out the relevant code structure and identifying key implementation points. I\u2019ll start by searching for model definitions, checkpoint logic, LoRA usage, and test infrastructure.</p> <p>I will:</p> <p>Search for model and LightningModule definitions. Locate checkpoint saving/loading code. Identify any LoRA or \u201cnew module\u201d integrations. Examine config/Hydra usage. Map the test directory and existing test patterns/utilities. Let\u2019s start with these searches.</p> <p>Semantic searched codebase 669 results Semantic searchedrna_predict/pipelinewith queryLightningModule Semantic searched codebase 755 results Semantic searchedrna_predict/pipelinewith querycheckpoint Semantic searched codebase 780 results Semantic searchedrna_predict/pipelinewith queryLoRA Semantic searched codebase 775 results Semantic searchedrna_predict/pipelinewith queryhydra Searched filesystem 2 results Searchedfordirectoriesin~/RNA_PREDICTwith depth3with patterntest Searched filesystem 99 results Searchedforfilesintestswith depth3with pattern.py Searched filesystem 0 results Searchedforfilesinrna_predict/utilswith depth2with patterncheckpoint* Here is a deep, codebase-specific analysis for implementing the M2 partial checkpoint cycle in the context of RNA_PREDICT, based on the latest code structure and findings:</p> <ol> <li>Model and Training Loop Structure Model Definitions: Most models are implemented as nn.Module (not PyTorch LightningModule), e.g. PairformerStack, PairformerBlock, PairformerWrapper (in stageB/pairwise/), and various predictors. There is a DummyPairformerModel in stageB/pairwise/dummy_pairformer.py for testing. No direct evidence of LightningModule usage for main pipeline models, but some test and utility code may use Lightning for convenience. Training Loops: Training and inference are managed by scripts like run_stageB_combined, run_pipeline (in stageB/main.py), and run_stageD (in stageD/run_stageD.py). These functions are not Lightning-native but could be adapted for minimal training/testing.</li> <li>Checkpointing Logic Checkpoint Saving/Loading: No dedicated checkpoint_utils.py or clear utility for partial loading in rna_predict/utils/. Checkpointing is referenced in some configs and model wrappers (e.g. use_checkpoint in PairformerWrapper), but actual save/load logic is not centralized. No existing partial_load_state_dict found\u2014this will need to be implemented, likely in a new utility file (e.g., rna_predict/utils/checkpoint.py).</li> <li>LoRA/Adapter Modules LoRA Integration: No explicit LoRA modules found in the main pipeline code. The test plan should use a dummy model with a simple trainable submodule to simulate LoRA/new module behavior.</li> <li>Hydra Configuration Hydra Usage: All main scripts and many tests use Hydra, with configs in /Users/tomriddle1/RNA_PREDICT/rna_predict/conf. For all tests and initialization, always use the absolute config path (/Users/tomriddle1/RNA_PREDICT/rna_predict/conf). Test configs exist under conf/test/.</li> <li>Test Infrastructure Test Directory: Main test directory is tests/ (not test/), with subfolders for integration/, unit/, and stage-specific tests. Integration tests like integration/test_full_pipeline.py and integration/test_pipeline_integration.py exist. Dummy models and data are sometimes defined in test files or in tests/common/ or tests/utils/.</li> <li>Proposed Implementation Plan (RNA_PREDICT-Specific) A. Utility Implementation Create rna_predict/utils/checkpoint.py with: partial_load_state_dict (loads only matching keys, ignores missing/unexpected). Unit test for this function in tests/unit/test_partial_load_state_dict.py. B. Dummy Model and Data Define a minimal nn.Module (with a dummy trainable submodule, simulating LoRA) in the integration test or in tests/utils/dummy_model.py. Use a synthetic DataLoader (random tensors). C. Minimal Training Loop In the integration test (tests/integration/test_partial_checkpoint_cycle.py): Instantiate the dummy model. Run a minimal manual training loop (since main models are not LightningModules). Save a checkpoint (using torch.save(model.state_dict(), ...) and torch.save(optimizer.state_dict(), ...)). D. Partial Checkpoint Cycle Test In the same integration test: Instantiate a new dummy model. Load checkpoint using partial_load_state_dict. Run inference, check output shape, ensure no all-zeros/NaNs. Optionally, compare checkpoint file sizes. E. Hydra Integration Use hydra.initialize(config_path=\"/Users/tomriddle1/RNA_PREDICT/rna_predict/conf\", ...) in all tests/configs. If needed, create a minimal config in conf/test/.</li> <li>Directory and File Structure CopyInsert rna_predict/   utils/     checkpoint.py  # (new, for partial_load_state_dict) tests/   integration/     test_partial_checkpoint_cycle.py   unit/     test_partial_load_state_dict.py   utils/     dummy_model.py  # (if needed)</li> <li> <p>Summary Table (RNA_PREDICT Context) | Step | File/Location | Notes | |---------------------|----------------------------------------------------|---------------------------------| | Partial Loader | utils/checkpoint.py, unit/test_partial_load_state_dict.py | New utility + unit test | | Dummy Model/Data | integration/test_partial_checkpoint_cycle.py or tests/utils/dummy_model.py | Use nn.Module, not Lightning | | Training Loop | integration/test_partial_checkpoint_cycle.py | Manual loop, torch.save | | Checkpoint Cycle | integration/test_partial_checkpoint_cycle.py | Save, partial load, infer, check| | Hydra Config | Use absolute path in all tests | Use conf/test/ if needed |</p> </li> <li> <p>Next Steps Implement partial_load_state_dict in rna_predict/utils/checkpoint.py. Write its unit test in tests/unit/test_partial_load_state_dict.py. Scaffold the integration test in tests/integration/test_partial_checkpoint_cycle.py using a dummy model and manual training loop. Use Hydra with the correct config path in all test code. Would you like me to scaffold any of these files or show concrete code snippets for any step? Let me know your preferred next step!</p> </li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#task-master-data-for-m2-partial-checkpoint-cycle-as-of-2025-04-21","title":"Task Master Data for M2 Partial Checkpoint Cycle (as of 2025-04-21)","text":""},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#project-dashboard","title":"Project Dashboard","text":"<ul> <li>Tasks Progress: 60% (25 done, 17 pending)</li> <li>Subtasks Progress: 43% (15/35 completed)</li> <li>High priority: 14, Medium: 23, Low: 5</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#next-task-to-work-on","title":"Next Task to Work On","text":"<ul> <li>ID 25: M2-Test Verify Partial Checkpoint Cycle (high priority)</li> <li>Dependencies: 40</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#task-25-m2-test-verify-partial-checkpoint-cycle","title":"Task #25 - M2-Test Verify Partial Checkpoint Cycle","text":"<p>Description: We save only trainable parameters (LoRA + new modules). This test ensures partial checkpoints reload cleanly without missing/unexpected key errors and that inference still works.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#tasks","title":"Tasks","text":"<ol> <li>New test: <code>tests/integration/test_checkpoint_cycle.py</code>.</li> <li>Dummy training: A few steps, produce partial checkpoint.</li> <li>Reload: Use <code>partial_load_state_dict(...)</code> in a fresh model.</li> <li>Inference: Confirm no key mismatch, shape is correct.</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>Partial checkpoint is smaller than a full model.</li> <li>Reload is error-free.</li> <li>Forward pass works post-load.</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#potential-problems","title":"Potential Problems","text":"<ul> <li>Lightning\u2019s default checkpoint might still save the entire state.</li> <li>Param naming differences can cause mismatch warnings.</li> <li>Distinguishing intentionally excluded vs. truly missing keys can be tricky.</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#test-strategy","title":"Test Strategy","text":"<p>In <code>test_checkpoint_cycle.py</code>, train a minimal model for a few steps, save partial checkpoint, reload into a fresh model, run forward pass to confirm no shape mismatch or missing keys.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#subtasks","title":"Subtasks","text":"ID Status Title Dependencies 25.1 pending Create Test Infrastructure and Dummy Model None 25.2 pending Implement Partial Checkpoint Generation 25.1 25.3 pending Implement Partial State Dict Loading 25.1, 25.2 25.4 pending Validate Model Inference Post-Loading 25.1, 25.2, 25.3 <p>Subtask Progress: 0/4 (0%)</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#suggested-actions","title":"Suggested Actions","text":"<ul> <li>Mark as in-progress: <code>task-master set-status --id=25 --status=in-progress</code></li> <li>Mark as done when completed: <code>task-master set-status --id=25 --status=done</code></li> <li>Update subtask status: <code>task-master set-status --id=25.1 --status=done</code></li> </ul> <p>This section was generated by exporting current Task Master data for the M2 partial checkpoint cycle feature. For updates, rerun <code>task-master list</code> and <code>task-master show 25</code>.</p> <p>Below is a bolt\u2011on guide that shows where Lightning and LanceDB can add value to the \u201cpartial\u2011checkpoint cycle\u201d work (and, more broadly, to your RNA\u202fpipeline) without forcing a ground\u2011up rewrite.  Think of it as two mix\u2011and\u2011match \u201cupgrade paths\u201d you can adopt independently or together.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#1-pytorch-lightning-painless-training-surgical-checkpointing","title":"1\u202f\u202f|\u202f\u202fPyTorch\u202fLightning \u279c painless training\u202f+\u202fsurgical checkpointing","text":"What you get Why it helps here Trainer loop\u2011as\u2011a\u2011service Eliminates the 30\u201340\u202flines of manual train/val boilerplate you planned to embed in the integration test. Pluggable checkpoint callback (<code>ModelCheckpoint</code>) Lets you decide at callback time which parts of the state\u2011dict to write \u2192 perfect for \u201csave only LoRA/adapter\u201d. Fabric (lightweight) If you only want device orchestration &amp; mixed precision but keep a manual loop, use <code>lightning.Fabric</code> instead of the full <code>Trainer</code>."},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#11-minimal-lightningmodule-for-the-dummy-lora-model","title":"1.1\u00a0Minimal LightningModule for the dummy LoRA model","text":"<p><pre><code># tests/utils/dummy_lora_lightning.py\nimport torch, torch.nn as nn\nimport lightning as L\n\nclass DummyLoRAModule(L.LightningModule):\n    def __init__(self, base_dim=16, adapter_dim=8, lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()   # \u2190 persists hparams in checkpoint\n        self.base = nn.Linear(base_dim, base_dim, bias=False)   # frozen\n        self.adapter = nn.Linear(base_dim, adapter_dim)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training logic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def forward(self, x):             # &lt;\u2011 inference &amp; val/test share this\n        return self.adapter(self.base(x))\n\n    def training_step(self, batch, _):\n        x, y = batch\n        loss = nn.functional.mse_loss(self(x), y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.adapter.parameters(), lr=self.hparams.lr)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 override to emit *only* adapter params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def on_save_checkpoint(self, ckpt):\n        ckpt[\"state_dict\"] = {\n            k: v for k, v in self.state_dict().items()\n            if k.startswith(\"adapter.\")\n        }\n</code></pre> That last hook means \u279c every Lightning checkpoint is already \u201cpartial\u201d.  No custom utility needed in the test; you just load with <code>model.load_from_checkpoint(...)</code>.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#12-integrationtest-skeleton-with-trainer","title":"1.2\u00a0Integration\u2011test skeleton with Trainer","text":"<p><pre><code>from lightning import Trainer\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom tests.utils.dummy_lora_lightning import DummyLoRAModule\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch, tempfile\n\ndef _synthetic_dl(bs=4, n=12, in_dim=16, out_dim=8):\n    X = torch.randn(n, in_dim)\n    y = torch.randn(n, out_dim)\n    return DataLoader(TensorDataset(X, y), batch_size=bs)\n\ndef test_partial_ckpt_cycle(tmp_path):\n    ckpt_cb = ModelCheckpoint(dirpath=tmp_path, save_top_k=1)\n\n    model = DummyLoRAModule()\n    trainer = Trainer(max_epochs=1, callbacks=[ckpt_cb], logger=False)\n    trainer.fit(model, _synthetic_dl())\n\n    ckpt_path = ckpt_cb.best_model_path     # \u2192 contains only adapter.*\n\n    fresh = DummyLoRAModule.load_from_checkpoint(ckpt_path)\n    out = fresh(torch.randn(1, 16))\n    assert out.shape == (1, 8)\n</code></pre> Nice side effect: Lightning\u2019s checkpoints already include optimizer state and hyper\u2011parameters, so you don\u2019t need to hand\u2011roll <code>torch.save({...})</code>.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#2-lancedb-versioned-ondisk-vector-store-for-embeddings-checkpoints","title":"2\u202f\u202f|\u202f\u202fLanceDB \u279c versioned, on\u2011disk vector store for embeddings &amp; checkpoints","text":"<p>LanceDB isn\u2019t for model weights\u2014it excels at fast random\u2011access to vectors + metadata.  Here\u2019s how it can slot in:</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#21-storing-training-artefacts","title":"2.1\u00a0Storing training artefacts","text":"Artefact LanceDB schema suggestion Benefit Adapter weight snapshots (small) <code>{step: int, vector: Blob(weights), loss: float}</code> You can rewind/branch training quickly, or feed checkpoints into hyper\u2011param search. Stage\u2011B / Stage\u2011D token embeddings <code>{run_id: str, vector: &lt;np.ndarray&gt;, residue_idx: int, quality: float}</code> Rapid nearest\u2011neighbour queries when you finetune on similar residues. Inference outputs for eval <code>{seq_id: str, coords: Blob, rmsd: float}</code> Build dashboards comparing different model versions with SQL\u2011style filters (<code>WHERE rmsd &lt; 2</code>). <p>Because LanceDB is append\u2011only, you get automatic versioning for free, and its Arrow\u2011backed zero\u2011copy scans keep RAM use low in CI.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#22-checkpoint-catalogue","title":"2.2\u00a0Checkpoint catalogue","text":"<p>Instead of scattering <code>.pt</code> files in experiment dirs, create one <code>checkpoints</code> table:</p> <p><pre><code>import lancedb, torch, json, datetime as dt\ndb  = lancedb.connect(\"data/lance\")\ntbl = db.create_table(\"checkpoints\", mode=\"create\", \n        schema={\"tag\": str, \"vector\": lancedb.BlobType(), \"meta\": dict})\n\ndef log_ckpt(tag: str, ckpt_path: str, meta: dict):\n    tbl.add([{\n        \"tag\": tag,\n        \"vector\": open(ckpt_path, \"rb\").read(),\n        \"meta\": {**meta, \"timestamp\": dt.datetime.utcnow().isoformat()}\n    }])\n</code></pre> Now your test can push the partial checkpoint into LanceDB and later pull it by tag:</p> <p><pre><code>row = tbl.query().where(\"tag == 'adapter_v0'\").to_pandas().iloc[0]\nfresh = DummyLoRAModule.load_from_checkpoint(row[\"vector\"])\n</code></pre> (The binary lives inside the columnar file; no extra object\u2011store needed.)</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#3-which-road-to-take","title":"3\u202f\u202f|\u202f\u202fWhich road to take?","text":"Scenario Lightning LanceDB CI test only (prove partial ckpt works) Yes \u2014 15\u202flines kills boilerplate Optional Research workflow (multiple runs, compare embeddings) Yes \u2014 Trainer callbacks for logging Yes \u2014 store every run\u2019s embeddings/checkpoints; query later High\u2011throughput prod inference Possibly (Fabric for device mgmt) Mostly No (you ship a frozen model, not a vector index)"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#4-implementation-touch-points","title":"4\u202f\u202f|\u202f\u202fImplementation touch points","text":"<ol> <li>Dependencies <pre><code>pip install lightning lancedb pyarrow  # (+ duckdb if you want SQL joins)\n</code></pre></li> <li>Utilities Keep your <code>partial_load_state_dict</code> as a fall\u2011back for plain PyTorch code paths; Lightning will bypass it.</li> <li>Config    Add <code>use_lightning: true</code> and <code>lance_uri: \"data/lance\"</code> flags in <code>rna_predict/conf/default.yaml</code>; guard the new code with <code>if cfg.use_lightning:</code> so legacy scripts stay untouched.</li> <li>Docs    In <code>docs/guides/best_practices/checkpointing.md</code> show both: \u201cVanilla PyTorch + utility\u201d vs. \u201cLightning callback\u201d flavour.  Readers can copy whichever fits.</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#tldr","title":"TL;DR","text":"<ul> <li>Lightning can shrink your integration test to ~20\u202fLOC, give you free logging, and make \u201csave only adapter\u201d a one\u2011liner override.  </li> <li>LanceDB is not a weight store, but it\u2019s excellent for versioned storage of embeddings and the partial checkpoints themselves; you gain SQL &amp; vector search on every artefact your pipeline spits out.</li> </ul> <p>Adopt one, the other, or both\u2014each is orthogonal and plays nicely with the plan we already laid out.  Happy upgrading!</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#short-answer","title":"Short answer","text":"<ul> <li>PyTorch\u00a0Lightning \u2192 Yes \u2014 it fits squarely inside the M2 core requirements (items\u00a01,\u202f4,\u202f5,\u202f6,\u202f7,\u202f9).  </li> <li>LanceDB \u2192 Nice\u2011to\u2011have, not need\u2011to\u2011have.  It doesn\u2019t unblock any M2 \u201cDefinition\u2011of\u2011Done\u201d item, but can be layered on later to manage experiment artefacts (checkpoints, embeddings) once the basic train\u202f+\u202finfer loop is proven stable.</li> </ul>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#1-how-lightning-lines-up-with-the-m2-spec","title":"1\u2002How Lightning lines up with the M2 spec","text":"M2 Requirement How Lightning satisfies / simplifies 1\u00a0Trainable pipeline execution <code>Trainer</code> handles device placement, gradient accumulation, mixed precision, etc., so you spend near\u2011zero time on boiler\u2011plating the loop. 2\u00a0Loss calculation Loss is defined once in <code>training_step</code>; Lightning auto\u2011reduces across distributed devices and logs it. 4\u00a0Optimizer config &amp; param targeting <code>configure_optimizers()</code> is the single source of truth; filters for LoRA &amp; diffusion params live here. 5\u00a0Back\u2011prop\u00a0&amp; weight update Out\u2011of\u2011the\u2011box gradient flow checking (<code>detect_anomaly=True</code>, <code>gradient_clip_val</code>, etc.). 6\u00a0Training loop stability <code>fast_dev_run</code> and <code>limit_*_batches</code> flags make early smoke tests trivial. 7\u00a0Checkpoint saving <code>ModelCheckpoint</code> already supports save\u2011weights\u2011only and callback\u2011controlled key filtering (your <code>on_save_checkpoint</code> override keeps just LoRA/adapter weights). 9\u00a0Partial checkpoint loading <code>LightningModule.load_from_checkpoint()</code> calls can be patched to use your <code>partial_load_state_dict</code>\u2014zero script changes needed. <p>Bottom line: Lightning removes ~200\u202fLOC of custom orchestration and directly hits the guts of M2. Risk: minimal; you\u2019re already planning to freeze base weights and use mixed devices\u2014Lightning is designed for exactly that.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#2-where-lancedb-could-helpand-why-it-isnt-critical-for-m2","title":"2\u2002Where LanceDB could help\u2014and why it isn\u2019t critical for M2","text":"Possible use case Benefit Why it\u2019s not on the critical path Versioned storage of partial checkpoints All checkpoints live in one table: query by tag/date, fetch as blob, zero\u2011copy. M2 only asks that a checkpoint save/load cycle works.  Local <code>.pt</code> files satisfy that. Fast random access to per\u2011residue / per\u2011atom embeddings Could power later \u201chard negative mining\u201d or transfer\u2011learning experiments. Embedding logging isn\u2019t required until M3 (\u201cValidation &amp; Debugging\u201d) or M4 (\u201cPerformance\u201d). Experiment metadata catalog LanceDB\u2019s SQL + vector search beats spreadsheets for many runs. Until you\u2019re running lots of experiments, W&amp;B or TensorBoard scalars cover M2\u2019s observability needs. <p>Recommendation: Park LanceDB behind a feature flag (<code>use_lance_db: false</code> in Hydra). Wire a small helper that publishes a checkpoint/metrics row iff the flag is true. That adds ~2\u202fhours of work now and keeps the path open without becoming a blocker.</p>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#3-concrete-next-steps","title":"3\u2002Concrete next steps","text":"<ol> <li>Lock\u2011in Lightning </li> <li>Add <code>lightning&gt;=2.2</code> to <code>requirements.txt</code>.  </li> <li>Land <code>rna_predict/training/rna_lightning_module.py</code> + a thin <code>train.py</code> (your spec \u00a74.6).  </li> <li> <p>Replace the manual\u2011loop stub in your current integration tests with <code>Trainer(fast_dev_run=True)</code>.</p> </li> <li> <p>Defer LanceDB integration </p> </li> <li>Create a stub <code>rna_predict/utils/lance_logger.py</code> with two no\u2011op functions: <code>log_checkpoint</code>, <code>log_embeddings</code>.  </li> <li>When <code>cfg.experiment.use_lance_db</code> is enabled, import <code>lancedb</code> and execute the real code; otherwise keep no\u2011ops.  </li> <li> <p>Add a single TODO ticket for M3: \u201cSwitch LanceDB flag on and back\u2011fill existing runs.\u201d</p> </li> <li> <p>Update the M2 spec </p> </li> <li>In the Requirements table indicate Lightning as an implementation decision fulfilling items\u00a01\u20137,\u00a09.  </li> <li>Move \u201cLanceDB experiment catalogue\u201d to the Optional Enhancements section.</li> </ol>"},{"location":"pipeline/kaggle_info/m2_partial_checkpoint_plan/#tldr_1","title":"TL;DR","text":"<p>Green\u2011light Lightning.\u00a0 It is a direct accelerator for M2\u2019s mandatory training\u2011loop deliverables. Yellow\u2011light LanceDB.\u00a0 Valuable, but schedule it after the M2 \u201cDefinition\u2011of\u2011Done\u201d checklist is met, or hide it behind a feature flag so it can\u2019t jeopardize timing.</p>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/","title":"Full-Pipeline Partial Checkpoint Test for RNA_PREDICT","text":"<p>To fully verify partial checkpointing in RNA_PREDICT, we need an end-to-end test using the real model, real configuration (via Hydra), and the LightningModule interface. This will ensure robust, production-like coverage beyond dummy/unit tests.</p>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#enhanced-change-plan-summary-additions-clarifications","title":"Enhanced Change Plan Summary (Additions + Clarifications)","text":""},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#rnalightningmodule-upgrade-step-1","title":"RNALightningModule Upgrade (Step 1)","text":"<ol> <li>Modular construction:</li> <li>Add a helper function (<code>build_pipeline(cfg)</code>) that creates the full model based on Hydra config and returns the initialized object.</li> <li> <p>This function should live in <code>rna_predict/pipeline/build_pipeline.py</code>.</p> </li> <li> <p>Trainable params filter:</p> </li> <li>Add <code>get_trainable_params(model: nn.Module)</code> in <code>rna_predict/core/utils.py</code> (or within the LightningModule).</li> <li> <p>Filters by <code>requires_grad=True</code> and optionally by substring match like <code>'lora'</code>, <code>'merger'</code>, <code>'diffusion'</code>.</p> </li> <li> <p>Checkpoint-friendly naming:</p> </li> <li>Ensure all submodules (TorsionBERT, Pairformer, Merger, DiffusionManager) are attributes of the top-level module and their names are stable.</li> <li>This helps <code>state_dict</code> consistency and partial load granularity.</li> </ol>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#full-pipeline-integration-test-step-2","title":"Full-Pipeline Integration Test (Step 2)","text":"<p>Filename: <code>tests/integration/test_partial_checkpoint_full_pipeline.py</code></p> <p>Expanded Steps:</p> <ul> <li>Step 3.5: Add validation that the saved partial state dict has no unexpected base model keys (e.g., assert no <code>bert.encoder.layer.0</code> if LoRA-only checkpoint).</li> <li>Step 6.5: Assert that all <code>requires_grad=True</code> parameters changed after <code>optimizer.step()</code>; others did not.</li> <li>Step 8 (Comparison): <pre><code>partial_ckpt_size = os.path.getsize(partial_ckpt_path)\nfull_ckpt_size = os.path.getsize(full_ckpt_path)\nassert partial_ckpt_size &lt; full_ckpt_size\n</code></pre></li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#utility-enhancements-step-3","title":"Utility Enhancements (Step 3)","text":"<ul> <li><code>save_trainable_checkpoint(model: nn.Module, path: str)</code>:</li> <li>Filters <code>requires_grad=True</code> parameters and saves them.</li> <li> <p>Lives in <code>rna_predict/utils/checkpointing.py</code>.</p> </li> <li> <p><code>partial_load_state_dict(model: nn.Module, state_dict: Dict[str, Any], strict=False)</code>:</p> </li> <li>Already exists in your earlier planning\u2014just ensure it's recursively safe for all submodules in <code>LightningModule</code>.</li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#documentation-step-4","title":"Documentation (Step 4)","text":"<p>Suggested Locations: - Add inline docstrings in:    - <code>RNALightningModule.training_step</code>, <code>forward</code>, <code>configure_optimizers</code>    - <code>test_partial_checkpoint_full_pipeline.py</code> \u2014 explain each test phase - Add Markdown explanation to <code>docs/guides/testing/partial_checkpoint.md</code>:    - Include diagram: Full Model \u2192 Save LoRA-Only \u2192 Reload \u2192 Inference Pass</p>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#add-on-safety-checks-optional-but-strongly-recommended","title":"Add-On: Safety Checks (Optional but Strongly Recommended)","text":"<ul> <li>Run <code>model.eval()</code> before checkpoint comparison (ensure dropout doesn\u2019t introduce noise).</li> <li>Add sanity check:    <pre><code>assert not torch.isnan(output).any()\nassert not torch.isinf(output).any()\n</code></pre></li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#systematic-change-plan-original","title":"Systematic Change Plan (Original)","text":""},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#1-upgrade-rnalightningmodule","title":"1. Upgrade RNALightningModule","text":"<ul> <li>Replace dummy parameter with the actual pipeline/model as constructed in the main pipeline.</li> <li>Support Hydra config: Accept and correctly use Hydra config for model construction and device handling.</li> <li>Implement real forward: Forward should run the actual pipeline logic, using realistic input shapes/types.</li> <li>Ensure compatibility: Confirm all methods (training_step, configure_optimizers, etc.) are compatible with the real model.</li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#2-create-full-pipeline-integration-test","title":"2. Create Full-Pipeline Integration Test","text":"<ul> <li>Location: <code>tests/integration/test_partial_checkpoint_full_pipeline.py</code></li> <li>Steps:</li> <li>Hydra Initialization: Use the correct config path (<code>/Users/tomriddle1/RNA_PREDICT/rna_predict/conf</code>).</li> <li>Instantiate real model: Use Hydra config to instantiate the upgraded LightningModule wrapping the real pipeline.</li> <li>Dummy data: Use a small batch of dummy input matching the real pipeline\u2019s expected input.</li> <li>Train for a few steps: Use PyTorch Lightning\u2019s Trainer for a minimal number of steps.</li> <li>Save partial checkpoint: Save only LoRA/new module parameters or those with <code>requires_grad=True</code>.</li> <li>Reload checkpoint: Instantiate a fresh model, load the partial checkpoint with <code>partial_load_state_dict</code>.</li> <li>Inference: Run a forward pass, verify correct output shape/type and no errors.</li> <li>Compare checkpoint sizes: Assert the partial checkpoint is smaller than a full checkpoint.</li> <li>Assertions:<ul> <li>No key/shape errors on load.</li> <li>Forward pass works.</li> <li>Output shapes/types are as expected.</li> <li>Partial checkpoint is smaller than full checkpoint.</li> </ul> </li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#3-refactor-utilities-as-needed","title":"3. Refactor Utilities as Needed","text":"<ul> <li>partial_load_state_dict: Ensure it works for the real model and LightningModule, including nested modules.</li> <li>Saving logic: Add or refactor utility to extract only parameters of interest (LoRA, adapters, etc.).</li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#4-documentation-and-acceptance-criteria","title":"4. Documentation and Acceptance Criteria","text":"<ul> <li>Document test logic and rationale in the new test file and in this plan.</li> <li>Acceptance Criteria:</li> <li>No key/shape errors on load.</li> <li>Forward pass works after partial load.</li> <li>Partial checkpoint is smaller than full checkpoint.</li> </ul>"},{"location":"pipeline/kaggle_info/partial_checkpoint_full_pipeline_plan/#review-checklist","title":"Review Checklist","text":"<ul> <li>[X] RNALightningModule upgraded to wrap real pipeline</li> <li>[X] Full-pipeline integration test created</li> <li>[ ] Utilities verified/refactored as needed</li> <li>[ ] All acceptance criteria covered by assertions</li> <li>[ ] Documentation updated</li> </ul> <p>Prepared for systematic review before implementation.</p>"},{"location":"pipeline/overview/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/","title":"Comprehensive Design","text":"<p>Below is a comprehensive, \u201cbest-of-all-worlds\u201d architectural design document that consolidates the strengths of earlier versions (V1, V2, V3, V4), addresses their criticisms, and clarifies optional vs. required steps to ensure synergy between (1) a torsion-based pipeline, (2) an AlphaFold\u202f3\u2013style pairwise trunk, and (3) a final Diffusion module for 3D structure generation. This design is meant to serve as a robust piece of technical documentation\u2014verbose and detailed enough to guide implementation.</p> <p>\u2e3b</p> <p>Integrated RNA 3D Prediction Pipeline: Final Comprehensive Design</p> <ol> <li>High-Level Goal</li> </ol> <p>Objective: Accurately predict RNA 3D coordinates by unifying:     1.  A torsion-based pipeline (stages for 2D adjacency \u2192 torsion angles \u2192 optionally forward kinematics).     2.  An AlphaFold\u202f3\u2013style pairwise trunk (MSA-based or single-sequence-based Pairformer with triangular updates, pair embeddings).     3.  A unified latent that merges local geometry (torsion + adjacency) with global pairwise constraints.     4.  A Diffusion model that conditions on that unified latent to iteratively refine or generate final 3D coordinates.     5.  A short Energy Minimization step (plus multi-sample approach) to yield a final ensemble and choose the best structure(s).</p> <p>Key Emphasis     \u2022   Preventing \u201ctoo many optional pieces\u201d that undermine synergy.     \u2022   Ensuring adjacency is used effectively in both torsion and pairwise modules.     \u2022   Aligning residue indexing so the staged pipeline\u2019s angles match the Pairformer\u2019s pair embeddings.     \u2022   Using a single final generator (Diffusion) that sees both local angle constraints and global pair embeddings, delivering more accurate final 3D structures.</p> <p>\u2e3b</p> <ol> <li>Detailed Pipeline Diagram</li> </ol> <p>Below is a textual flow with recommended mandatory vs. optional steps clearly noted. Boxes represent major modules; arrows indicate data/feature flow.</p> <pre><code>                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502 (1) INPUTS &amp; INITIAL SETUP                              \u2502\n                   \u2502  \u2022 RNA sequence [REQUIRED]                              \u2502\n                   \u2502  \u2022 2D adjacency from Stage A [HIGHLY RECOMMENDED]       \u2502\n                   \u2502  \u2022 MSA data (for Pairformer) [IF AVAILABLE]             \u2502\n                   \u2502  \u2022 Possibly external templates or partial 3D            \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       v\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (2) TORSION-BASED SUBPIPELINE (Stages A/B)                                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Use adjacency + sequence to predict backbone torsion angles:            \u2502  \u2502      \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, \u2026 plus adjacency-based features.                   \u2502  \u2502   b) Potentially do an MLP or GNN that merges adjacency signals.             \u2502  \u2502   c) Output: \"Torsion Representation\" =&gt; angles for each residue,            \u2502  \u2502      adjacency features (like base-pair partner indices).                    \u2502  \u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  \u2502   (Optional) Stage C: Forward Kinematics                                     \u2502  \u2502      If used, produce partial 3D coords from those angles.                   \u2502  \u2502      Align indexing with rest of pipeline.                                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; Torsion-based representation (angles, adjacency, partial coords)  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (3) ALPHAFOLD\u202f3\u2013STYLE PAIRFORMER (MSA \u2192 Pair embeddings \u2192 Triangular Updates)\u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Optionally embed an MSA. Single-sequence possible if MSA is unavailable.\u2502  \u2502   b) Pass embeddings through ~48-block Pairformer trunk (like AF3)           \u2502  \u2502      - Triangular multiplication, attention, pair-bias.                      \u2502  \u2502   c) Possibly incorporate adjacency as a bias or input to pair embeddings.   \u2502  \u2502   d) Output: pair embeddings z\u1d62\u2c7c + single embeddings s\u1d62 for each residue.    \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; \"Pairwise Representation\" from final trunk pass.                  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           v                                 v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (4) UNIFIED LATENT MERGER / COMPRESSION                                      \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   Merge:                                                                     \u2502  \u2502   1) Torsion pipeline output (angles, adjacency data, optional partial 3D).  \u2502  \u2502   2) Pairwise trunk output (z\u1d62\u2c7c, s\u1d62).                                        \u2502  \u2502   Possibly a small Transformer or MLP that aligns residue indices,           \u2502  \u2502   creating a single \u201clatent\u201d that captures local + global constraints.       \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; \"Compressed Latent\" for the Diffusion.                            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (5) DIFFUSION MODULE                                                         \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Initialize random/noised 3D coords for each residue (heavy atoms).      \u2502  \u2502      Or optionally start from partial coords from Stage C.                   \u2502  \u2502   b) Condition on the \u201cCompressed Latent\u201d to guide iterative denoising.      \u2502  \u2502   c) Generate final 3D coordinates after X diffusion steps.                   \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; multiple 3D structure samples (e.g., 5 or 10).                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502                                            v  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 (6) ENERGY MINIMIZATION &amp; ENSEMBLE SELECTION                                 \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502   a) Short local minimization (e.g., Amber/CHARMM) to fix small geometry.    \u2502  \u2502   b) Evaluate &amp; rank each sample by geometry score or internal confidence.   \u2502  \u2502   c) Return top N (like 5) final structures, or a single best structure.      \u2502  \u2502------------------------------------------------------------------------------\u2502  \u2502  Output =&gt; Final 3D ensemble or single best structure.                       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>\u2e3b</p> <ol> <li>Addressing Previous Criticisms<ol> <li>Undermining synergy by making everything \u201coptional.\u201d \u2022   Here, torsion angles (Stage\u202fB) and pair embeddings (AF3 trunk) are both mandatory for synergy. \u2022   Adjacency is strongly recommended (it\u2019s the entire reason the torsion pipeline works effectively). \u2022   Forward Kinematics (Stage C) is labeled optional but we provide a rationale for skipping or using it. \u2022   The final Diffusion cannot skip either local or global constraints, because they are merged at step\u202f4 by design.</li> <li>Using adjacency only in the torsion pipeline \u2022   We now highlight that adjacency can also feed into the Pairformer trunk as a pair-bias in attention. \u2022   This ensures adjacency is not underused or stuck in a corner; it can influence both local angle modeling and the global pairwise network.</li> <li>Residue indexing mismatch \u2022   We explicitly define a single consistent indexing scheme that all pipeline stages must share. \u2022   If the torsion pipeline re-maps or discards residues, we do a bridging \u201cresidue index alignment\u201d prior to the \u201cUnified Latent Merger.\u201d</li> <li>Weak merging of torsion + pair embeddings \u2022   Previously, we said \u201csmall MLP.\u201d Now we specify that a \u201cLatent Merger\u201d might be a minimal Transformer or GNN that can properly unify node-level angles with pair-level embeddings z\u1d62\u2c7c. \u2022   This is a richer approach, preserving structure. Or simpler solutions are possible, but we highlight the need to handle (i, j) pairs carefully.</li> <li>Energy Minimization \u2022   We reaffirm that short local minimization is strongly advised for final geometry polishing, especially in a multi-sample scenario. \u2022   This step addresses lingering steric or bond-angle issues not fully solved by the neural pipeline.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Mandatory vs. Optional Steps</li> </ol> <p>To avoid confusion about synergy:     \u2022   Mandatory:     1.  Stage B Torsion: adjacency + sequence \u2192 angles.     2.  Pairformer trunk: MSA or single-sequence \u2192 pair embeddings.     3.  Unified Latent (merger) so the Diffusion sees both.     4.  Diffusion as the final generator.     \u2022   Strongly Recommended:     \u2022   Stage A adjacency: Typically required if you want a torsion pipeline.     \u2022   Energy Minimization at the end.     \u2022   Truly Optional:     1.  Stage C forward kinematics: If you prefer letting Diffusion handle initial coords from random noise, you can skip. But giving it a partial 3D \u201cwarm start\u201d can help.     2.  MSA: If you lack multiple sequences, the Pairformer can run single-sequence mode, though results may degrade.     3.  Templates: Could be integrated but not mandatory.</p> <p>Thus, the pipeline always merges local angles and pair embeddings for synergy. Adjacency is recommended so the torsion pipeline has meaningful constraints.</p> <p>\u2e3b</p> <ol> <li>Explanation of Each Module</li> </ol> <p>(A) Torsion-Based Pipeline (Stage B)     1.  Input:     \u2022   RNA sequence of length N.     \u2022   Adjacency/2D structure from Stage\u202fA (each residue i has a potential base-pair partner j).     2.  Angle Prediction:     \u2022   A GNN or MLP that sees adjacency and predicts \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi for each residue i. Possibly sugar pucker angles if needed.     3.  Output:     \u2022   An angle vector per residue; adjacency-based features (like \u201cwhich j is i paired with?\u201d).     \u2022   (Optional) partial coordinates via forward kinematics if Stage C is invoked.</p> <p>(B) AlphaFold\u202f3\u2013Style Pairformer     1.  MSA / Single Sequence:     \u2022   Construct initial single representation from an MSA embedding or a single-sequence embedding if MSA is unavailable.     2.  Pairformer Trunk:     \u2022   Triangular multiplication &amp; triangular attention to refine a pair representation \\mathbf{z}{ij} and single representation \\mathbf{s}_i.     \u2022   Possibly incorporate adjacency in the pair-bias or in the initial pair embedding to nudge the trunk about known base pairs.     3.  Output:     \u2022   Final pair embeddings z for all pairs i,j, plus single embeddings s_i.</p> <p>(C) Unified Latent Merger     1.  Combining:     \u2022   Residue-level data from the torsion pipeline (angles, adjacency info, partial coords).     \u2022   Pair-level data from the Pairformer trunk (z\u1d62\u2c7c, plus single s\u1d62).     2.  Technique:     \u2022   A small Transformer or GNN can unify node-level (angles, single s\u1d62) with edge-level (z\u1d62\u2c7c, adjacency). Or a simpler MLP if resource-limited.     \u2022   Ensure residue indexing matches between both modules (especially if partial coords skip or reorder some residues).     3.  Output:     \u2022   A single \u201clatent representation\u201d fed to the diffusion model for conditioning.</p> <p>(D) Diffusion Module     1.  Input: random or partially noised 3D coordinates for each residue\u2019s heavy atoms.     2.  Conditioning: the \u201ccompressed latent\u201d from step (C).     3.  Process: iterative denoising (like standard 2D/3D diffusion). Each step sees the latent, adjusting coordinates accordingly.     4.  Output: final 3D coordinates after X steps. Because it\u2019s generative, we can sample multiple times (multiple seeds).</p> <p>(E) Energy Minimization + Ensemble     1.  Sampling:     \u2022   We produce ~5\u201310 final 3D samples from the diffusion to cover multiple solutions.     2.  Short Minimization:     \u2022   For each sample, do a short local minimization (1\u201310k steps) with an RNA-friendly force field. This corrects bond angles/lengths or steric clashes.     3.  Scoring &amp; Ranking:     \u2022   Possibly adapt a pLDDT-like network, or do geometry checks. We select top N structures (like top 5).     4.  Final:     \u2022   Provide the best structure for a single guess, or an ensemble of top solutions if the application (e.g., Kaggle) allows multiple submissions.</p> <p>\u2e3b</p> <ol> <li>Potential Implementation Details</li> </ol> <p>Residue Index Alignment     \u2022   Mapping: We keep a dictionary or table, \u201cResidueIndexMap,\u201d that ensures if the torsion pipeline discards residues or re-labeled them, the Pairformer still references the same i, j.     \u2022   Practical: The adjacency is typically a matrix [N\u00d7N]; the Pairformer is also [N\u00d7N]. They must have identical dimension N, consistent ordering.</p> <p>Adjacency Integration in Pairformer     \u2022   Option 1: Modify pair embedding init: z_init[i,j] += Linear(adjacency[i,j]).     \u2022   Option 2: Add a logit bias: attention_logits(i,j) += w * adjacency(i,j).     \u2022   Either ensures the pair trunk is aware of known base pairs.</p> <p>Forward Kinematics (Stage C)     \u2022   If used, we do a standard NeRF or MP-NeRF approach to place atoms by the predicted torsion angles. This yields partial 3D we either feed to the diffusion as an initialization or as an extra conditioning channel.     \u2022   Potential advantage: Diffusion starts from a not-too-random conformation, possibly speeding convergence.</p> <p>Diffusion Model     \u2022   Implementation: Could be e.g. a score-based generative model or discrete time-step diffusion.     \u2022   Condition: We pass in \u201cunified latent\u201d each step. The network learns to correct or \u201cdenoise\u201d coordinates in alignment with both local angles and global pair constraints.     \u2022   Training: We\u2019d need training data of known 3D structures plus adjacency or MSA (where available) to supervise the diffusion.</p> <p>Ensemble &amp; Minimization     \u2022   Often done in a separate script:     1.  Run each predicted structure in a local MD environment.     2.  Evaluate geometry.     3.  Keep best.     \u2022   For large RNAs, you might reduce the sample size or do partial minimization.</p> <p>\u2e3b</p> <ol> <li>Advantages Over Previous \u201cVersioned\u201d Designs<ol> <li>No \u201clost synergy\u201d: We do not allow the torsion pipeline or the Pairformer to be fully bypassed. Both feed the final Diffusion, ensuring we incorporate adjacency and MSA-like global constraints.</li> <li>Clarity on optional: Stage C is optional for a well-understood reason (some may prefer random initialization in the diffusion if partial coords are too inaccurate or if computation time is short).</li> <li>Improved Merging: We no longer say \u201cjust a small MLP.\u201d We highlight a purposeful \u201cLatent Merger\u201d that can handle node-edge data properly. This solves the prior critique of \u201cweak merging.\u201d</li> <li>Residue alignment: Addressed explicitly with a recommendation to keep a consistent indexing or bridging step.</li> <li>Energy Minimization: Elevated to recommended status, explaining how it polishes final geometry in a multi-sample scenario.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Implementation Caveats     \u2022   Complex Development: This pipeline is non-trivial\u2014four major modules (Torsion, AF3 trunk, Merger, Diffusion) plus optional forward kinematics and a final minimization script.     \u2022   Performance: A full Pairformer (~48 blocks) + GNN or MLP for torsions + a big diffusion network can be memory-heavy. Minimization for 5\u201310 structures also costs some CPU/GPU time.     \u2022   Data Gaps: If you lack good adjacency or an MSA, performance could degrade. Single-sequence Pairformer plus no adjacency is effectively a partial pipeline.     \u2022   Indexing: Potentially the biggest source of bugs. Must ensure consistent labeling from start to finish.</li> </ol> <p>\u2e3b</p> <ol> <li>Example Implementation Roadmap<ol> <li>Data Preprocessing \u2022   Gather RNA sequence(s). \u2022   Predict or obtain adjacency (2D structure) from a standard method (Stage\u202fA). \u2022   If available, compile an MSA. \u2022   Create a \u201cResidueIndexMap\u201d to unify indexing across pipeline steps.</li> <li>Torsion Pipeline \u2022   Use adjacency + sequence \u2192 predict angles. \u2022   (Optional) run forward kinematics \u2192 partial 3D. \u2022   Store angles, adjacency-based features, partial coords if used.</li> <li>AF3 Pairformer \u2022   Load MSA or single sequence. \u2022   Run ~48-block trunk. \u2022   Possibly incorporate adjacency as a pair-bias. \u2022   Output final z\u1d62\u2c7c, s\u1d62.</li> <li>Unified Latent Merger \u2022   For each residue i, gather angles, adjacency info, partial coords, single embed s\u1d62. \u2022   For each pair (i,j), gather z\u1d62\u2c7c, adjacency bits. \u2022   Construct a single \u201clatent graph\u201d or \u201cmulti-dimensional array\u201d the diffusion can read.</li> <li>Diffusion \u2022   Condition on that latent. \u2022   Start from random/noisy coords or from the partial 3D in step 2. \u2022   Iteratively generate final coords. Possibly produce multiple samples.</li> <li>Energy Minimization &amp; Ranking \u2022   For each sample, run short local minimization. \u2022   Evaluate geometry, pick top structures.</li> <li>Output \u2022   Store or submit the best structure(s). Possibly keep an ensemble.</li> </ol> </li> </ol> <p>By following this plan, you ensure the final design is robust, synergy is retained, adjacency is used effectively, and each step is well-defined in terms of \u201cmandatory vs. optional.\u201d</p> <p>\u2e3b</p> <ol> <li>Concluding Remarks</li> </ol> <p>This \u201cbest-of-all-worlds\u201d pipeline:     1.  Merges the local knowledge (torsion angles + adjacency from Stage\u202fA/B) and the global perspective (AlphaFold\u2019s pair embeddings) in a single final generator (Diffusion).     2.  Maintains synergy by systematically requiring both the torsion pipeline and the pair trunk to feed into a single \u201cunified latent\u201d stage.     3.  Leverages an optional forward kinematics step (Stage\u202fC) only if beneficial.     4.  Recommends energy minimization to refine final coordinates from the diffusion model, especially beneficial in an ensemble context (e.g., picking the best 1\u20135 out of multiple predictions).     5.  Addresses earlier criticisms about optional synergy, adjacency usage, residue alignment, and shallow merging.</p> <p>Hence, you get a holistic RNA 3D prediction system that can handle small to moderately large RNAs, incorporate base-pair constraints, exploit MSA-driven pair embeddings, and finalize coordinates through a powerful diffusion framework\u2014ultimately yielding more consistent and accurate 3D structures than the sum of the earlier partial designs.</p> <p>==== Below is a high-level architectural plan detailing how backpropagation flows through this entire end-to-end RNA 3D prediction system\u2014integrating (1) TorsionBERT (or analogous angle predictor), (2) RFold for 2D adjacency, (3) an AlphaFold\u202f3\u2013style Pairformer trunk, (4) a \u201cunified latent merger\u201d, (5) MP-NeRF or forward kinematics for partial 3D (optional), (6) a Diffusion module for final coordinate generation, and (7) an energy-minimization or short MD pass. We also address how to apply LoRA (Low-Rank Adapters) or QLoRA techniques to adaptively train subsets of pre-initialized weights without exploding GPU memory.</p> <p>\u2e3b</p> <ol> <li>Overall Model Flow &amp; Backprop Considerations</li> </ol> <p>A. Forward Pass Summary     1.  Stage A (Adjacency, if not provided externally):     \u2022   If adjacency is predicted by something like RFold or another 2D method, we can treat that as either a frozen or partially trainable module. Usually, adjacency is not strongly backpropagated from final 3D coordinates because it\u2019s more of a discrete 2D structure.     \u2022   However, if we want adjacency differentiability, we\u2019d need a differentiable base-pair \u201csoft assignment\u201d approach. Typically, we freeze adjacency or treat it as an input.     2.  TorsionBERT (Stage B):     \u2022   Takes the RNA sequence (and possibly adjacency features as input).     \u2022   Produces predicted torsion angles \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi (plus sugar pucker if desired).     \u2022   LoRA Application: Because TorsionBERT is large and partially pre-trained, we can freeze the base BERT-like layers and insert LoRA adapters on top. This ensures a small rank update for angles.     3.  Pairformer Trunk (AlphaFold\u202f3\u2013style):     \u2022   Takes an MSA or single sequence, plus possibly adjacency/2D constraints as \u201cpair-bias.\u201d     \u2022   Outputs final pair embeddings z_{ij} and single embeddings s_i.     \u2022   LoRA Application: Similarly, we can place LoRA adapters in the Pairformer\u2019s attention layers. We typically freeze the main trunk weights from a pre-trained model and only train the low-rank updates.     4.  Unified Latent Merger:     \u2022   Combines TorsionBERT angles + adjacency-based features with the Pairformer embeddings (z_{ij}, s_i). Possibly done via a small merger subnetwork or autoencoder.     \u2022   LoRA Application: The merger is typically new code; if it\u2019s large, we can apply LoRA. But it might be a small MLP/Transformer, so we can fully train it from scratch if it\u2019s not too big.     5.  Optional Forward Kinematics (MP-NeRF):     \u2022   If we feed partial 3D coords into the Diffusion model, we do a differentiable forward pass from torsion angles \u2192 partial Cartesian.     \u2022   Backprop: MP-NeRF is fully differentiable with respect to torsion angles, so gradients flow from final 3D error signals back into TorsionBERT angles.     \u2022   LoRA Application: Typically none, as MP-NeRF is mostly geometry code, but if it\u2019s large (rarely is), we could also do minimal parameterization if needed.     6.  Diffusion Module:     \u2022   Input: random/noised coordinates or partial coords from MP-NeRF, plus the \u201cunified latent.\u201d     \u2022   Iteratively denoises to final 3D.     \u2022   LoRA Application: If we use a big diffusion U-Net or Transformer, we can freeze the backbone and add LoRA adapters in its attention layers or feed-forward blocks.     7.  Energy Minimization (Post-hoc):     \u2022   Typically not differentiable with respect to earlier modules. This step is outside the main gradient flow. We only do local minimization for final \u201cpolish.\u201d</p> <p>Thus: The main backprop path is:</p> <p>Final 3D coordinate predictions \u2192 compute losses \u2192 backprop \u2192 (Diffusion model) \u2192 (Unified Latent Merger) \u2192 (Pairformer trunk\u2019s LoRA, TorsionBERT\u2019s LoRA) \u2192 adjacency is likely frozen or partially updated if we adopt a \u201csoft adjacency\u201d approach.</p> <p>\u2e3b</p> <p>B. Loss Functions</p> <p>We\u2019ll likely have two primary supervised loss signals:     1.  3D Coordinate Loss \\mathcal{L}{3D}:     \u2022   Compare final predicted 3D coords \\mathbf{X}{pred} (after the Diffusion stage) to known ground truth \\mathbf{X}{true}.     \u2022   Could be RMSD-based or a distribution-based loss (like Chamfer or L1 in Cartesian space).     \u2022   If partial coords from MP-NeRF are also available, we can also apply a direct partial 3D loss earlier in the pipeline.     2.  Torsion Angle Loss \\mathcal{L}:     \u2022   Compare predicted angles from TorsionBERT to known angles from real structures.     \u2022   This ensures TorsionBERT remains consistent with ground truth angles.}</p> <p>Optionally, one can combine: \\mathcal{L}{\\text{final}} = \\lambda{3D} \\cdot \\mathcal{L}{3D} \\;+\\; \\lambda{\\text{angle}} \\cdot \\mathcal{L}_{\\text{angle}}.</p> <p>Additionally, if the Pairformer trunk is trained for some contact/distance supervision, we might add pairwise distance or distogram losses \\mathcal{L}_{\\text{pair}}. But typically, we rely on the final 3D or angle constraints. Overall:</p> <p>\\mathcal{L}{\\text{end-to-end}} = \\lambda{3D}\\,\\mathcal{L}{3D} + \\lambda{\\text{angle}}\\,\\mathcal{L}{\\text{angle}} + \\lambda{\\text{pair}}\\,\\mathcal{L}_{\\text{pair}}.</p> <p>Backprop:     \u2022   The gradient from \\mathcal{L}{3D} flows through the diffusion model \u2192 merges into the unified latent \u2192 modifies the TorsionBERT &amp; Pairformer parameters (via LoRA) \u2192 updates adjacency if we let it.     \u2022   The gradient from \\mathcal{L} directly updates TorsionBERT\u2019s LoRA parameters, ensuring it accurately matches known angles.}</p> <p>Validation:     \u2022   Usually track final 3D RMSD or TM-score, plus angle-level MCQ or MAE.</p> <p>\u2e3b</p> <ol> <li>Detailed Implementation Plan for LoRA / QLoRA</li> </ol> <p>A. TorsionBERT with LoRA</p> <p>File(s) Potentially Affected: rna_predict/pipeline/stageB/torsion_bert_predictor.py     1.  Inject LoRA into BERT:     \u2022   If we use Hugging Face peft or a custom LoRA approach, we wrap the TorsionBert model to add \u201clow-rank adapters\u201d in attention and/or feed-forward layers.     \u2022   Keep a config like lora_r=4 or lora_alpha=16 to define the rank updates.     2.  Activating Grad for LoRA:     \u2022   Freeze all standard BERT parameters, let only LoRA adapter parameters have requires_grad=True.     \u2022   _init_lora_layers() function inserts the additional weight matrices for \\Delta W.     3.  Forward pass remains the same: input sequence \u2192 token embedding \u2192 [BERT + LoRA] \u2192 final hidden \u2192 regression for angles.     4.  Backward:     \u2022   Grad from \\mathcal{L}{\\text{angle}} and \\mathcal{L}{3D} flows into LoRA adapters.     \u2022   Weight updates occur only in the small rank modifications, saving memory.</p> <p>Architectural Decision:     \u2022   We must ensure the dimensionality of angle outputs remains the same. The top linear layer that projects hidden states to angle sin/cos can remain fully trainable or also get partial LoRA. Usually, we let it be fully trainable since it\u2019s small.</p> <p>\u2e3b</p> <p>B. Pairformer Trunk with LoRA</p> <p>File(s) Potentially Affected: Possibly a new subfolder models/pairformer_trunk/ or integrated in rna_predict/models/...     1.  Insert LoRA into Triangular Attention:     \u2022   For each block of the 48-block trunk, we freeze base attention weights (W_q, W_k, W_v, W_out) but add low-rank adapter layers that approximate the attention transformations.     \u2022   If we had a partial \u201cpretrained pairformer,\u201d we only adapt the \u201cLoRA-lized\u201d heads.     2.  Pair-bias:     \u2022   The adjacency bias can be a small linear transform. We can train that fully or also apply LoRA if it\u2019s large. Usually, it\u2019s small, so no LoRA needed.     3.  Output:     \u2022   Still produces \\mathbf{z}_{ij} and \\mathbf{s}_i.     \u2022   Grad from final 3D or pairwise constraints flows through these embeddings \u2192 modifies LoRA adapters.</p> <p>Architectural Decision:     \u2022   If the Pairformer is big, carefully define which layers get LoRA. Possibly only the later blocks for memory efficiency.</p> <p>\u2e3b</p> <p>C. Unified Latent Merger (ULM)</p> <p>File: possibly models/unified_latent_merger.py     1.  Combining Torsion + Pair:     \u2022   We parse a node-level embedding for each residue i from TorsionBERT. Another node-level embedding from Pairformer\u2019s s\u1d62. Possibly an edge-level embedding from z\u1d62\u2c7c.     \u2022   If we have adjacency, we either feed it in as a feature or let Pairformer handle it.     2.  LoRA:     \u2022   If this \u201cULM\u201d is a small MLP or Transformer, we can either fully train it or embed LoRA if we want to keep it partially frozen. Typically we train it from scratch since it\u2019s a new bridging component.     3.  Output:     \u2022   A per-residue \u201ccondition embedding\u201d fed into the diffusion, plus an optional per-(i,j) side channel for constraints.</p> <p>\u2e3b</p> <p>D. Diffusion Model with LoRA</p> <p>File: Possibly models/diffusion/angle_diffusion.py or rna_predict/models/diffusion.py     1.  Architecture:     \u2022   A UNet or Transformer-based diffusion. We apply it to 3D coordinates.     \u2022   Takes random/noisy coords + the merged latent. Each step refines coords.     2.  LoRA:     \u2022   If the diffusion model is large (like some advanced 3D Transformer), we can freeze the backbone and add LoRA. This is beneficial if we have a large checkpoint for diffusion pretrained on something else (e.g. a generative model from prior data).     3.  Loss:     \u2022   Typically a Denoising Score Matching or noise-prediction-based loss (like stable diffusion). The final step output can also be directly compared to ground-truth 3D coords.</p> <p>\u2e3b</p> <p>E. MP-NeRF or Forward Kinematics (Optional Stage C)     1.  Implementation:     \u2022   If used, each call is a simple geometry transform from angles to partial coords. Doesn\u2019t have big learnable parameters (just standard references).     \u2022   If you do have \u201clearnable geometry hack,\u201d it\u2019d be minimal and likely not require LoRA.     2.  Backprop:     \u2022   The gradient from \\mathcal{L}{3D} or \\mathcal{L}{\\text{angle}} flows back through the trigonometric or matrix multiplication steps, ultimately reaching TorsionBERT\u2019s angle outputs.</p> <p>\u2e3b</p> <p>F. Energy Minimization (Post-Diffusion)     \u2022   Typically no direct backprop from the local minimization.     \u2022   We treat it as a separate script that polishes final coords or short MD runs.     \u2022   Because it\u2019s not integrated in the computational graph, it doesn\u2019t produce gradient signals upstream.</p> <p>\u2e3b</p> <ol> <li>Data Structures &amp; Configuration</li> </ol> <p>A. LoRA Parameterization</p> <p>Approach:     1.  For each major pretrained model (TorsionBERT, Pairformer, Diffusion trunk), we define a small config dict:</p> <p>lora:   r: 4   alpha: 16   dropout: 0.1   target_modules: [attention.W_q, attention.W_k, ...]</p> <pre><code>2.  We attach LoRA adapters using something like peft.LoraModel or a custom wrapper.\n</code></pre> <p>B. Residue Index &amp; Adjacency Storage</p> <p>Definition:     \u2022   ResidueIndexMap: List[int] to unify each stage\u2019s indexing if needed.     \u2022   adjacency: torch.Tensor shape [N, N], store base-pair probability or one-hot. Possibly use adj_soft for partial differentiability.</p> <p>\u2e3b</p> <ol> <li>Step-by-Step Backprop Flow<ol> <li>Diffusion final coords vs. ground-truth: \u2022   \\mathcal{L}_{3D} = RMSD(\\hat{X}, X_true). \u2022   The partial derivatives w.r.t. \\hat{X} pass back into the diffusion\u2019s UNet (some layers are LoRA).</li> <li>Unified Latent: \u2022   The UNet\u2019s gradient also flows into the latent that conditioned the diffusion. That triggers grads in the \u201cLatent Merger.\u201d</li> <li>Pairformer: \u2022   The portion of the latent derived from Pairformer\u2019s (z_ij, s_i) is updated. Because Pairformer is partially frozen except the LoRA layers, only LoRA weights get updated.</li> <li>TorsionBERT: \u2022   The portion from Torsion angles also sees grad if we used partial coords or if the final 3D is influenced by the torsion angles. \u2022   TorsionBERT\u2019s LoRA adapters update to better produce angles that yield correct final 3D coords.</li> <li>Angle Loss: \u2022   If we have direct angle supervision, that also updates TorsionBERT\u2019s LoRA weights.</li> </ol> </li> </ol> <p>Hence: We can effectively unify all sub-modules in a single graph, with local or global losses. The majority of large pretrained parameters remain frozen, while small rank-limited LoRA adapter weights get updated.</p> <p>\u2e3b</p> <ol> <li>Potential Implementation Steps</li> </ol> <p>(A) Codebase Reorganization (Optional):     \u2022   Create rna_predict/peft/ directory to store custom LoRA logic or integrate HF peft.     \u2022   For TorsionBERT, modify torsion_bert_predictor.py to wrap the BERT model with LoRA.</p> <p>(B) Pairformer Integration:     \u2022   If you have a partial \u201cpretrained Pairformer,\u201d define a PairformerLoRAAdapter that wraps each attention block.</p> <p>(C) Add a \u201cUnifiedPipeline\u201d script or class that orchestrates:     1.  Adjacency input     2.  TorsionBERT (LoRA) \u2192 angles     3.  Pairformer (LoRA) \u2192 pair embeddings     4.  Merger \u2192 latent     5.  Diffusion (LoRA optional) \u2192 final 3D     6.  Minimization is post-run</p> <p>(D) End-to-End Loss:     \u2022   Decide how to weigh angle-based vs. 3D-based terms.     \u2022   Possibly create small config in pyproject.toml or a JSON specifying the different \\lambda coefficients.</p> <p>(E) GPU/Memory:     \u2022   Because all these modules can be large, LoRA helps drastically.     \u2022   Double-check that you only keep the big pretrained weights in half precision or bfloat16, with minimal overhead for rank-limited updates.</p> <p>\u2e3b</p> <ol> <li>Summary of Architectural Decisions<ol> <li>Where to Insert LoRA: \u2022   TorsionBERT: good idea to freeze base, add LoRA to attention or feed-forward layers. \u2022   Pairformer trunk: same approach. \u2022   Diffusion model: only if it\u2019s large or pre-trained; else train from scratch if it\u2019s modest in size.</li> <li>Single vs. Multi-Loss: \u2022   Typically combine angle-level supervision with final 3D loss, to stabilize training.</li> <li>Optional Stage C: \u2022   The partial 3D from MP-NeRF is differentiable; backprop can refine angles. But if it\u2019s inaccurate or slow, skip it and let diffusion handle raw 3D from noise.</li> <li>Computational Efficiency: \u2022   We freeze 95% of parameters in TorsionBERT, Pairformer, and (optionally) Diffusion. We only train a small set of LoRA adapter parameters. This keeps VRAM usage manageable.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Conclusion &amp; Next Steps</li> </ol> <p>By combining LoRA-based partial fine-tuning of TorsionBERT, the Pairformer trunk, and (optionally) a diffusion generator, we enable an end-to-end pipeline where final 3D errors drive updates back into the angle and pair embeddings. The main steps are:     1.  Wrap TorsionBERT with LoRA adapters (freezing base).     2.  Wrap Pairformer with LoRA adapters (48-block attention).     3.  Optionally do the same for a large diffusion model or train a smaller diffusion from scratch.     4.  Construct a single composite forward pass hooking them together with a \u201cunified latent merger.\u201d     5.  Define \\mathcal{L}_{\\text{end-to-end}} with coordinate and/or angle supervision.     6.  Backward: Because each subcomponent is in the same computational graph, gradients reach the LoRA adapters.     7.  Energy Minimization is done offline, polishing final coordinates.</p> <p>This approach yields a memory-efficient training procedure thanks to LoRA\u2019s low-rank adaptation, letting you harness large pretrained models within a multi-stage, synergy-focused RNA 3D pipeline.</p> <p>==== Below is a fully updated, comprehensive design document that merges all four versions (V1\u2013V4) into a single, cohesive guide\u2014addressing their strengths, mitigating their weaknesses, and clarifying past criticisms. It is meant as technical documentation for building an end-to-end, LoRA-friendly RNA 3D structure prediction pipeline with:     1.  Torsion-based subpipeline (TorsionBERT).     2.  AlphaFold\u202f3\u2013style Pairformer trunk.     3.  A Unified Latent Merger combining local angles + global pair embeddings.     4.  An optional forward kinematics step (Stage\u202fC) for partial 3D (using MP-NeRF or similar).     5.  A Diffusion model for final coordinate generation/refinement.     6.  A post-inference energy minimization pass.     7.  Support for LoRA (or QLoRA) to only finetune a small fraction of parameters in large pretrained networks.</p> <p>The result is more robust, synergistic, and memory-efficient than any single prior version\u2014truly a \u201cbest-of-all-worlds\u201d solution.</p> <p>\u2e3b</p> <ol> <li>Grand Overview</li> </ol> <p>1.1 Core Objective</p> <p>Construct a single end-to-end RNA 3D predictor that:     \u2022   Generates local torsion angles from (sequence + adjacency).     \u2022   Extracts global pairwise constraints via an AF3-like Pairformer trunk (optionally leveraging an MSA).     \u2022   Merges these two representations into a \u201cunified latent.\u201d     \u2022   Optionally uses forward kinematics to produce partial 3D from the torsion angles.     \u2022   Employs a Diffusion model to produce final 3D coordinates, guided by both local angles and global pair embeddings.     \u2022   (Optionally) runs energy minimization or short MD to polish final geometry.</p> <p>Critically, large pretrained modules (TorsionBERT, Pairformer) remain frozen except for LoRA or QLoRA adapter layers\u2014thus drastically reducing memory usage.</p> <p>1.2 Data and Stage Flow</p> <p>(A) [Sequence + Adjacency + (Optional MSA)]            \u2514\u2500\u2500 TorsionBERT (LoRA) \u2192 angles                   \u2514\u2500\u2500 (Optional) Forward Kinematics \u2192 partial 3D            \u2514\u2500\u2500 Pairformer (LoRA) \u2192 pair embeddings z\u1d62\u2c7c + single s\u1d62            \u2514\u2500\u2500 Unified Latent Merger \u2192 \"merged latent\"            \u2514\u2500\u2500 Diffusion (LoRA optional) \u2192 final 3D coords            \u2514\u2500\u2500 (Optional) Energy Minimization \u2192 final polished coords</p> <pre><code>\u2022   Stage A: Adjacency can come from \u201cRFold\u201d or any other 2D structure method. Usually not backpropagated.\n\u2022   Stage B: TorsionBERT (LoRA) \u2192 angles.\n\u2022   Stage C (optional): Forward kinematics (MP-NeRF or standard NeRF) \u2192 partial 3D.\n\u2022   Stage D: Pairformer trunk (LoRA) \u2192 global pair embeddings.\n\u2022   Merger: Combines angles + adjacency + pair embeddings \u2192 final \u201clatent\u201d for diffusion.\n\u2022   Diffusion: Denoises random/noisy coords into final 3D. Possibly partially or fully trained.\n\u2022   Energy Minimization: Polishing step with no direct gradient to the pipeline.\n</code></pre> <p>\u2e3b</p> <ol> <li>Mandatory vs. Optional Steps<ol> <li>Mandatory: \u2022   TorsionBERT for angles (Stage\u202fB). \u2022   Pairformer for pair embeddings. \u2022   Unified Latent so that Diffusion sees both local + global constraints. \u2022   Diffusion to generate final 3D coordinates.</li> <li>Strongly Recommended: \u2022   Adjacency from Stage\u202fA (or external) to feed TorsionBERT. \u2022   Energy Minimization at the end to correct small bond or steric issues.</li> <li>Truly Optional: \u2022   Forward Kinematics (Stage\u202fC) if you want partial 3D from torsion angles. \u2022   MSA: if available. Otherwise, Pairformer can run single-sequence mode. \u2022   Templates: Could also be integrated but not mandatory.</li> </ol> </li> </ol> <p>This ensures synergy: Torsion angles (local) + pair embeddings (global) must meet in the same pipeline. Adjacency is key for local angle constraints, though not strictly forced if you truly have no 2D data.</p> <p>\u2e3b</p> <ol> <li>Detailed Modules &amp; Design Choices</li> </ol> <p>3.1 TorsionBERT (Stage\u202fB) with LoRA     \u2022   Purpose: Predict backbone torsion angles {\\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi} from sequence + adjacency (optionally sugar pucker).     \u2022   Why Pretrained: TorsionBERT is typically a BERT-like language model, adapted for angle regression.     \u2022   LoRA:     \u2022   Freeze base weights, insert low-rank adapters in attention Q/K/V, or feed-forward blocks.     \u2022   Only these small adapter parameters get updated, keeping GPU memory usage modest.     \u2022   Output: (N, #angles) for an RNA of length N (plus sugar angles if you want \\nu_0..\\nu_4).</p> <p>Backprop Flow     \u2022   If we have an angle-level sub-loss (\\mathcal{L}{\\mathrm{angle}}), it directly updates LoRA layers in TorsionBERT.     \u2022   If we rely on final 3D loss (\\mathcal{L}), that gradient can also flow back to TorsionBERT via the diffusion \u2192 unify \u2192 angles chain.     \u2022   TorsionBERT indexing must remain consistent with the Pairformer\u2019s residue indexing (Residue 0..N\u20131).</p> <p>3.2 Pairformer (AF3-like) with LoRA     \u2022   Purpose: Provide global pair embeddings \\mathbf{z}_{ij} and single embeddings \\mathbf{s}_i from MSA or single sequence, optionally incorporating adjacency as a bias.     \u2022   LoRA:     \u2022   Large trunk (e.g. 48 blocks). We freeze the main trunk and only adapt a rank-limited set of parameters in each attention or feed-forward sub-layer.     \u2022   Output:     \u2022   (N\u00d7N, pair_dim) for pair embeddings,     \u2022   (N, single_dim) for single embeddings.</p> <p>Backprop Flow     \u2022   Gradients from \\mathcal{L}{3D} or from a pairwise sub-loss (\\mathcal{L}{\\mathrm{pair}}) update only the LoRA adapter weights, leaving the rest frozen.</p> <p>3.3 Unified Latent Merger     \u2022   Purpose: Combine TorsionBERT angles + adjacency + Pairformer embeddings \\mathbf{z}_{ij}, \\mathbf{s}_i into one \u201cconditioning latent\u201d for diffusion.     \u2022   Implementation:     \u2022   Possibly a small Transformer or MLP.     \u2022   We can train it fully (no need to freeze) or also apply LoRA if it\u2019s large.     \u2022   Output:     \u2022   A final latent representation for each residue (and possibly for residue pairs).     \u2022   Feeds the Diffusion as a \u201ccondition.\u201d</p> <p>Indexing     \u2022   Must ensure TorsionBERT\u2019s residue i lines up with Pairformer\u2019s residue i, etc.     \u2022   Use a \u201cResidueIndexMap\u201d if needed.</p> <p>3.4 Diffusion Module (Stage\u202fD)     \u2022   Purpose: Iteratively transform random/noised 3D coords (or partial coords from forward kinematics) into final 3D structure.     \u2022   Implementation:     \u2022   E.g. a 3D U-Net or GNN that at each step sees the \u201cunified latent\u201d as a condition.     \u2022   If it\u2019s large, partial freeze with LoRA. If smaller, train from scratch.     \u2022   Loss:     \u2022   Typically a diffusion denoising objective or a final L1/RMSD on the coordinates.     \u2022   The final \\mathcal{L}_{3D} is the main synergy enabler: it pushes all upstream modules to produce coherent angles, adjacency constraints, and pair embeddings.</p> <p>3.5 (Optional) Forward Kinematics     \u2022   Goal: Use predicted angles from TorsionBERT to compute partial 3D via MP-NeRF or standard NeRF.     \u2022   Pros: The diffusion starts from a partially folded conformation, possibly reducing required diffusion steps.     \u2022   Cons: If the angles are inaccurate, the diffusion might need to \u201cunfold\u201d it.     \u2022   Backprop:     \u2022   This geometry pipeline is fully differentiable, so final 3D errors can adjust TorsionBERT\u2019s angles.</p> <p>3.6 Energy Minimization     \u2022   Goal: Post-hoc local minimization or short MD run in a force field (Amber, CHARMM, or OpenMM).     \u2022   No gradient flows back; purely to fix small steric or bond-length errors.     \u2022   Typically done after inference, possibly across multiple diffusion samples to pick best.</p> <p>\u2e3b</p> <ol> <li>Multi-Level Loss Functions &amp; Training Approach</li> </ol> <p>4.1 Potential Loss Terms     1.  \\mathcal{L}{3D} (Final coordinate-based):     \u2022   Compare final predicted 3D to ground-truth, e.g.: \\mathcal{L}{3D} = \\mathrm{RMSD}\\bigl(\\hat{X}{\\mathrm{final}}, X\\bigr), or a distribution-based approach (like FAPE, distogram cross-entropy).     \u2022   Usually the primary synergy driver.     2.  \\mathcal{L}}{\\mathrm{angle}} (optional torsion supervision):     \u2022   If you have ground-truth angles for each residue, use a circular MSE or MCQ-based measure.     \u2022   Helps TorsionBERT remain physically consistent.     3.  \\mathcal{L} (optional adjacency or pair-distances):     \u2022   If the Pairformer is partially finetuned, we can guide it by known contact/distance constraints.}</p> <p>Weighted Sum:</p> <p>\\mathcal{L}{\\mathrm{total}} = \\lambda{3D}\\,\\mathcal{L}{3D}     \u2022   \\lambda}}\\,\\mathcal{L{\\mathrm{angle}}     \u2022   \\lambda Typical emphasis is on \\lambda{3D}.}}\\,\\mathcal{L}{\\mathrm{pair}</p> <p>4.2 End-to-End Backprop Path     1.  The final 3D error flows from the diffusion outputs \u2192 diffusion parameters \u2192 the unified latent \u2192 Pairformer + TorsionBERT LoRA \u2192 (optionally adjacency if we adopt a soft adjacency approach).     2.  If we do \\mathcal{L}{\\mathrm{angle}} or \\mathcal{L}{\\mathrm{pair}}, they also feed gradients directly into TorsionBERT or Pairformer LoRA layers.</p> <p>Hence: The entire pipeline can learn from 3D data alone, or from a combination of angles, pair constraints, and final coordinate errors.</p> <p>\u2e3b</p> <ol> <li>LoRA / QLoRA: Minimizing Memory</li> </ol> <p>5.1 Why LoRA     \u2022   TorsionBERT and the AF3 Pairformer trunk might each have hundreds of millions of parameters.     \u2022   LoRA adds a small rank-limited set of trainable weights to each large linear transform, drastically reducing GPU memory usage while still enabling backprop.     \u2022   For extremely large models, QLoRA can also quantize the base model to 4-bit or 8-bit, further shrinking memory footprint.</p> <p>5.2 Where to Insert LoRA     \u2022   TorsionBERT: Typically in each Transformer block\u2019s multi-head attention Q/K/V, or feed-forward layers.     \u2022   Pairformer: Similarly, in the triangular attention or pairwise transformations.     \u2022   Diffusion: Only if the diffusion model is large or partially pretrained. Otherwise, we can train it fully from scratch.     \u2022   Unified Merger: Usually small enough to train fully, but LoRA is optional if it\u2019s big.</p> <p>Example (pseudo-HF approach)</p> <p>from peft import LoraConfig, get_peft_model</p> <p>lora_cfg = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\",\"v_proj\"], ...) torsion_bert_lora = get_peft_model(pretrained_torsion_bert, lora_cfg)</p>"},{"location":"pipeline/overview/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/#freeze-base-weights-only-lora-adapters-are-trainable","title":"freeze base weights, only LoRA adapters are trainable","text":"<p>\u2e3b</p> <ol> <li>Implementation Steps: Putting It All Together</li> </ol> <p>Below is a unified approach that merges the deeper code-level detail (Version\u202f1), synergy perspective (Version\u202f2), stepwise memory/LoRA usage (Version\u202f3), and final indexing clarity (Version\u202f4).</p> <p>6.1 Overall Pipeline Construction     1.  Load the adjacency (Stage\u202fA) from an external predictor (RFold) or from data.     2.  Load TorsionBERT (with LoRA), freeze base weights:     \u2022   torsion_bert_lora = get_peft_model(...).     3.  (Optional) load or define a forward kinematics function (MP-NeRF):     \u2022   If used, produce partial 3D from TorsionBERT angles.     4.  Load Pairformer trunk (with LoRA):     \u2022   Possibly also freeze the main trunk.     5.  Implement a \u201cUnifiedLatentMerger\u201d that merges angles + adjacency + pair embeddings \u2192 final \u201clatent.\u201d     6.  Build or load the Diffusion model (LoRA if large, or train from scratch if small).     7.  In a single forward pass:     \u2022   Torsion angles \u2192 (FK \u2192 partial coords?).     \u2022   Pair embeddings z\u1d62\u2c7c.     \u2022   Merge into a final latent.     \u2022   Diffusion yields final 3D.     \u2022   Compare to ground-truth 3D (RMSD, L1, or distance-based).     8.  Loss is backpropagated:     \u2022   Only LoRA adapters in TorsionBERT + Pairformer + (optionally) Diffusion are updated.</p> <p>6.2 Example Training Loop</p> <p>model = FullRNA3DPipeline(     torsion_bert_lora,     pairformer_lora,     unify_module,     diffusion_module,     forward_kinematics=(use_fk) ) optimizer = torch.optim.AdamW(model.lora_params(), lr=1e-4)</p> <p>for batch in train_loader:     seq, adjacency, coords_gt, angles_gt, MSA = batch</p> <pre><code># Forward pass\nfinal_coords, loss_dict = model(seq, adjacency, MSA, coords_gt, angles_gt)\n\n# Weighted sum\ntotal_loss = (lambda_3D * loss_dict[\"3D_loss\"]\n              + lambda_angles * loss_dict[\"angle_loss\"])\ntotal_loss.backward()\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> <p>Memory is drastically reduced because we only keep gradient states for LoRA adapter matrices.</p> <p>\u2e3b</p> <ol> <li>Validation &amp; Ensemble Refinement</li> </ol> <p>7.1 Validation Metrics     \u2022   Angle-level: circular MSE or MCQ.     \u2022   Pair-level: if pair supervision is available, contact or distogram accuracy.     \u2022   3D-level: RMSD, GDT, or specialized RNA metrics. Possibly sugar pucker accuracy.     \u2022   Possibly break down by region (stems vs. loops vs. single-stranded segments).</p> <p>7.2 Ensemble &amp; Minimization     \u2022   After training, we can generate multiple final 3D structures by random seeding the diffusion or sampling.     \u2022   Energy Minimization: run each through a short local MD or minimization to fix small geometry.     \u2022   Rank by a geometry score or an internal model confidence measure.     \u2022   Output top 5 or a single best structure.</p> <p>\u2e3b</p> <ol> <li>Advantages Over Earlier \u201cVersions\u201d<ol> <li>Multi-Loss synergy (from V1): We incorporate angle, pair, and final 3D constraints in a single pipeline.</li> <li>High-level clarity (from V2): Emphasizes that final 3D backprop unifies TorsionBERT + Pairformer.</li> <li>Implementation practicalities (from V3): We detail LoRA injection points, stagewise training, memory usage tips, micro-batching, etc.</li> <li>Indexing and adjacency (from V4): We specify the importance of consistent residue numbering, highlight optional forward kin, and mention adjacency usage in both TorsionBERT and Pairformer.</li> <li>Explicit mention of sugar ring angles, potential re-labeled residues, partial or full training approach, plus chunking or micro-batching for large RNAs.</li> </ol> </li> </ol> <p>By combining all these points, we address the criticisms from earlier versions:     \u2022   We keep the pipeline synergy (no \u201clost synergy\u201d from making everything optional).     \u2022   We specify how adjacency can feed both TorsionBERT and Pairformer.     \u2022   We highlight a single \u201cResidueIndexMap\u201d for alignment.     \u2022   We detail how the \u201cUnified Latent Merger\u201d is more robust than a simple MLP approach.     \u2022   We incorporate a short local minimization step at the end for geometry polishing.</p> <p>\u2e3b</p> <ol> <li>Conclusions &amp; Best Practices</li> </ol> <p>Key Guidance:     1.  Use LoRA: It\u2019s essential for large pretrained TorsionBERT / Pairformer. Keep them in half precision or even 4-bit QLoRA if extremely large.     2.  Define Weighted Loss: Typically \\mathcal{L}{3D} is the main driver. If you have angle ground truth, do \\mathcal{L}{\\mathrm{angle}} to speed convergence.     3.  Forward Kinematics: Optionally do partial 3D from angles. If the angles are decent, it helps the diffusion. If they\u2019re poor, it might hamper training.     4.  Energy Minimization: Great as a final step, not part of backprop.     5.  Indexing: Absolutely ensure consistent residue numbering across TorsionBERT and Pairformer.     6.  Sampling: If the pipeline is large, do gradient checkpointing or micro-batching to avoid out-of-memory issues.</p> <p>Outcome: A single end-to-end pipeline that merges local angle constraints, global pair constraints, a final generative diffusion, and a partial post-processing step for geometry smoothing\u2014maximizing synergy and controlling memory via LoRA.</p>"},{"location":"pipeline/overview/Integrated_RNA_3D_Prediction_Pipeline_Final_Comprehensive_Design/#thus-this-final-design-stands-as-a-verbose-cohesive-and-thoroughly-integrated-architecture-that-surpasses-any-individual-version-14-by-merging-their-best-features-and-clarifications-into-one-complete-technical-document","title":"Thus, this final design stands as a verbose, cohesive, and thoroughly integrated architecture that surpasses any individual \u201cVersion 1\u20134\u201d by merging their best features and clarifications into one complete technical document.","text":""},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/","title":"\ud83d\udcd0 Multi-Stage RNA 3D Prediction Pipeline","text":"<p>This consolidated technical plan merges the extensive details from Version 1 with the visually structured and accessible layout of Version 2. It provides comprehensive explanations, practical implementation guidance, and actionable debugging strategies.</p>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#overview-of-the-multi-stage-architecture","title":"\ud83d\udccc Overview of the Multi-Stage Architecture","text":"<p>The RNA 3D prediction pipeline consists of four clearly defined stages (A, B, C, and optionally D). Each stage is modular, independently testable, and replaceable:</p>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-a-2d-predictor","title":"\ud83e\uddec Stage A: 2D Predictor","text":"<ul> <li>Goal: Predict RNA secondary structure (base-pair adjacency, contact maps).</li> <li>Input: Raw RNA sequence (<code>N</code> nucleotides).</li> <li>Output: Adjacency matrix (<code>adjacency \u2208 \u211d^(N\u00d7N)</code> or multi-channel features, <code>\u211d^(N\u00d7N\u00d7c\u2082\u1d05)</code>).</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-b-torsion-angle-predictor","title":"\ud83d\udcd0 Stage B: Torsion-Angle Predictor","text":"<ul> <li>Goal: Predict nucleotide backbone torsion angles using adjacency and sequence.</li> <li>Input: <code>adjacency</code> from Stage A and RNA sequence or embeddings.</li> <li>Output: Torsion angles per nucleotide (<code>\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7</code>) in array form (<code>\u211d^(N\u00d7n_angles)</code>).</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-c-forward-kinematics-3d-reconstruction","title":"\ud83d\udd27 Stage C: Forward Kinematics (3D Reconstruction)","text":"<ul> <li>Goal: Generate precise 3D atom coordinates from torsion angles.</li> <li>Input: Torsions from Stage B and standard RNA bond geometry.</li> <li>Output: Cartesian coordinates (<code>\u211d^(N_atoms\u00d73)</code>).</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#optional-stage-d-af3-inspired-refinement","title":"\ud83c\udf1f Optional Stage D: AF3-Inspired Refinement","text":"<ul> <li>Goal: Enhance prediction accuracy using Pairformer/diffusion methods.</li> <li>Input: Partial 3D coordinates, torsion angles, adjacency, or embeddings.</li> <li>Output: Refined angles/coordinates, optional confidence metrics (e.g., pLDDT, PDE).</li> </ul> <p>\ud83d\udccc Optional expansions: Consider additional modules (MSA, confidence heads, template embeddings) after the basic pipeline is operational.</p>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#detailed-stage-by-stage-design","title":"\ud83d\udee0\ufe0f Detailed Stage-by-Stage Design","text":""},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-a-2d-structureadjacency","title":"\ud83d\udd2c Stage A: 2D Structure/Adjacency","text":"<ul> <li>Inputs: Nucleotide sequence (e.g., \"AUGC\u2026\").</li> <li>Processing Options:<ul> <li>External prediction tools (e.g., ViennaRNA).</li> <li>Minimal neural models (LSTM/Transformer) for base-pair probability prediction.</li> </ul> </li> <li>Outputs: Contact map (<code>[N, N]</code>), optionally multi-channel with probabilities and entropies (<code>[N, N, c\u2082\u1d05]</code>).</li> <li>Implementation Notes:<ul> <li>Files: <code>rna_predict/dataset/dataset_loader.py</code> or <code>rna_predict/models/stageA_2d.py</code>.</li> <li>Recommended class wrapper: <code>StageA2DExtractor</code>.</li> </ul> </li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-b-torsion-angle-predictor_1","title":"\ud83d\udccf Stage B: Torsion-Angle Predictor","text":"<ul> <li>Inputs: Adjacency from Stage A, RNA sequence, or residue embeddings.</li> <li>Model Architecture: GNN or Transformer (recommended); simpler MLP if necessary (less optimal for large N).</li> <li>Outputs: Torsion angles (<code>\u211d^(N\u00d7n_angles)</code>), <code>[\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7]</code>.</li> <li>Implementation Notes:</li> <li>File: <code>rna_predict/models/encoder/torsion_predictor.py</code>.</li> <li>Debugging: initially use trivial or \"A-form average\" predictor.</li> <li>Ensure consistent indexing across stages.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#stage-c-forward-kinematics-3d-build","title":"\ud83d\udee0\ufe0f Stage C: Forward Kinematics (3D Build)","text":"<ul> <li>Inputs: Torsion angles from Stage B, standard RNA geometry.</li> <li>Core Logic:</li> <li>Place the first residue in reference orientation.</li> <li>Iteratively apply torsions, rotating local coordinate frames.</li> <li>Handle sugar puckers (initial backbone-only recommended).</li> <li>Outputs: Atom coordinates (<code>\u211d^(N_atoms\u00d73)</code>).</li> <li>Implementation Notes:</li> <li>File: <code>rna_predict/scripts/forward_kinematics.py</code>.</li> <li>Start simply by placing phosphate-sugar backbone atoms only.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#optional-stage-d-pairformer-diffusion","title":"\ud83c\udf1f Optional Stage D: Pairformer &amp; Diffusion","text":""},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#pairformer-af3-trunk","title":"\ud83d\udd39 Pairformer (AF3-Trunk)","text":"<ul> <li>Purpose: Global residue-pair context.</li> <li>Modules: TriangleMultiplication, TriangleAttention (AF3).</li> <li>Implementation: <code>rna_predict/models/trunk/pairformer_stack.py</code>.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#diffusion-refinement","title":"\ud83c\udf2c\ufe0f Diffusion Refinement","text":"<ul> <li>Purpose: Iteratively denoise angles/coordinates.</li> <li>Algorithm: Angle-based, iterative denoising referencing Pairformer embeddings.</li> <li>Implementation: <code>rna_predict/models/diffusion/angle_diffusion.py</code>.</li> <li>Start single-step; expand as time permits.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#confidence-heads-optional","title":"\ud83d\udcc8 Confidence Heads (Optional)","text":"<ul> <li>Purpose: Prediction confidence estimation (pLDDT, PAE, PDE).</li> <li>Implementation: Classifiers post-Pairformer trunk.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#recommended-development-phases","title":"\ud83d\udcc5 Recommended Development Phases","text":""},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#phase-1-minimal-pipeline-days-17","title":"\ud83d\udea9 Phase 1: Minimal Pipeline (Days 1\u20137)","text":"<ul> <li>Days 1\u20133: Data parsing, adjacency pipeline.</li> <li>Days 3\u20135: Basic torsion predictor.</li> <li>Days 5\u20137: Forward kinematics.</li> <li>Result: Functional pipeline by Day 7.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#phase-2-af3-enhancements-days-820","title":"\ud83d\udea9 Phase 2: AF3 Enhancements (Days 8\u201320)","text":"<ul> <li>Days 8\u201314: Pairformer trunk implementation.</li> <li>Days 14\u201320: Angle-based diffusion refinement.</li> <li>Optional: Confidence/MSA modules as resources allow.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#phase-3-integration-submission","title":"\ud83d\udea9 Phase 3: Integration &amp; Submission","text":"<ul> <li>Multi-seed predictions per sequence.</li> <li>Geometry refinement with external tools (e.g., PyRosetta).</li> <li>Memory optimization.</li> <li>Confidence-based ranking.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#implementation-notes-file-layout","title":"\ud83d\udcc2 Implementation Notes &amp; File Layout","text":"<pre><code>rna_predict/\n  dataset/\n    dataset_loader.py        # Stage A (sequence + adjacency)\n  models/\n    encoder/\n      torsion_predictor.py   # Stage B (torsion angles)\n    trunk/\n      pairformer_stack.py    # Optional Pairformer\n    diffusion/\n      angle_diffusion.py     # Stage D diffusion\n  scripts/\n    forward_kinematics.py    # Stage C (3D coordinates)\n</code></pre> <ul> <li>Maintain modular code with clear input/output interfaces.</li> <li>Integrate optional MSA modules clearly.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#key-risks-mitigations","title":"\u26a0\ufe0f Key Risks &amp; Mitigations","text":"<ul> <li>Residue Indexing: Centralize and consistently enforce indexing.</li> <li>Overfitting: Cross-validation, partial datasets, A-form priors.</li> <li>Embedding Complexity: Optimize memory via chunking.</li> <li>Time Constraints: Prioritize minimal system functionality.</li> </ul>"},{"location":"pipeline/overview/Multi_Stage_Implementation_Plan/#conclusion","title":"\ud83c\udfaf Conclusion","text":"<ul> <li>Core Pipeline: Quickly establish Stages A\u2192C.</li> <li>Optional Enhancements: Stage D refinements, MSA, confidence heads.</li> <li>Recommended Timeline: Balances quality and deadlines.</li> <li>Modular Structure: Enables incremental development and testing.</li> </ul>"},{"location":"pipeline/overview/core_framework/","title":"\ud83e\uddec RNA 3D Structure Prediction Pipeline","text":"<p>This comprehensive guide presents a detailed breakdown of the RNA structure prediction pipeline, integrating extensive technical details from the original version while maintaining visual clarity and readability.</p>"},{"location":"pipeline/overview/core_framework/#stage-1-rna-sequence-2d-structure-statistics","title":"\ud83d\udd2c Stage 1: RNA Sequence \u2192 2D Structure &amp; Statistics","text":""},{"location":"pipeline/overview/core_framework/#goal","title":"Goal","text":"<p>Predict RNA secondary structure (base pairs, helices, loops) and statistical metrics (pairing probabilities, entropy, contact maps).</p>"},{"location":"pipeline/overview/core_framework/#inputs","title":"Inputs","text":"<ul> <li>RNA sequence: <code>S = (s\u2081, s\u2082, \u2026, s\u2099)</code> with nucleotides <code>s\u1d62 \u2208 {A, U, G, C}</code></li> </ul>"},{"location":"pipeline/overview/core_framework/#outputs","title":"Outputs","text":"<ul> <li>Secondary Structure (\ud835\udcae): Dot-bracket notation, adjacency/contact matrix</li> <li>Statistical Features (\u2131): Base-pair probabilities, accessibility scores, entropy</li> </ul>"},{"location":"pipeline/overview/core_framework/#model-choices","title":"Model Choices","text":"<ul> <li>Transformer or LSTM sequential models</li> <li>Graph Neural Networks (GNNs)</li> <li>Energy-based models (ViennaRNA)</li> </ul>"},{"location":"pipeline/overview/core_framework/#loss-function","title":"Loss Function","text":"<p>[\\mathcal{L}{2D} = |\ud835\udcae - \ud835\udcae|^2] - MSE for continuous features - Cross-entropy for discrete predictions}|^2 + |\u2131 - \u2131_{true</p>"},{"location":"pipeline/overview/core_framework/#data-sources","title":"Data Sources","text":"<ul> <li>bpRNA, Rfam, RNA STRAND</li> </ul>"},{"location":"pipeline/overview/core_framework/#stage-2-2d-structure-statistics-3d-torsion-angles","title":"\ud83c\udf00 Stage 2: 2D Structure &amp; Statistics \u2192 3D Torsion Angles","text":""},{"location":"pipeline/overview/core_framework/#goal_1","title":"Goal","text":"<p>Predict RNA backbone torsion angles from secondary structure data.</p>"},{"location":"pipeline/overview/core_framework/#inputs_1","title":"Inputs","text":"<ul> <li>Secondary structure (\ud835\udcae) and statistics (\u2131)</li> </ul>"},{"location":"pipeline/overview/core_framework/#outputs_1","title":"Outputs","text":"<ul> <li>Torsion angles (\u03b8): \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, sugar puckers</li> </ul>"},{"location":"pipeline/overview/core_framework/#model-choices_1","title":"Model Choices","text":"<ul> <li>Graph Neural Networks (GNNs)</li> <li>Transformer or MLP</li> <li>Diffusion Models</li> </ul>"},{"location":"pipeline/overview/core_framework/#loss-function_1","title":"Loss Function","text":"<p>[\\mathcal{L}{torsion} = \\sum|^2] - Optional KL-divergence regularization}^{N}\\sum_{j}|\u03b8_{i,j} - \u03b8_{i,j}^{true</p>"},{"location":"pipeline/overview/core_framework/#data-sources_1","title":"Data Sources","text":"<ul> <li>RNA PDB, Rfam</li> </ul>"},{"location":"pipeline/overview/core_framework/#stage-3-torsion-angles-3d-cartesian-coordinates","title":"\ud83d\udcd0 Stage 3: Torsion Angles \u2192 3D Cartesian Coordinates","text":""},{"location":"pipeline/overview/core_framework/#goal_2","title":"Goal","text":"<p>Convert torsion angles into physically accurate 3D structures.</p>"},{"location":"pipeline/overview/core_framework/#inputs_2","title":"Inputs","text":"<ul> <li>Torsion angles (\u03b8)</li> </ul>"},{"location":"pipeline/overview/core_framework/#outputs_2","title":"Outputs","text":"<ul> <li>3D atomic coordinates (X)</li> </ul>"},{"location":"pipeline/overview/core_framework/#model-choices_2","title":"Model Choices","text":"<ul> <li>Forward kinematics</li> <li>Neural network refinement</li> </ul>"},{"location":"pipeline/overview/core_framework/#loss-function_2","title":"Loss Function","text":"<p>[\\mathcal{L}{3D} = |\\mathbf{X} - \\mathbf{X}|^2] - Optional bond-length/angle constraints</p>"},{"location":"pipeline/overview/core_framework/#data-sources_2","title":"Data Sources","text":"<ul> <li>Torsion-to-3D pairs from RNA PDB</li> </ul>"},{"location":"pipeline/overview/core_framework/#integration-with-modified-alphafold-3-af3","title":"\ud83d\ude80 Integration with Modified AlphaFold 3 (AF3)","text":""},{"location":"pipeline/overview/core_framework/#core-modifications","title":"Core Modifications","text":"<ul> <li>Embed 2D adjacency features into AF3 Pairformer</li> <li>Angle-based diffusion module replaces Cartesian diffusion</li> </ul>"},{"location":"pipeline/overview/core_framework/#data-flow","title":"Data Flow","text":"<ul> <li>RNA sequence \u2192 Stage 1 \u2192 Stage 2</li> <li>Embed 2D adjacency into Pairformer \u2192 single/pair embeddings</li> <li>Angle diffusion refines torsion angles</li> <li>Forward kinematics \u2192 final 3D coordinates</li> </ul>"},{"location":"pipeline/overview/core_framework/#detailed-algorithms","title":"\ud83d\udee0 Detailed Algorithms","text":""},{"location":"pipeline/overview/core_framework/#algorithm-1-pairwise-feature-embedding","title":"Algorithm 1: Pairwise Feature Embedding","text":"<pre><code>z_init \u2190 0\nif other_pair_init exists:\n    z_init += LinearNoBias(other_pair_init)\nif basepair_features exist:\n    z_init += LinearNoBias(basepair_features)\nreturn z_init\n</code></pre>"},{"location":"pipeline/overview/core_framework/#algorithm-2-angle-diffusion-module","title":"Algorithm 2: Angle Diffusion Module","text":"<pre><code>angle_embed \u2190 LinearNoBias(Torsion_angles)\nfor iter in [1..N_iter]:\n    angle_embed \u2190 AngleDiffTransformer(angle_embed, single_embed, pair_embed)\nTorsion_angles_refined \u2190 LinearNoBias(angle_embed)\nreturn Torsion_angles_refined\n</code></pre>"},{"location":"pipeline/overview/core_framework/#algorithm-3-main-inference-loop","title":"Algorithm 3: Main Inference Loop","text":"<pre><code>2D_feats \u2190 stageA_model(seq)\nTorsion_angles \u2190 stageB_model(seq, 2D_feats)\nz_init \u2190 PairInitEmbedding(2D_feats, other_feats)\nz_embed, single_embed \u2190 PairformerStack(z_init, MSA_embed)\nTorsion_angles_refined \u2190 angle_diffusion(Torsion_angles, z_embed, single_embed)\ncoords \u2190 forward_kinematics(Torsion_angles_refined)\nreturn coords\n</code></pre>"},{"location":"pipeline/overview/core_framework/#algorithm-4-comprehensive-training-step","title":"Algorithm 4: Comprehensive Training Step","text":"<pre><code>2D_feats \u2190 stageA_model(seq)\nTorsion_angles_pred \u2190 stageB_model(seq, 2D_feats)\nz_init \u2190 PairInitEmbedding(2D_feats, other_feats)\nz_embed, single_embed \u2190 PairformerStack(z_init)\nTorsion_angles_diff \u2190 angle_diffusion(Torsion_angles_pred, z_embed, single_embed)\ncoords \u2190 forward_kinematics(Torsion_angles_diff)\nloss = w2D*L2D + w_torsion*Ltorsion + w3D*Lcoords\nbackpropagation(loss)\n</code></pre>"},{"location":"pipeline/overview/core_framework/#advantages-implementation-tips","title":"\u2705 Advantages &amp; Implementation Tips","text":""},{"location":"pipeline/overview/core_framework/#advantages","title":"Advantages","text":"<ul> <li>Smooth integration: Maintains original Stage 1 &amp; 2 structures</li> <li>Angle-based diffusion: Reduces complexity, ensures local geometry</li> <li>Scalable architecture: Suitable for various RNA sizes</li> </ul>"},{"location":"pipeline/overview/core_framework/#implementation-tips","title":"Implementation Tips","text":"<ul> <li>Handle angle wrap-around carefully (use trigonometric methods)</li> <li>Sugar puckers as special angular parameters</li> <li>Optimize GPU utilization and memory management</li> </ul>"},{"location":"pipeline/overview/core_framework/#conclusion","title":"\ud83c\udfaf Conclusion","text":"<p>Embedding 2D adjacency into AlphaFold Pairformer and employing angle-based diffusion achieves: - Enhanced long-range modeling - Accurate local geometric constraints - Efficient RNA 3D predictions from sequence data</p> <p>This structured guide provides comprehensive technical clarity and practical implementation feasibility.</p>"},{"location":"pipeline/overview/full_pipeline_specification/","title":"Full Specification","text":"<p>Unified RNA 3D Prediction Pipeline: \u201cBest-of-All-Worlds\u201d Technical Documentation</p> <p>Below is a comprehensive, high-level-to-concrete technical guide that merges and improves upon the previous versions (V1\u2013V5). It addresses criticisms from each, consolidates their strengths, and incorporates a cohesive plan for seamlessly integrating LoRA, Pairformer embeddings, a Unified Latent Merger, and Diffusion (Stage\u202fD). It is verbose and detailed\u2014designed to serve as a robust blueprint for developers working on an end-to-end RNA structure prediction pipeline.</p> <p>\u2e3b</p> <ol> <li>Introduction &amp; Objectives</li> </ol> <p>This \u201cbest-of-all-worlds\u201d architecture aims to unify the entire multi-stage RNA 3D pipeline:     1.  Stage A: 2D adjacency predictor (e.g., RFold or an external method).     2.  Stage B:     \u2022   TorsionBERT (with LoRA) \u2192 local angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, \u2026).     \u2022   Pairformer (with LoRA) \u2192 global pairwise embeddings \\mathbf{z}_{ij} + single embeddings \\mathbf{s}_i.     3.  (Optional) Stage C: MP-NeRF or partial 3D geometry reconstruction from angles.     4.  Unified Latent Merger: Merges adjacency, angles, partial coords, Pairformer outputs into a single \u201cunified latent\u201d representation.     5.  Stage D: Diffusion-based refinement (with LoRA optional), which takes the unified latent as conditioning to generate final 3D coordinates.     6.  (Optional) Energy Minimization: A post-hoc step for short local refinement in a force field.</p> <p>Key motivations:     \u2022   Bring local angle predictions and global pair embeddings into a single representation.     \u2022   Support LoRA to minimize GPU memory usage in large pretrained models (TorsionBERT, Pairformer).     \u2022   Achieve synergy by letting the Diffusion step see both adjacency-based local constraints and Pairformer-driven global context.     \u2022   Provide a single top-level pipeline function so users can run from sequence \u2192 final 3D with minimal confusion.</p> <p>\u2e3b</p> <ol> <li>High-Level Data Flow<pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Stage A \u2502\n    \u2502Adjacency\u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 adjacency_matrix\n         v\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Stage B Combined Runner (Torsion+Pair)    \u2502 \u2502  - TorsionBERT (LoRA) -&gt; angles      \u2502 \u2502  - Pairformer (LoRA) -&gt; (s, z)       \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 {torsion_angles, s, z}  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 (Optional) Stage C: MP-NeRF/ partial 3D \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 partial_coords (optional)  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Unified Latent Merger (angles, s, z,  \u2502 \u2502 adjacency, partial_coords) -&gt; unified \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 unified_latent  v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Stage D Diffusion (LoRA optional)        \u2502 \u2502  Condition on 'unified_latent'           \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 final_3D_coords  v  (Optional) Energy Minimization &amp; Output</p> </li> </ol> <p>\u2e3b</p> <ol> <li>LoRA Integration</li> </ol> <p>3.1 Where LoRA is Applied     1.  TorsionBERT: We load a base BERT-like model (for angle regression) and freeze its main weights. Insert LoRA in attention or feed-forward layers.     2.  Pairformer: The large trunk (TriangleAttention, TriangularMultiplication blocks). We freeze base layers, attach LoRA to minimal modules.     3.  (Optional) Diffusion: If we have a large pretrained diffusion model, we can also freeze and inject LoRA.</p> <p>3.2 Implementation Sketch</p> <p>In practice, you might create rna_predict/peft/lora_utils.py with a function like:</p> <p>from peft import LoraConfig, get_peft_model</p> <p>def apply_lora(model, lora_cfg):     # lora_cfg might contain r, alpha, dropout, target_modules, etc.     lora_config = LoraConfig(**lora_cfg)     return get_peft_model(model, lora_config)</p> <p>Then in TorsionBertPredictorWithLoRA:</p> <p>class TorsionBertPredictorWithLoRA:     def init(self, model_name_or_path, lora_cfg, device=\"cpu\", angle_mode=\"sin_cos\"):         # load base TorsionBERT         self.base_model = TorsionBertModel(model_name_or_path, device=device)         # apply LoRA to self.base_model.model (the underlying HF model)         self.model = apply_lora(self.base_model.model, lora_cfg)         # freeze base         for name, param in self.base_model.model.named_parameters():             if \"lora\" not in name.lower():                 param.requires_grad = False         # store angle_mode, device, etc.         self.angle_mode = angle_mode         self.device = device</p> <pre><code>def __call__(self, sequence, adjacency=None):\n    # same logic as StageBTorsionBertPredictor\n    out = self.base_model.predict_angles_from_sequence(sequence)\n    # convert sin/cos if needed\n    return {\"torsion_angles\": out}  # e.g. shape [N, 2 * num_angles]\n</code></pre> <p>A similar approach is used for PairformerWithLoRA.</p> <p>\u2e3b</p> <ol> <li>Stage-by-Stage Implementation Outline</li> </ol> <p>4.1 Stage A: Adjacency</p> <p>We assume a script (e.g. rna_predict/pipeline/stageA/run_stageA.py) that can produce an [N, N] adjacency. Possibly:</p> <p>def run_stageA(sequence: str, predictor, device=\"cpu\") -&gt; torch.Tensor:     # predictor might be StageARFoldPredictor or a wrapper     adjacency_matrix = predictor.predict_adjacency(sequence)     return adjacency_matrix.to(device)</p> <p>In the advanced pipeline: We only need the adjacency as a 2D float/bool tensor for the subsequent steps.</p> <p>4.2 Stage B: Combined Runner</p> <p>Create run_stageB_combined.py. This merges TorsionBERT + Pairformer:</p> <p>def run_stageB_combined(     sequence: str,     adjacency_matrix: torch.Tensor,     torsion_bert_model,  # TorsionBertPredictorWithLoRA     pairformer_model,     # PairformerWithLoRA     device=\"cpu\" ) -&gt; dict:     # 1) Torsion angles     torsion_output = torsion_bert_model(sequence, adjacency=adjacency_matrix)     angles = torsion_output[\"torsion_angles\"].to(device)  # shape [N, ...]</p> <pre><code># 2) Prepare input for Pairformer\n# e.g. create initial_s [1, N, c_s], initial_z [1, N, N, c_z], pair_mask [1, N, N]\n# possibly incorporate adjacency as bias\n\n# 3) Run Pairformer -&gt; s_embeddings, z_embeddings\ns_updated, z_updated = pairformer_model(initial_s, initial_z, pair_mask)\n\nreturn {\n  \"torsion_angles\": angles,   # [N, angle_dim]\n  \"s_embeddings\": s_updated.squeeze(0), # [N, c_s]\n  \"z_embeddings\": z_updated.squeeze(0)  # [N, N, c_z]\n}\n</code></pre> <p>Key Points:     \u2022   You must define how initial_s and initial_z are constructed (some shape [1, N, c_s], [1, N, N, c_z]).     \u2022   If you have an MSA, incorporate that or fallback to single-sequence mode.     \u2022   If adjacency is used for Pairformer attention bias, you add it to z_init or inside the Pairformer code.</p> <p>4.3 (Optional) Stage C: MP-NeRF</p> <p>If the pipeline uses partial 3D from angles:</p> <p>def run_stageC(sequence, torsion_angles, method=\"mp_nerf\", device=\"cpu\", **kwargs):     # e.g., build_scaffolds_rna_from_torsions -&gt; rna_fold -&gt; coords     # Return {\"coords\": shape [N, #atoms, 3], \"atom_count\": ...}     ...</p> <p>Note: This step can be skipped if you let the diffusion start from random noise.</p> <p>4.4 Unified Latent Merger</p> <p>We create a flexible module, e.g. in merger/unified_latent_merger.py.</p> <p>class SimpleUnifiedLatentMerger(nn.Module):     def init(self, angle_dim, s_dim, z_dim, hidden_dim, output_dim):         super().init()         # Various sub-layers         # Potential adjacency + partial_coords processing</p> <pre><code>def forward(self, angles, adjacency, s_embeddings, z_embeddings, partial_coords=None):\n    # merges into unified_latent\n    return unified_latent\n</code></pre> <p>Implementation Details:     \u2022   Possibly embed angles with nn.Linear(angle_dim, hidden_dim).     \u2022   Convert adjacency [N,N] into node features [N,1] by row sum or a GNN layer.     \u2022   Pool z [N,N,c_z] -&gt; [N,c_z].     \u2022   Concatenate all. You get [N, \\text{some_total_dim}].     \u2022   Possibly run a small MLP or Transformer block.     \u2022   Produce final shape [N, output_dim] or a single global [output_dim].</p> <p>4.5 Stage D: Diffusion</p> <p>Adapt your diffusion manager to accept \u201cunified_latent\u201d:</p> <p>def run_stageD_diffusion(     partial_coords: Optional[torch.Tensor],     unified_latent: torch.Tensor,     diffusion_manager,     device=\"cpu\",     inference_steps=20 ):     # 1) if partial_coords is None, create random noise     # 2) pass unified_latent as condition     final_coords = diffusion_manager.inference_conditioned(         coords_init=partial_coords,         conditioning_latent=unified_latent,         steps=inference_steps     )     return final_coords</p> <p>In practice: If your code uses s_trunk, z_trunk, you might do {\"unified_latent\": ...} or merge them inside. The actual method in ProtenixDiffusionManager must be changed to handle that single latent.</p> <p>\u2e3b</p> <ol> <li>A \u201cFull Pipeline\u201d Orchestrator</li> </ol> <p>5.1 Proposed File: rna_predict/run_full_pipeline.py</p> <p>Pseudocode:</p> <p>def run_full_pipeline(sequence, config, device=\"cuda\"):     # Stage A     adjacency = run_stageA(sequence, stageA_predictor, device=device)</p> <pre><code># Stage B\nb_outputs = run_stageB_combined(\n    sequence, adjacency,\n    torsion_bert_model,\n    pairformer_model,\n    device=device\n)\nangles = b_outputs[\"torsion_angles\"]\ns_emb = b_outputs[\"s_embeddings\"]\nz_emb = b_outputs[\"z_embeddings\"]\n\n# Stage C (optional)\npartial_coords = None\nif config[\"use_stageC\"]:\n    partial_coords_out = run_stageC(sequence, angles, device=device)\n    partial_coords = partial_coords_out[\"coords\"]\n\n# Merger\nunified_latent = merger_module(\n    angles, adjacency, s_emb, z_emb, partial_coords\n)\n\n# Stage D\nfinal_coords = run_stageD_diffusion(\n    partial_coords, unified_latent, diffusion_manager, device=device,\n    inference_steps=config[\"stageD\"][\"n_steps\"]\n)\n\n# (Optional) Minimization\nif config.get(\"run_minimization\"):\n    final_coords = run_energy_minimization(final_coords, config[\"minimization\"])\n\nreturn final_coords\n</code></pre> <p>Config can store file paths for TorsionBERT &amp; Pairformer LoRA checkpoints, adjacency predictor settings, etc.</p> <p>\u2e3b</p> <ol> <li>Key Architectural Details and Decisions<ol> <li>Index Consistency \u2022   Ensure Stage A\u2019s adjacency matrix matches sequence indices used by TorsionBERT, Pairformer, MP-NeRF, etc. A single reference for residue indexing is critical.</li> <li>LoRA Implementation \u2022   Each sub-model is loaded in a partially-frozen mode with small LoRA adapters. \u2022   Keep a method like get_trainable_parameters() to only optimize LoRA layers, or rely on PEFT\u2019s built-in parameter filtering.</li> <li>Pooling of z-embeddings \u2022   In the simplest approach, we do a row-wise average of z_{ij} across j to get a node-level feature for each residue i. More advanced methods might do specialized GNN or attention.</li> <li>Stage D Condition \u2022   By default, older code might expect s_trunk or z_trunk. We now unify them with unified_latent. The diffusion code must be updated accordingly\u2014 e.g., if it had a line like:</li> </ol> </li> </ol> <p>condition = self.model.build_condition(s_trunk, z_trunk, ...)</p> <p>it becomes:</p> <p>condition = self.model.build_condition(unified_latent=unified_latent, ...)</p> <p>The details are up to the existing diffusion architecture.</p> <pre><code>5.  Memory\n\u2022   The pipeline can get large. Use gradient checkpointing or micro-batching in Pairformer.\n\u2022   Keep angles as minimal float32 or float16.\n\u2022   LoRA helps reduce training memory by only learning small rank updates.\n6.  Angle Format\n\u2022   TorsionBERT might output sin/cos pairs or direct angles in degrees. The MP-NeRF pipeline might require radians. Either unify them or carefully convert in the Stage B \u2192 Stage C handoff.\n7.  Energy Minimization\n\u2022   If you have PyRosetta, OpenMM, or MDAnalysis, define a function run_energy_minimization(coords, config). This is purely optional, but recommended for final polishing.\n</code></pre> <p>\u2e3b</p> <ol> <li>Example Repository Layout</li> </ol> <p>rna_predict/ \u251c\u2500\u2500 pipeline \u2502   \u251c\u2500\u2500 stageA \u2502   \u2502   \u2514\u2500\u2500 run_stageA.py \u2502   \u251c\u2500\u2500 stageB \u2502   \u2502   \u251c\u2500\u2500 run_stageB_combined.py    # NEW \u2502   \u2502   \u251c\u2500\u2500 torsion \u2502   \u2502   \u2502   \u2514\u2500\u2500 torsion_bert_lora.py  # LoRA-enabled TorsionBERT \u2502   \u2502   \u2514\u2500\u2500 pairwise \u2502   \u2502       \u251c\u2500\u2500 pairformer_lora.py    # LoRA-enabled Pairformer \u2502   \u2502       \u2514\u2500\u2500 pairformer_wrapper.py # existing code, adapted \u2502   \u251c\u2500\u2500 stageC \u2502   \u2502   \u2514\u2500\u2500 stage_c_reconstruction.py \u2502   \u251c\u2500\u2500 stageD \u2502   \u2502   \u2514\u2500\u2500 run_stageD_unified.py     # or run_stageD_diffusion.py \u2502   \u2514\u2500\u2500 merger \u2502       \u2514\u2500\u2500 unified_latent_merger.py  # NEW \u251c\u2500\u2500 run_full_pipeline.py              # NEW orchestrator \u251c\u2500\u2500 peft \u2502   \u2514\u2500\u2500 lora_utils.py \u2514\u2500\u2500 postprocess     \u2514\u2500\u2500 energy_minimization.py</p> <p>\u2e3b</p> <ol> <li>Final Guidance, Next Steps<ol> <li>Implementation: \u2022   Start by creating placeholders for each new file. Copy in the pseudocode or code stubs from above. \u2022   Validate shapes at each step by printing tensor shapes\u2014especially adjacency, angles, s/z embeddings, partial coords, final coords. \u2022   Integrate your actual adjacency predictor code from Stage A. \u2022   Integrate real TorsionBERT &amp; Pairformer model loading with LoRA. \u2022   Flesh out the Diffusion manager so it can read unified_latent.</li> <li>Testing: \u2022   Write unit tests for each stage (A, B, Merger, C, D). Then add an integration test that calls run_full_pipeline on a short synthetic sequence (like \u201cACGUACG\u201d) and checks for shape correctness.</li> <li>Performance: \u2022   If training, ensure you only optimize LoRA parameters. Double-check memory usage. \u2022   If your pipeline is large, use half precision or BF16 on a modern GPU.</li> <li>Refinement: \u2022   Once you get the pipeline working on small RNAs, do QA metrics (RMSD, pLDDT equivalents). \u2022   Tweak adjacency usage, z pooling, or unify angle modes if the geometry fails.</li> </ol> </li> </ol> <p>\u2e3b</p> <p>Conclusion</p> <p>This document merges the strengths of Versions 1\u20135:     \u2022   It provides file-by-file structural guidance (V1, V3, V4).     \u2022   It includes concrete code snippets and final orchestrator pseudocode (V2, V5).     \u2022   It addresses LoRA hooking points, data shape alignment, synergy with partial 3D, and an integrated pipeline function\u2014all in a single, verbose reference.</p> <p>Next Steps: Implement the placeholders, ensure shapes align, confirm the diffusion model sees the unified_latent, and incorporate LoRA in your TorsionBERT &amp; Pairformer code. Once complete, you\u2019ll have a fully functional Stage A \u2192 B \u2192 (C) \u2192 Merger \u2192 D pipeline that reflects the advanced synergy described in your original design documents\u2014truly better than the sum of its parts.</p> <p>Below is a comprehensive technical document that unifies the best qualities of all four previous versions (V1\u2013V4) while addressing their criticisms. It includes a detailed, color-coded Mermaid diagram referencing specific code modules (as in Version\u202f3), visual clarity (Version\u202f2), top-down flow (Version\u202f1 &amp; V4), optional stages, LoRA integration points, shape details, and unified latent synergy\u2014ultimately creating a verbose, all-in-one technical overview of the multistage RNA 3D structure prediction pipeline.</p> <p>\u2e3b</p> <p>Multistage RNA 3D Structure Prediction Pipeline (Comprehensive Final Version)</p> <p>High-Level Summary</p> <p>We have a five-step pipeline for generating RNA 3D structures:     1.  Stage A \u2013 Predict 2D Adjacency (e.g., via RFold).     2.  Stage B \u2013 Torsion &amp; Pair Embeddings:     \u2022   TorsionBERT (LoRA-enabled) to get backbone torsion angles.     \u2022   Pairformer (LoRA-enabled) to generate single-residue and pairwise embeddings.     3.  Stage C \u2013 (Optional) Partial 3D Reconstruction (e.g., MP-NeRF), using torsions to build backbone coords.     4.  Unified Latent Merger \u2013 Combine adjacency, torsions, partial coords, single/pair embeddings into a single \u201clatent.\u201d     5.  Stage D \u2013 Diffusion (LoRA-optional) for final 3D refinement, optionally followed by short energy minimization.</p> <p>Implementation Files:     \u2022   Stage A: rna_predict/pipeline/stageA/run_stageA.py, rfold_predictor.py, model.py     \u2022   Stage B:     \u2022   Torsion: torsion/torsion_bert_predictor.py, torsionbert_inference.py     \u2022   Pairformer: pairwise/pairformer_wrapper.py, pairformer.py     \u2022   Stage C: stage_c_reconstruction.py + mp_nerf/rna.py + final_kb_rna.py     \u2022   Unified Latent: could be a simple MLP or attention block (not always singled out in code)     \u2022   Stage D: stageD/diffusion/, protenix_diffusion_manager.py, generator.py, diffusion.py</p> <p>Shape Conventions:     \u2022   Adjacency: [N, N] (binary or real-valued).     \u2022   Torsion Angles: [N, K] (e.g., K=7 for alpha..zeta + chi) or [N, 2*K] if sin/cos.     \u2022   Single Embeddings: [N, c_s]     \u2022   Pair Embeddings: [N, N, c_z]     \u2022   Partial 3D coords: [N, #atoms, 3]     \u2022   Diffusion: might internally handle [batch, N_sample, N_atom, 3] arrays, plus trunk embeddings.</p> <p>\u2e3b</p> <p>Comprehensive Mermaid Diagram</p> <p>Below is a left-to-right color-coded diagram with subgraphs for each stage, referencing code modules, shape details, LoRA usage, and optional steps. You can copy this into a Mermaid-compatible environment to view a rendered version.</p> <p>flowchart LR</p> <pre><code>%% -------------------------------------------------\n%% STYLES\n%% -------------------------------------------------\nclassDef stageA fill:#bbdefb,stroke:#1a237e,stroke-width:2px,color:#000\nclassDef stageB fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\nclassDef stageC fill:#fff9c4,stroke:#fdd835,stroke-width:2px,color:#000\nclassDef unify fill:#e1bee7,stroke:#6a1b9a,stroke-width:2px,color:#000\nclassDef stageD fill:#f8bbd0,stroke:#ad1457,stroke-width:2px,color:#000\nclassDef optional fill:#cfd8dc,stroke:#455a64,stroke-width:2px,color:#000,stroke-dasharray:5 5\nclassDef data fill:#ffffff,stroke:#999999,stroke-width:1px,color:#000,rx:5,ry:5\nclassDef code fill:#f5f5f5,stroke:#999999,stroke-width:1px,color:#000,stroke-dasharray:3 3\n\n%% -------------------------------------------------\n%% INPUTS\n%% -------------------------------------------------\nS((RNA Sequence)):::data\n\n%% ========== Stage A Subgraph ==========\nsubgraph A_subgraph [**Stage A**: 2D Adjacency Prediction (RFold)]\ndirection TB\nclass A_subgraph stageA\n\nA1[[run_stageA.py\\n(rfold_predictor.py / model.py)]:::code]\nA2((Adjacency NxN)):::data\nS --&gt; A1\nA1 --&gt; A2\nend\n\n%% ========== Stage B Subgraph ==========\nsubgraph B_subgraph [**Stage B**: TorsionBERT &amp; Pairformer (LoRA)]\ndirection TB\nclass B_subgraph stageB\n\nB1[[TorsionBert Predictor\\n(torsion_bert_predictor.py)\\nLoRA-enabled]]:::code\nB2((Torsion Angles\\n[N,K or N,2K])):::data\n\nB3[[Pairformer\\n(pairformer_wrapper.py)\\nLoRA-enabled]]:::code\nB4((Single Embs s:\\n[N, c_s])):::data\nB5((Pair Embs z:\\n[N,N,c_z])):::data\n\nS --&gt; B1\nA2 -. optional .-&gt; B1\nB1 --&gt; B2\n\nS --&gt; B3\nA2 -. optional .-&gt; B3\nB3 --&gt; B4\nB3 --&gt; B5\n\nend\n\n%% ========== Stage C Subgraph ==========\nsubgraph C_subgraph [**Stage C** (Optional): MP-NeRF Partial 3D]\ndirection TB\nclass C_subgraph stageC\n\nC1[[stage_c_reconstruction.py\\n+ mp_nerf/rna.py\\n+ final_kb_rna.py]]:::code\nC2((Partial 3D Coords\\n[N, #atoms, 3])):::data\n\nB2 --&gt; C1\nS --&gt; C1\nC1 --&gt; C2\nend\nclass C_subgraph optional\n\n%% ========== Unified Latent Subgraph ==========\nsubgraph M_subgraph [Unified Latent Merger]\nclass M_subgraph unify\n\nM1[[Merge angles,\\nadjacency, s, z,\\npartial coords]]:::code\nM2((Unified\\nLatent)):::data\nend\n\n%% Connect them\nA2 --&gt; M1\nB2 --&gt; M1\nB4 --&gt; M1\nB5 --&gt; M1\nC2 -. optional .-&gt; M1\nM1 --&gt; M2\n\n%% ========== Stage D Subgraph ==========\nsubgraph D_subgraph [**Stage D**: Diffusion Refinement]\ndirection TB\nclass D_subgraph stageD\n\nD1[[ProtenixDiffusionManager\\n(protenix_diffusion_manager.py)\\n+ DiffusionModule\\nLoRA optional]]:::code\nD2((Final 3D Structures\\n(N, #atoms, 3)\\nor multiple)):::data\nM2 --&gt; D1\nD1 --&gt; D2\nend\n\n%% ========== Post-Processing Subgraph ==========\nsubgraph PP_subgraph [Optional Post-Processing: Energy Minimization]\nclass PP_subgraph optional\ndirection TB\nPP1[[Local MD / Minimization\\ne.g. OpenMM, GROMACS]]:::code\nPP2((Polished 3D\\nStructure(s))):::data\nD2 --&gt; PP1\nPP1 --&gt; PP2\nend\n\n%% -------------------------------------------------\n%% STYLING\n%% -------------------------------------------------\nlinkStyle default stroke-width:2px,fill:none,stroke:#888\n</code></pre> <p>Diagram Explanation     1.  Input     \u2022   RNA Sequence S((\u2026)): A raw string representing nucleotides.     2.  Stage\u202fA: Adjacency (RFold)     \u2022   In run_stageA.py + rfold_predictor.py, the pipeline obtains an adjacency matrix [N, N].     \u2022   This matrix typically indicates base-pair contacts. Optionally fed into Stage\u202fB if the TorsionBERT or Pairformer uses adjacency as a feature.     3.  Stage\u202fB: TorsionBERT + Pairformer     1.  TorsionBERT (LoRA)     \u2022   Reads the RNA sequence and optionally adjacency, producing backbone torsion angles [N, K] or [N, 2K] if sin/cos.     \u2022   Code references: torsion_bert_predictor.py, torsionbert_inference.py.     2.  Pairformer (LoRA)     \u2022   Potentially uses the same sequence and adjacency to generate single [N, c_s] and pair [N, N, c_z] embeddings (like z_trunk).     \u2022   Code references: pairformer_wrapper.py, pairformer.py.     4.  Stage\u202fC (Optional): Partial 3D     \u2022   If used, we pass torsion angles + sequence + standard geometry from final_kb_rna.py into mp_nerf/rna.py or stage_c_reconstruction.py.     \u2022   Produces partial 3D coords [N, #atoms, 3], typically backbone only.     \u2022   This is optional; the pipeline can skip it and rely purely on Diffusion or initial random coords.     5.  Unified Latent Merger     \u2022   Merges everything: adjacency, angles, partial coords, single/pair embeddings.     \u2022   This synergy can be an MLP or a small attention block. Usually not a separate file, but references are integrated in stageD or a separate \u201cmerger\u201d class.     \u2022   Yields a single \u201cunified latent\u201d vector or array [N, ...] used by the next stage.     6.  Stage\u202fD: Diffusion     \u2022   The ProtenixDiffusionManager plus the DiffusionModule (optionally LoRA-enabled) use the unified latent as a condition to refine or generate final 3D coordinates.     \u2022   The code references: rna_predict/pipeline/stageD/diffusion/.py (including generator.py, protenix_diffusion_manager.py, diffusion.py).     \u2022   Produces final 3D coordinates [N, #atoms, 3] or an ensemble from multiple samples.     7.  Optional Energy Minimization     \u2022   Tools like OpenMM or GROMACS for local minimization or short MD runs, producing a final polished structure.     \u2022   Often done in a separate script or environment, not strictly part of the Python pipeline code.</p> <p>\u2e3b</p> <p>Detailed Stage-by-Stage Description</p> <p>Stage A: Adjacency (2D)     \u2022   Code: run_stageA.py, rfold_predictor.py, referencing an RFold_Model in model.py.     \u2022   Input: RNA sequence (e.g. \"AUGCA...\").     \u2022   Output: adjacency \u2208 \u211d^(N\u00d7N) (binary or probability).     \u2022   Comment: Typically no LoRA is used here, though you could do so if your adjacency predictor is large.</p> <p>Stage B: Torsion &amp; Pair Embeddings     1.  TorsionBERT     \u2022   LoRA: partial fine-tuning if model_name is huge (\"sayby/rna_torsionbert\").     \u2022   Produces angles in either sin/cos or direct rad/deg.     \u2022   Key shapes: [N, 2\u00d7num_angles] or [N, num_angles].     2.  Pairformer     \u2022   LoRA: partial fine-tuning again.     \u2022   Generates single embeddings s [N, c_s] and pair embeddings z [N, N, c_z].     \u2022   Possibly uses adjacency as a \u201cbias\u201d to handle base-pair info or skip if not needed.</p> <p>Stage C: (Optional) Partial 3D Reconstruction     \u2022   Code: stage_c_reconstruction.py \u2192 calls mp_nerf/rna.py, plus geometry from final_kb_rna.py.     \u2022   Input: Torsion angles + (optionally) adjacency or other constraints.     \u2022   Output: partial coords, typically [N, #atoms, 3] if building a backbone. This can be used as an initial conformation for Diffusion or a final fallback if Stage\u202fD is skipped.     \u2022   Sugar Pucker: default \u201cC3\u2032-endo\u201d for standard A-form. Could also handle \u201cC2\u2032-endo\u201d or ring closure logic.</p> <p>Unified Latent Merger     \u2022   Combines:     1.  Torsion angles     2.  Adjacency [N, N]     3.  Single embeddings [N, c_s] + pair embeddings [N, N, c_z]     4.  Possibly partial coords [N, #atoms, 3]     \u2022   Typically an MLP or small attention-based aggregator that outputs a single \u201cconditioning latent.\u201d Not always singled out as a separate .py, but recognized conceptually for synergy.</p> <p>Stage D: Diffusion Refinement     \u2022   Code: stageD/diffusion/, e.g. protenix_diffusion_manager.py, generator.py, diffusion.py.     \u2022   LoRA: optional if the base diffusion model is large.     \u2022   Process:     1.  Possibly start from partial coords (Stage\u202fC) or random noise.     2.  Condition on the \u201cunified latent.\u201d     3.  Iteratively denoise to generate refined 3D coords [N, #atoms, 3]. Possibly produce multiple samples.     \u2022   Output: final or near-final 3D structure(s).</p> <p>Post-Processing (Optional)     \u2022   Might run short local MD in OpenMM or GROMACS to fix minor geometry or steric issues.     \u2022   Not strictly in the code, but invoked for final polishing.     \u2022   If used, it yields an improved structure (lowest-energy or an ensemble).</p> <p>\u2e3b</p> <p>Why This Comprehensive Diagram Excels     1.  Complete Flow:     \u2022   We integrate the straightforward top-down approach (V1, V4) with color-coded subgraphs (V2) plus code references and shape details (V3).     2.  LoRA Markings:     \u2022   TorsionBERT &amp; Pairformer are explicitly shown as LoRA-enabled; Diffusion\u2019s LoRA is noted.     3.  Implementation Mapping:     \u2022   We reference .py files and configuration references (like model.py, protenix_diffusion_manager.py), for a developer-friendly approach.     4.  Optional Paths:     \u2022   Stage C (MP-NeRF) is visually \u201coptional,\u201d connected with a dashed arrow.     \u2022   Post-processing is also a separate optional subgraph.     5.  Shape / Data:     \u2022   Key data artifacts (adjacency NxN, angles NxK, single embeddings Nx c_s, pair NxNx c_z, partial coords Nx(#atoms), final coords Nx(#atoms), etc.) are all labeled.     6.  Unified Latent Synergy:     \u2022   The \u201cMerger\u201d is singled out as a subgraph, clarifying we combine adjacency, angles, partial coords, s, z, etc., exactly how we want.</p> <p>\u2e3b</p> <p>Key Configuration Points     1.  LoRA:     \u2022   TorsionBERT in Stage B: set model_name_or_path to a large pretrained model and insert LoRA adapters with \u201crank=8\u201d or so.     \u2022   Pairformer: similarly add LoRA to attention or feed-forward layers.     \u2022   Diffusion: optionally insert LoRA if the diffusion model is huge.     2.  Stage A:     \u2022   rfold_predictor.py might load RNAStralign_trainset_pretrained.pth.     \u2022   Output adjacency is used in Stages B and the Unified Merger, if we want adjacency-based synergy.     3.  Stage B:     \u2022   torsion_bert_predictor.py has angle_mode=\"degrees\" or \"sin_cos\".     \u2022   pairformer_wrapper.py config might specify n_blocks=48, c_z=128, c_s=384.     4.  Stage C:     \u2022   mp_nerf/rna.py: Usually sets sugar pucker to \u201cC3\u2032-endo.\u201d     \u2022   If do_ring_closure is False, we skip ring closure.     5.  Stage D:     \u2022   protenix_diffusion_manager.py might define a schedule in generator.py (InferenceNoiseScheduler) for 50\u2013100 denoising steps.     \u2022   We unify single embeddings (s_trunk), pair embeddings (z_trunk), partial coords, etc., in a single \u201cconditioning\u201d dictionary.</p> <p>\u2e3b</p> <p>Recommended Usage Flow     1.  Obtain adjacency from Stage A.     2.  Run Stage B to get angles + single/pair embeddings. Possibly pass adjacency in to TorsionBERT or Pairformer if they require it.     3.  (Optional) Stage C: If you want an initial 3D for diffusion, run MP-NeRF.     4.  Merge all data (angles, adjacency, partial coords, single/pair embeddings) into a unified latent.     5.  Stage D: Condition the diffusion model on that unified latent to refine final 3D.     6.  Optionally do a short local MD to minimize bond strains or fix sterics.</p> <p>You can skip Stage C if you want to let diffusion start from random noise. You can skip energy minimization if you trust the final diffusion geometry. However, each step may improve the final structure\u2019s accuracy.</p> <p>\u2e3b</p> <p>Conclusion</p> <p>This final combined architectural document:     \u2022   Merges the clarity of a color-coded flow (V2) with the concrete code references and shape details (V3).     \u2022   Incorporates the straightforward top-down perspective (V1) plus an emphasis on synergy and the \u201cunified latent\u201d concept (V4).     \u2022   Highlights LoRA usage in TorsionBERT, Pairformer, and (optionally) Diffusion.     \u2022   Shows optional partial 3D (Stage\u202fC) and optional energy minimization.</p> <p>Hence, this pipeline covers everything from adjacency (2D) \u2192 torsion + pair embeddings \u2192 (optional) partial 3D \u2192 unified synergy \u2192 diffusion-based refinement \u2192 final or post-processed 3D. It should serve as a verbose and complete reference for both developers and advanced users interested in each module\u2019s role, shapes, code references, and how they integrate to produce high-quality RNA 3D structures. Multistage RNA 3D Structure Prediction Pipeline (Comprehensive Final Version, ASCII Edition)</p> <p>Below is a comprehensive technical document unifying the strengths of previous versions (V1\u2013V4), addressing their criticisms, and presenting a verbose ASCII-based diagram clearly referencing code modules, shapes, optional paths, LoRA integration, and unified latent synergy.</p> <p>\u2e3b</p> <p>High-Level Summary</p> <p>The RNA 3D prediction pipeline consists of five primary stages:</p> <ol> <li>Stage A \u2013 2D Adjacency Prediction (RFold)</li> <li>Stage B \u2013 Torsion &amp; Pair Embeddings:</li> <li>TorsionBERT (LoRA-enabled) \u2192 backbone torsion angles</li> <li>Pairformer (LoRA-enabled) \u2192 single-residue and pairwise embeddings</li> <li>Stage C \u2013 (Optional) Partial 3D Reconstruction (MP-NeRF)</li> <li>Unified Latent Merger \u2013 Combines adjacency, angles, partial coords, embeddings</li> <li>Stage D \u2013 Diffusion-based Refinement (LoRA-optional)</li> <li>Optional Post-processing (Energy Minimization)</li> </ol> <p>\u2e3b</p> <p>Implementation Files</p> <p>Stage A:   - rna_predict/pipeline/stageA/run_stageA.py   - rfold_predictor.py   - model.py</p> <p>Stage B:   - TorsionBERT:     - torsion/torsion_bert_predictor.py     - torsionbert_inference.py   - Pairformer:     - pairwise/pairformer_wrapper.py     - pairformer.py</p> <p>Stage C:   - stage_c_reconstruction.py   - mp_nerf/rna.py   - final_kb_rna.py</p> <p>Unified Latent:   - Typically a small MLP or attention block (often within Stage D code)</p> <p>Stage D:   - stageD/diffusion/   - protenix_diffusion_manager.py   - generator.py   - diffusion.py</p> <p>\u2e3b</p> <p>Shape Conventions</p> <ul> <li>Adjacency:         [N, N] (binary/probability)</li> <li>Torsion Angles:    [N, K] or [N, 2K] if sin/cos encoding</li> <li>Single Embeddings: [N, c_s]</li> <li>Pair Embeddings:   [N, N, c_z]</li> <li>Partial 3D Coords: [N, #atoms, 3]</li> <li>Diffusion:         [batch, N_sample, N_atom, 3] + trunk embeddings</li> </ul> <p>\u2e3b</p> <p>Detailed ASCII Diagram of the Pipeline</p> <p>RNA Sequence (String: \"ACGU...\")          |          v +---------------------------------------------+ | Stage A: 2D Adjacency Prediction (RFold)    | | [run_stageA.py, rfold_predictor.py, model.py] (No LoRA) +---------------------------------------------+          |          | Adjacency Matrix [N,N]          v +----------------------------------------------------------+ | Stage B: Torsion Angles &amp; Pair Embeddings (LoRA-enabled) | |                                                          | | - TorsionBERT: angles [N,K or N,2K]                      | |   [torsion_bert_predictor.py, torsionbert_inference.py]  | |                                                          | | - Pairformer:                                            | |   Single embeddings [N,c_s]                              | |   Pair embeddings [N,N,c_z]                              | |   [pairformer_wrapper.py, pairformer.py]                 | +----------------------------------------------------------+          |          +------------+---------------+          |            |               |          |            |(Optional)     |          |            v               |          |  +--------------------------------------------+          |  | Stage C: Partial 3D Reconstruction         |          |  | [stage_c_reconstruction.py, mp_nerf/rna.py,|          |  | final_kb_rna.py] (Optional)                |          |  +--------------------------------------------+          |            |               |          | Partial Coords [N,#atoms,3]|          |            v               |          +------------+---------------+                       |                       v +----------------------------------------------------+ | Unified Latent Merger                              | |                                                    | | Combines adjacency, angles, partial coords,        | | single &amp; pair embeddings into Unified Latent       | | (MLP/attention-based merger, usually in Stage D)   | +----------------------------------------------------+                       |                       | Unified Latent                       v +-------------------------------------------------------------+ | Stage D: Diffusion-based Refinement (LoRA optional)        | | [protenix_diffusion_manager.py, generator.py, diffusion.py]| | - Conditions on Unified Latent                             | | - Produces Final 3D structure(s) [N,#atoms,3] or ensemble  | +-------------------------------------------------------------+                       |                       v +---------------------------------------+ | Optional Post-processing:             | | Short MD / Energy Minimization        | | (OpenMM, GROMACS, Amber, etc.)        | | Polished Final Structures             | +---------------------------------------+</p> <p>\u2e3b</p> <p>Stage-by-Stage Breakdown (Detailed)</p> <p>Stage A: 2D Adjacency (RFold)     \u2022   Input: RNA sequence string.     \u2022   Output: Adjacency matrix [N,N].     \u2022   Code: run_stageA.py, rfold_predictor.py, model.py     \u2022   Comment: Usually no LoRA integration here; output optionally used downstream.</p> <p>Stage B: Torsion &amp; Pair Embeddings</p> <p>TorsionBERT (LoRA):     \u2022   Inputs: Sequence (optionally adjacency).     \u2022   Outputs: Backbone torsion angles [N,K] or [N,2K] if sin/cos encoded.     \u2022   Code: torsion_bert_predictor.py, torsionbert_inference.py</p> <p>Pairformer (LoRA):     \u2022   Inputs: Sequence (optionally adjacency).     \u2022   Outputs:     \u2022   Single embeddings [N,c_s]     \u2022   Pair embeddings [N,N,c_z]     \u2022   Code: pairformer_wrapper.py, pairformer.py</p> <p>Stage C (Optional): Partial 3D Reconstruction (MP-NeRF)     \u2022   Inputs: Torsion angles, sequence, standard geometry.     \u2022   Outputs: Partial coordinates [N,#atoms,3].     \u2022   Code: stage_c_reconstruction.py, mp_nerf/rna.py, final_kb_rna.py     \u2022   Comment: Typically backbone atoms only; optional ring closure.</p> <p>Unified Latent Merger     \u2022   Inputs: Adjacency, torsion angles, single/pair embeddings, partial coords.     \u2022   Outputs: Unified latent vector (used as conditioning input to diffusion).     \u2022   Implementation: Often small MLP or attention layers embedded within Stage D.</p> <p>Stage D: Diffusion-based Refinement     \u2022   Inputs: Unified latent, optionally partial 3D coords from Stage C.     \u2022   Outputs: Final refined 3D structure(s) [N,#atoms,3].     \u2022   Code: stageD/diffusion/, protenix_diffusion_manager.py, generator.py, diffusion.py     \u2022   LoRA: Optional if diffusion model is large-scale.</p> <p>Post-processing (Optional)     \u2022   Methods: Short molecular dynamics or energy minimization runs.     \u2022   Tools: OpenMM, GROMACS, Amber.     \u2022   Output: Polished final 3D structure(s).</p> <p>\u2e3b</p> <p>Key Configuration &amp; LoRA Integration Points     \u2022   Stage B (TorsionBERT &amp; Pairformer):     \u2022   Insert LoRA adapters (rank=8) into large pretrained models.     \u2022   Angle modes configurable (degrees vs. sin_cos).     \u2022   Pairformer configurable (n_blocks=48, c_z=128, c_s=384).     \u2022   Stage D (Diffusion):     \u2022   Optional LoRA insertion for large diffusion models.     \u2022   Denoising schedule configurable (~50-100 steps typical).     \u2022   Stage C (Optional):     \u2022   Sugar pucker geometry defaults (C3'-endo).     \u2022   Ring closure toggle (do_ring_closure=True/False).</p> <p>\u2e3b</p> <p>Recommended Usage Flow</p> <ol> <li>Obtain adjacency matrix from Stage A.</li> <li>Generate angles &amp; embeddings in Stage B, optionally using adjacency.</li> <li>(Optional) Generate partial 3D structure in Stage C.</li> <li>Merge data into unified latent.</li> <li>Diffusion-based refinement (Stage D) using unified latent.</li> <li>(Optional) Post-processing MD/energy minimization.</li> </ol> <p>Stages C and post-processing are optional, but inclusion typically enhances accuracy.</p> <p>\u2e3b</p> <p>Why This Comprehensive ASCII Diagram Excels     \u2022   Clarity: Clearly marked stages, inputs/outputs, optional steps.     \u2022   LoRA integration: Explicitly highlighted integration points.     \u2022   Implementation mapping: Clear references to code modules.     \u2022   Data shape specification: Explicit and consistent.     \u2022   Unified latent synergy: Clearly defined aggregation step for advanced conditioning.</p> <p>\u2e3b</p> <p>Conclusion</p> <p>This comprehensive ASCII-based overview merges clarity, technical detail, code referencing, optional paths, LoRA integration, and latent synergy to provide a complete, developer-friendly resource for understanding, implementing, and customizing the RNA 3D prediction pipeline from initial sequence to polished final structures.</p>"},{"location":"pipeline/stageA/RFold_code/","title":"RFold Code","text":"<p>RFold CODE: Directory Structure:</p> <p>\u2514\u2500\u2500 ./     \u251c\u2500\u2500 API     \u2502   \u251c\u2500\u2500 init.py     \u2502   \u251c\u2500\u2500 dataloader.py     \u2502   \u251c\u2500\u2500 dataset.py     \u2502   \u2514\u2500\u2500 metric.py     \u251c\u2500\u2500 colab_utils.py     \u251c\u2500\u2500 main.py     \u251c\u2500\u2500 model.py     \u251c\u2500\u2500 module.py     \u251c\u2500\u2500 parser.py     \u251c\u2500\u2500 rfold.py     \u2514\u2500\u2500 utils.py</p>"},{"location":"pipeline/stageA/RFold_code/#file-apiinitpy","title":"File: /API/init.py","text":"<p>from .dataloader import load_data from .metric import evaluate_result</p>"},{"location":"pipeline/stageA/RFold_code/#file-apidataloaderpy","title":"File: /API/dataloader.py","text":"<p>from .dataset import RNADataset from torch.utils.data import DataLoader</p> <p>def load_data(data_name, batch_size, data_root, num_workers=8, **kwargs):     if data_name == 'RNAStralign':         test_set = RNADataset(path=data_root, dataname='test_600')     elif data_name == 'ArchiveII':         test_set = RNADataset(path=data_root, dataname='all_600')     elif data_name == 'bpRNA':         test_set = RNADataset(path=data_root, dataname='test')     test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True)     return test_loader</p>"},{"location":"pipeline/stageA/RFold_code/#file-apidatasetpy","title":"File: /API/dataset.py","text":"<p>import numpy as np import os.path as osp import _pickle as cPickle from tqdm import tqdm from torch.utils import data</p> <p>def get_cut_len(data_len,set_len):     l = data_len     if l &lt;= set_len:         l = set_len     else:         l = (((l - 1) // 16) + 1) * 16     return l</p> <p>class cached_property(object):     \"\"\"     Descriptor (non-data) for building an attribute on-demand on first use.     \"\"\"     def init(self, factory):         \"\"\"          is called such: factory(instance) to build the attribute.         \"\"\"         self._attr_name = factory.name         self._factory = factory <pre><code>def __get__(self, instance, owner):\n    # Build the attribute.\n    attr = self._factory(instance)\n\n    # Cache the value; hide ourselves.\n    setattr(instance, self._attr_name, attr)\n    return attr\n</code></pre> <p>class RNADataset(data.Dataset):     def init(self, path, dataname):         self.path = path         self.dataname = dataname         self.data = self.cache_data</p> <pre><code>def __len__(self):\n    return len(self.data)\n\ndef get_data(self, dataname):\n    filename = dataname + '.pickle'\n    pre_data = cPickle.load(open(osp.join(self.path, filename), 'rb'))\n\n    data = []\n    for instance in tqdm(pre_data):\n        data_x, _, seq_length, name, pairs = instance\n        l = get_cut_len(seq_length, 80)\n        # contact\n        contact = np.zeros((l, l))\n        contact[tuple(np.transpose(pairs))] = 1. if pairs != [] else 0.\n        # data_seq\n        data_seq = np.zeros((l, 4))\n        data_seq[:seq_length] = data_x[:seq_length]\n        data.append([contact, seq_length, data_seq])\n    return data\n\n@cached_property\ndef cache_data(self):\n    return self.get_data(self.dataname)\n\ndef __getitem__(self, index):\n    return self.data[index]\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#file-apimetricpy","title":"File: /API/metric.py","text":"<p>import torch</p> <p>def evaluate_result(pred_a, true_a, eps=1e-11):     tp_map = torch.sign(torch.Tensor(pred_a) * torch.Tensor(true_a))     tp = tp_map.sum()     pred_p = torch.sign(torch.Tensor(pred_a)).sum()     true_p = true_a.sum()     fp = pred_p - tp     fn = true_p - tp     recall = (tp + eps)/(tp+fn+eps)     precision = (tp + eps)/(tp+fp+eps)     f1_score = (2tp + eps)/(2tp + fp + fn + eps)     return precision, recall, f1_score</p>"},{"location":"pipeline/stageA/RFold_code/#file-colab_utilspy","title":"File: /colab_utils.py","text":"<p>import torch import os.path as osp import numpy as np import torch.nn.functional as F</p> <p>seq_dict = {     'A': 0,     'U': 1,     'C': 2,     'G': 3 }</p> <p>def base_matrix(_max_length, device):     base_matrix = torch.ones(_max_length, _max_length)     for i in range(_max_length):         st, en = max(i-3, 0), min(i+3, _max_length-1)         for j in range(st, en + 1):             base_matrix[i, j] = 0.     return base_matrix.to(device)</p> <p>def constraint_matrix(x):     base_a, base_u, base_c, base_g = x[:, :, 0], x[:, :, 1], x[:, :, 2], x[:, :, 3]     batch = base_a.shape[0]     length = base_a.shape[1]     au = torch.matmul(base_a.view(batch, length, 1), base_u.view(batch, 1, length))     au_ua = au + torch.transpose(au, -1, -2)     cg = torch.matmul(base_c.view(batch, length, 1), base_g.view(batch, 1, length))     cg_gc = cg + torch.transpose(cg, -1, -2)     ug = torch.matmul(base_u.view(batch, length, 1), base_g.view(batch, 1, length))     ug_gu = ug + torch.transpose(ug, -1, -2)     return (au_ua + cg_gc + ug_gu) * base_matrix(x.shape[1], x.device)</p> <p>def sequence2onehot(seq, device):     seqs = list(map(lambda x: seq_dict[x], seq))     return torch.tensor(seqs).unsqueeze(0).to(device)</p> <p>def get_cut_len(l):     return (((l - 1) // 16) + 1) * 16</p> <p>def process_seqs(seq, device):     seq_len = len(seq)     seq = sequence2onehot(seq, device=device)     nseq_len = get_cut_len(seq_len)     nseq = F.pad(seq, (0, nseq_len - seq_len))     nseq_one_hot = F.one_hot(nseq).float()     return nseq, nseq_one_hot, seq_len</p> <p>def row_col_softmax(y):     row_softmax = torch.softmax(y, dim=-1)     col_softmax = torch.softmax(y, dim=-2)     return 0.5 * (row_softmax + col_softmax)</p> <p>def row_col_argmax(y):     y_pred = row_col_softmax(y)     y_hat = y_pred + torch.randn_like(y) * 1e-12     col_max = torch.argmax(y_hat, 1)     col_one = torch.zeros_like(y_hat).scatter(1, col_max.unsqueeze(1), 1.0)     row_max = torch.argmax(y_hat, 2)     row_one = torch.zeros_like(y_hat).scatter(2, row_max.unsqueeze(2), 1.0)     int_one = row_one * col_one      return int_one</p> <p>def ct_file_output(pairs, seq, seq_name, save_result_path):     col1 = np.arange(1, len(seq) + 1, 1)     col2 = np.array([i for i in seq])     col3 = np.arange(0, len(seq), 1)     col4 = np.append(np.delete(col1, 0), [0])     col5 = np.zeros(len(seq), dtype=int)</p> <pre><code>for i, I in enumerate(pairs):\n    col5[I[0]-1] = int(I[1])\ncol6 = np.arange(1, len(seq) + 1, 1)\ntemp = np.vstack((np.char.mod('%d', col1), col2, np.char.mod('%d', col3), np.char.mod('%d', col4),\n                  np.char.mod('%d', col5), np.char.mod('%d', col6))).T\nnp.savetxt(osp.join(save_result_path, seq_name.replace('/','_'))+'.ct', (temp), delimiter='\\t', fmt=\"%s\", header='&gt;seq length: ' + str(len(seq)) + '\\t seq name: ' + seq_name.replace('/','_') , comments='')\nreturn\n</code></pre> <p>def seq2dot(seq):     idx = np.arange(1, len(seq) + 1)     dot_file = np.array(['_'] * len(seq))     dot_file[seq &gt; idx] = '('     dot_file[seq &lt; idx] = ')'     dot_file[seq == 0] = '.'     dot_file = ''.join(dot_file)     return dot_file</p> <p>def save_ct(predict_matrix, seq_ori, name):     seq_tmp = torch.mul(predict_matrix.cpu().argmax(axis=1), predict_matrix.cpu().sum(axis = 1).clamp_max(1)).numpy().astype(int)     seq_tmp[predict_matrix.cpu().sum(axis = 1) == 0] = -1     dot_list = seq2dot((seq_tmp+1).squeeze())     letter = 'AUCG'     seq_letter = ''.join([letter[item] for item in np.nonzero(seq_ori)[:,1]])     seq = ((seq_tmp + 1).squeeze(), torch.arange(predict_matrix.shape[-1]).numpy() + 1)     cur_pred = [(seq[0][i],seq[1][i]) for i in np.arange(len(seq[0])) if seq[0][i] != 0]     ct_file_output(cur_pred, seq_letter, name, './')     return </p> <p>def visual_get_bases(seq):     a_bases, u_bases, c_bases, g_bases = [], [], [], []     for ii, s in enumerate(seq):         if s == 'A': a_bases.append(ii+1)         if s == 'U': u_bases.append(ii+1)         if s == 'C': c_bases.append(ii+1)         if s == 'G': g_bases.append(ii+1)     a_bases = ''.join([str(s)+',' for s in a_bases])[:-1]     u_bases = ''.join([str(s)+',' for s in u_bases])[:-1]     c_bases = ''.join([str(s)+',' for s in c_bases])[:-1]     g_bases = ''.join([str(s)+',' for s in g_bases])[:-1]     return a_bases, u_bases, c_bases, g_bases</p>"},{"location":"pipeline/stageA/RFold_code/#file-mainpy","title":"File: /main.py","text":"<p>import json import torch import logging import collections import os.path as osp</p>"},{"location":"pipeline/stageA/RFold_code/#from-parser-import-create_parser","title":"from parser import create_parser","text":"<p>import warnings warnings.filterwarnings('ignore')</p> <p>from utils import * from rfold import RFold</p> <p>class Exp:     def init(self, args):         self.args = args         self.config = args.dict         self.device = self._acquire_device()         self.total_step = 0         self._preparation()         print_log(output_namespace(self.args))</p> <pre><code>def _acquire_device(self):\n    if self.args.use_gpu:\n        device = torch.device('cuda:0')\n        print('Use GPU:',device)\n    else:\n        device = torch.device('cpu')\n        print('Use CPU')\n    return device\n\ndef _preparation(self):\n    set_seed(self.args.seed)\n    # log and checkpoint\n    self.path = osp.join(self.args.res_dir, self.args.ex_name)\n    check_dir(self.path)\n\n    self.checkpoints_path = osp.join(self.path, 'checkpoints')\n    check_dir(self.checkpoints_path)\n\n    sv_param = osp.join(self.path, 'model_param.json')\n    with open(sv_param, 'w') as file_obj:\n        json.dump(self.args.__dict__, file_obj)\n\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(level=logging.INFO, filename=osp.join(self.path, 'log.log'),\n                        filemode='a', format='%(asctime)s - %(message)s')\n    # prepare data\n    self._get_data()\n    # build the method\n    self._build_method()\n\ndef _build_method(self):\n    self.method = RFold(self.args, self.device)\n\ndef _get_data(self):\n    self.test_loader = get_dataset(self.config)\n\ndef test(self):\n    test_f1, test_precision, test_recall, test_runtime = self.method.test_one_epoch(self.test_loader)\n    print_log('Test F1: {0:.4f}, Precision: {1:.4f}, Recall: {2:.4f}, Runtime: {3:.4f}\\n'.format(test_f1, test_precision, test_recall, test_runtime))\n    return test_f1, test_precision, test_recall, test_runtime\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#if-name-main","title":"if name == 'main':","text":""},{"location":"pipeline/stageA/RFold_code/#rna_ss_data-collectionsnamedtuplerna_ss_data-seq-ss_label-length-name-pairs","title":"RNA_SS_data = collections.namedtuple('RNA_SS_data', 'seq ss_label length name pairs')","text":""},{"location":"pipeline/stageA/RFold_code/#args-create_parser","title":"args = create_parser()","text":""},{"location":"pipeline/stageA/RFold_code/#config-argsdict","title":"config = args.dict","text":""},{"location":"pipeline/stageA/RFold_code/#exp-expargs","title":"exp = Exp(args)","text":""},{"location":"pipeline/stageA/RFold_code/#print-training","title":"print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; training &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;')","text":""},{"location":"pipeline/stageA/RFold_code/#exptest","title":"exp.test()","text":""},{"location":"pipeline/stageA/RFold_code/#print-testing","title":"print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; testing  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;')","text":""},{"location":"pipeline/stageA/RFold_code/#test_f1-test_precision-test_recall-test_runtime-exptest","title":"test_f1, test_precision, test_recall, test_runtime = exp.test()","text":""},{"location":"pipeline/stageA/RFold_code/#file-modelpy","title":"File: /model.py","text":"<p>import torch import torch.nn as nn from module import conv_block, up_conv, Attn</p> <p>class Encoder(nn.Module):     def init(self, C_lst=[17, 32, 64, 128, 256]):         super(Encoder, self).init()         self.enc = nn.ModuleList([conv_block(ch_in=C_lst[0],ch_out=C_lst[1])])         for ch_in, ch_out in zip(C_lst[1:-1], C_lst[2:]):             self.enc.append(                 nn.Sequential(*[                     nn.MaxPool2d(kernel_size=2, stride=2),                     conv_block(ch_in=ch_in, ch_out=ch_out)                 ])             )</p> <pre><code>def forward(self, x):\n    skips = []\n    for i in range(0, len(self.enc)):\n        x = self.enc[i](x)\n        skips.append(x)\n    return x, skips[:-1]\n</code></pre> <p>class Decoder(nn.Module):     def init(self, C_lst=[512, 256, 128, 64, 32]):         super(Decoder, self).init()         self.dec = nn.ModuleList([])         for ch_in, ch_out in zip(C_lst[0:-1], C_lst[1:]):             self.dec.append(                 nn.ModuleList([                     up_conv(ch_in=ch_in, ch_out=ch_out),                     conv_block(ch_in=ch_out * 2, ch_out=ch_out)                 ])             )</p> <pre><code>def forward(self, x, skips):\n    skips.reverse()\n    for i in range(0, len(self.dec)):\n        upsample, conv = self.dec[i]\n        x = upsample(x)\n        x = conv(torch.cat((x, skips[i]), dim=1))\n    return x\n</code></pre> <p>class Seq2Map(nn.Module):     def init(self,                   input_dim=4,                  num_hidden=128,                  dropout=0.1,                   device=torch.device('cuda'),                  max_length=3000,                  kwargs):         super(Seq2Map, self).init(kwargs)         self.device = device         self.dropout = nn.Dropout(dropout)         self.scale = torch.sqrt(torch.FloatTensor([num_hidden])).to(device)</p> <pre><code>    self.tok_embedding = nn.Embedding(input_dim, num_hidden)\n    self.pos_embedding = nn.Embedding(max_length, num_hidden)\n    self.layer = Attn(dim=num_hidden, query_key_dim=num_hidden, dropout=dropout)\n\ndef forward(self, src):\n    batch_size, src_len = src.shape[:2]\n    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n    src = self.tok_embedding(src) * self.scale\n    src = self.dropout(src + self.pos_embedding(pos))\n    attention = self.layer(src)\n    return attention\n</code></pre> <p>class RFold_Model(nn.Module):     def init(self, args):         super(RFold_Model, self).init()</p> <pre><code>    c_in, c_out, c_hid = 1, 1, 32\n    C_lst_enc = [c_in, 32, 64, 128, 256, 512]\n    C_lst_dec = [2*x for x in reversed(C_lst_enc[1:-1])] + [c_hid]\n\n    self.encoder = Encoder(C_lst=C_lst_enc)\n    self.decoder = Decoder(C_lst=C_lst_dec)\n    self.readout = nn.Conv2d(c_hid, c_out, kernel_size=1, stride=1, padding=0)\n    self.seq2map = Seq2Map(input_dim=4, num_hidden=args.num_hidden, dropout=args.dropout)\n\ndef forward(self, seqs):\n    attention = self.seq2map(seqs)\n    x = (attention * torch.sigmoid(attention)).unsqueeze(0)\n    latent, skips = self.encoder(x)\n    latent = self.decoder(latent, skips)\n    y = self.readout(latent).squeeze(1)\n    return torch.transpose(y, -1, -2) * y\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#file-modulepy","title":"File: /module.py","text":"<p>import torch from torch import nn, einsum import torch.nn.functional as F</p> <p>class conv_block(nn.Module):     def init(self,ch_in,ch_out,residual=False):         super(conv_block,self).init()         self.conv = nn.Sequential(             nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),             nn.BatchNorm2d(ch_out),             nn.ReLU(inplace=True),             nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),             nn.BatchNorm2d(ch_out),             nn.ReLU(inplace=True)         )         self.residual = residual</p> <pre><code>def forward(self,x):\n    if self.residual:\n        return x + self.conv(x)\n    return self.conv(x)\n</code></pre> <p>class up_conv(nn.Module):     def init(self,ch_in,ch_out):         super(up_conv,self).init()         self.up = nn.Sequential(             nn.Upsample(scale_factor=2),             nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),             nn.BatchNorm2d(ch_out),             nn.ReLU(inplace=True)         )</p> <pre><code>def forward(self,x):\n    x = self.up(x)\n    return x\n</code></pre> <p>class OffsetScale(nn.Module):     def init(self, dim, heads = 1):         super().init()         self.gamma = nn.Parameter(torch.ones(heads, dim))         self.beta = nn.Parameter(torch.zeros(heads, dim))         nn.init.normal_(self.gamma, std = 0.02)</p> <pre><code>def forward(self, x):\n    out = einsum('... d, h d -&gt; ... h d', x, self.gamma) + self.beta\n    return out.unbind(dim = -2)\n</code></pre> <p>class Attn(nn.Module):     def init(self, *, dim, query_key_dim=128, expansion_factor=2.,         dropout=0.1):         super().init()         self.norm = nn.LayerNorm(dim)         self.dropout = nn.Dropout(dropout)         self.to_qk = nn.Sequential(             nn.Linear(dim, query_key_dim),             nn.SiLU()         )         self.offsetscale = OffsetScale(query_key_dim, heads=2)</p> <pre><code>def forward(self, x):\n    seq_len = x.shape[-2]\n    normed_x = self.norm(x)\n    qk = self.to_qk(normed_x)\n    q, k = self.offsetscale(qk)\n    sim = einsum('b i d, b j d -&gt; b i j', q, k) / seq_len\n    attn = F.relu(sim) ** 2\n    return attn\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#file-parserpy","title":"File: /parser.py","text":"<p>import argparse</p> <p>def create_parser():     parser = argparse.ArgumentParser()     # Set-up parameters     parser.add_argument('--device', default='cuda', type=str, help='Name of device to use for tensor computations (cuda/cpu)')     parser.add_argument('--display_step', default=10, type=int, help='Interval in batches between display of training metrics')     parser.add_argument('--res_dir', default='./results', type=str)     parser.add_argument('--ex_name', default='Debug', type=str)     parser.add_argument('--use_gpu', default=True, type=bool)     parser.add_argument('--gpu', default=0, type=int)     parser.add_argument('--seed', default=111, type=int)</p> <pre><code># dataset parameters\nparser.add_argument('--data_name', default='ArchiveII', choices=['ArchiveII', 'RNAStralign', 'bpRNA'])\nparser.add_argument('--data_root', default='./data/archiveII_all')\nparser.add_argument('--batch_size', default=1, type=int)\nparser.add_argument('--num_workers', default=8, type=int)\n\n# Training parameters\nparser.add_argument('--epoch', default=1, type=int, help='end epoch')\nparser.add_argument('--log_step', default=1, type=int)\nparser.add_argument('--lr', default=0.001, type=float, help='Learning rate')\n\n# debug parameters\nparser.add_argument('--num_hidden', default=128, type=int)\nparser.add_argument('--pf_dim', default=128, type=int)\nparser.add_argument('--num_heads', default=2, type=int)\nparser.add_argument('--dropout', default=0.3, type=float)\nreturn parser.parse_args()\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#file-rfoldpy","title":"File: /rfold.py","text":"<p>import time import torch import numpy as np from tqdm import tqdm from utils import cuda from API import evaluate_result from model import RFold_Model</p>"},{"location":"pipeline/stageA/RFold_code/#predefine-a-base_matrix","title":"predefine a base_matrix","text":"<p>_max_length = 1005 base_matrix = torch.ones(_max_length, _max_length) for i in range(_max_length):     st, en = max(i-3, 0), min(i+3, _max_length-1)     for j in range(st, en + 1):         base_matrix[i, j] = 0.</p> <p>def constraint_matrix(x):     base_a, base_u, base_c, base_g = x[:, :, 0], x[:, :, 1], x[:, :, 2], x[:, :, 3]     batch = base_a.shape[0]     length = base_a.shape[1]     au = torch.matmul(base_a.view(batch, length, 1), base_u.view(batch, 1, length))     au_ua = au + torch.transpose(au, -1, -2)     cg = torch.matmul(base_c.view(batch, length, 1), base_g.view(batch, 1, length))     cg_gc = cg + torch.transpose(cg, -1, -2)     ug = torch.matmul(base_u.view(batch, length, 1), base_g.view(batch, 1, length))     ug_gu = ug + torch.transpose(ug, -1, -2)     return (au_ua + cg_gc + ug_gu) * base_matrix[:length, :length].to(x.device)</p> <p>def row_col_softmax(y):     row_softmax = torch.softmax(y, dim=-1)     col_softmax = torch.softmax(y, dim=-2)     return 0.5 * (row_softmax + col_softmax)</p> <p>def row_col_argmax(y):     y_pred = row_col_softmax(y)     y_hat = y_pred + torch.randn_like(y) * 1e-12     col_max = torch.argmax(y_hat, 1)     col_one = torch.zeros_like(y_hat).scatter(1, col_max.unsqueeze(1), 1.0)     row_max = torch.argmax(y_hat, 2)     row_one = torch.zeros_like(y_hat).scatter(2, row_max.unsqueeze(2), 1.0)     int_one = row_one * col_one      return int_one</p> <p>class RFold(object):     def init(self, args, device):         self.args = args         self.device = device         self.config = args.dict</p> <pre><code>    self.model = self._build_model()\n    self.criterion = torch.nn.MSELoss()\n\ndef _build_model(self, **kwargs):\n    return RFold_Model(self.args).to(self.device)\n\ndef test_one_epoch(self, test_loader, **kwargs):\n    # note that the model is under the training mode for bn/dropout\n    self.model.train()\n    eval_results, run_time = [], []\n    test_pbar = tqdm(test_loader)\n    for batch in test_pbar:\n        contacts, seq_lens, seq_ori = batch\n        contacts, seq_ori = cuda(\n            (contacts.float(), seq_ori.float()), device=self.device)\n\n        # predict\n        seqs = torch.argmax(seq_ori, axis=-1)\n        s_time = time.time()\n        with torch.no_grad():\n            pred_contacts = self.model(seqs)\n\n        pred_contacts = row_col_argmax(pred_contacts) * constraint_matrix(seq_ori)\n\n        # interval time\n        interval_t = time.time() - s_time\n        run_time.append(interval_t)\n\n        eval_result = list(map(lambda i: evaluate_result(pred_contacts.cpu()[i],\n                                                                 contacts.cpu()[i]), range(contacts.shape[0])))\n        eval_results += eval_result\n\n    p, r, f1 = zip(*eval_results)\n    return np.average(f1), np.average(p), np.average(r), np.average(run_time)\n</code></pre>"},{"location":"pipeline/stageA/RFold_code/#file-utilspy","title":"File: /utils.py","text":"<p>import os import logging import numpy as np import torch from torch import optim import random  import torch.backends.cudnn as cudnn from collections.abc import Mapping, Sequence</p> <p>def set_seed(seed):     random.seed(seed)     np.random.seed(seed)     torch.manual_seed(seed)     cudnn.deterministic = True</p> <p>def print_log(message):     print(message)     logging.info(message)</p> <p>def output_namespace(namespace):     configs = namespace.dict     message = ''     for k, v in configs.items():         message += '\\n' + k + ': \\t' + str(v) + '\\t'     return message</p> <p>def check_dir(path):     if not os.path.exists(path):         os.makedirs(path)</p> <p>def get_dataset(config):     from API import load_data     return load_data(**config)</p> <p>def cuda(obj, args, kwargs):     \"\"\"     Transfer any nested conatiner of tensors to CUDA.     \"\"\"     if hasattr(obj, \"cuda\"):         return obj.cuda(args, kwargs)     elif isinstance(obj, Mapping):         return type(obj)({k: cuda(v, *args, kwargs) for k, v in obj.items()})     elif isinstance(obj, Sequence):         return type(obj)(cuda(x, args, kwargs) for x in obj)     elif isinstance(obj, np.ndarray):         return torch.tensor(obj, args, **kwargs)     raise TypeError(\"Can't transfer object type <code>%s</code>\" % type(obj))</p>"},{"location":"pipeline/stageA/RFold_paper/","title":"RFold Paper","text":"<p>Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Cheng Tan 1 2 * Zhangyang Gao 2 1 * Hanqun Cao 3 Xingran Chen 4 Ge Wang 2 Lirong Wu 2 Jun Xia 2 Jiangbin Zheng 2 Stan Z. Li 2 Abstract The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we reformulate the RNA secondary structure prediction as a K-Rook prob- lem, thereby simplifying the prediction process into probabilistic matching within a finite solution space. Building on this innovative perspective, we introduce RFold, a simple yet effective method that learns to predict the most matching K-Rook solution from the given sequence. RFold em- ploys a bi-dimensional optimization strategy that decomposes the probabilistic matching problem into row-wise and column-wise components to reduce the matching complexity, simplifying the solving process while guaranteeing the validity of the output. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art approaches. The code is avail- able at github.com/A4Bio/RFold. 1. Introduction The functions of RNA molecules are determined by their structure (Sloma &amp; Mathews, 2016). The secondary struc- ture, which contains the nucleotide base pairing information, as shown in Figure 1, is crucial for the correct functions of RNA molecules (Fallmann et al., 2017). Although experi- mental assays such as X-ray crystallography (Cheong et al., 2004), nuclear magnetic resonance (F \u00a8urtig et al., 2003), and *Equal contribution 1Zhejiang University 2Westlake University 3The Chinese University of Hong Kong 4University of Michigan. Correspondence to: Stan Z. Li Stan.ZQ.Li@westlake.edu.cn. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).A A C C U G G U C A G G C C C G G A A G G G A G C A G C C A A A C C U G G U C A G G C C C G G A A G G G A G C A G C C A graph representation matrix representation (contact map) Figure 1. The graph and matrix representation of an RNA sec- ondary structure example. cryogenic electron microscopy (Fica &amp; Nagai, 2017) can be implemented to determine RNA secondary structure, they suffer from low throughput and expensive cost. Computational RNA secondary structure prediction meth- ods have been favored for their high efficiency in recent years (Iorns et al., 2007). Currently, mainstream meth- ods can be broadly classified into two categories (Rivas, 2013; Szikszai et al., 2022): (i) comparative sequence anal- ysis and (ii) single sequence folding algorithm. Compara- tive sequence analysis determines the secondary structure conserved among homologous sequences but the limited known RNA families hinder its development (Gutell et al., 2002; Griffiths-Jones et al., 2003; Gardner et al., 2009; Nawrocki et al., 2015). Researchers thus resort to single RNA sequence folding algorithms that do not need mul- tiple sequence alignment information. A classical cate- gory of computational RNA folding algorithms is to use dynamic programming (DP) that assumes the secondary structure is a result of energy minimization (Bellaousov et al., 2013; Nicholas &amp; Zuker, 2008; Lorenz et al., 2011; Zuker, 2003; Mathews &amp; Turner, 2006; Do et al., 2006). However, energy-based approaches usually require the base pairs have a nested structure while ignoring some valid yet biologically essential structures such as pseudoknots, i.e., non-nested base pairs (Chen et al., 2019; Seetin &amp; Math- ews, 2012; Xu &amp; Chen, 2015), as shown in Figure 2. Since predicting secondary structures with pseudoknots under the energy minimization framework has shown to be hard and 1 arXiv:2212.14041v5 [q-bio.BM] 19 Jun 2024 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective NP-complete (Wang &amp; Tian, 2011; Fu et al., 2022), deep learning techniques are introduced as an alternative.A AC U G U A AC U G U nested structure non-nested structure Figure 2. Examples of nested and non-nested secondary structures. Attempts to overcome the limitations of energy-based meth- ods have motivated deep learning methods that predict RNA secondary structures in the absence of DP. SPOT- RNA (Singh et al., 2019) is a seminal work that ensembles ResNet (He et al., 2016) and LSTM (Hochreiter &amp; Schmid- huber, 1997) and applies transfer learning to identify molec- ular recognition features. SPOT-RNA does not constrain the output space into valid RNA secondary structures, which de- grades its generalization ability on new datasets (Jung et al.). E2Efold (Chen et al., 2019) employs an unrolled algorithm for constrained programming that post-processes the net- work output to satisfy the constraints. E2Efold introduces a convex relaxation to make the constrained optimization tractable, leading to possible structural constraint violations and poor generalization ability (Sato et al., 2021; Fu et al., 2022; Franke et al., 2023; 2022). RTfold (Jung et al.) uti- lizes the Fenchel-Young loss (Berthet et al., 2020) to en- able differentiable discrete optimization with perturbations, but the approximation cannot guarantee the satisfaction of constraints. Developing an appropriate optimization that enforces the output to be valid becomes a crucial concern. Since deep learning-based approaches cannot directly out- put valid RNA secondary structures, existing approaches usually formulate the problem into a constrained optimiza- tion problem and optimize the output of the model to fulfill specific constraints as closely as possible. However, these methods typically necessitate iterative optimization, leading to reduced efficiency. Moreover, the extensive optimization space involved does not ensure the complete satisfaction of these constraints. In this study, we introduce a novel perspective for predicting RNA secondary structures by re- framing the challenge as a K-Rook problem. Recognizing the alignment between the solution spaces of the K-Rook problem and RNA secondary structure prediction, our objec- tive is to identify the most compatible K-Rook solution for each RNA sequence. This is achieved by training the deep learning model on prior data to learn matching patterns. Considering the high complexity of the solution space in the symmetric K-Rook problem, we introduced RFold, an inno- vative approach. This method utilizes a bi-dimensional opti- mization strategy, effectively decomposing the problem into separate row-wise and column-wise components. This de- composition significantly reduces the matching complexity, thereby simplifying the solving process while guaranteeing the validity of the output. We conduct extensive experiments to compare RFold with state-of-the-art methods on several benchmark datasets and show the superior performance of our proposed method. Moreover, RFold has faster inference efficiency than those methods due to its simplicity. 2. Related work Comparative Sequence Analysis Comparative sequence analysis determines base pairs conserved among homolo- gous sequences (Gardner &amp; Giegerich, 2004; Knudsen &amp; Hein, 2003; Gorodkin et al., 2001). ILM (Ruan et al., 2004) combines thermodynamic and mutual information content scores. Sankoff (Hofacker et al., 2004) merges the sequence alignment and maximal-pairing folding methods (Nussinov et al., 1978). Dynalign (Mathews &amp; Turner, 2002) and Carnac (Touzet &amp; Perriquet, 2004) are the subsequent vari- ants of Sankoff algorithms. RNA forester (Hochsmann et al., 2003) introduces a tree alignment model for global and local alignments. However, the limited number of known RNA families (Nawrocki et al., 2015) impedes the development. Energy-based Folding Algorithms When the structures consist only of nested base pairing, dynamic programming can predict the structure by minimizing energy. Early works include Vienna RNAfold (Lorenz et al., 2011), Mfold (Zuker, 2003), RNAstructure (Mathews &amp; Turner, 2006), and CONTRAfold (Do et al., 2006). Faster imple- mentations that speed up dynamic programming have been proposed, such as Vienna RNAplfold (Bernhart et al., 2006), LocalFold (Lange et al., 2012), and LinearFold (Huang et al., 2019). However, they cannot accurately predict structures with pseudoknots, as predicting the lowest free energy struc- tures with pseudoknots is NP-complete (Lyngs\u00f8 &amp; Pedersen, 2000), making it difficult to improve performance. Learning-based Folding Algorithms Deep learning methods have inspired approaches in bioengineering appli- cations (Wu et al., 2024a;b; Lin et al., 2022; 2023; Tan et al., 2024; 2023). SPOT-RNA (Singh et al., 2019) is a seminal work that employs deep learning for RNA secondary struc- ture prediction. SPOT-RNA2 (Singh et al., 2021) improves its predecessor by using evolution-derived sequence pro- files and mutational coupling. Inspired by Raptor-X (Wang et al., 2017) and SPOT-Contact (Hanson et al., 2018), SPOT- RNA uses ResNet and bidirectional LSTM with a sigmoid function. MXfold (Akiyama et al., 2018) combines sup- port vector machines and thermodynamic models. CDP- fold (Zhang et al., 2019), DMFold (Wang et al., 2019), and MXFold2 (Sato et al., 2021) integrate deep learning tech- niques with energy-based methods. E2Efold (Chen et al., 2019) constrains the output to be valid by learning unrolled algorithms. However, its relaxation for making the optimiza- tion tractable may violate the constraints. UFold (Fu et al., 2022) introduces U-Net model to improve performance. 2 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective 3. Background 3.1. Preliminaries The primary structure of RNA is a sequence of nucleotide bases A, U, C, and G, arranged in order and represented as X \u201c px1, ..., xLq, where each xi denotes one of these bases. The secondary structure is the set of base pairings within the sequence, modeled as a sparse matrix M P t0, 1uL\u02c6L, where M ij \u201c 1 indicates a bond between bases i and j. The key challenges include (i) designing a model, characterized by parameters \u0398, that captures the complex transformations from the sequence X to the pairing matrix M and (ii) cor- rectly identifying the sparsity of the secondary structure, which is determined by the nature of RNA. Thus, the trans- formation F\u0398 : X \u00de \u00d1 M is usually decomposed into two stages for capturing the sequence-to-structure relationship and optimizing the sparsity of the target matrix: F\u0398 :\u201c G\u03b8g \u02dd H\u03b8h , (1) where H\u03b8h : X \u00de \u00d1 H represents the initial processing step, transforming the RNA sequence into an intermediate, unconstrained representation H P RL\u02c6L. Subsequently, G\u03b8g : H \u00de \u00d1 M parameterizes the optimization stage for the intermediate distribution into the final sparse matrix M . 3.2. Constrained Optimization-based Approaches The core problem of secondary structure prediction lies in sparsity identification. Numerous studies regard this task as a constrained optimization problem, seeking the optimal refinement mappings by gradient descent. Besides, keeping the hard constraints on RNA secondary structures is also essential, which ensures valid biological functions (Steeg, 1993). These constraints can be formally described as: \u2022 (a) Only three types of nucleotide combinations can form base pairs: B :\u201c tAU, UAu Y tGC, CGu Y tGU, UGu. For any base pair xixj where xixj R B, M ij \u201c 0. \u2022 (b) No sharp loop within three bases. For any adjacent bases within a distance of three nucleotides, they cannot form pairs with each other. For all |i \u00b4 j| \u0103 4, M ij \u201c 0. \u2022 (c) There can be at most one pair for each base. For all i and j, \u0159L j\u201c1 M ij \u010f 1, \u0159L i\u201c1 M ij \u010f 1. The search for valid secondary structures is thus a quest for symmetric sparse matrices P t0, 1uL\u02c6L that adhere to the constraints above. The first two constraints can be sat- isfied by defining a constraint matrix \u010eM as: \u010eM ij :\u201c 1 if xixj P B and |i \u00b4 j| \u011b 4, and \u010eM ij :\u201c 0 otherwise. Addressing the third constraint is critical as it necessitates employing sparse optimization techniques. Therefore, our primary objective is to devise an effective sparse optimiza- tion strategy. This strategy is based on the symmetric in- herent distribution H and M , which support constraints (a) and (b), and additionally addresses constraint (c). SPOT-RNA subtly enforces the principles of sparsity. It streamlines the pathway from the raw neural network output H by harnessing the Sigmoid function to distill a sparse pattern. The transformation applies a threshold to yield a binary sparse matrix. This process can be represented as: GpHq \u201c 1rSigmoidpHq\u0105ss. (2) In this approach, a fixed threshold s of 0.5 is applied, typical for inducing sparsity. It omits complex constraints or extra parameters \u03b8g , simply using this cutoff to achieve sparse structure representations. E2Efold introduces a non-linear transformation to the in- termediate value xM P RL\u02c6L and an additional regulariza- tion term } xM }1. 1 2 A H \u00b4 s, T p xM q E \u00b4 \u03c1} xM }1, (3) where T p xM q \u201c 1 2 p xM d xM <code>p xM d xM qT q d \u010eM ensures symmetry and adherence to RNA base-pairing constraints (a) and (b), s is the log-ratio bias term set to logp9.0q, and the \u21131 penalty \u03c1} xM }1 promotes sparsity. To fulfill constraint (c), the objective is combined with conditions T p xM q1 \u010f 1. Denote \u03bb P RL</code> as the Lagrange multiplier, the formulation for the sparse optimization is expressed as: min \u03bb\u011b0 max xM 1 2 A H \u00b4 s, T p xM q E \u00b4 \u03c1} xM }1 \u00b4 A \u03bb, ReLUpT p xM q1 \u00b4 1q E , (4) In the training stage, the optimization objective is the output of score function S dependent on xM and H. It can be regarded as an optimization function G parameterized by \u03b8g : G\u03b8g pHq \u201c T parg max xM PRL\u02c6L Sp xM , Hqq. (5) Although the complicated design to the constraints is ex- plicitly formulated, the iterative updates may fall into sub- optimal or invalid solutions. Besides, it requires additional parameters \u03b8g , making the model training complicated. RTfold introduces a differentiable function that incorpo- rates an additional Gaussian perturbation W . The objective function is expressed as: min xM 1 N N\u00ff i\u201c1 T \u00b4 H <code>\u03f5W piq\u00af \u00b4 xM (6) where T denotes the non-linear transformation to constrain the initial output H, and N is the number of random sam- ples. The random perturbation W piq adjusts the distribution by leveraging the gradient during the optimization process. 3 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective While RTFold designs an efficient differential objective function, the constraints imposed by the non-linear transfor- mation on a noisy hidden distribution may lead to biologi- cally implausible structures. 4. RFold 4.1. Probabilistic K-Rook Matching The symmetric K-Rook arrangement (Riordan, 2014; Elkies &amp; Stanley, 2011) is a classic combinatorial problem in- volving the placement of KpK \u010f Lq non-attacking Rooks on an L \u02c6 L chessboard, where the goal is to arrange the Rooks such that they form a symmetric pattern. The term \u2019non-attacking\u2019 means that no two Rooks are positioned in the same row or column. An interesting parallel can be drawn between this combinatorial scenario and the domain of RNA secondary structure prediction, as illustrated in Fig- ure 3. This analogy stems from the conceptual similarity in the arrangement patterns required in both cases. The RNA sequence can be regarded as a chessboard of size L and the base pairs are the Rooks. The core problem is to determine an optimal arrangement of these base pairs.a d heb c f g 8 4 1 3 2 7 6 5 A C AUA C f C U G C A A A C C (a) symmetric Rooks arrangement (b) RNA secondary structure Figure 3. The analogy between the symmetric K-Rook arrange- ment and the RNA secondary structure prediction. Given a finite solution space M defined by the symmetric K-Rook arrangement, we reformulate our objective as a probabilistic matching problem. The goal is to find the most matching solution M \u02da P M for the given sequence X. The optimal solution M \u02da is defined as: M \u02da \u201c arg max M PM PpM |Xq. (7) According to Bayes\u2019 theorem, the posterior probability can be represented as PpM |Xq \u201c PpX|M qPpM q PpXq . Since the denominator P pXq is constant for all M , and assuming that the solution space is finite and each solution within it is equally likely, we can adopt a uniform prior PpM q in this context. Therefore, maximizing the posterior probability is equivalent to maximizing the likelihood PpX|M q. This leads to the following equation: M \u02da \u201c arg max M PM PpX|M q. (8) Therefore, our primary task becomes computing the like- lihood PpX|M q for the given sequence X under each possible solution M . 4.2. Bi-dimensional Optimization Computing the likelihood PpX|M q directly poses sig- nificant challenges. To address this, we propose a bi- dimensional optimization strategy that simplifies the prob- lem by decomposing it into row-wise and column-wise com- ponents. This approach is mathematically represented as: PpX|M q \u201c PpX|RqPpX|Cq, (9) where M is the product of the row-wise component R P RL\u02c6L and the column-wise component C P RL\u02c6L, i.e., M \u201c R d C. Each component represents the optimal solu- tion for the row-wise and column-wise matching problems, respectively. Importantly, the row-wise and column-wise components are independent, and the comprehensive solu- tion for the entire problem is derived from the product of the optimal solutions for these two sub-problems. Applying Bayes\u2019 theorem, for the row-wise component, we have PpR|Xq \u201c PpX|RqPpRq PpXq . Given that the solution space of R is both finite and valid, we can regard it as a uniform distribution. The analysis for the column-wise com- ponent, PpC|Xq, follows a similar approach. Therefore, the optimal solution M \u02da can be represented as: M \u02da \u201c arg max R,C PpR|XqPpC|Xq \u201c arg max R PpR|Xq arg max C PpC|Xq (10) The next phase involves establishing proxies for PpR|Xq and PpC|Xq. To this end, we introduce the basic sym- metric hidden distribution, xH \u201c pH d HT q d \u010eM . The row-wise and column-wise components are then derived by applying Softmax functions to xH, resulting in their respec- tive probability distributions: RpxHq \u201c exppxHij q \u0159L k\u201c1 exppxHikq , CpxHq \u201c exppxHij q \u0159L k\u201c1 exppxHkj q . (11) The final output is the element-wise product of the row-wise component RpxHq and the column-wise component CpxHq. This operation integrates the individual insights from both dimensions, leading to the optimized matrix M \u02da: M \u02da \u201c arg max RpxHq d arg max CpxHq. (12) As illustrated in Figure 4, we consider a random symmetric 6 \u02c6 6 matrix as an example. For simplicity, we disregard 4 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspectiveargmax \u211b \ud835\udc6f) \u2299 argmax \ud835\udc9e(\ud835\udc6f)\u0302 ) symmetric matrix \ud835\udc6f) argmax \u211b(\ud835\udc6f) ) argmax \ud835\udc9e(\ud835\udc6f) ) Figure 4. The visualization of arg max Rp xHq d arg max Cp xHq. the constraints (a-b) from \u010eM . This example demonstrates the outputs of Rp\u00a8q, Cp\u00a8q, and their element-wise product Rp\u00a8q d Cp\u00a8q. The row-wise and column-wise components jointly select the value that has the maximum in both its row and column while keeping the output matrix symmetric. Given the definition of xH \u201c pH d HT q d \u010eM , it is evident that xH inherently forms a symmetric and non- negative matrix. Regarding optimization, the operation RpxHq d CpxHq can be equivalently simplified to optimizing 1 2 pRpxHq</code> CpxHqq. This is because both approaches fun- damentally aim to maximize the congruence between the row-wise and column-wise components of xH. The underly- ing reason for this equivalence is that both optimizing the Hadamard product and the arithmetic mean of RpxHq and CpxHq focus on reinforcing the alignment and coherence across the various dimensions of the matrix. Moreover, examining the gradients of these operations sheds light on their computational efficiencies. The gradient of RpxHqdCpxHq entails a blend of partial derivatives intercon- nected via element-wise multiplication. It can be formally expressed as follows: BpRpxHq d CpxHqqij B xHij \u201cCpxHqij \u00a8 BRpxHqij B xHij <code>RpxHqij \u00a8 BCpxHqij B xHij . (13) In contrast, the gradient of 1 2 pRpxHq</code> CpxHqq is character- ized by a straightforward sum of partial derivatives: BpRpxHq <code>CpxHqqij B xHij \u201c BRpxHqij B xHij</code> BCpxHqij B xHij . (14) Element-wise addition, as used in the latter, tends to be numerically more stable and less susceptible to issues like floating-point precision errors, which are more common in element-wise multiplication operations. This stability is particularly beneficial when dealing with large-scale matri- ces or when the gradients involve extreme values, where numerical instability can pose significant challenges. The proposed simplification not only maintains the math- ematical integrity of the optimization problem but also provides computational advantages, making it a desirable strategy in practical scenarios involving large and intricate datasets. Consequently, we define the overall loss function as the mean square error (MSE) between the averaged row- wise and column-wise components of xH and the ground truth secondary structure M : LpM \u02da, M q \u201c 1 L2 \u203a \u203a \u203a 1 2 pRpxHq <code>CpxHqq \u00b4 M \u203a \u203a \u203a2 . (15) 4.3. Practical Implementation We identify the problem of predicting H P RL\u02c6L from the given sequence attention map pZ P RL\u02c6L as an image-to- image segmentation problem and apply the U-Net model to extract pair-wise information, as shown in Figure 5.sequence one-hot \ud835\udc7f: \ud835\udc3f\u00d74 Token embedding \ud835\udc3f\u00d7\ud835\udc37 Seq2map Attention \ud835\udc3f\u00d7\ud835\udc3f\u00d71 + Positional embedding \ud835\udc3f\u00d7\ud835\udc37 1 32 3264 128 64 256 512 256 128 \ud835\udc3f\u00d7\ud835\udc3f (\ud835\udc3f/2)\u00d7(\ud835\udc3f/2) (\ud835\udc3f/4)\u00d7(\ud835\udc3f/4) (\ud835\udc3f/8)\u00d7(\ud835\udc3f/8) (\ud835\udc3f/16)\u00d7(\ud835\udc3f/16) 1 \ud835\udc3f\u00d7\ud835\udc3f \ud835\udc6f \ud835\udc81+ \ud835\udc81 Figure 5. The overview model architecture of RFold. To automatically learn informative representations from sequences, we propose a Seq2map attention module. Given a sequence in one-hot form X P RL\u02c64, we first obtain the sum of the token embedding and positional embedding as the input of the Seq2map attention. We denote the input as Z P RL\u02c6D for convenience, where D is the hidden layer size of the token and positional embeddings. Motivated by the recent progress in attention mecha- nisms (Vaswani et al., 2017; Choromanski et al., 2020; Katharopoulos et al., 2020; Hua et al., 2022), we aim to develop a highly effective sequence-to-map transforma- tion based on pair-wise attention. We obtain the query Q P RL\u02c6D and key K P RL\u02c6D by applying per-dim scalars and offsets to Z: Q \u201c \u03b3QZ</code> \u03b2Q, K \u201c \u03b3K Z <code>\u03b2K , (16) where \u03b3Q, \u03b3K , \u03b2Q, \u03b2K P RL\u02c6D are learnable parameters. Then, the pair-wise attention map is obtained by: sZ \u201c ReLU2pQKT {Lq, (17) 5 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective where ReLU2 is an activation function that can be recog- nized as a simplified Softmax function in vanilla Transform- ers (So et al., 2021). The output of Seq2map is the gated representation of sZ: pZ \u201c sZ d \u03c3p sZq, (18) where \u03c3p\u00a8q is the Sigmoid function that performs as a gate. 5. Experiments We conduct experiments to compare our proposed RFold with state-of-the-art and commonly used approaches. Mul- tiple experimental settings are taken into account, includ- ing standard structure prediction, generalization evaluation, large-scale benchmark evaluation, cross-family evaluation, pseudoknot prediction and inference time comparison. De- tailed experimental setups can be found in the Appendix B. 5.1. Standard RNA Secondary Structure Prediction Following (Chen et al., 2019), we split the RNAStralign dataset into train, validation, and test sets by stratified sam- pling. We report the results in Table 1. Energy-based meth- ods achieve relatively weak F1 scores ranging from 0.420 to 0.633. Learning-based folding algorithms like E2Efold and UFold significantly improve performance by large mar- gins, while RFold obtains even better performance among all the metrics. Moreover, RFold obtains about 8% higher precision than the state-of-the-art method. This suggests that our optimization strategy is strict to satisfy all the hard constraints for predicting valid structures. Table 1. Results on RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.450 0.398 0.420 RNAfold 0.516 0.568 0.540 RNAstructure 0.537 0.568 0.550 CONTRAfold 0.608 0.663 0.633 LinearFold 0.620 0.606 0.609 CDPfold 0.633 0.597 0.614 E2Efold 0.866 0.788 0.821 UFold 0.905 0.927 0.915 RFold 0.981 0.973 0.977 5.2. Generalization Evaluation To verify the generalization ability of our proposed RFold, we directly evaluate the performance on another benchmark dataset ArchiveII using the pre-trained model on the RNAS- tralign training dataset. Following (Chen et al., 2019), we exclude RNA sequences in ArchiveII that have overlapping RNA types with the RNAStralign dataset for a fair compari- son. The results are reported in Table 2. It can be seen that traditional methods achieve F1 scores in the range of 0.545 to 0.842. A recent learning-based method, MXfold2, obtains an F1 score of 0.768, which is even lower than some energy-based methods. Another state- of-the-art learning-based method improves the performance to the F1 score of 0.905. RFold further improves the F1 score to 0.921, even higher than UFold. It is worth noting that RFold has a relatively lower result in the recall metric and a significantly higher result in the precision metric. The reason for this phenomenon may be the strict constraints of RFold. While none of the current learning-based methods can satisfy all the constraints we introduced in Sec. 3.2, the predictions of RFold are guaranteed to be valid. Thus, RFold may cover fewer pair-wise interactions, leading to a lower recall metric. However, the highest F1 score still suggests the great generalization ability of RFold. Table 2. Results on ArchiveII dataset. Results in bold and under- lined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.668 0.590 0.621 CDPfold 0.557 0.535 0.545 RNAfold 0.663 0.613 0.631 RNAstructure 0.664 0.606 0.628 CONTRAfold 0.696 0.651 0.665 LinearFold 0.724 0.605 0.647 RNAsoft 0.665 0.594 0.622 Eternafold 0.667 0.622 0.636 E2Efold 0.734 0.660 0.686 SPOT-RNA 0.743 0.726 0.711 MXfold2 0.788 0.760 0.768 Contextfold 0.873 0.821 0.842 RTfold 0.891 0.789 0.814 UFold 0.887 0.928 0.905 RFold 0.938 0.910 0.921 5.3. Large-scale Benchmark Evaluation The bpRNA dataset is a large-scale benchmark, comprises fixed training (TR0), evaluation (VL0), and testing (TS0) sets. Following previous works (Singh et al., 2019; Sato et al., 2021; Fu et al., 2022), we train the model in bpRNA- TR0 and evaluate the performance on bpRNA-TS0 by using the best model learned from bpRNA-VL0. The detailed results can be found in Table 3. RFold outperforms the prior state-of-the-art method, SPOT- RNA, by a notable 4.0% in terms of the F1 score. This improvement in the F1 score can be attributed to the con- sistently superior performance of RFold in the precision metric when compared to baseline models. However, it is important to note that the recall metric remains constrained, 6 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 3. Results on bpRNA-TS0 set. Method Precision Recall F1 E2Efold 0.140 0.129 0.130 RNAstructure 0.494 0.622 0.533 RNAsoft 0.497 0.626 0.535 RNAfold 0.494 0.631 0.536 Mfold 0.501 0.627 0.538 Contextfold 0.529 0.607 0.546 LinearFold 0.561 0.581 0.550 MXfold2 0.519 0.646 0.558 Externafold 0.516 0.666 0.563 CONTRAfold 0.528 0.655 0.567 SPOT-RNA 0.594 0.693 0.619 UFold 0.521 0.588 0.553 RFold 0.692 0.635 0.644 likely due to stringent constraints applied during prediction. Table 4. Results on long-range bpRNA-TS0 set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.315 0.450 0.356 RNAfold 0.304 0.448 0.350 RNAstructure 0.299 0.428 0.339 CONTRAfold 0.306 0.439 0.349 LinearFold 0.281 0.355 0.305 RNAsoft 0.310 0.448 0.353 Externafold 0.308 0.458 0.355 SPOT-RNA 0.361 0.492 0.403 MXfold2 0.318 0.450 0.360 Contextfold 0.332 0.432 0.363 UFold 0.543 0.631 0.584 RFold 0.803 0.765 0.701 Following (Fu et al., 2022), we conduct an experiment on long-range interactions. Given a sequence of length L, the long-range base pairing is defined as the paired and unpaired bases with intervals longer than L{2. As shown in Table 4, RFold performs unexpectedly well on these long-range base pairing predictions and improves UFold in all metrics by large margins, demonstrating its strong predictive ability. 5.4. Cross-family Evaluation The bpRNA-new dataset is a cross-family benchmark dataset comprising 1,500 RNA families, presenting a signifi- cant challenge for pure deep learning approaches. UFold, for instance, relies on the thermodynamic method Contrafold for data augmentation to achieve satisfactory results. As shown in Table 5, the standard UFold achieves an F1 score of 0.583, while RFold reaches 0.616. When the same data augmentation technique is applied, UFold\u2019s performance increases to 0.636, whereas RFold yields a score of 0.651. This places RFold second only to the thermodynamics-based method, Contrafold, in terms of F1 score. Table 5. Results on bpRNA-new. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 E2Efold 0.047 0.031 0.036 SPOT-RNA 0.635 0.641 0.620 Contrafold 0.620 0.736 0.661 UFold 0.500 0.736 0.583 UFold + aug 0.570 0.742 0.636 RFold 0.614 0.619 0.616 RFold + aug 0.618 0.687 0.651 5.5. Predict with Pseudoknots Following E2Efold (Chen et al., 2019), we consider a se- quence to be a true positive if it is correctly identified as containing a pseudoknot. For this analysis, we extracted all sequences featuring pseudoknots from the RNAStralign test dataset and assessed their predictive accuracy. The results of this analysis are summarized in the following table: Table 6. Results on RNA structures with pseudoknots. Method Precision Recall F1 Score RNAstructure 0.778 0.761 0.769 SPOT-RNA 0.677 0.978 0.800 E2Efold 0.844 0.990 0.911 UFold 0.962 0.990 0.976 RFold 0.971 0.993 0.982 RFold demonstrates superior performance compared to its counterparts across all evaluated metrics, i.e., precision, recall, and F1 score. This consistent outperformance across multiple dimensions of accuracy underscores the efficacy and robustness of the RFold approach in predicting RNA structures with pseudoknots. 5.6. Inference Time Comparison We compared the running time of various methods for pre- dicting RNA secondary structures using the RNAStralign testing set with the same experimental setting and the hard- ware environment as in (Fu et al., 2022). The results are pre- sented in Table 7, which shows the average inference time per sequence. The fastest energy-based method, LinearFold, takes about 0.43s for each sequence. The learning-based baseline, UFold, takes about 0.16s. RFold has the highest inference speed, costing only about 0.02s per sequence. In particular, RFold is about eight times faster than UFold and sixteen times faster than MXfold2. 5.7. Ablation Study Bi-dimensional Optimization To validate the effective- ness of our proposed bi-dimensional optimization strategy, we conduct an experiment that replaces them with other op- 7 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 7. Inference time on the RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Time CDPfold (Tensorflow) 300.11 s RNAstructure (C) 142.02 s CONTRAfold (C++) 30.58 s Mfold (C) 7.65 s Eternafold (C++) 6.42 s RNAsoft (C++) 4.58 s RNAfold (C) 0.55 s LinearFold (C++) 0.43 s SPOT-RNA(Pytorch) 77.80 s (GPU) E2Efold (Pytorch) 0.40 s (GPU) MXfold2 (Pytorch) 0.31 s (GPU) UFold (Pytorch) 0.16 s (GPU) RFold (Pytorch) 0.02 s (GPU) timization methods. The results are summarized in Table 8, where RFold-E and RFold-S denote our model with the opti- mization strategies of E2Efold and SPOT-RNA, respectively. While precision, recall, and F1 score are evaluated at base- level, we report the validity which is a sample-level metric evaluating whether the predicted structure satisfies all the constraints. It can be seen that though RFold-E has compa- rable performance in the first three metrics with ours, many of its predicted structures are invalid. The optimization strategy of SPOT-RNA has incorporated no constraint that results in its low validity. Moreover, its strategy seems to not fit our model well, which may be caused by the simplicity of our proposed RFold model. Table 8. Ablation study on different optimization strategies (RNAStralign testing set). Method Precision Recall F1 Validity RFold 0.981 0.973 0.977 100.00% RFold-E 0.888 0.906 0.896 50.31% RFold-S 0.223 0.988 0.353 0.00% Seq2map Attention We also conduct an experiment to evaluate the proposed Seq2map attention. We replace the Seq2map attention with the hand-crafted features from UFold and the outer concatenation from SPOT-RNA, which are denoted as RFold-U and RFold-SS, respectively. In ad- dition to performance metrics, we also report the average inference time for each RNA sequence to evaluate the model complexity. We summarize the result in Table 9. It can be seen that RFold-U takes much more inference time than our RFold and RFold-SS due to the heavy computational cost when loading and learning from hand-crafted features. Moreover, it is surprising to find that RFold-SS has a little better performance than RFold-U, with the least inference time for its simple outer concatenation operation. However, neither RFold-U nor RFold-SS can provide informative rep- resentations like our proposed Seq2map attention. With comparable inference time with the simplest RFold-SS, our RFold outperforms baselines by large margins. Table 9. Ablation study on different pre-processing strategies (RNAStralign testing set). Method Precision Recall F1 Time RFold 0.981 0.973 0.977 0.0167 RFold-U 0.875 0.941 0.906 0.0507 RFold-SS 0.886 0.945 0.913 0.0158 Row-wise and Column-wise Componenets We con- ducted comprehensive ablation studies on the row-wise and column-wise components of our proposed model, RFold, by modifying the inference mechanism using pre-trained checkpoints. These studies were meticulously designed to isolate and understand the individual contributions of these components to our model\u2019s performance in RNA secondary structure prediction. The results, presented across three datasets\u2014RNAStralign (Table 10), ArchiveII (Table 11), and bpRNA-TS0 (Table 12)\u2014highlight two key findings: (i) Removing both the row-wise and column-wise compo- nents leads to a substantial drop in the model\u2019s performance, underscoring their pivotal role within our model\u2019s archi- tecture. This dramatic reduction in effectiveness clearly demonstrates that both components are integral to achieving high accuracy. The significant decline in performance when these components are omitted highlights their essential func- tion in capturing the complex dependencies within RNA sequences. (ii) The performance metrics when isolating either the row-wise or column-wise components are remark- ably similar across all datasets. This uniformity suggests that the training process, which incorporates row-wise and column-wise softmax functions, likely yields symmetric outputs. Consequently, this symmetry implies that each component contributes in an almost equal measure to the model\u2019s overall predictive capacity. Table 10. Ablation study on row-wise and column-wise compo- nents (RNAStralign testing set). Method Precision Recall F1 Validity RFold 0.981 0.973 0.977 100.00% RFold w/o C 0.972 0.975 0.973 75.99% RFold w/o R 0.972 0.975 0.973 75.99% RFold w/o R,C 0.016 0.031 0.995 0.00% 5.8. Visualization We visualize two examples predicted by RFold and UFold in Figure 6. The corresponding F1 scores are denoted at 8 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 11. Ablation study on row-wise and column-wise compo- nents (ArchiveII). Method Precision Recall F1 Validity RFold 0.938 0.910 0.921 100.00% RFold w/o C 0.919 0.914 0.914 49.14% RFold w/o R 0.919 0.914 0.914 49.14% RFold w/o R,C 0.013 0.997 0.025 0.00% Table 12. Ablation study on row-wise and column-wise compo- nents (bpRNA-TS0). Method Precision Recall F1 Validity RFold 0.693 0.635 0.644 100.00% RFold w/o C 0.652 0.651 0.637 12.97% RFold w/o R 0.652 0.651 0.637 12.97% RFold w/o R,C 0.021 0.995 0.040 0.00% the bottom right of each plot. The first row of secondary structures is a simple example of a nested structure. It can be seen that UFold may fail in such a case. The second row of secondary structures is much more difficult that contains over 300 bases of the non-nested structure. While UFold fails in such a complex case, RFold can predict the structure accurately. Due to the limited space, we provide more visualization comparisons in Appendix D.RFoldTrue UFold A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U UG U U C G G GG A C C A C C CG C U A A A U U A C A U G C U A C CG C A G C A G U G C U GA A A G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C GA A G U U CU A A G G C C U U C U G C U A C G A A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U A G U C G A A C CC C U C A G A G A U G A G G A U G G A A U C A A U G 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U UG U U C G G GG A C C A C C CG C U A A A U U A C A U G C U A C CG C A G C A G U G C U GA A A G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C GA A G U U CU A A G G C C U U C U G C U A C G A A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U A G U C G A A C CC C U C A G A G A U G A G G A U G G A A U C A A U G 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U U G U U C G GGG A C C A C C CG C U A A A U U A C A U G C U A C C G CA G C AGUGCU G A AA G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C G A A G U U C U A A G G C C U U C U G C U A CGA A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 RFoldTrue UFold 1.000 0.995 0.558 0.823 Figure 6. Visualization of the true and predicted structures. 6. Conclusion In this study, we reformulate RNA secondary structure pre- diction as a K-Rook problem, thus transforming the predic- tion process into probabilistic matching. Subsequently, we introduce RFold, an efficient learning-based model, which utilizes a bidimensional optimization strategy to decom- pose the probabilistic matching into row-wise and column- wise components, simplifying the solving process while guaranteeing the validity of the output. Comprehensive experiments demonstrate that RFold achieves competitive performance with faster inference speed. The limitations of RFold primarily revolve around its strin- gent constraints. This strictness in constraints implies that RFold is cautious in predicting interactions, leading to higher precision but possibly at the cost of missing some true interactions. Though we have provided a naive so- lution in Appendix C, it needs further studies to obtain a better strategy that leads to more balanced precision-recall trade-offs and more comprehensive structural predictions. Acknowledgements This work was supported by National Science and Technol- ogy Major Project (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake Univer- sity and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake Univer- sity Industries of the Future Research Funding. Impact Statement RFold is the first learning-based method that guarantees the validity of predicted RNA secondary structures. Its capability to ensure accurate predictions. It can be a valuable tool for biologists to study the structure and function of RNA molecules. Additionally, RFold stands out for its speed, significantly surpassing previous methods, marking it as a promising avenue for future developments in this field. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Akiyama, M., Sato, K., and Sakakibara, Y. A max-margin training of rna secondary structure prediction integrated with the thermodynamic model. Journal of bioinformatics and computational biology, 16(06):1840025, 2018. Andronescu, M., Aguirre-Hernandez, R., Condon, A., and Hoos, H. H. Rnasoft: a suite of rna secondary struc- ture prediction and design software tools. Nucleic acids research, 31(13):3416\u20133422, 2003. Bellaousov, S., Reuter, J. S., Seetin, M. G., and Mathews, D. H. Rnastructure: web servers for rna secondary struc- ture prediction and analysis. Nucleic acids research, 41 (W1):W471\u2013W474, 2013. 9 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Bernhart, S. H., Hofacker, I. L., and Stadler, P. F. Local rna base pairing probabilities in large sequences. Bioinfor- matics, 22(5):614\u2013615, 2006. Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.- P., and Bach, F. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508\u20139519, 2020. Chen, X., Li, Y., Umarov, R., Gao, X., and Song, L. Rna secondary structure prediction by learning unrolled algo- rithms. In International Conference on Learning Repre- sentations, 2019. Cheong, H.-K., Hwang, E., Lee, C., Choi, B.-S., and Cheong, C. Rapid preparation of rna samples for nmr spectroscopy and x-ray crystallography. Nucleic acids research, 32(10):e84\u2013e84, 2004. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mo- hiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. Do, C. B., Woods, D. A., and Batzoglou, S. Contrafold: Rna secondary structure prediction without physics-based models. Bioinformatics, 22(14):e90\u2013e98, 2006. Elkies, N. and Stanley, R. P. Chess and mathematics. Recu- perado el, 11, 2011. Fallmann, J., Will, S., Engelhardt, J., Gr \u00a8uning, B., Backofen, R., and Stadler, P. F. Recent advances in rna folding. Journal of biotechnology, 261:97\u2013104, 2017. Fica, S. M. and Nagai, K. Cryo-electron microscopy snap- shots of the spliceosome: structural insights into a dy- namic ribonucleoprotein machine. Nature structural &amp; molecular biology, 24(10):791\u2013799, 2017. Franke, J., Runge, F., and Hutter, F. Probabilistic trans- former: Modelling ambiguities and distributions for rna folding and molecule design. Advances in Neural Infor- mation Processing Systems, 35:26856\u201326873, 2022. Franke, J. K., Runge, F., and Hutter, F. Scalable deep learning for rna secondary structure prediction. arXiv preprint arXiv:2307.10073, 2023. Fu, L., Cao, Y., Wu, J., Peng, Q., Nie, Q., and Xie, X. Ufold: fast and accurate rna secondary structure prediction with deep learning. Nucleic acids research, 50(3):e14\u2013e14, 2022. F \u00a8urtig, B., Richter, C., W \u00a8ohnert, J., and Schwalbe, H. Nmr spectroscopy of rna. ChemBioChem, 4(10):936\u2013962, 2003. Gardner, P. P. and Giegerich, R. A comprehensive compari- son of comparative rna structure prediction approaches. BMC bioinformatics, 5(1):1\u201318, 2004. Gardner, P. P., Daub, J., Tate, J. G., Nawrocki, E. P., Kolbe, D. L., Lindgreen, S., Wilkinson, A. C., Finn, R. D., Griffiths-Jones, S., Eddy, S. R., et al. Rfam: updates to the rna families database. Nucleic acids research, 37 (suppl 1):D136\u2013D140, 2009. Gorodkin, J., Stricklin, S. L., and Stormo, G. D. Discovering common stem\u2013loop motifs in unaligned rna sequences. Nucleic Acids Research, 29(10):2135\u20132144, 2001. Griffiths-Jones, S., Bateman, A., Marshall, M., Khanna, A., and Eddy, S. R. Rfam: an rna family database. Nucleic acids research, 31(1):439\u2013441, 2003. Gutell, R. R., Lee, J. C., and Cannone, J. J. The accuracy of ribosomal rna comparative structure models. Current opinion in structural biology, 12(3):301\u2013310, 2002. Hanson, J., Paliwal, K., Litfin, T., Yang, Y., and Zhou, Y. Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks. Bioinfor- matics, 34(23):4039\u20134045, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Hochsmann, M., Toller, T., Giegerich, R., and Kurtz, S. Local similarity in rna secondary structures. In Computa- tional Systems Bioinformatics. CSB2003. Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003, pp. 159\u2013168. IEEE, 2003. Hofacker, I. L., Bernhart, S. H., and Stadler, P. F. Alignment of rna base pairing probability matrices. Bioinformatics, 20(14):2222\u20132227, 2004. Hua, W., Dai, Z., Liu, H., and Le, Q. Transformer quality in linear time. In International Conference on Machine Learning, pp. 9099\u20139117. PMLR, 2022. Huang, L., Zhang, H., Deng, D., Zhao, K., Liu, K., Hen- drix, D. A., and Mathews, D. H. Linearfold: linear-time approximate rna folding by 5\u2019-to-3\u2019dynamic program- ming and beam search. Bioinformatics, 35(14):i295\u2013i304, 2019. Iorns, E., Lord, C. J., Turner, N., and Ashworth, A. Utilizing rna interference to enhance cancer drug discovery. Nature reviews Drug discovery, 6(7):556\u2013568, 2007. 10 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Jung, A. J., Lee, L. J., Gao, A. J., and Frey, B. J. Rtfold: Rna secondary structure prediction using deep learning with domain inductive bias. Kalvari, I., Nawrocki, E. P., Ontiveros-Palacios, N., Argasin- ska, J., Lamkiewicz, K., Marz, M., Griffiths-Jones, S., Toffano-Nioche, C., Gautheret, D., Weinberg, Z., et al. Rfam 14: expanded coverage of metagenomic, viral and microrna families. Nucleic Acids Research, 49(D1):D192\u2013 D200, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. Knudsen, B. and Hein, J. Pfold: Rna secondary structure prediction using stochastic context-free grammars. Nu- cleic acids research, 31(13):3423\u20133428, 2003. Lange, S. J., Maticzka, D., M \u00a8ohl, M., Gagnon, J. N., Brown, C. M., and Backofen, R. Global or local? predicting secondary structure and accessibility in mrnas. Nucleic acids research, 40(12):5215\u20135226, 2012. Lin, H., Huang, Y., Liu, M., Li, X. C., Ji, S., and Li, S. Z. Diffbp: Generative diffusion of 3d molecules for target protein binding. ArXiv, abs/2211.11214, 2022. URL https://api.semanticscholar. org/CorpusID:253734621. Lin, H., Huang, Y., Zhang, H., Wu, L., Li, S., Chen, Z., and Li, S. Z. Functional-group- based diffusion for pocket-specific molecule gen- eration and elaboration. ArXiv, abs/2306.13769, 2023. URL https://api.semanticscholar. org/CorpusID:259251644. Lorenz, R., Bernhart, S. H., H \u00a8oner zu Siederdissen, C., Tafer, H., Flamm, C., Stadler, P. F., and Hofacker, I. L. Viennarna package 2.0. Algorithms for molecular biology, 6(1):1\u201314, 2011. Lyngs\u00f8, R. B. and Pedersen, C. N. Rna pseudoknot predic- tion in energy-based models. Journal of computational biology, 7(3-4):409\u2013427, 2000. Mathews, D. H. and Turner, D. H. Dynalign: an algorithm for finding the secondary structure common to two rna sequences. Journal of molecular biology, 317(2):191\u2013 203, 2002. Mathews, D. H. and Turner, D. H. Prediction of rna sec- ondary structure by free energy minimization. Current opinion in structural biology, 16(3):270\u2013278, 2006. Nawrocki, E. P., Burge, S. W., Bateman, A., Daub, J., Eber- hardt, R. Y., Eddy, S. R., Floden, E. W., Gardner, P. P., Jones, T. A., Tate, J., et al. Rfam 12.0: updates to the rna families database. Nucleic acids research, 43(D1): D130\u2013D137, 2015. Nicholas, R. and Zuker, M. Unafold: Software for nucleic acid folding and hybridization. Bioinformatics, 453:3\u201331, 2008. Nussinov, R., Pieczenik, G., Griggs, J. R., and Kleitman, D. J. Algorithms for loop matchings. SIAM Journal on Applied mathematics, 35(1):68\u201382, 1978. Riordan, J. An introduction to combinatorial analysis. 2014. Rivas, E. The four ingredients of single-sequence rna sec- ondary structure prediction. a unifying perspective. RNA biology, 10(7):1185\u20131196, 2013. Ruan, J., Stormo, G. D., and Zhang, W. An iterated loop matching approach to the prediction of rna secondary structures with pseudoknots. Bioinformatics, 20(1):58\u2013 66, 2004. Sato, K., Akiyama, M., and Sakakibara, Y. Rna secondary structure prediction using deep learning with thermody- namic integration. Nature communications, 12(1):1\u20139, 2021. Seetin, M. G. and Mathews, D. H. Rna structure prediction: an overview of methods. Bacterial regulatory RNA, pp. 99\u2013122, 2012. Singh, J., Hanson, J., Paliwal, K., and Zhou, Y. Rna sec- ondary structure prediction using an ensemble of two- dimensional deep neural networks and transfer learning. Nature communications, 10(1):1\u201313, 2019. Singh, J., Paliwal, K., Zhang, T., Singh, J., Litfin, T., and Zhou, Y. Improved rna secondary structure and tertiary base-pairing prediction using evolutionary profile, mu- tational coupling and two-dimensional transfer learning. Bioinformatics, 37(17):2589\u20132600, 2021. Sloma, M. F. and Mathews, D. H. Exact calculation of loop formation probability identifies folding motifs in rna secondary structures. RNA, 22(12):1808\u20131818, 2016. So, D., Ma \u00b4nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Searching for efficient transformers for language modeling. Advances in Neural Information Processing Systems, 34:6010\u20136022, 2021. Steeg, E. W. Neural networks, adaptive optimization, and rna secondary structure prediction. Artificial intelligence and molecular biology, pp. 121\u2013160, 1993. Szikszai, M., Wise, M. J., Datta, A., Ward, M., and Mathews, D. Deep learning models for rna secondary structure prediction (probably) do not generalise across families. bioRxiv, 2022. 11 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Tan, C., Zhang, Y., Gao, Z., Hu, B., Li, S., Liu, Z., and Li, S. Z. Hierarchical data-efficient representation learning for tertiary structure-based rna design. In The Twelfth International Conference on Learning Representations, 2023. Tan, C., Gao, Z., Wu, L., Xia, J., Zheng, J., Yang, X., Liu, Y., Hu, B., and Li, S. Z. Cross-gate mlp with protein com- plex invariant embedding is a one-shot antibody designer. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 38, pp. 15222\u201315230, 2024. Tan, Z., Fu, Y., Sharma, G., and Mathews, D. H. Turbofold ii: Rna structural alignment and secondary structure pre- diction informed by multiple homologs. Nucleic acids research, 45(20):11570\u201311581, 2017. Touzet, H. and Perriquet, O. Carnac: folding families of related rnas. Nucleic acids research, 32(suppl 2):W142\u2013 W145, 2004. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Wang, L., Liu, Y., Zhong, X., Liu, H., Lu, C., Li, C., and Zhang, H. Dmfold: A novel method to predict rna sec- ondary structure with pseudoknots based on deep learning and improved base pair maximization principle. Frontiers in genetics, 10:143, 2019. Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J. Accu- rate de novo prediction of protein contact map by ultra- deep learning model. PLoS computational biology, 13(1): e1005324, 2017. Wang, X. and Tian, J. Dynamic programming for np-hard problems. Procedia Engineering, 15:3396\u20133400, 2011. Wayment-Steele, H. K., Kladwang, W., Strom, A. I., Lee, J., Treuille, A., Participants, E., and Das, R. Rna sec- ondary structure packages evaluated and improved by high-throughput experiments. BioRxiv, pp. 2020\u201305, 2021. Wu, L., Huang, Y., Tan, C., Gao, Z., Hu, B., Lin, H., Liu, Z., and Li, S. Z. Psc-cpi: Multi-scale protein sequence-structure contrasting for efficient and gener- alizable compound-protein interaction prediction. arXiv preprint arXiv:2402.08198, 2024a. Wu, L., Tian, Y., Huang, Y., Li, S., Lin, H., Chawla, N. V., and Li, S. Z. Mape-ppi: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. arXiv preprint arXiv:2402.14391, 2024b. Xu, X. and Chen, S.-J. Physics-based rna structure predic- tion. Biophysics reports, 1(1):2\u201313, 2015. Zakov, S., Goldberg, Y., Elhadad, M., and Ziv-Ukelson, M. Rich parameterization improves rna structure prediction. Journal of Computational Biology, 18(11):1525\u20131542, 2011. Zhang, H., Zhang, C., Li, Z., Li, C., Wei, X., Zhang, B., and Liu, Y. A new method of rna secondary structure prediction based on convolutional neural network and dynamic programming. Frontiers in genetics, 10:467, 2019. Zuker, M. Mfold web server for nucleic acid folding and hybridization prediction. Nucleic acids research, 31(13): 3406\u20133415, 2003. 12 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective A. Comparison of mainstream RNA secondary structure prediction methods We compare our proposed method RFold with several other leading RNA secondary structure prediction methods and summarize the results in Table 13. RFold satisfies all three constraints (a)-(c) for valid RNA secondary struc- tures, while the other methods do not fully meet some of the constraints. RFold utilizes a sequence-to-map attention mechanism to capture long-range dependencies, whereas SPOT-RNA simply concatenates pairwise sequence infor- mation and E2Efold/UFold uses hand-crafted features. In terms of prediction accuracy on the RNAStralign benchmark test set, RFold achieves the best F1 score of 0.977, outper- forming SPOT-RNA, E2Efold and UFold by a large margin. Regarding the average inference time, RFold is much more efficient and requires only 0.02 seconds to fold the RNAS- tralign test sequences. In summary, RFold demonstrates superior performance over previous methods for RNA sec- ondary structure prediction in both accuracy and speed. B. Experimental Details Datasets We use three benchmark datasets: (i) RNAS- tralign (Tan et al., 2017), one of the most comprehensive collections of RNA structures, is composed of 37,149 struc- tures from 8 RNA types; (ii) ArchiveII (Sloma &amp; Mathews, 2016), a widely used benchmark dataset in classical RNA folding methods, containing 3,975 RNA structures from 10 RNA types; (iii) bpRNA (Singh et al., 2019), is a large scale benchmark dataset, containing 102,318 structures from 2,588 RNA types. (iv) bpRNA-new (Sato et al., 2021), de- rived from Rfam 14.2 (Kalvari et al., 2021), containing sequences from 1500 new RNA families. Baselines We compare our proposed RFold with base- lines including energy-based folding methods such as Mfold (Zuker, 2003), RNAsoft (Andronescu et al., 2003), RNAfold (Lorenz et al., 2011), RNAstructure (Mathews &amp; Turner, 2006), CONTRAfold (Do et al., 2006), Con- textfold (Zakov et al., 2011), and LinearFold (Huang et al., 2019); learning-based folding methods such as SPOT- RNA (Singh et al., 2019), Externafold (Wayment-Steele et al., 2021), E2Efold (Chen et al., 2019), MXfold2 (Sato et al., 2021), and UFold (Fu et al., 2022). Metrics We evaluate the performance by precision, recall, and F1 score, which are defined as: Precision \u201c TP TP</code> FP , Recall \u201c TP TP <code>FN , F1 \u201c 2 Precision \u00a8 Recall Precision</code> Recall , (19) where TP, FP, and FN denote true positive, false positive and false negative, respectively. Implementation details Following the same experimental setting as (Fu et al., 2022), we train the model for 100 epochs with the Adam optimizer. The learning rate is 0.001, and the batch size is 1 for sequences with different lengths. C. Discussion on Abnormal Samples Although we have illustrated three hard constraints in 3.2, there exist some abnormal samples that do not satisfy these constraints in practice. We have analyzed the datasets used in this paper and found that there are some abnormal sam- ples in the testing set that do not meet these constraints. The ratio of valid samples in each dataset is summarized in the table below: As shown in Table 8, RFold forces the validity to be 100.00%, while other methods like E2Efold only achieve about 50.31%. RFold is more accurate than other methods in reflecting the real situation. Nevertheless, we provide a soft version of RFold to relax the strict constraints. A possible solution to relax the rigid pro- cedure is to add a checking mechanism before the Argmax function in the inference. Specifically, if the confidence given by the Softmax is low, we do not perform Argmax and assign more base pairs. It can be implemented as the following pseudo-code: 1 y_pred = row_col_softmax(y) 2 int_one = row_col_argmax(y_pred) 3 4 # get the confidence for each position 5 conf = y_pred * int_one 6 all_pos = conf &gt; 0.0 7 8 # select reliable position 9 conf_pos = conf &gt; thr1 10 11 # select unreliable position with the full row and column 12 uncf_pos = get_unreliable_pos(all_pos, conf_pos) 13 14 # assign \"1\" for the positions with the confidence higher than thr2 15 # note that thr2 &lt; thr1 16 y_pred[uncf_pos] = (y_pred[uncf_pos] &gt; thr2 ).float() 17 int_one[uncf_pos] = y_pred[uncf_pos] We conduct experiments to compare the soft-RFold and the original version of RFold in the RNAStralign dataset. The results are summarized in the Table 15. It can be seen that soft-RFold improves the recall metric by a small margin. The minor improvement may be because the number of abnormal samples is small. We then select those samples that do not obey the three constraints to further analyze the performance. The total number of such samples is 179. It can be seen that soft-RFold can deal with abnormal samples well. The improvement of the recall metric is more obvious. 13 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 13. Comparison between RNA secondary structure prediction methods and RFold. Method SPOT-RNA E2Efold UFold RFold pre-processing pairwise concat pairwise concat hand-crafted seq2map attention optimization approach \u02c6 unrolled algorithm unrolled algorithm bi-dimensional optimization constraint (a) \u02c6 \u2713 \u2713 \u2713 constraint (b) \u02c6 \u2713 \u2713 \u2713 constraint (c) \u02c6 \u02c6 \u02c6 \u2713 F1 score 0.711 0.821 0.915 0.977 Inference time 77.80 s 0.40 s 0.16 s 0.02 s Table 14. The ratio of valid samples in the datasets. Dataset RNAStralign ArchiveII bpRNA Validity 93.05% 96.03% 96.51% Table 15. The results of soft-RFold and RFold on the RNAStralign. Method Precision Recall F1 RFold 0.981 0.973 0.977 soft-RFold 0.978 0.974 0.976 Table 16. The results of soft-RFold and RFold on the abnormal samples on the RNAStralign. Method Precision Recall F1 RFold 0.956 0.860 0.905 soft-RFold 0.949 0.889 0.918 D. Visualization 14 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching PerspectiveRFoldTrue UFold Figure 7. Visualization of the true and predicted structures. 15 =====</p>"},{"location":"pipeline/stageA/StageA_RFold/","title":"Overview","text":""},{"location":"pipeline/stageA/StageA_RFold/#technical-documentation-integrating-rfold-into-a-multi-stage-rna-3d-pipeline","title":"\ud83d\ude80 Technical Documentation: Integrating RFold into a Multi-Stage RNA 3D Pipeline","text":""},{"location":"pipeline/stageA/StageA_RFold/#focusing-on-stage-a-2d-structureadjacency","title":"\ud83c\udfaf Focusing on Stage\u202fA (2D Structure/Adjacency)","text":""},{"location":"pipeline/stageA/StageA_RFold/#1-overview","title":"\ud83d\udccc 1. Overview","text":"<p>This document outlines how to incorporate RFold\u2014a K-Rook-based RNA secondary structure predictor\u2014into Stage\u202fA of a multi-stage RNA 3D pipeline. Stage\u202fA is responsible for predicting base pairs (2D adjacency or contact maps) from an RNA sequence. Subsequent stages (torsion angle prediction and 3D reconstruction) use these adjacency maps as inputs. By integrating RFold into Stage\u202fA, valid secondary structures are guaranteed, and the overall workflow is streamlined.</p>"},{"location":"pipeline/stageA/StageA_RFold/#2-why-use-rfold-for-stage-a","title":"\u2705 2. Why Use RFold for Stage\u202fA?","text":"<ul> <li>Validity Guarantee \ud83d\udd12</li> <li>RFold formulates RNA folding as a symmetric K-Rook matching problem, ensuring nucleotides pair at most once and adhere strictly to base-type/distance constraints.</li> <li> <p>Other neural methods often require iterative relaxations or additional steps to approximate constraints.</p> </li> <li> <p>High Accuracy \ud83c\udfaf</p> </li> <li> <p>RFold demonstrates superior F1 scores across RNA datasets (RNAStralign, ArchiveII, bpRNA), notably excelling in precision.</p> </li> <li> <p>Fast Inference \u26a1</p> </li> <li> <p>Runs approximately 0.02\u2009s per sequence (on GPU), ideal for rapidly processing large RNA datasets.</p> </li> <li> <p>Straightforward Integration \ud83d\udd17</p> </li> <li>Produces an N\u00d7N adjacency (or probability) matrix easily fed into subsequent stages (torsion prediction and 3D reconstruction).</li> </ul>"},{"location":"pipeline/stageA/StageA_RFold/#3-stage-a-2d-structure-adjacency","title":"\ud83d\udee0\ufe0f 3. Stage\u202fA: 2D Structure / Adjacency","text":"<p>In Stage\u202fA, an RNA sequence of length N is used to generate a contact map (adjacency matrix <code>M[i,j] = 1</code> indicates paired nucleotides).</p> \ud83d\udcd0 Constraints Typically Enforced  - **Base-type**: Only permitted pairs are **A\u2013U**, **G\u2013C**, or **G\u2013U**. - **Minimum Loop Length**: No pairs allowed between indices `i, j` if `|i\u2212j| &lt; 4`. - **One Pair per Nucleotide**: Each row/column in the adjacency matrix can have at most one \"1.\"   <p>RFold inherently adheres to these constraints, representing base pairs as non-attacking rooks on a chessboard, thus guaranteeing a valid adjacency matrix.</p>"},{"location":"pipeline/stageA/StageA_RFold/#4-implementation-steps","title":"\ud83d\udd27 4. Implementation Steps","text":""},{"location":"pipeline/stageA/StageA_RFold/#41-files-class-structure","title":"\ud83d\udcc2 4.1 Files &amp; Class Structure","text":"<ul> <li><code>rna_predict/models/stageA_2d.py</code></li> <li>Class: <code>RFold2DPredictor</code></li> <li> <p>Responsibilities:</p> <ul> <li>Implements RFold logic (Seq2map attention, U-Net backbone, row/column softmax factorization).</li> <li>Method: <code>predict_2d_structure(sequence)</code> \u2192 returns adjacency/probability map.</li> </ul> </li> <li> <p><code>rna_predict/dataset/dataset_loader.py</code> (optional)</p> </li> <li>Handles unified data loading, tokenization, and necessary preprocessing.</li> </ul>"},{"location":"pipeline/stageA/StageA_RFold/#minimal-example-sketch","title":"\ud83d\udcbb Minimal Example Sketch","text":"<pre><code># rna_predict/models/stageA_2d.py\nimport torch\nimport torch.nn as nn\n\nclass RFold2DPredictor(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 1) Token embeddings + Seq2map attention\n        # 2) U-Net backbone\n        # 3) Row/col softmax heads\n        # 4) Load pre-trained weights ('rfold_checkpoint.pt')\n\n    def forward(self, seq_str: str):\n        \"\"\"\n        Args:\n          seq_str: e.g., 'AUGC...'\n        Returns:\n          adjacency_matrix: [N, N] binary adjacency\n        \"\"\"\n        pass\n\ndef build_rfold_predictor(config):\n    model = RFold2DPredictor(config)\n    # model.load_state_dict(torch.load(\"rfold_checkpoint.pt\"))\n    return model\n</code></pre>"},{"location":"pipeline/stageA/StageA_RFold/#42-stage-a-integration","title":"\ud83e\udde9 4.2 Stage\u202fA Integration","text":"<p>Define an extractor class for seamless integration:</p> <pre><code>class StageA2DExtractor:\n    def __init__(self, config):\n        self.model = build_rfold_predictor(config)\n\n    def run(self, sequence: str) -&gt; np.ndarray:\n        adjacency = self.model(sequence)  # [N, N]\n        return adjacency\n</code></pre> <p>Pipeline usage example:</p> <pre><code># pipeline.py\ndef run_pipeline(seq: str):\n    # Stage A\n    adjacency = stageA2D.run(seq)  # [N, N]\n    # Stage B\n    angles = torsionPredictor.forward(seq, adjacency)\n    # Stage C\n    coords = forwardKinematics.run(angles)\n</code></pre>"},{"location":"pipeline/stageA/StageA_RFold/#5-notes-on-usage-configuration","title":"\u2699\ufe0f 5. Notes on Usage &amp; Configuration","text":"<ul> <li> <p>Pretrained Weights \ud83d\udce6</p> <ul> <li>RFold typically requires training on datasets (RNAStralign, bpRNA). Checkpoints from the RFold authors can be utilized.</li> </ul> </li> <li> <p>Symmetry &amp; Masking \ud83c\udfad</p> <ul> <li>Internally employs <code>(H * H^T) * mask</code> to remove invalid pairs.</li> <li>Combines row/column-softmax outputs to form final adjacency probabilities.</li> </ul> </li> <li> <p>Binary Output \ud83d\udd18</p> <ul> <li>Apply thresholding (\u22650.5) or argmax on probabilities for binary adjacency matrices, maintaining valid row-column constraints.</li> </ul> </li> <li> <p>Performance Gains \ud83d\ude85</p> <ul> <li>Rapid inference (~0.02\u2009s per sequence, for RNA lengths 100\u2013200 nt).</li> <li>Better scalability than iterative/relaxation-based methods.</li> <li>For extremely long RNAs (1000+ nucleotides), monitor memory usage in the N\u00d7N computation step.</li> </ul> </li> <li> <p>Soft Version \ud83c\udf25\ufe0f</p> <ul> <li>Slight constraint relaxation for datasets with rare exceptions (see the \u201csoft-RFold\u201d discussion in original publications).</li> </ul> </li> </ul> <p>Note on Configuration (Hydra)</p> <p>Stage A's behavior is controlled via Hydra. The default parameters are set in: <code>rna_predict/conf/model/stageA.yaml</code></p> <p>Here's a snippet illustrating the structure and some key default values:</p> <p><pre><code># rna_predict/conf/model/stageA.yaml\n\n# --- Top-Level Parameters ---\nnum_hidden: 128       # Corresponds to query_key_dim in Seq2Map? (Check RFold code)\ndropout: 0.3          # General dropout rate\nmin_seq_length: 80    # Minimum sequence length for padding\ndevice: \"cuda\"        # Target device (\"cuda\" or \"cpu\")\ncheckpoint_path: \"RFold/checkpoints/RNAStralign_trainset_pretrained.pth\"\ncheckpoint_url: \"https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1\"\nbatch_size: 32        # Batch size (if applicable during inference/training)\nlr: 0.001             # Learning rate (for training/fine-tuning)\nthreshold: 0.5        # Threshold for converting probabilities to binary map\n\n# --- Nested Configurations ---\nvisualization:\n  enabled: true\n  varna_jar_path: \"tools/varna-3-93.jar\" # Default path to VARNA jar\n  resolution: 8.0\n\nmodel:\n  # U-Net Architecture\n  conv_channels: [64, 128, 256, 512]\n  residual: true\n  c_hid: 32 # Hidden channels in U-Net bottleneck\n\n  # Seq2Map Component\n  seq2map:\n    attention_heads: 8\n    attention_dropout: 0.1\n    # ... other seq2map params ...\n\n  # Decoder Component\n  decoder:\n    skip_connections: true\n    # ... other decoder params ...\n</code></pre> (Note: Refer to <code>rna_predict/conf/config_schema.py</code> for the complete, typed definition of all parameters and their defaults.)</p> <p>Overriding Defaults via Command Line:</p> <p>You can easily override any parameter without editing the YAML file using command-line arguments. Since the main <code>default.yaml</code> loads <code>model/stageA</code>, these parameters are accessed under the <code>stageA</code> group:</p> <ul> <li>Change Dropout: <pre><code>python rna_predict/pipeline/stageA/run_stageA.py stageA.dropout=0.1\n</code></pre></li> <li>Use CPU: <pre><code>python rna_predict/pipeline/stageA/run_stageA.py stageA.device=cpu\n</code></pre></li> <li>Change Nested Parameter (U-Net hidden channels): <pre><code>python rna_predict/pipeline/stageA/run_stageA.py stageA.model.c_hid=64\n</code></pre></li> <li>Change Deeper Nested Parameter (Seq2Map attention heads): <pre><code>python rna_predict/pipeline/stageA/run_stageA.py stageA.model.seq2map.attention_heads=12\n</code></pre></li> <li>Disable Visualization: <pre><code>python rna_predict/pipeline/stageA/run_stageA.py stageA.visualization.enabled=false\n</code></pre></li> </ul>"},{"location":"pipeline/stageA/StageA_RFold/#combine-multiple-overrides-as-needed-this-allows-for-flexible-experimentation","title":"Combine multiple overrides as needed. This allows for flexible experimentation.","text":""},{"location":"pipeline/stageA/StageA_RFold/#6-benefits-summary","title":"\ud83c\udf89 6. Benefits &amp; Summary","text":"<p>Adopting RFold in Stage\u202fA provides:</p> <ul> <li> <p>Constraint Satisfaction \u2705</p> <ul> <li>Guaranteed valid base pairs, proper loop lengths, and strict nucleotide pairing constraints.</li> </ul> </li> <li> <p>Precision \ud83d\udccc</p> <ul> <li>Minimal spurious pair predictions, leveraging row-column factorization.</li> </ul> </li> <li> <p>Efficiency \u23f1\ufe0f</p> <ul> <li>Substantial speed advantage, ideal for high-throughput RNA folding applications.</li> </ul> </li> </ul> <p>Thus, RFold is optimal for pipelines requiring fast, accurate, and constraint-valid adjacency predictions prior to downstream torsion-angle estimation and 3D reconstruction.</p>"},{"location":"pipeline/stageA/StageA_RFold/#references","title":"\ud83d\udcda References","text":"<ul> <li>Chen, et al. \u201cRNA Secondary Structure Prediction by Learning Unrolled Algorithms.\u201d ICLR, 2019.</li> <li>Fu, et al. \u201cUFold: Fast and Accurate RNA Secondary Structure Prediction with Deep Learning.\u201d NAR, 2022.</li> <li>Tan, et al. \u201cDeciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective.\u201d arXiv, 2024.</li> </ul>"},{"location":"pipeline/stageA/rfold-demo/","title":"-- coding: utf-8 --","text":"<p>\"\"\"demo.ipynb</p> <p>Automatically generated by Colab.</p> <p>Original file is located at     https://colab.research.google.com/drive/1rAWP7evVLc7cbIP3KzPr5ZlHTVo9A57g</p>"},{"location":"pipeline/stageA/rfold-demo/#reproduce-the-results-in-the-paper","title":"Reproduce the results in the paper","text":""},{"location":"pipeline/stageA/rfold-demo/#download-the-rfold-code-checkpoints-and-datasets","title":"Download the RFold code, checkpoints, and datasets","text":"<p>\"\"\"</p> <p>!git clone https://github.com/A4Bio/RFold !wget -O RFold/checkpoints.zip https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=0 !wget -O RFold/data.zip https://www.dropbox.com/s/wzbkd3q43haax0r/data.zip?dl=0 !unzip -o RFold/checkpoints.zip -d RFold/ !unzip -o RFold/data.zip -d RFold/</p> <p>\"\"\"### Import packages\"\"\"</p> <p>import os os.chdir('/content/RFold')</p> <p>import torch import json import argparse import collections from RFold.main import Exp</p> <p>RNA_SS_data = collections.namedtuple('RNA_SS_data', 'seq ss_label length name pairs')</p> <p>\"\"\"### Predefined paths of checkpoints and configs\"\"\"</p> <p>pretrained_file = {     'RNAStralign': {         'config': './checkpoints/RNAStralign.json',         'checkpoint': './checkpoints/RNAStralign_trainset_pretrained.pth'     },     'ArchiveII': {         'config': './checkpoints/ArchiveII.json',         'checkpoint': './checkpoints/RNAStralign_trainset_pretrained.pth'     },     'bpRNA': {         'config': './checkpoints/bpRNA.json',         'checkpoint': './checkpoints/bpRNA_trainset_pretrained.pth'     } }</p> <p>\"\"\"### Reproduce the results\"\"\"</p> <p>for data_name in ['RNAStralign', 'ArchiveII', 'bpRNA']:     config = json.load(open(pretrained_file[data_name]['config'], 'r'))     args = argparse.Namespace(**config)     exp = Exp(args)     exp.method.model.load_state_dict(torch.load(pretrained_file[data_name]['checkpoint']))     exp.test()</p> <p>\"\"\"## Predict a single sequence\"\"\"</p> <p>name = 'test' sequence = 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC'</p> <p>import json from RFold.colab_utils import process_seqs, row_col_argmax, constraint_matrix, save_ct, visual_get_bases</p> <p>config = json.load(open(pretrained_file['RNAStralign']['config'], 'r')) args = argparse.Namespace(**config) exp = Exp(args) exp.method.model.load_state_dict(torch.load(pretrained_file['RNAStralign']['checkpoint']))</p> <p>\"\"\"### process the sequence\"\"\"</p> <p>nseq, nseq_one_hot, seq_len = process_seqs(sequence, exp.device)</p> <p>\"\"\"### make the predicion\"\"\"</p> <p>raw_pred = exp.method.model(nseq) preds = row_col_argmax(raw_pred) * constraint_matrix(nseq_one_hot)</p> <p>\"\"\"### save ct file\"\"\"</p> <p>save_ct(preds[0, :seq_len, :seq_len], nseq_one_hot[0, :seq_len], name)</p> <p>\"\"\"### visualize the contact map\"\"\"</p> <p>import matplotlib.pyplot as plt</p> <p>plt.figure() plt.axis('off') plt.imshow(preds.cpu().numpy()[0], cmap='GnBu')</p> <p>\"\"\"### visualize the RNA secondary structure\"\"\"</p> <p>import subprocess from RFold.colab_utils import process_seqs, row_col_argmax, constraint_matrix, save_ct, visual_get_bases</p>"},{"location":"pipeline/stageA/rfold-demo/#name-test","title":"name = 'test'","text":"<p>a_bases, u_bases, c_bases, g_bases = visual_get_bases(sequence) subprocess.Popen([\"java\", \"-cp\", \"VARNAv3-93.jar\",     \"fr.orsay.lri.varna.applications.VARNAcmd\",     '-i', './'+name+'.ct',     '-basesStyle1', 'fill=#FF0000,outline=#FF0000,label=#FFFFFF,number=#FF0000',     '-applyBasesStyle1on', a_bases,     '-basesStyle2', 'fill=#FF6600,outline=#FF6600,label=#FFFFFF,number=#FF6600',     '-applyBasesStyle2on', u_bases,     '-basesStyle3', 'fill=#FFFF008,outline=#FFFF008,label=#000000,number=#FFFF008',     '-applyBasesStyle3on', c_bases,     '-basesStyle4', 'fill=#00FFFF,outline=#00FFFF,label=#000000,number=#00FFFF',     '-applyBasesStyle4on', g_bases,     '-o', './'+name+'.png',     '-resolution', '8.0'],     stderr=subprocess.STDOUT, stdout=subprocess.PIPE).communicate()[0]</p> <p>from PIL import Image import matplotlib.pyplot as plt</p> <p>plt.figure(figsize=(10, 10)) plt.axis('off') plt.imshow(Image.open(name+'.png'))</p>"},{"location":"pipeline/stageA/stage_a_extra/","title":"Extra Notes","text":"<p>Below is a concise yet in-depth look at how Stage\u202fA (secondary-structure prediction) can be implemented using RFold in your multi-stage RNA 3D pipeline. We\u2019ll walk through the essential concepts, how RFold fits into Stage\u202fA, and recommended design patterns to ensure the pipeline remains clean, maintainable, and aligned with general software engineering principles.</p> <p>\u2e3b</p> <ol> <li>What Stage\u202fA Does</li> </ol> <p>Stage\u202fA\u2019s goal is to generate an N\u00d7N adjacency (or contact) matrix that indicates which nucleotides are base-paired. In practice:     1.  You input an RNA sequence of length N.     2.  The Stage\u202fA model predicts whether each pair (i,j) is paired (M[i,j] = 1) or not (M[i,j] = 0).     3.  Later pipeline stages (e.g., torsion-angle prediction, forward kinematics) can incorporate this adjacency (or 2D structure) to drive more accurate 3D predictions.</p> <p>\u2e3b</p> <ol> <li>RFold: A Quick Refresher</li> </ol> <p>RFold reframes RNA 2D structure prediction as a K-Rook problem:     \u2022   Each row and column can have at most one \u201cRook,\u201d analogous to \u201ceach nucleotide can form at most one base-pair.\u201d     \u2022   To do that, RFold uses a bi-dimensional optimization approach: it computes row-wise and column-wise probabilities separately (via softmax), merges them, and ensures that the final adjacency matrix is valid (no conflicting pair assignments, no short loops, etc.).</p> <p>Key Assets in RFold     \u2022   Seq2Map Attention Module: Learns a token embedding + positional embedding from one-hot nucleotides, then produces a pairwise attention map (size L\u00d7L) that encodes potential base pair signals.     \u2022   UNet-like Encoder\u2013Decoder: Interprets the attention map as an \u201cimage\u201d and refines it, capturing local and global patterns.     \u2022   Row\u2013Column Factorization: Applies row-wise and column-wise constraints so you end up with a valid adjacency matrix that obeys base-pair constraints.</p> <p>\u2e3b</p> <ol> <li>Incorporating RFold Into Stage\u202fA</li> </ol> <p>3.1 Inputs &amp; Outputs     \u2022   Input: Single RNA sequence (string of A/U/C/G, or with custom expansions).     \u2022   Output: Binary adjacency matrix [N, N] or a probability matrix from which you can binarize.</p> <p>Suggested: Typically, you produce a final binary adjacency (1 = base-pair, 0 = no pair), or possibly a real-valued contact matrix if you want a probability threshold. But the standard usage in your pipeline is a final discrete adjacency.</p> <p>3.2 Where RFold\u2019s Code Sits</p> <p>If your pipeline is structured as:     1.  Stage\u202fA: 2D structure (RFold)     2.  Stage\u202fB: Torsion angles     3.  Stage\u202fC: Forward kinematics     4.  Stage\u202fD: Optional refinements</p> <p>Then you\u2019d do something like:</p> <p>def stageA_predict_adjacency(sequence: str, rfold_model) -&gt; np.ndarray:     \"\"\"     Return an [N x N] adjacency from the given RNA sequence using RFold.     \"\"\"     # 1) Convert sequence to one-hot or numeric form     # 2) Let the rfold_model produce the contact map     # 3) Binarize the contact map if needed     # 4) Return adjacency     return adjacency</p> <p>Implementation Tip: Keep the Stage\u202fA logic in a dedicated class or function. That ensures single responsibility (SRP) \u2013 your code for secondary-structure prediction stays separate from other pipeline pieces.</p> <p>\u2e3b</p> <ol> <li>Design Patterns &amp; Best Practices</li> </ol> <p>Below are some best-practice pointers, blending the KISS/DRY/YAGNI principles with Domain-Driven Design (DDD) ideas:     1.  SRP (Single Responsibility Principle)     \u2022   Have a single class (e.g., RFoldPredictor) that loads the model, processes the sequence, and returns adjacency.     \u2022   This keeps the rest of your pipeline decoupled from the internal details of how exactly RFold does row\u2013column factorization or how the UNet is structured.     2.  Keep It Simple, Stupid (KISS)     \u2022   Resist adding extra \u201chooks\u201d or toggles unless your pipeline truly needs them.     \u2022   Let your RFoldPredictor handle the entire \u201csequence \u2192 adjacency\u201d step with minimal additional friction.     3.  DRY     \u2022   If you integrate RFold into multiple places (like for pre-training or for analysis), unify repeated code in utility functions (e.g., process_seqs in colab_utils.py).     \u2022   That ensures you aren\u2019t duplicating the same data prep logic in different corners of your pipeline.     4.  YAGNI     \u2022   Don\u2019t embed advanced alphaFold-like or 3D logic in your Stage\u202fA. The goal is simply to produce adjacency.     \u2022   If you see yourself wanting partial adjacency constraints for specialized tasks, consider a separate refinement or \u201csoft adjacency\u201d post-processing function. But don\u2019t build that unless you need it.     5.  DDD / Bounded Context     \u2022   Stage\u202fA can be considered its own Bounded Context: \u201c2D-structure domain.\u201d     \u2022   The pipeline is more easily managed if each stage is loosely coupled. That means you just pass the adjacency matrix as the main \u201cartifact\u201d from Stage\u202fA \u2192 Stage\u202fB.     6.  Testing     \u2022   Even if you\u2019re not doing formal TDD, try to keep a handful of unit tests (or small test sequences) that quickly confirm \u201cGiven sequence \u2018AAGU\u2026\u2019, does the adjacency from Stage\u202fA match the expected pairs?\u201d     \u2022   This ensures your pipeline always has a robust foundation for subsequent angles or coordinate building.</p> <p>\u2e3b</p> <ol> <li>Practical Implementation Flow</li> </ol> <p>Here\u2019s a suggested step-by-step outline for hooking RFold into Stage\u202fA:     1.  Initialize &amp; Load     \u2022   Create a Python module or class named stageA_rfold.py or similar.     \u2022   Inside, define RFoldPredictor which:     \u2022   Loads the RFold config and checkpoint (like in the colab example).     \u2022   Prepares the model for inference (sets .eval() if needed).     2.  Preprocessing     \u2022   Provide a function, e.g., prepare_sequence(seq: str) to convert 'AUCG...' into the right numeric form (like the code in colab_utils.process_seqs).     \u2022   Possibly refine it so it can handle variable lengths, padding to multiples of 16, etc.     3.  Prediction     \u2022   In RFoldPredictor.predict_adjacency(sequence: str) -&gt; np.ndarray:     1.  Convert the sequence to tensor form, shape [1, L].     2.  Pass it through the RFold_Model.     3.  Extract the raw contact map. E.g., raw_pred = model(...)     4.  Binarize with row-col ArgMax plus constraint matrix:</p> <p>pred_contacts = row_col_argmax(raw_pred) * constraint_matrix(...)</p> <pre><code>5.  Crop to the original length if you used any padding.\n6.  Return an N\u00d7N NumPy array.\n\n4.  Integration\n\u2022   In your pipeline, define:\n</code></pre> <p>def run_stageA(sequence: str, rfold_predictor: RFoldPredictor) -&gt; np.ndarray:     return rfold_predictor.predict_adjacency(sequence)</p> <pre><code>\u2022   Then from your main pipeline code:\n</code></pre> <p>adjacency = run_stageA(my_seq, rfold_predictor)</p> <pre><code>\u2022   That adjacency is then used in Stage\u202fB (torsion angles) or whichever logic you have for 3D building.\n\n5.  (Optional) Post-Processing\n\u2022   If you need \u201csoft adjacency\u201d (probabilities) or \u201csoft-RFold\u201d logic (like the partial snippet they showed in the paper\u2019s Appendix C with thresholds thr1, thr2), you can either:\n\u2022   Add a small method that returns \u201csoft adjacency\u201d or \u201cconfidence matrix,\u201d or\n\u2022   Keep a separate function that merges adjacency with a confidence-based approach.\n</code></pre> <p>\u2e3b</p> <ol> <li>Additional Considerations<ol> <li>Edge Cases \u2022   Very short sequences (&lt;4 nucleotides) might be trivial or not well-handled by big UNet. Just ensure you handle them gracefully (RFold does a quick check internally). \u2022   Very large sequences might require you to watch GPU memory usage. You can chop them into smaller segments or rely on the batch logic in your code.</li> <li>Potential Performance Tuning \u2022   RFold is already quite fast (~0.02\u202fs per sequence). If you have extremely large volumes of sequences, you might consider a batch approach\u2014though in practice, a single-sequence approach is often fine.</li> <li>Testing \u2022   Have a small test script that runs RFoldPredictor.predict_adjacency() on 2\u20133 known test sequences. Compare the adjacency with an expected .ct file or known pairs. \u2022   This ensures future code changes don\u2019t break Stage\u202fA\u2019s output.</li> <li>Future Expansions \u2022   If you want to incorporate extra domain-specific rules (e.g., no triple helices, specialized isosteric constraints), these generally go in the final adjacency logic or a separate \u201cpost-processing refinement.\u201d \u2022   Keep the core \u201cRFold\u201d code unmodified if possible, since it\u2019s well-tested for standard constraints.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Summary of Best Design Approach     \u2022   Create a dedicated \u201cStageA_RFold\u201d module or class that loads the pre-trained RFold, processes sequences, and returns adjacency.     \u2022   Keep SRP by isolating all the 2D structure logic inside that class and not mixing it with torsion-angle or 3D concerns.     \u2022   Use small, well-documented functions (like prepare_sequence, predict_adjacency) to keep code clear.     \u2022   Validate the adjacency on a few sample sequences to confirm correctness.</li> </ol> <p>By structuring Stage\u202fA in this way, you gain a maintainable, testable foundation. You also ensure minimal friction when hooking adjacency into later pipeline stages. With RFold\u2019s speed and accuracy, your pipeline can robustly handle 2D structure generation before moving on to angle or 3D predictions.</p> <p>\u2e3b</p> <p>Closing Thoughts</p> <p>Integrating RFold in Stage\u202fA is straightforward if you keep an eye on single responsibility and structured pipeline design. RFold\u2019s row\u2013column factorization and K-Rook approach produce valid 2D structures quickly, letting you devote attention to subsequent 3D steps. By wrapping RFold\u2019s usage in a small, cohesive class or function, you keep the pipeline clean, modular, and easy to extend in the future.</p>"},{"location":"pipeline/stageB/Stage_B/","title":"\ud83d\ude80 Stage B: Comprehensive RNA Torsion Angle Predictor","text":""},{"location":"pipeline/stageB/Stage_B/#domain-inputs-outputs","title":"\ud83d\udccc Domain, Inputs &amp; Outputs","text":""},{"location":"pipeline/stageB/Stage_B/#inputs","title":"\ud83d\udce5 Inputs","text":"<ol> <li>RNA Sequence</li> <li>Length: N residues</li> <li> <p>Residues: <code>{A, C, G, U}</code> or modification tokens</p> </li> <li> <p>2D Adjacency (Base-Pair Matrix) (from Stage A)</p> </li> <li>Matrix size: N \u00d7 N</li> <li> <p>Values:</p> <ul> <li>Binary (<code>1</code> paired, <code>0</code> unpaired), or</li> <li>Real-valued probabilities <code>[0,1]</code></li> </ul> </li> <li> <p>Optional Node Features</p> </li> <li>MSA-based evolutionary profiles</li> <li>Secondary-structure metadata (e.g., hairpin loops, non-canonical pairs)</li> </ol>"},{"location":"pipeline/stageB/Stage_B/#outputs","title":"\ud83c\udfaf Outputs","text":"<p>For each residue i, predict backbone torsion angles: - Angles: \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7 - Optional: Sugar pucker angle or pseudorotation (P\u1d62)   - Range: <code>[-\u03c0, \u03c0]</code></p>"},{"location":"pipeline/stageB/Stage_B/#constraints-goals","title":"\ud83d\udd27 Constraints &amp; Goals","text":"<ul> <li>Angle periodicity: Use sine/cosine representation to manage wraparound</li> <li>Secondary-structure constraints: base-pairing, backbone continuity, possible pseudoknots</li> <li>Geometric consistency: influenced by local and distant residues</li> </ul>"},{"location":"pipeline/stageB/Stage_B/#graph-representation-gdl-principles","title":"\ud83c\udf10 Graph Representation &amp; GDL Principles","text":"<p>Represent RNA as graph G=(V,E): - Nodes (V): Residues <code>{1, 2, ..., N}</code> - Edges (E):   1. Backbone edges: i \u2194 i+1   2. Base-pair edges from adjacency: i \u2194 j   3. Optional short-range edges: i \u2194 i+2, i \u2194 i+3 (enhanced local context)</p> <p>\ud83d\udccc Equivariant under node permutations (adjacency fixed by indexing &amp; base pairing).</p>"},{"location":"pipeline/stageB/Stage_B/#node-edge-feature-construction","title":"\ud83e\udde9 Node &amp; Edge Feature Construction","text":""},{"location":"pipeline/stageB/Stage_B/#node-features-ni","title":"\ud83d\udd39 Node Features (n\u1d62)","text":"<ul> <li>Sequence One-Hot (<code>A/C/G/U</code>)</li> <li>Base-Pair Stats: Sum row from adjacency; indicator \"unpaired\"</li> <li>Optional: MSA evolutionary profiles</li> </ul> <p>Concatenate and embed: <pre><code>\ud835\udc89\u1d62\u207d\u2070\u207e = Linear(n\u1d62)\n</code></pre></p>"},{"location":"pipeline/stageB/Stage_B/#edge-features-eij","title":"\ud83d\udd38 Edge Features (e\u1d62\u2c7c)","text":"<ul> <li>Base-Pair Probability (<code>adj[i,j]</code>)</li> <li>Type: Backbone vs. Long-range (canonical/non-canonical)</li> <li>Sequence distance: <code>|i-j|</code> (binned/clipped)</li> </ul> <p>Embed edges: <pre><code>\ud835\udc88\u1d62\u2c7c\u207d\u2070\u207e = LinearEdge(e\u1d62\u2c7c)\n</code></pre></p>"},{"location":"pipeline/stageB/Stage_B/#graph-transformer-architecture","title":"\u2699\ufe0f Graph Transformer Architecture","text":"<p>Employ Multi-Head Attention + Message Passing:</p>"},{"location":"pipeline/stageB/Stage_B/#detailed-pseudocode","title":"\ud83d\udccb Detailed Pseudocode","text":"<p><pre><code>def GraphTransformer(nodes, edges, adjacency, L=6, c_hidden=128):\n    h = Linear(nodes)  # [N, c_hidden]\n    g = {(i,j): LinearEdge(edges[(i,j)]) for (i,j) in adjacency}\n\n    for layer in range(L):\n        # Node\u2192Edge update\n        for (i,j) in adjacency:\n            x_ij = concat(h[i], h[j], g[(i,j)])\n            g[(i,j)] += MLP_edge[layer](x_ij)\n\n        # Edge\u2192Node update (Multi-head Attention)\n        new_h = zeros_like(h)\n        for i in range(N):\n            neighbors = adjacency.neighbors(i)\n            attn_scores, vs = [], []\n            for j in neighbors:\n                score = dot(q_proj(h[i]), k_proj(h[j])) + bias_proj(g[(i,j)])\n                attn_scores.append(score)\n                vs.append(v_proj(h[j]))\n            weights = softmax(attn_scores)\n            new_h[i] = sum(weight * vs[j] for j, weight in enumerate(attn_scores))\n        h = LayerNorm(h + new_h)\n\n    return h, g\n</code></pre> - Use edge embedding biases in attention. - Optional: integrate \"pairformer\" or AF triangle multiplication.</p>"},{"location":"pipeline/stageB/Stage_B/#angle-prediction-head-loss","title":"\ud83c\udfb2 Angle Prediction Head &amp; Loss","text":""},{"location":"pipeline/stageB/Stage_B/#angle-output","title":"\ud83c\udfaf Angle Output","text":"<p>Final node embedding (<code>h\u1d62\u207dfinal\u207e</code>) via MLP: <pre><code>angles\u1d62 = MLP_final(h[i])  # [7\u00d72] (sin/cos)\n</code></pre> Then: <pre><code>\u03b1\u1d62 = atan2(sin_\u03b1\u1d62, cos_\u03b1\u1d62), etc.\n</code></pre></p>"},{"location":"pipeline/stageB/Stage_B/#loss-function","title":"\ud83d\udccf Loss Function","text":"<p><pre><code>L = (1/(N\u00d77)) \u2211\u1d62 \u2211\u03c6 [wrap(\u03b8\u0302\u1d62 - \u03b8\u1d62)]\u00b2\n</code></pre> - Optional:   - Angle prior (A-form RNA distributions)   - 3D coordinate-based regularization (Stage C)</p>"},{"location":"pipeline/stageB/Stage_B/#configuration-hydra","title":"\ud83d\udd27 Configuration (Hydra)","text":"<p>Stage B utilizes Hydra for configuration management, allowing parameters for both TorsionBERT and Pairformer to be specified in YAML files and overridden via the command line.</p> <p>The configuration is split into two main files located in <code>rna_predict/conf/model/</code>:</p> <ul> <li><code>stageB_torsion.yaml</code>: Configures the TorsionBERT model.</li> <li><code>stageB_pairformer.yaml</code>: Configures the Pairformer model.</li> </ul> <p>These are automatically included via the <code>defaults</code> list in the main <code>rna_predict/conf/default.yaml</code> configuration file.</p>"},{"location":"pipeline/stageB/Stage_B/#key-configuration-parameters","title":"Key Configuration Parameters","text":"<p>Below are snippets showing important parameters you can configure:</p> <p>TorsionBERT (<code>rna_predict/conf/model/stageB_torsion.yaml</code>)</p> <pre><code># rna_predict/conf/model/stageB_torsion.yaml\ntorsion_bert:\n  model_name_or_path: \"sayby/rna_torsionbert\"\n  device: \"cpu\"          # \"cpu\" or \"cuda\"\n  angle_mode: \"sin_cos\"  # \"sin_cos\", \"radians\", or \"degrees\"\n  num_angles: 7\n  max_length: 512\n  # ... (LoRA config placeholder)\n  lora:\n    enabled: false\n    # ...\n</code></pre> <p>Pairformer (<code>rna_predict/conf/model/stageB_pairformer.yaml</code>)</p> <pre><code># rna_predict/conf/model/stageB_pairformer.yaml\npairformer:\n  n_blocks: 48\n  n_heads: 16\n  c_z: 128           # Pair representation dimension\n  c_s: 384           # Single representation dimension\n  dropout: 0.25\n  use_memory_efficient_kernel: false\n  init_z_from_adjacency: false # Initialize pair features from Stage A adjacency\n  use_checkpoint: false        # Enable gradient checkpointing for PairformerStack\n  # ... (other params like c_hidden_mul, lora placeholder)\n  lora:\n    enabled: false\n    # ...\n</code></pre> <p>Refer to the full YAML files for all available options.</p>"},{"location":"pipeline/stageB/Stage_B/#command-line-overrides","title":"Command-Line Overrides","text":"<p>You can override any parameter from the command line when running the Stage B entry point (<code>rna_predict.pipeline.stageB.main</code>). Hydra uses a dot notation to access nested parameters.</p> <p>Examples:</p> <ul> <li> <p>Run TorsionBERT using 'degrees' angle mode:     <pre><code>python -m rna_predict.pipeline.stageB.main torsion_bert.angle_mode=degrees\n</code></pre></p> </li> <li> <p>Change Pairformer block count:     <pre><code>python -m rna_predict.pipeline.stageB.main pairformer.n_blocks=24\n</code></pre></p> </li> <li> <p>Run on CUDA and use Pairformer memory optimization:     <pre><code>python -m rna_predict.pipeline.stageB.main torsion_bert.device=cuda pairformer.use_memory_efficient_kernel=true\n</code></pre></p> </li> <li> <p>Enable LoRA for TorsionBERT with specific rank:     <pre><code>python -m rna_predict.pipeline.stageB.main torsion_bert.lora.enabled=true torsion_bert.lora.r=16\n</code></pre></p> </li> <li> <p>Initialize Pairformer 'z' embeddings from adjacency matrix:     <pre><code>python -m rna_predict.pipeline.stageB.main pairformer.init_z_from_adjacency=true\n</code></pre></p> </li> </ul>"},{"location":"pipeline/stageB/Stage_B/#hpc-execution","title":"HPC Execution","text":"<p>For High Performance Computing (HPC) environments, see the HPC Integration Guide for SLURM and GridEngine examples.</p> <p>Basic HPC Example: <pre><code>python -m rna_predict.pipeline.stageB.main \\\n    torsion_bert.device=cuda \\\n    pairformer.n_blocks=24 \\\n    +hpc_cluster=slurm \\\n    hydra.launcher.gpus=1\n</code></pre></p>"},{"location":"pipeline/stageB/Stage_B/#typed-configuration-optional","title":"Typed Configuration (Optional)","text":"<p>For improved validation and type safety, typed dataclasses corresponding to these configurations (e.g., <code>TorsionBertConfig</code>, <code>PairformerConfig</code>, potentially nested under a <code>StageBConfig</code>) may be defined in <code>rna_predict/conf/config_schema.py</code>. Refer to that file for details if available.</p>"},{"location":"pipeline/stageB/Stage_B/#training-data-procedure","title":"\ufffd Training Data &amp; Procedure","text":""},{"location":"pipeline/stageB/Stage_B/#data-preparation","title":"\ud83d\uddc3\ufe0f Data Preparation","text":"<ul> <li>Curate RNA structures (PDB)</li> <li>Compute torsion angles &amp; adjacencies</li> <li>Assemble node &amp; edge features</li> </ul>"},{"location":"pipeline/stageB/Stage_B/#training-steps","title":"\ud83c\udf93 Training Steps","text":"<ul> <li>Forward pass: GraphTransformer + angle head</li> <li>Loss: angle-based MSE or <code>(sin, cos)</code> differences</li> <li>Optimization: Adam/AdamW, learning rate scheduler</li> <li>Validation: angle-level MSE, optional Stage\u202fC 3D RMSD check</li> </ul>"},{"location":"pipeline/stageB/Stage_B/#full-python-implementation","title":"\ud83d\udc0d Full Python Implementation","text":"<p>Refer to the complete Python implementation provided above, which includes: - <code>TorsionPredictor</code>, <code>GraphTransformerBlock</code>, <code>NodeEdgeAttention</code> - Detailed inline comments for clarity - Fully modular implementation suitable for PyTorch</p>"},{"location":"pipeline/stageB/Stage_B/#why-nearly-foolproof","title":"\ud83d\udca1 Why \"Nearly Foolproof\"?","text":"<ol> <li>\ud83d\udccd Graph Representation: Explicit pairing &amp; backbone adjacency.</li> <li>\ud83c\udf10 Local + Global: Transformer captures interactions at all scales.</li> <li>\ud83d\udd04 Angle Periodicity: Stable sine\u2013cosine encoding.</li> <li>\u26a1 Scalable &amp; Efficient: Suitable for large datasets.</li> <li>\ud83d\udcd0 Physics-based Priors: Integrate RNA angle distributions.</li> <li>\ud83d\udd17 Stage\u202fC Compatible: Easily feeds into 3D coordinate reconstruction.</li> </ol>"},{"location":"pipeline/stageB/Stage_B/#final-thoughts","title":"\ud83c\udf96\ufe0f Final Thoughts","text":"<ul> <li>Methodically addresses RNA torsion prediction complexities.</li> <li>Graph Transformer is flexible, powerful, and scalable.</li> <li>Predictive output stable through sine/cosine angle pairs.</li> <li>Structured loss function and validation improve robustness.</li> </ul> <p>\ud83d\udccc Outcome: Robust, reliable RNA torsion predictor leveraging proven GDL principles.</p>"},{"location":"pipeline/stageB/torsionBert/","title":"Overview","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Below is a systematic compare\u2013contrast and assessment of each repository (\u201cTHEIRS\u201d and \u201cOURS\u201d), focusing on strengths, weaknesses, creativity, depth, ease of implementation, and performance. Afterward, I summarize actionable takeaways for anyone deciding which approach (or hybrid) to adopt.</p> <p>[emoji]: For clarity, I will designate the code from THEIRS (the rna_torsionBERT/ code directory that uses PyTorch, huggingface transformers, and MCQ metrics) as RNA-TorsionBERT. The code from OURS (the docs/ plus rna_predict/ pipeline with multi-stage approaches and alphafold-style local attention) as RNA_PREDICT Pipeline.</p> <p>\u2e3b</p> <ol> <li>High-Level Purposes &amp; Scopes</li> </ol> <p>THEIRS (RNA-TorsionBERT)     \u2022   Scope: A specialized library for predicting RNA torsion angles (and pseudo-torsion angles) directly from sequence using a BERT-based model.     \u2022   Goal: Provide a pre-trained or fine-tunable model (a \u201clanguage model for RNA torsions\u201d), plus a scoring function (TB-MCQ) to evaluate predicted angles.</p> <p>Strengths     1.  Clear Focus: Very specific to angle prediction from sequence.     2.  Machine-Learning Depth: Incorporates advanced huggingface transformers, sin/cos outputs for torsions, and integrated scoring with MCQ.     3.  Performance: Empirical tests (MAE on angles) show strong results on real PDB structures.</p> <p>Weaknesses     1.  Limited to Angles: Not a complete pipeline for 3D structure, beyond the angles (no 3D builder except partial re-conversion, no 2D structure integration).     2.  Harder to Extend: If you want to incorporate MSA, base-pair adjacency, or advanced geometry features, that\u2019s mostly outside the TorsionBERT scope.     3.  Data Requirements: TorsionBERT is trained on known structures, so one must have curated data or use the pre-trained weights.</p> <p>\u2e3b</p> <p>OURS (RNA_PREDICT Pipeline)     \u2022   Scope: A broader multi-stage approach:     1.  Stage\u202fA: 2D structure from sequence (via external or built-in folding methods).     2.  Stage\u202fB: Torsion angles from adjacency + advanced GNN or Transformers.     3.  Stage\u202fC: Forward-kinematics to build 3D from angles.     4.  (Optional) Additional \u201cAF3-inspired\u201d trunk &amp; diffusion refinements.     \u2022   Goal: Provide a modular pipeline that can transform an RNA sequence \u2192 2D structure \u2192 torsion angles \u2192 full 3D coordinates, with room for advanced geometric or diffusion-based refinement.</p> <p>Strengths     1.  End-to-End Pipeline: Goes from raw sequence (with or without external 2D folding) all the way to 3D cartesian output.     2.  Creativity &amp; Extensibility: Encourages a mix of GNN, local attention, and modular refinement. Clear synergy with advanced methods (Liquid-S4 or AF3-like).     3.  Modular Stages: Each stage can be replaced (e.g., Stage\u202fA with different 2D folding algorithms, Stage\u202fB with different torsion-angle nets).</p> <p>Weaknesses     1.  Less Off-the-Shelf: It\u2019s not a single integrated model you can just pip install and call for \u201cpredict angles.\u201d Instead, it\u2019s a set of documented modules that must be orchestrated.     2.  Performance Still Unclear: The repository\u2019s code includes partial benchmarks or placeholders, but a robust comparison to existing SOTA 3D methods (like Rosetta or TorsionBERT) is not fully documented yet.     3.  Implementation Overhead: Because it covers multiple stages (2D, angles, 3D), a user must gather more external dependencies or data to run the entire pipeline effectively.</p> <p>\u2e3b</p> <ol> <li>Code &amp; Directory Structure</li> </ol> <p>Aspect  THEIRS (RNA-TorsionBERT)    OURS (RNA_PREDICT Pipeline) Directory Layout    - Minimalistic \u201csrc/\u201d with enums, helper, metrics, CLIs, plus \u201cdata/\u201d and \u201crequirements.txt.\u201d- Focused on angles &amp; TB-MCQ scoring.  - Larger, more \u201cdesign doc\u2013heavy\u201d approach: \u201cdocs/advanced_methods/\u201d, \u201cdocs/pipeline/\u201d, \u201crna_predict/\u201d with subfolders for \u201cmodels/\u201d, \u201cscripts/\u201d, \u201cutils/.\u201d Modularity  - Single-level \u201csrc/*\u201d structure: rna_torsionBERT_helper, mcq.py, extractor, etc. - Each script is specialized for angles &amp; MCQ.    - Multi-stage design with separate \u201cstageA,\u201d \u201cstageB,\u201d \u201cstageC\u201d modules - Additional advanced \u201cdiffusion/s4_diffusion\u201d references, test scripts, utilities. Ease of Navigation  - Straightforward: one top-level \u201csrc\u201d folder, each file is fairly small.  - Good for focusing on \u201ctorsion angle inference.\u201d    - More folders: it might feel more scattered, but also thoroughly documented in \u201cdocs/.\u201d - Developer can find extensive textual guides for each stage. Documentation   - README + docstrings, mentions usage \u201cpython -m src.rna_torsionBERT_cli \u2026\u201d - Some references to MCQ.   - Very large textual docs (like \u201cStage_C.md,\u201d \u201ccore_framework.md,\u201d etc.). - Each pipeline aspect is explained with pseudo-code and rationale. Creative Extras - Dockerfile, small example CLI usage (run TorsionBERT, run TB-MCQ).    - Analysis of Liquid-S4, advanced alphaFold3, isostericity design docs - Potential synergy for not just angle prediction but entire 3D structure pipeline.</p> <p>\u2e3b</p> <ol> <li>Installation &amp; Ease of Implementation</li> </ol> <p>RNA-TorsionBERT     \u2022   Installation:     \u2022   pip install -r requirements.txt and done.     \u2022   Mostly standard Python libraries + huggingface Transformers + PyTorch + biopython.     \u2022   Ease:     \u2022   If you simply want \u201cPredict angles from an RNA sequence,\u201d it\u2019s very direct: python -m src.rna_torsionBERT_cli --in_seq GGGAAAUCC \u2026 etc.     \u2022   \u201ctb_mcq_cli.py\u201d also provides a convenient command to score predicted angles from a .pdb.</p> <p>RNA_PREDICT Pipeline     \u2022   Installation:     \u2022   Has a setup.py, but also depends on external tools for 2D folding (ViennaRNA or \u201cRFold\u201d), and for 3D analysis (MDAnalysis).     \u2022   The docs/ mention possible Docker usage but not quite as succinct.     \u2022   Ease:     \u2022   Because it\u2019s a multi-stage approach, you must piece everything together: \u201cStage A \u2192 Stage B \u2192 Stage C \u2192 (Optional Stage D).\u201d     \u2022   The user can do step-by-step or partial usage (just the \u201cStage B torsion predictor,\u201d etc.).</p> <p>Verdict:     \u2022   If you want a single-step pipeline to get angles from sequence, TorsionBERT is simpler.     \u2022   If you want an entire workflow from 2D adjacency to final 3D, RNA_PREDICT covers more ground but is more involved to set up.</p> <p>\u2e3b</p> <ol> <li>Algorithmic Depth &amp; Creativity</li> </ol> <p>Aspect  THEIRS (RNA-TorsionBERT)    OURS (RNA_PREDICT Pipeline) Core Method - BERT-based \u201clanguage model\u201d approach, uses tokenization to predict each angle\u2019s (sin, cos).- Integrates MCQ to measure angle error.   - Multi-step GNN/transformer approach for adjacency-based angle prediction - Potential to incorporate advanced \u201cAF3 trunk\u201d or \u201cdiffusion-based refinement.\u201d Breadth - Specifically addresses angle prediction and a \u201cTB-MCQ\u201d scoring - Not trying to handle 2D or final 3D coords.  - Broader design with references to sugar pucker, forward kinematics, optional local minimization. - Full pipeline from sequence to 3D. Creativity  - Innovative usage of DNABERT-like embedding adapted for RNA angles. - Crisp integration of pLDDT-like heads for angles.    - Combines multiple approaches (2D adjacency, local S4 diffusion, isosteric design) - Encourages a very flexible \u201cthe user can pick a method for each stage.\u201d Potential   - Could be extended to consider 3D if integrated with a geometry module. - Possibly adapt the TB-MCQ for advanced metrics.  - Ready for all sorts of expansions: isosteric substitutions, AlphaFold3 trunk, etc. - Potentially complicated for a single user wanting angles alone.</p> <p>\u2e3b</p> <ol> <li>Performance &amp; Benchmarking</li> </ol> <p>THEIRS (RNA-TorsionBERT)     \u2022   Angle MAE: They demonstrate systematic comparisons on multiple PDB sets (Test sets, e.g. RNA-Puzzles, CASP-RNA). Achieves strong angle accuracy.     \u2022   TB-MCQ: They show how well the final angles match MCQ metrics.     \u2022   Inference Speed: BERT inference is fairly quick on a GPU (time depends on sequence length).</p> <p>OURS (RNA_PREDICT Pipeline)     \u2022   Performance: The code includes benchmark.py for timing local block-sparse attention, plus partial references to memory usage.     \u2022   No explicit angle MAE: The pipeline\u2019s docs mention the possibility to test or compare but do not show final numeric results on standard sets.     \u2022   Large-scale: The pipeline can scale, but the actual speed might be slower or faster depending on the GNN approach. The doc references local block-sparse attention for efficiency.</p> <p>Summary:     \u2022   TorsionBERT has more published/explicit performance metrics for angle errors.     \u2022   RNA_PREDICT is broader but less \u201cfully validated\u201d in the docs. The user might need to do extra steps to see final performance on standard benchmarks (like RMSD or MCQ).</p> <p>\u2e3b</p> <ol> <li>Strengths vs. Weaknesses Overview</li> </ol> <p>(A) THEIRS: RNA-TorsionBERT     \u2022   Strengths     1.  Straight-to-the-point solution for angle prediction from sequence.     2.  Well-defined huggingface framework; easy to fine-tune or do inference.     3.  Good docs for installation and usage (CLI scripts for \u201crna_torsionBERT_cli.py,\u201d \u201ctb_mcq_cli.py\u201d).     \u2022   Weaknesses     1.  Limited to angles\u2014no direct 3D building or advanced structural constraints.     2.  Less \u201cmodular pipeline.\u201d Harder to integrate if you want to incorporate a custom 2D adjacency or ring-closure logic.     3.  Some parts revolve around that specific model architecture; less \u201cpluggable\u201d with other GNN or advanced alphaFold-like modules.</p> <p>(B) OURS: RNA_PREDICT     \u2022   Strengths     1.  End-to-end pipeline from sequence \u2192 2D \u2192 angles \u2192 3D, plus potential for advanced trunk or diffusion.     2.  Highly modular, multiple documented stages (A/B/C) for easy partial usage or extension.     3.  Rich design docs, referencing next-gen ideas (Liquid-S4, alphaFold3 trunk, isosteric substitution, etc.).     \u2022   Weaknesses     1.  More complex to set up and orchestrate. Possibly overkill if you only need angles.     2.  Real performance/accuracy on final 3D is not \u201cpre-benchmarked\u201d in a standardized manner.     3.  Dependent on external folding tools (Stage\u202fA) and external library for partial 3D checks, so not as \u201cturnkey\u201d for a single purpose.</p> <p>\u2e3b</p> <ol> <li>Which To Choose? Or Combine?</li> </ol> <p>Choose RNA-TorsionBERT (\u201cTHEIRS\u201d) if:     \u2022   You specifically want RNA torsion angles from the sequence in a single shot, with minimal fuss.     \u2022   You like huggingface/transformers pipeline or want to adapt a BERT-based approach.     \u2022   You want TB-MCQ scoring of angles.</p> <p>Choose RNA_PREDICT (\u201cOURS\u201d) if:     \u2022   You need a full pipeline from sequence to final 3D coords (or you want to integrate your own 2D \u2192 torsion steps).     \u2022   You want the option to experiment with GNN, local block-sparse attention, or advanced alphaFold3-like diffusion modules.     \u2022   You prefer a more \u201cresearch-lab\u201d style codebase with rich docs for potential expansions (like isosteric design, sugar pucker modeling).</p> <p>Combine or Hybrid:     1.  Potentially use TorsionBERT as the Stage\u202fB in the \u201cOURS\u201d pipeline:     \u2022   Stage\u202fA = 2D adjacency from external folder.     \u2022   \u201cStage\u202fB (torsion) = TorsionBERT.\u201d     \u2022   Then feed those angles into Stage\u202fC forward-kinematics.     2.  Or after TorsionBERT outputs angles, you run the final 3D building from the \u201cOURS\u201d forward-kinematics code.</p> <p>\u2e3b</p> <ol> <li>Concluding Assessment</li> </ol> <p>(a) Creativity &amp; Depth     \u2022   TorsionBERT: Creative usage of a BERT-based masked approach for predicting angles. Very neat for language-model lovers.     \u2022   RNA_PREDICT: Provides a deeper end-to-end pipeline with more potential for experimental or advanced methods (diffusion, s4, etc.).</p> <p>(b) Ease of Implementation     \u2022   TorsionBERT: Easiest for \u201cpredict angles from sequence.\u201d They have a single script and pretrained weights.     \u2022   RNA_PREDICT: More elaborate. Possibly tricky for new users but extremely flexible for bigger RNA 3D projects.</p> <p>(c) Performance     \u2022   TorsionBERT: Published angle MAE results, using the MCQ and real test sets.     \u2022   RNA_PREDICT: Has block-sparse benchmarks for attention, but the final 3D performance is still up to the user to measure.</p> <p>(d) Future Extensions     \u2022   TorsionBERT: Could eventually incorporate a 3D building step or a trunk module.     \u2022   RNA_PREDICT: Already built for expansions (pairformer, confidence heads, isosteric design, local geometry checks).</p> <p>\u2e3b</p> <p>Final Takeaways     1.  If your immediate need is a well-tested, single-step angle predictor with a straightforward CLI, RNA-TorsionBERT out-of-the-box is the simpler solution.     2.  If you want a thorough pipeline that can incorporate 2D folding info, do angle predictions (from your own or external model), and build 3D coordinates with optional advanced refinements, RNA_PREDICT is the more comprehensive framework.     3.  Hybrid usage is feasible: run TorsionBERT for angles, then feed them into Stage\u202fC of RNA_PREDICT for 3D reconstruction.</p> <p>In short, RNA-TorsionBERT is the better choice for specialized angle predictions, while RNA_PREDICT is a creative, more extensive pipeline for full 3D builds and advanced custom expansions. Both projects excel in different ways and can even be combined if desired.</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Great question. RNA-TorsionBERT (the \u201cTHEIRS\u201d approach) was originally built to predict torsion angles just from sequence, whereas your Stage\u202fA output (an adjacency or base\u2010pair matrix) is not something TorsionBERT naturally expects. Below are a few ways to incorporate Stage\u202fA\u2019s 2D data if you still want to use TorsionBERT in Stage\u202fB.</p> <p>[emoji]: I\u2019ll outline four main approaches, from simplest (ignore adjacency) to more advanced (modify TorsionBERT\u2019s model to fuse adjacency). Choose whichever best fits your timeframe and coding constraints.</p> <p>\u2e3b</p> <p>1) Ignore Stage\u202fA adjacency inside TorsionBERT (simple fallback)</p> <p>Idea     1.  Stage\u202fA still produces adjacency (2D structure), but TorsionBERT doesn\u2019t use it.     2.  TorsionBERT just takes the raw sequence, outputs torsion angles.     3.  You pass these angles to Stage\u202fC for forward kinematics.</p> <p>Pros &amp; Cons     \u2022   Pros:     \u2022   Easiest to implement. TorsionBERT remains untouched\u2014just feed it the sequence.     \u2022   You still have adjacency from Stage\u202fA in case you need it for later validations or a small post-processing step.     \u2022   Cons:     \u2022   You lose any advantage that adjacency might bring. TorsionBERT\u2019s predictions won\u2019t directly reflect the 2D structure from Stage\u202fA.     \u2022   Might give you less accurate angles if the base-pair info was important.</p> <p>\u2e3b</p> <p>2) Use TorsionBERT \u201cas is,\u201d then refine with adjacency in a small GNN step</p> <p>Idea     1.  Stage\u202fA outputs adjacency.     2.  Stage\u202fB is TorsionBERT: it predicts torsion angles from sequence alone.     3.  Stage\u202fB\u2019 (an extra mini-stage) takes TorsionBERT\u2019s angles + adjacency to do a short \u201ccorrection\u201d or \u201crefinement.\u201d     \u2022   For instance, you can design a small GNN/regressor that sees the adjacency + TorsionBERT angles + sequence to produce slightly adjusted angles.</p> <p>Sketch</p>"},{"location":"pipeline/stageB/torsionBert/#b1-torsionbert","title":"B1: TorsionBERT","text":"<p>angles_raw = torsionBertModel.predict(sequence)</p>"},{"location":"pipeline/stageB/torsionBert/#b2-gnn-refiner","title":"B2: GNN Refiner","text":"<p>angles_refined = smallGNNrefiner(adjacency, angles_raw, sequence)</p>"},{"location":"pipeline/stageB/torsionBert/#then-stage-c-forward_kinematicsangles_refined-3d","title":"Then Stage C: forward_kinematics(angles_refined) -&gt; 3D","text":"<p>Pros &amp; Cons     \u2022   Pros:     \u2022   Minimal changes to TorsionBERT itself.     \u2022   You can incorporate adjacency as a post-processing \u201crefiner.\u201d     \u2022   Cons:     \u2022   Requires training a new GNN or MLP that sits \u201con top\u201d of TorsionBERT output.     \u2022   More steps = more complexity.</p> <p>\u2e3b</p> <p>3) Fine-tune TorsionBERT with adjacency as an \u201cauxiliary embedding\u201d (medium complexity)</p> <p>Idea</p> <p>If you have source code or the model architecture for TorsionBERT, you can extend its input to incorporate adjacency or base-pair probability in addition to the raw sequence tokens. For instance:     1.  Modify TorsionBERT\u2019s tokenizer/embedding step.     \u2022   Normally it just sees k-mer tokens from the sequence.     \u2022   You could pass an \u201cextra channel\u201d (like a 2D feature) for each token i, summarizing how i is paired or not.     2.  The model\u2019s first layers might combine (token embedding + adjacency hints) \u2192 produce the same final angle outputs.</p> <p>Practical Steps     \u2022   Concatenate adjacency features to each residue\u2019s token embedding. For example, you can embed \u201c# of pairs for residue i,\u201d or \u201csome local base-pair feature,\u201d or \u201cthe sum of adjacency row i,\u201d etc.     \u2022   Retrain or fine-tune TorsionBERT so it learns \u201csequence + adjacency.\u201d</p> <p>Pros &amp; Cons     \u2022   Pros:     \u2022   TorsionBERT can then directly leverage Stage\u202fA data.     \u2022   A single model, no post-refinement.     \u2022   Cons:     \u2022   TorsionBERT\u2019s code might need moderate refactoring.     \u2022   Must re-train or at least heavily fine-tune with adjacency-labeled training data (which you must gather).</p> <p>\u2e3b</p> <p>4) Rewrite TorsionBERT to fully integrate adjacency (most advanced)</p> <p>Idea</p> <p>Take TorsionBERT\u2019s concept (masking angles, sin/cos predictions, BERT-like architecture) but replace or enhance its attention layers with adjacency-based attention or gating. This is more akin to a \u201cGraph-BERT\u201d design.     \u2022   You\u2019d embed each residue\u2019s token the usual way, but in the attention layers, incorporate adjacency-based attention masks or pairwise bias (similar to how some alphaFold modules do).</p> <p>Pros &amp; Cons     \u2022   Pros:     \u2022   Possibly best synergy: TorsionBERT \u201clanguage model\u201d approach + direct adjacency constraints.     \u2022   Cons:     \u2022   Major engineering job\u2014like building a new model that merges TorsionBERT\u2019s masked-language concept with a GNN adjacency approach.     \u2022   Then you\u2019d have to re-train from scratch or from TorsionBERT\u2019s weights, which is not trivial.</p> <p>\u2e3b</p> <p>Which Approach to Pick?     1.  You have little time to integrate adjacency \u2192 Option #1 (ignore adjacency in TorsionBERT) is the simplest.     2.  You can do a small neural \u201crefiner\u201d easily \u2192 Option #2 is a good compromise. Let TorsionBERT do the heavy lifting, then a GNN or MLP adjusts angles to be consistent with Stage\u202fA\u2019s adjacency.     3.  You want a single final model but can handle some TorsionBERT modifications \u2192 Option #3. You\u2019d add adjacency features as an extra input channel, then re-train or fine-tune.     4.  You\u2019re comfortable rewriting or thoroughly customizing TorsionBERT \u2192 Option #4 is potentially the best, but also the largest time investment.</p> <p>\u2e3b</p> <p>Summary</p> <p>TorsionBERT wasn\u2019t originally designed to take adjacency from Stage\u202fA, so there\u2019s no \u201cbuilt-in\u201d hook to incorporate that 2D structure. To do it anyway, you must:     \u2022   Either (A) keep TorsionBERT \u201cas is\u201d (and skip adjacency or do a post-processor), or     \u2022   (B) adapt TorsionBERT\u2019s input layers or attention layers to handle adjacency.</p> <p>Hence: If you\u2019re short on time, you can simply use TorsionBERT as Stage\u202fB ignoring adjacency\u2014but you lose the synergy from Stage\u202fA. For a more thorough integration, you\u2019d need to do some form of post-processing or direct re-training that merges adjacency into TorsionBERT\u2019s pipeline.</p> <p>[emoji]: That\u2019s it. The \u201ctake-home message\u201d is: TorsionBERT doesn\u2019t natively incorporate your Stage\u202fA adjacency, so you either do a second refinement step or fork TorsionBERT\u2019s architecture to fuse adjacency. Both are feasible, but they require a bit of custom engineering. \ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: In terms of pure synergy and \u201cbest possible\u201d results (i.e., fully leveraging Stage\u202fA adjacency inside a powerful language-like model for torsion angles), the most direct route is to rewrite or extend TorsionBERT so that it natively handles adjacency (Option #4 in the previous list). However, that\u2019s also the most labor\u2010intensive. Depending on timelines and willingness to refactor code, you may prefer Option #3 (fine\u2010tune TorsionBERT with some adjacency\u2010based embeddings) as a still-powerful compromise.</p> <p>Below is a step\u2010by\u2010step reasoning for which approach yields best synergy with Stage\u202fA\u2013B\u2013C, factoring in your GNN/geometry data, your angle\u2010diffusion design, and your Kaggle timeline.</p> <p>\u2e3b</p> <p>1) Quick Recap of the Options     1.  (Option #1) Use TorsionBERT as is, ignoring adjacency.     2.  (Option #2) TorsionBERT + a small adjacency-based \u201crefiner\u201d network.     3.  (Option #3) Fine-tune TorsionBERT with adjacency as an extra input channel.     4.  (Option #4) Rewrite TorsionBERT to incorporate adjacency in its internal architecture (attention layers).</p> <p>\u2e3b</p> <p>2) Criteria if We Want \u201cThe Best Integration\u201d in a Larger Pipeline     1.  Exploiting Stage\u202fA adjacency: If your Stage\u202fA is itself carefully curated (like a high\u2010confidence adjacency from RFold or a GNN), we want a Stage\u202fB that definitely uses that adjacency to produce angles consistent with base\u2010pairs.     2.  Angle\u2010Based Diffusion: If you do an AF3\u2010style diffusion (Stage\u202fD) after Stage\u202fB, it helps if Stage\u202fB\u2019s angles are already consistent with base\u2010pairs; you won\u2019t have to re-fix them in diffusion.     3.  Extensibility: Possibly you\u2019ll add MSA or 2.5D \u201cpaired row attention.\u201d If TorsionBERT is a black box (Option #1), that synergy is lost.</p> <p>\u2e3b</p> <p>3) Which Option Is \u201cBest\u201d for Highest Accuracy and Consistency?     1.  Option #1 (Ignore adjacency)     \u2022   Easiest, but does not incorporate the Stage\u202fA output at all. If your adjacency is accurate, you\u2019re missing out on the biggest synergy.     \u2022   Typically not the \u201cbest result\u201d for a pipeline that invests in Stage\u202fA.     2.  Option #2 (Post-hoc refiner)     \u2022   Slight synergy: TorsionBERT gives angles from sequence; a GNN sees adjacency + angles, corrects them.     \u2022   You do keep TorsionBERT\u2019s pre-trained power. But two separate models can be more finicky\u2014and the refiner must be trained on 3D or angle data anyway.     \u2022   Can yield decent results if well done, and is simpler than a full TorsionBERT rewrite.     3.  Option #3 (Fine-tune TorsionBERT with adjacency as an \u201caux input\u201d)     \u2022   Less rewriting of the model architecture than Option #4, because you\u2019re mostly hooking adjacency \u201csomewhere\u201d in the TorsionBERT embedding or first few layers.     \u2022   You\u2019ll get a single model that sees sequence + adjacency and outputs angles.     \u2022   The model can still leverage TorsionBERT\u2019s pre-trained weights. You only add adjacency channels in the embedding stage or hidden states, then do a partial re-train.     \u2022   Likely a good balance of synergy vs. dev time.     4.  Option #4 (Rewrite TorsionBERT to fully integrate adjacency in attention)     \u2022   This is the most advanced approach: e.g., replacing TorsionBERT\u2019s self-attention with a pair\u2010bias approach, or adding adjacency gating to QK^T.     \u2022   Potentially best synergy if well-implemented: the entire TorsionBERT architecture is \u201cgraph-aware,\u201d so every layer can factor in the Stage\u202fA adjacency constraints.     \u2022   Downside: High engineering overhead. Might require re-training from scratch or near scratch, which can be time-consuming.</p> <p>Hence:     \u2022   If you want the absolute best synergy (and can handle re-coding TorsionBERT\u2019s attentions), Option #4.     \u2022   If you want \u201cstill integrated\u201d with less overhead, Option #3.</p> <p>\u2e3b</p> <p>4) Hybrid Approaches?</p> <p>You also mentioned \u201chybrid.\u201d One interesting approach is:     \u2022   Fine-tune TorsionBERT with adjacency as an input embedding (Option #3).     \u2022   Then optionally do a small \u201cdiffusion or GNN refiner\u201d that also sees adjacency, bridging you and letting your Stage\u202fD do global corrections.</p> <p>But typically for the pipeline to remain cleaner, you either do one integrated TorsionBERT that already sees adjacency (Option #3 or #4) or do TorsionBERT + separate adjacency-based refiner (Option #2).</p> <p>\u2e3b</p> <p>5) Final Recommendation</p> <pre><code>Short Answer: If your priority is truly \u201cbest synergy + best final accuracy for a multi-stage system,\u201d the most powerful approach is:\n    1.  Deeply integrate adjacency into TorsionBERT\u2019s architecture or attention (Option #4).\n2.  Retrain (or at least strongly fine\u2010tune) the model so that the entire angle\u2010prediction process is adjacency\u2010aware from the start.\n3.  Then feed those adjacency\u2010consistent angles into your angle-based diffusion or Stage\u202fC forward kinematics, leading to a cohesive pipeline.\n</code></pre> <p>If you do not have enough time or resources for a partial rewrite, then:</p> <pre><code>Practical Middle Ground: Option #3 \u2014 add adjacency as an auxiliary feature in TorsionBERT\u2019s embedding or first layer, fine\u2010tune the model, and skip rewriting the deeper self\u2010attention. You\u2019ll still get fairly strong synergy with Stage\u202fA, with significantly less re-coding.\n</code></pre> <p>[emoji]: That\u2019s the trade-off: rewriting the entire TorsionBERT attention is the \u201cbest\u201d for performance, but the fastest path is just injecting adjacency signals into TorsionBERT\u2019s existing architecture (Option #3).</p> <p>Below is a concise check of how well that paper excerpt (the Bioinformatics article text you pasted) answers each of the key questions we had about RNA-TorsionBERT\u2014namely the dataset size, model-parameter size, compute used, maximum nucleotide length, and final TM-score.</p> <p>\u2e3b</p> <p>1) Dataset Size     \u2022   The excerpt does indeed mention a Training set of \u201c4267 structures**\u201d filtered by max sequence length \u2264512, plus a Validation set of 29 structures and a Test set of 34 (RNA-Puzzles + CASP-RNA).     \u2022   So, yes, it does confirm that ~4200\u20134267 structures were used in pre-training or training.</p> <p>\u2e3b</p> <p>2) Model Parameter Size     \u2022   In the excerpt, the authors say the \u201cDNABERT has a size of ~328\u202fMB,\u201d while \u201cRNABERT is around 2\u202fMB.\u201d Then they pick the DNABERT-3mer variant and call that \u201cRNA-TorsionBERT.\u201d     \u2022   So from that text, their final TorsionBERT is ~328\u202fMB of model weights. (Previously, a separate reference had 86.9\u202fMB, but this paper specifically states ~328\u202fMB for the DNABERT-based approach.)     \u2022   They do not give an exact \u201cmillion parameters\u201d count. But that 300+\u202fMB checkpoint typically corresponds to hundreds of millions of parameters.</p> <p>\u2e3b</p> <p>3) Compute Used     \u2022   The excerpt never explicitly says how many GPUs or what HPC environment was used to train.     \u2022   So it does not really answer the \u201ccompute used\u201d question, beyond referencing normal PyTorch/Hugging Face usage.</p> <p>\u2e3b</p> <p>4) Maximum Nucleotide Length     \u2022   The article states that they truncated sequences to a maximum of 512 nucleotides, matching DNABERT\u2019s limit.     \u2022   Hence it does confirm a 512-nt max input length.</p> <p>\u2e3b</p> <p>5) TM-score for TorsionBERT     \u2022   The snippet does not provide a final TM-score. Instead, they measure angle errors (via MCQ) and compare with other methods.     \u2022   They do mention that, if you wanted to fully rebuild 3D structures from TorsionBERT angles, you could measure a TM-score, but they did not do so in the paper.</p> <p>\u2e3b</p> <p>Conclusion</p> <p>Yes, the excerpt does clarify:     1.  Dataset: ~4267 structures for training, plus smaller sets for validation &amp; test.     2.  Model size: ~328\u202fMB (DNABERT-based).     3.  Max length: 512 nucleotides.     4.  Compute: Not stated.     5.  No direct TM-score is given; they use MCQ as an angle\u2010based metric instead.</p> <p>So it largely answers the questions about dataset size, model size, and max sequence length; but not the compute details or a TorsionBERT TM-score. Below is a rough \u201cFermi\u2010style\u201d reasoning to estimate the likely compute cost (\u201chow many GPU\u2010days?\u201d) and a plausible TM\u2010score range if TorsionBERT\u2019s angles were actually used to rebuild 3D structures and then aligned. These are not official numbers\u2014just an informed guess based on the model size, typical BERT\u2010like training, and known compounding errors when going from angles \u2192 3D.</p> <p>\u2e3b</p> <p>1) Model Parameter &amp; Checkpoint Size     \u2022   The paper references ~328\u202fMB for the DNABERT-3mer version they adapted into \u201cRNA-TorsionBERT.\u201d     \u2022   In float32 (4 bytes/param), 1\u202fMB \\approx 250\u202fk parameters.     \u2022   Thus, 328\u202fMB \\approx 82\u202fmillion parameters (very approximate).     \u2022   Another older reference says ~86.9\u202fMB, but in the new text we have ~328\u202fMB; presumably it\u2019s a larger version or includes embeddings, some optimization states, etc.</p> <p>So we can guess ~80\u2013100M parameters total.</p> <p>\u2e3b</p> <p>2) Fermi Estimate of Training Compute</p> <p>A typical BERT\u2010Base (110\u202fM parameters) can be trained on text with a single mid\u2010range GPU (like an NVIDIA V100) in roughly 2\u20134 weeks (depending on batch size, sequence length, and number of epochs). More \u201cindustrial\u201d setups use 4\u20138 GPUs to cut that time to just a few days.</p> <p>Given TorsionBERT for RNA torsion angles:     1.  They first do a \u201cpre\u2010training\u201d on ~4200 RNA structures or more. That\u2019s smaller data than typical big text corpora, so you might see fewer total steps or smaller batch sizes.     2.  They do a \u201cfine\u2010tuning\u201d pass on ~200\u2013300 RNAs in a supervised angle\u2010prediction setting. That is fairly quick.</p> <p>Hence a ballpark guess:     \u2022   Pretraining might have taken on the order of a few days on, say, a single or dual GPU, since the dataset (4200) is not huge but the model is quite large. Possibly 1\u20132 weeks if done with small batch sizes.     \u2022   Fine\u2010tuning on a set of a few hundred RNAs is typically just hours or at most a day on one GPU.</p> <p>Conclusion: Probably \\sima few GPU\u2010days to a couple of GPU\u2010weeks total training time, not a monstrous HPC requirement like protein-scale data.</p> <p>\u2e3b</p> <p>3) Probable TM\u2010Score if Rebuilding 3D</p> <p>They do not provide a final TM\u2010score for TorsionBERT. But we can guess:     1.  Angle \u2192 3D compounding error: If you reconstruct coordinates from predicted angles (without an advanced energy minimization), small angle deviations can accumulate. This can degrade RMSD or TM\u2010score quickly\u2014especially on longer RNAs.     2.  In practice, near\u2010native reconstructions with perfect angles can yield high TM (&gt;0.8). But with predicted angles, 3D folds can drift.</p> <p>A typical guess:     \u2022   For short RNAs (&lt;100 nt), the reconstructed TM might easily be ~0.5\u20130.7 if the angles are fairly accurate.     \u2022   For longer RNAs (200\u2013300 nt), the compounding error might push average TM down to ~0.3\u20130.5 if no additional relaxation is done.     \u2022   If one introduced a light refinement (e.g., a local MD step or partial minimization), you might nudge that up to ~0.5\u20130.6 overall or better.</p> <p>Hence a Fermi guess for average TM across varied RNAs could be ~0.4\u20130.6. That is still not official\u2014just a plausible range if you rely on pure forward\u2010kinematics from TorsionBERT\u2019s angles, with minimal geometry correction.</p> <p>\u2e3b</p> <p>4) Summary of the Fermi Estimates     1.  Model Size: ~80\u2013100 million parameters, given a 300+\u202fMB checkpoint.     2.  Compute: Possibly a few GPU\u2010days to 2 GPU\u2010weeks for the entire pretrain/fine\u2010tune cycle, on a standard HPC environment.     3.  TM\u2010Score: If you rebuilt 3D from TorsionBERT angles, a typical \u201cno advanced refinement\u201d guess might be 0.4\u20130.6 range overall, varying by RNA length. With a short local minimization, you might push that higher.</p> <p>These are all best\u2010effort guesses based on typical BERT training times and the known difficulty of angle\u21923D reconstructions for RNA.</p> <p>===</p>"},{"location":"pipeline/stageB/torsionBert_full_paper/","title":"Paper","text":"<p>FULL PAPER: Structural bioinformatics RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction Cl\ufffdement Bernard 1,2, Guillaume Postic 1, Sahar Ghannay 2, Fariza Tahi 1,\ufffd 1Universit\ufffde Paris Saclay, Univ Evry, IBISC, Evry-Courcouronnes 91020, France 2LISN\u2014CNRS/Universit\ufffde Paris-Saclay, Orsay 91400, France \ufffdCorresponding author. Universit\ufffde Paris Saclay, Univ Evry, IBISC, Evry-Courcouronnes 91020, France. E-mail: fariza.tahi@univ-evry.fr Associate Editor: Jianlin Cheng Abstract Motivation: Predicting the 3D structure of RNA is an ongoing challenge that has yet to be completely addressed despite continuous advance- ments. RNA 3D structures rely on distances between residues and base interactions but also backbone torsional angles. Knowing the torsional angles for each residue could help reconstruct its global folding, which is what we tackle in this work. This paper presents a novel approach for directly predicting RNA torsional angles from raw sequence data. Our method draws inspiration from the successful application of language models in various domains and adapts them to RNA. Results: We have developed a language-based model, RNA-TorsionBERT, incorporating better sequential interactions for predicting RNA torsional and pseudo-torsional angles from the sequence only. Through extensive benchmarking, we demonstrate that our method improves the prediction of torsional angles compared to state-of-the-art methods. In addition, by using our predictive model, we have inferred a torsion angle-dependent scoring function, called TB-MCQ, that replaces the true reference angles by our model prediction. We show that it accurately evaluates the quality of near-native predicted structures, in terms of RNA backbone torsion angle values. Our work demonstrates promising results, suggesting the potential utility of language models in advancing RNA 3D structure prediction. Availability and implementation: Source code is freely available on the EvryRNA platform: https://evryrna.ibisc.univ-evry.fr/evryrna/RNA- TorsionBERT. 1 Introduction RNA is a macromolecule that plays various biological func- tions in organisms. Similarly to proteins, the biological func- tion of an RNA may be directly linked to its 3D structure. Experimental methods such as NMR, X-ray crystallography, or cryo-EM can determine the 3D structure of RNAs, but they remain tedious in cost and time. Computational meth- ods have been developed for predicting the 3D structure from the sequence, with three different approaches: ab initio, template-based, and deep learning-based (Bernard et al. 2024c). Currently, no existing method matches the perfor- mance of AlphaFold 2 for proteins (Jumper et al. 2021), as shown with the last results on the CASP-RNA challenge (Das et al. 2023). Reaching AlphaFold\u2019s (Jumper et al. 2021) level of accuracy is a long shot, notably due to the lack of data (Schneider et al. 2023). Very recently, the release of AlphaFold 3 (Abramson et al. 2024) has extended its predic- tions to a wide range of molecules like DNA, ligand, ion, and RNA, but the results remain limited for RNA (Abramson et al. 2024, Bernard et al. 2024a). RNA can adopt various secondary motifs, along with a wide range of complex interactions that contribute to its 3D structure. Research efforts have focused on classifying both the canonical and noncanonical pairs, further supported by the description of the backbone conformation (Schneider et al. 2004). Unlike proteins, RNA backbone structures are defined by eight torsional angles, the natural manifold of all these dihedral angles combined being a 8D hypertorus, which presents a significant challenge both statistically and compu- tationally. Pyle and colleagues have shown that they can be approximated by two pseudo-torsional angles (see Fig. 1) (Wadley et al. 2007). Understanding these torsional angles is crucial for comprehending the 3D structures of RNA, which in turn could aid in predicting their folding. Current predictive methods for RNA 3D structure predic- tion do not always integrate torsional angles, missing impor- tant features to comprehend its folding. One work (Zok et al. 2015) has focused on constructing libraries of RNA conform- ers with torsional angles. It has been used for RNAfitme (Antczak et al. 2018), which allows editing and refining pre- dicted RNA 3D structures. Another work has been done to predict exclusively torsional angles from RNA sequence, SPOT-RNA-1D (Singh et al. 2021), using a residual convolu- tional neural network. In this work, we aim to leverage language models to better apprehend the prediction of RNA torsional angles from its se- quence. Indeed, works have been proposed through the years to work on biological sequences, inspired by the success of language models like BERT (Devlin et al. 2018). Its adapta- tion for RNA (Akiyama and Sakakibara 2022) or DNA (Ji et al. 2021) shows promising results which could be lever- aged for other RNA structural features prediction. Received: 8 July 2024; Revised: 11 December 2024; Editorial Decision: 30 December 2024; Accepted: 7 January 2025 \u00a9 The Author(s) 2025. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Bioinformatics, 2025, 41(1), btaf004 https://doi.org/10.1093/bioinformatics/btaf004 Advance Access Publication Date: 8 January 2025 Original Paper Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 Another interest in torsional angles for RNA 3D structures is for quality assessment. Without the help of reference structures, scoring functions have been developed to assess structural quality. These methods can be knowledge-based (Capriotti et al. 2011, Tan et al. 2022) using statistical potentials or deep learning (Townshend et al. 2021). The knowledge-based scoring functions consider RNA structural features as inputs like pairwise distances (Capriotti et al. 2011, Bottaro et al. 2014) or with the help of torsional angles (Tan et al. 2022). We propose here a new scoring function based on the extension of our model to predict RNA torsional angles. This scoring function allows us to assess structural quality in torsional angle space. This article is organized as follows: we describe our contributions in two separate points. Each section is divided into two parts: one for the work on torsional angle prediction and the other for our proposed scoring function. We detail our experiments for the torsional angles prediction and the structural quality assessment tasks before discussing our approaches\u2019 results and limitations. We then conclude by discussing the scope of our contributions. The results and the code of our RNA-TorsionBERT and TB-MCQ are easily reproducible and freely available on the EvryRNA platform: https://evryrna.ibisc.univ-evry.fr/evryrna/RNA-TorsionBERT. 2 Materials and methods This section presents our model for predicting RNA torsional angles and then the scoring function derived from our model. 2.1 Torsional angles prediction 2.1.1 RNA-TorsionBERT approach Current methods that use sequence as inputs for RNA-related approaches only represent sequences as one-hot-encoding vectors. This representation may be too sparse to consider se- quential interactions well. This encoding is usually associated with a convolutional neural network, which is commonly limited by long-range interactions. A solution could be using attention mechanisms. Attention-based architecture nonethe- less requires a huge amount of data to train well, which is not the case for RNA 3D structure data. To counter this problem, we can use models pre-trained on a large amount of unla- beled data. This could bring a better input representation of the raw sequence, which could then be fine-tuned to specific tasks. These pre-trained models could input either RNA or DNA sequences. Recent advances in language models started with BERT (Devlin et al. 2018), where the model was pre-trained on masking and next-sentence prediction tasks before being fine- tuned on diverse specific language tasks. DNA or RNA can be seen as a sequence of nucleotides, where their interactions have a biological meaning. Therefore, methods have been adapted from BERT to develop a language-based architecture for either RNA or DNA. The aim is to reproduce the success of language comprehension for another language. As the size of the vocabulary is different, modifications should be made to fit the current language. An example for DNA is DNABERT (Ji et al. 2021), where the training process was updated compared to the original BERT by removing the next sentence prediction and taking K-mers as inputs (contig- uous sequence of k nucleotides). It was trained on human- genome data. An example of the adaptation of BERT for RNA is called RNABERT (Akiyama and Sakakibara 2022). It is a six-transformer layer pre-trained on two tasks: masking and structural alignment learning (SAL). RNABERT was trained on 76 237 human-derived small ncRNAs. Other methods have been adapted to RNA language but uses MSA as inputs like RNA-MSM (Zhang et al. 2024). Nonetheless, they require multiple sequence alignment (MSA) as inputs, which restricts the use for RNAs. Indeed, there are a numer- ous amount of unknown structures (Kalvari et al. 2018), and MSA will restrict the adaptation to future unseen families. In this article, we decided to only consider sequences as inputs, and so for the language models. The aim of our method is, given a pre-trained language model (DNABERT or RNABERT), to adapt its neuronal weights to predict RNA torsional and pseudo-torsional angles from the sequence. We have added layers to adapt the methods to our multi-token label regression task. Each token in the input would have 28 labels: two values (sine and co- sine) for each of the eight torsional angles (the phase P being represented by its five ribose ring angles) and two pseudo- torsional angles. The use of pre-trained embedding would help the model not to start from scratch and update the learned attention layers for RNA structural features. 2.1.2 RNA-TorsionBERT architecture The architecture of our method, when based on DNABERT, is described in Fig. 2 (illustrated with 3-mers). An input se- quence of size L is tokenized and then fed to the network with token and positional embeddings. The tokenization pro- cess usually adds specific tokens (like the CLS and PAD tokens). As DNABERT could input a maximum of 512 tokens, we set the maximum sequence length to 512 nucleoti- des. The last hidden state is set to be 768 by the original DNABERT architecture. We then apply extra layers to map the hidden state outputs to the desired final output dimension (Lx28). These extra layers comprise layer normalization, a linear layer (from 768 to 1024), a GELU activation, another Figure 1. The eight RNA torsional angles and the two pseudo-torsional angles. (A) RNA backbone torsional angles. The angles are defined around O30i \u2212 1=Pi =O50i =C50i (\u03b1), Pi =O50i =C50i =C40i (\u03b2), O50i =C50i =C40i =C30i (\u03b3), C50i =C40i =C30i =O30i (\u03b4), C40i =C30i =O30i =Pi \u00fe 1 (\u03f5), C30i =O30i =Pi \u00fe 1=O50i \u00fe 1 (f) and the rotation of the base relative to the sugar (\u03c7) O40i =C10i =N1i =C2i for pyrimidines and O40i =C10i =N9i =C4i for purines. The ribose ring angles are defined as \u03bd0 (C40i =O40i =C10i =C20i ), \u03bd1 (O40i =C10i =C20i =C30i ), \u03bd2 (C10i =C20i =C30i =C40i ), \u03bd3 (C20i =C30i =C40i =O40i ), and \u03bd4 (C30i =C40i =O40i =C10i ). The ribose ring is usually described by a single sugar pucker pseudorotation phase P \u00bc arctan \u03bd1 \u00fe \u03bd4 \u2212 \u03bd0 \u2212 \u03bd3 2\u03bd2 \u00f0sin 36\ufffd \u00fe sin 72\ufffd \u00de \ufffd \ufffd. (B) RNA pseudo-torsional angles. \u03b7 is defined around C40i \u2212 1=Pi =C40i =Pi \u00fe 1 and \u03b8 around Pi =C40i =Pi \u00fe 1=C40i \u00fe 1. 2 Bernard et al. Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 linear layer (1024 to 28), and a Tanh final activation. The fi- nal output layer is of size 28 because it outputs a sine and a cosine for the eight torsional (\u03b1, \u03b2, \u03b3, \u03b4, \u03f5, f, \u03c7, and the phase P being predicted through the five ribose ring angles \u03bd0, \u03bd1, \u03bd2, \u03bd3, and \u03bd4) and two pseudo-torsional angles (\u03b7 and \u03b8). It allows the relief of the periodicity of the different angles. The Tanh activation maps the outputs to the cosine and sine range (between \u22121 and \u00fe1), which is then converted into angle predictions using the formula \u03b1 \u00bc tan\u2212 1 sin\u00f0\u03b1\u00de cos\u00f0\u03b1\u00de \ufffd \ufffd (adaptable for the other angles). Details on the training process are in the Supplementary File. 2.2 Model quality assessment based on torsional angles 2.2.1 Torsional-based quality assessment metrics Existing metrics have been developed to assess the quality of predicted RNA 3D structures with access to a reference. The most famous one is the RMSD (root-mean-square deviation), which assesses the general folding of structures. Other met- rics have been developed and adapted from proteins (Zhang and Skolnick 2004, Mariani et al. 2013). Some specific met- rics have also been designed to consider RNA specificities (Parisien et al. 2009). Only two metrics are torsional-angles- based: the MCQ (Zok et al. 2014) (mean of circular quanti- ties), and the Longest Continuous Segment in Torsion Angle space (LCS-TA) (Wiedemann et al. 2017). The MCQ com- putes the deviation in angle space without any superposition of structures and complements other existing metrics. LCS- TA computes the longest number of continuous residues with an MCQ below a threshold (usually 10\ufffd, 15\ufffd, 20\ufffd, and 25\ufffd). It is also a superposition-independent metric. In SPOT-RNA-1D (Singh et al. 2021), the authors intro- duced the mean-average error (MAE) metric to assess the per- formance of their method SPOT-RNA-1D in the prediction of torsional and pseudo-torsional angles. Nonetheless, the MAE is an arithmetic mean and is not designed for angles. To compute deviation for circular quantities, we use the mean of circular quantities (MCQ) (Zok et al. 2014). We do not consider the LCS-TA as it is more expensive to compute, and the MCQ is more widely used in RNA-Puzzles (Cruz et al. 2012; Miao et al. 2015, 2017, 2020). We define the set of angles for the torsional angles as T \u00bc f\u03b1; \u03b2; \u03b3; \u03b4; \u03f5; f; P; \u03c7g and for pseudo-torsional angles PT \u00bc f\u03b7; \u03b8g. Following the notation in (Zok et al. 2014), for a given structure S of L resi- dues, let\u2019s note ti;j the torsional angle of type j of the residue at position i. We denote the difference between two structures S and S0 as the MCQ(S, S\u2019), defined by: MCQ\u00f0S; S0\u00de \u00bc arctan Pr i\u00bc1 PjTj j\u00bc1 sin \u0394\u00f0ti;j; t0i;j\u00de Pr i\u00bc1 PjTj j\u00bc1 cos \u0394\u00f0ti;j; t0i;j\u00de 0 @ 1 A where r is the number of residues in S \\ S0 and with: \u0394\u00f0t; t0\u00de \u00bc 0 if both undefined; \u03c0 if either t or t0 is undefined; minfdiff\u00f0t; t0\u00de; 2\u03c0 \u2212 diff\u00f0t; t0\u00deg 8</p> <p>&lt; : and: diff\u00f0t; t0\u00de \u00bc jmod\u00f0t\u00de \u2212 mod\u00f0t0\u00dej The difference aims to consider periodicity of 2\u03c0 with mod\u00f0t\u00de \u00bc \u00f0t \u00fe 2\u03c0\u00de modulo 2\u03c0 To have more details on the performances for a specific angle, we define the MCQ for a specific type of angle j: MCQ\u00f0j\u00de\u00f0S; S0\u00de \u00bc arctan Pr i\u00bc1 sin \u0394\u00f0ti;j; t0i;j\u00de Pr i\u00bc1 cos \u0394\u00f0ti;j; t0i;j\u00de ! We extend the formulation for pseudo-torsion angles by just changing the set of angles used, and we name it MCQPT. 2.2.2 Quality assessment scoring functions Quality assessment of RNA 3D structures requires two struc- tures, with one being the experimentally solved structure. Having this reference is a strong asset that is hardly possible in practice. To rank near-native structures without a refer- ence, scoring functions have been developed, adapting free-energy (Capriotti et al. 2011, Tan et al. 2022). Other methods use deep learning approaches like ARES (Townshend et al. 2021). Figure 2. Schema of the proposed language model architecture for torsional and pseudo-torsional angles prediction. Given an RNA sequence, mapping is applied to each sequence\u2019s nucleotide into a token with embeddings from the language model. CLS and PAD tokens are added to the sequence tokens. We convert the Uracil (U) with its equivalent in DNA: Thymine (T) for DNABERT model. Then, the language model will output hidden states with a representation of each token. This is fed into extra layers before entering a last Tanh activation to have the cosine and sine per angle. A postprocessing is required to convert back to angles [and pseudo-torsional (PT) angles] from the sine and cosine. RNA-TorsionBERT 3 Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 To discriminate near-native structures in the torsional space, we have derived a scoring function from our RNA- TorsionBERT model. First, we have replicated a quality as- sessment metric that uses torsional angles features: the mean of circular quantities (MCQ) (Zok et al. 2014). Then, we replaced the true torsional angles with the predicted angles from our model to compute the MCQ over the near-native structure. Therefore, the MCQ computation compares the prediction of our model angles with the angles from the pre- dicted nonnative structures. This MCQ now becomes a scor- ing function, as it only takes as input a structure without any known native structure. We named this scoring function TB- MCQ for TorsionBERT-MCQ. Figure 3 shows the architec- ture of TB-MCQ. Given a structure, we extract the torsional angles and the sequence. The sequence is then pre-processed by RNA-TorsionBERT, and the inference gives predicted angles. Then, we compute the MCQ to finally output a struc- tural quality measure for an RNA 3D structure. 3 Results and discussion 3.1 Results on torsional angles prediction We present here the different experiments for the torsional angles prediction task. We used the MCQ presented above as a criterion to assess our model performances. We mainly fo- cus on the results for torsional angles, while the results of MCQPT for pseudo-torsional angles are available in the Supplementary File. 3.1.1 Datasets To validate the performances of torsional angle prediction models, we used different datasets of native structures: Training: we downloaded each available PDB structure and removed the structures from the nonredundant Validation and Test sets presented below. We also ensure the structures from this dataset have a sequence similarity below 80% compared to the other used datasets. We considered only the structures of a maximum sequence length of 512 (DNABERT can only input 512 tokens). The final set is com- posed of 4267 structures with sequences from 11 to 508 nucleotides. Validation: we used the validation structures from SPOT- RNA-1D (Singh et al. 2021). It contains 29 structures with sequences between 33 and 288 nucleotides. Test: we combined two well-known test sets: RNA-Puzzles (Cruz et al. 2012) and CASP-RNA (Das et al. 2023). We combined both of these datasets as a whole Test set to assess the robustness of our model. It leads to a Test set of 34 struc- tures (22 from single-stranded RNA of RNA-Puzzles and 12 from CASP-RNA), with sequences from 27 nucleotides to 512 [we cropped the RNA of PDB ID R1138 (720 nt) to 512 nucleotides]. The distribution of the eight torsional angles and the two pseudo-torsional angles is given in Fig. 4. As the pseudorota- tion phase P is defined with the five ribose ring angles, their distributions are shown in Supplementary Fig. S1 of the Supplementary File. These distributions are similar for the three datasets, meaning the learned distribution from the training set could allow good generalization for the model. 3.1.2 Language model selection Each language model has a different format of inputs (K- mers for DNABERT and single nucleotides for RNABERT), we had to select the best tokenization of our RNA sequences. We also had to decide which pre-trained model was the best for our task. Therefore, we trained the same DNABERT model with the different values of K (3, 4, 5, or 6) and RNABERT on the Training set and observed the performan- ces on the Validation set. The results are shown in Table 1 on the Validation set. In terms of K-mers, DNABERT trained on 3-mers has better results (MCQ of 19.0) than the other K-mers and RNABERT, even if it does not outperform them for each tor- sional angle. The results for the pseudorotation phase P show that the model does not change the prediction for this angle. RNABERT only outperforms the other methods for the P angle, which does not lead to any significant conclusion for the selection of this model. We observe that for some angles (\u03b2, P, and \u03c7), the choice of models does not have an impact on the performances. DNABERT with 3-mers outper- forms RNABERT, which does not input K-mers. This result remains surprising as we could have thought that RNABERT, as pre-trained specifically on RNA data, could Figure 3. TB-MCQ scoring function computation. Given a predicted RNA 3D structure, we extract the sequence and calculates the different torsional angles. The sequence is fed to our RNA-TorsionBERT model to predict torsional angles. The scoring function is computed by taking the MCQ between the predicted angles and the real angles. 4 Bernard et al. Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 have done better than the DNABERT model. This difference might be explained by the K-mers representation that is used by DNABERT compared to RNABERT, where the size of the vocabulary is extended, and thus a finer representation of the inputs is embedded. This could help the model learn a higher number of interactions and be more adaptable for other tasks. What could also explain the difference in performances is the size of the model: DNABERT has a size of around 328MB, whereas RNABERT has around 2MB. From now on, we name RNA-TorsionBERT (for RNA torsional BERT) the DNABERT with 3-mers. 3.1.3 Performances We present here the prediction results obtained by our method RNA-TorsionBERT on the Test set (presented above) compared to the state-of-the-art approach SPOT-RNA-1D (Singh et al. 2021), which is the only method that predicts RNA torsional angles from the sequence only. We repro- duced the architecture of SPOT-RNA-1D (because we only had the code to do inference) and trained it with the exact same data as RNA-TorsionBERT. We also included the results for the inferred angles from methods benchmarked in State-of-the-RNArt (Bernard et al. 2024c). The methods benchmarked included either ab initio with IsRNA (Zhang et al. 2021) and RNAJP (Li and Chen 2023), or template- based with RNAComposer (Popenda et al. 2012), Vfold- Pipeline (Li et al. 2022), MC-Sym (Parisien and Major 2008), and 3dRNA (Wang et al. 2019). We also include three deep learning methods: trRosettaRNA (Wang et al. 2023) and RhoFold (Shen et al. 2022) and the newly AlphaFold 3 (Abramson et al. 2024). We report the MCQ per angle on the Test Set in Table 2. MCQPT (pseudo-torsional) results are available in Supplementary Table S1 of the Supplementary File. Our RNA-TorsionBERT model has better performances than SPOT-RNA-1D for every angle. It has an average MCQ of 17.4 compared to 19.4 for SPOT-RNA-1D. The MCQ im- provement over SPOT-RNA-1D ranges between 0.2\ufffd (for \u03f5) and 4.3\ufffd (for \u03b4). It also outperforms the angles inferred from state-of-the-art methods for RNA 3D structure prediction, in- cluding the last published method, AlphaFold 3 (Abramson et al. 2024). Nonetheless, the performances compared to AlphaFold 3 remains close. RNA-TorsionBERT does not out- perform it for every angle. trRosettaRNA and RhoFold, two deep learning methods, have the worst MCQ compared to ab initio and template-based approaches. It can be explained by the use of physics in ab initio and template-based methods that are inferred in the torsional angles. The use of deep learning approaches might have the counterpart to not include physics enough, except for AlphaFold 3. Deep learning methods, while having the best overall results, as shown in the benchmark done in State-of-the-RNArt (Bernard et al. 2024c), remain limited in torsional angle predictions. 3.1.3.1 Results according to sequence length To study more in details the performances based on the RNA length, we report in Fig. 5, the MCQ obtained by our method, SPOT-RNA-1D and AlphaFold 3 depending on the sequence length for the Test set. We can see our method out- performs SPOT-RNA-1D for each of the sequence slot. AlphaFold 3 has lower MCQ for structures with sequences between 75 and 175 nt. For sequences &gt;200 nucleotides, our method demonstrates superior performances compared to both SPOT-RNA-1D and AlphaFold 3, showing the interest for long range sequences. Results for the MCQPT are shown in Supplementary Fig. S2 of the Supplementary File. 3.1.3.2 Results according to secondary structure motifs We report the results of MCQ for three types of secondary structure motifs (single-stranded, loop and stem) averaged over the Test set for RNA-TorsionBERT, AlphaFold 3 and SPOT-RNA-1D in Table 3. We observe that our method delivers improved performances for each secondary structure motif [extracted from RNApdbee (Zok et al. 2018)]. It has an overall MCQ higher for single-stranded than stem motifs. This behavior is also similar to SPOT-RNA-1D and AlphaFold 3, which could be explained by the fact that stem and loop motifs are easier to predict than single-stranded motifs (and so are the base pairings). Details on the results for pseudo-torsional angles are available in Supplementary Table S2 of the Supplementary File. 3.1.3.3 Results according to RNA types In CASP-RNA, structures can be described as either natural with or without homologs, or synthetic RNAs. To further study the different cases where our approach is better than existing tools, we report the results for the natural (with or without homologs) and synthetic RNAs in Table 4. Our method outperforms AlphaFold 3 and SPOT-RNA-1D for natural RNAs, with the largest gap for RNA without homo- logs. This could be explained by the reliability of AlphaFold Figure 4. Polar distribution of the eight torsional angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03f5, f, P, and \u03c7) and the two pseudo-torsional angles (\u03b7 and \u03b8) for the Training, Validation, and Test datasets. For each angle, the logarithm of the normalized count is depicted. Table 1. MCQ for each torsional angle and using all the torsional angles for the Validation set for DNABERT (3,4, 5, or 6-mers) and RNABERT. Bold values correspond to the best MCQ values per line. DNABERT RNABERT 3-mer 4-mer 5-mer 6-mer MCQ\u00f0\u03b1\u00de 32.3 31.0 31.8 33.9 36.3 MCQ\u00f0\u03b2\u00de 17.6 17.6 17.6 17.8 17.8 MCQ\u00f0\u03b3\u00de 26.3 22.8 26.5 26.7 28.1 MCQ\u00f0\u03b4\u00de 14.4 12.1 15.6 16.7 17.3 MCQ\u00f0\u03f5\u00de 16.1 15.8 16.0 16.0 16.0 MCQ\u00f0f\u00de 22.5 21.7 21.6 21.8 22.2 MCQ\u00f0P\u00de 8.6 8.7 8.8 8.7 8.7 MCQ\u00f0\u03c7\u00de 18.2 18.2 18.3 18.5 18.7 MCQ 20.2 19.0 20.2 20.7 21.4 RNA-TorsionBERT 5 Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 3 on multiple sequence alignment, and, thus, on the availabil- ity and quality of homologs for the prediction. AlphaFold 3 has better performances for synthetic RNAs. More details on the results for different RNA families on RNA-Puzzles are available in Supplementary Table S3 of the Supplementary File. Examples of structures are provided in Supplementary Fig. S3 of the Supplementary File. 3.2 Model quality assessment based on torsional angles In this part, we describe the different datasets used for evalu- ating our scoring function. We used correlation scores to compare the links of our scoring function to existing metrics. 3.2.1 Datasets Datasets of near-native structures (or decoys) are necessary to compare model quality assessment metrics. Indeed, scoring functions are used to discriminate between near-native struc- tures, meaning that we need to have nonnative structures to evaluate the quality of our scoring function. We used three different datasets with different strategies of structure generation: Decoy Test Set I is from RASP (Capriotti et al. 2011), com- posed of 85 native RNAs with decoys generated with a pre- dictive model (by applying different sets of Gaussian restraint parameters). Each RNA has 500 decoys, which are close to the native structure. We only kept 83 RNAs and removed the two RNAs that have sequence lengths &gt;512 nucleotides (PDB ID: 3df3A and 3f1hA). Decoy Test Set II corresponds to the prediction-models (PM) subset from rsRNASP (Tan et al. 2022). It has 20 nonredundant single-stranded RNAs. For each RNA, 40 decoys are generated with four RNA 3D structure prediction models The decoys are not as near to native structures as with the Decoy Test Set I. Decoy Test Set III is the RNA-Puzzles standardized dataset (Magnus et al. 2020). This dataset comprises 21 RNAs and dozens of decoy structures for each RNA. The decoys are not all close to the native structures. 3.2.2 Evaluation measures Scoring functions aim to discriminate near-native structures. The Pearson correlation coefficient (PCC) and the enrichment score (ES) are used to assess the correctness of a given scoring function. They assess the link between a scoring function and a given metric. The Pearson coefficient correlation (PCC) is computed be- tween the ranked structures based on scoring functions and structures ranked by metrics. It is defined as: Table 2. MCQ per torsional angle and for all torsional angles over the Test set for RNA-TorsionBERT compared to SPOT-RNA-1D. We also include inferred torsional angles from state-of-the-art methods that predict RNA 3D structures from State-of-the-RNArt (Bernard et al. 2024c). Methods are either deep learning (DL), ab initio (AB), or template-based (TP). Bold values correspond to the best MCQ values per column. Type MCQ\u00f0\u03b1\u00de MCQ\u00f0\u03b2\u00de MCQ\u00f0\u03b3\u00de MCQ\u00f0\u03b4\u00de MCQ\u00f0\u03f5\u00de MCQ\u00f0f\u00de MCQ\u00f0P\u00de MCQ\u00f0\u03c7\u00de MCQ RNA-TorsionBERT DL 29.9 19.0 23.7 9.5 15.3 19.1 8.4 12.2 17.4 SPOT-RNA-1D (Singh et al. 2021) DL 32.5 19.6 26.6 13.7 15.5 20.2 9.8 13.2 19.4 AlphaFold3 (Abramson et al. 2024) DL 29.8 19.9 23.9 8.8 15.2 18.8 8.8 14.1 17.8 IsRNA1 (Zhang et al. 2021) AB 41.9 23.8 33.5 12.5 22.8 31.3 18.9 17.5 24.9 Vfold-Pipeline (Li et al. 2022) TP 41.3 24.1 32.3 14.7 23.4 29.3 17.3 20.6 25.3 RNAComposer (Popenda et al. 2012) TP 43.6 27.5 38.8 13.3 21.4 27.4 16.4 20.6 25.9 RNAJP (Li and Chen 2023) AB 41.3 28.0 33.3 14.0 24.4 32.1 11.2 20.7 26.6 3dRNA (Wang et al. 2019) TP 50.4 31.9 42.3 21.4 31.0 36.1 24.2 23.2 32.5 MC-Sym (Parisien and Major 2008) TP 66.5 26.0 57.9 27.8 22.1 39.6 17.5 23.4 36.0 trRosettaRNA (Wang et al. 2023) DL 59.1 33.8 60.2 21.9 27.9 41.1 28.3 55.4 40.4 RhoFold (Shen et al. 2022) DL 91.4 61.3 67.4 48.1 45.0 53.6 46.7 32.3 54.8 Figure 5. MCQ depending on sequence length (with a window of 25 nt from 25 nt to 200 nt) for RNA-TorsionBERT, SPOT-RNA-1D, and AlphaFold 3 for the Test set. Table 3. MCQ for torsional angles averaged over the Test set for RNA- TorsionBERT, AlphaFold 3, and SPOT-RNA-1D for three secondary structure motifs: single-stranded, loops, and stems.a Bold values correspond to the best MCQ values per line. Motifs RNA-TorsionBERT AlphaFold 3 SPOT-RNA-1D Single-stranded 24.9 25.3 25.3 Loops 24.1 24.9 24.8 Stems 16.2 16.3 17.7 a Motifs are extracted using RNApdbee (Zok et al. 2018). Table 4. MCQ per RNA type for the CASP-RNA dataset for RNA- TorsionBERT, AlphaFold 3, and SPOT-RNA-1D. Molecules are either natural RNAs with homolog(s) [Natural (H)], natural RNAs without homolog(s) [Natural (nH)], or synthetic RNAs. The number of times each model outperforms the others is described in parentheses. Bold values correspond to the best MCQ values per line. Type RNA-TorsionBERT AlphaFold 3 SPOT-RNA-1D Natural (H) 20.6 (3/5) 22.3 (1/5) 22.3 (1/5) Natural (nH) 11.8 (3/3) 15.5 (0/3) 13.6 (0/3) Synthetic 16.1 (2/4) 15.9 (2/4) 19.1 (0/4) All 16.9 (8/12) 18.5 (3/12) 18.8 (1/12) 6 Bernard et al. Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 PCC \u00bc PNdecoys i\u00bc1 \u00f0En \u2212 \ufffdE\u00de\u00f0Rn \u2212 \ufffdR\u00de ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi PNdecoys n\u00bc1 \u00f0En \u2212 \ufffdE\u00de2 q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi PNdecoys n\u00bc1 \u00f0Rn \u2212 \ufffdR\u00de2 q where En and Rn the energy and metric of the nth structure, respectively. PCC ranges from 0 to 1, where a PCC of 1 means the relationship between metric and energy is completely linear. The enrichment score (ES) considers top-ranked structures from both scoring function and metric. It is defined as: ES \u00bc 100 \u00d7 jEtop10% \\ Rtop10%j Ndecoys where jEtop10% \\ Rtop10%j is the number of common structures from the top 10% of structures (measured by the metric) and the top 10% of structures with the lowest scoring function. ES ranges between 0 and 10 (perfect scoring). An enrichment score of 1 means a random prediction, whereas below 1 means a bad score. 3.2.3 TB-MCQ as scoring function To assess the validity of our scoring function, we computed with RNAdvisor (Bernard et al. 2024b) the available scoring functions RASP (Capriotti et al. 2011), \u03f5SCORE (Bottaro et al. 2014), DFIRE-RNA (Capriotti et al. 2011), and rsRNASP (Tan et al. 2022) for the three different Decoys test sets. We compared TB-MCQ with the state-of-the-art scoring functions using PCC and ES with the MCQ. The averaged values are shown in Fig. 6. TB-MCQ is the scoring function that is the more correlated to MCQ (PCC of 0.87 and ES of 5.39). rsRNASP still shows a high correlation to MCQ (PCC of 0.67 and ES of 4.41), which is surprising as it does not integrate explicit torsional angles in its computation. What is missing for both scoring functions to reproduce the MCQ metric perfectly is the accu- racy of predicted torsional angles. It might be ineffective for structures that are really close to the native one and where the inferred angles from these structures are closer to the na- tive than the predicted ones from RNA-TorsionBERT. PCC and ES for other distance-based metrics are shown in Supplementary Fig. S4 of the Supplementary File. 4 Conclusion In this work, we have developed a language-based model, RNA-TorsionBERT, to predict RNA torsional and pseudo- torsional angles from the sequence. With a DNABERT 3- mers model, the learned embeddings have been used as a starting point to infer structural features from the sequence. We have achieved improvement compared to SPOT-RNA-1D (Singh et al. 2021), the only tool for RNA torsional angle pre- diction from the raw sequence. Through an extensive benchmark of state-of-the-art meth- ods, we have outperformed the angles inferred from the predic- tive models. We have also included in the benchmark the new release of AlphaFold, named AlphaFold 3 (Abramson et al. 2024), which gives the best results compared to ab initio, template-based and deep learning solutions in terms of MCQ on inferred angles. Our method, RNA-TorsionBERT, remains better for the prediction of RNA torsional angles with only the sequence as input, while AlphaFold 3 uses MSA as inputs. Most protein methods or current deep learning methods for predicting RNA 3D structures use MSA as inputs, which is a huge restriction. Indeed, significant families are still un- known (Kalvari et al. 2018). It also increases the inference time, where a homology search should be made for each pre- diction. Our method leverages language model without the need of homology, which is a benefit for the prediction of RNA from unknown families. Through the evaluation of our model for backbone torsional angles prediction, we have extended this evaluation as a model quality assessment for RNA 3D structures. Then, we have in- ferred a scoring function named TB-MCQ. This scoring func- tion could help the selection of near-native structures in terms of angle deviation. It is also specific to torsional angles and, thus, is more related to the angle-based metric MCQ. Improvements could be made for both RNA-TorsionBERT and TB-MCQ. The RNA-TorsionBERT performances remain limited to reconstruct the structures from just the torsional angles. MCQ remains of high values for the different test sets, meaning there are still improvements to be made to tor- sional angle prediction. Indeed, the reconstruction from tor- sional angles alone is difficult as small angle deviation could lead to high cumulative divergence. The number of solved structures remain the main bottleneck to train robust meth- ods. Different structural tasks could be added to the model, with the prediction of secondary structure, interatomic dis- tances, hydrogen bonds, or noncanonical base interactions. Efforts could be made to improve the language-based model used, where a model pre-trained more efficiently on RNA data could help improve the overall performances. The qual- ity of the scoring function could be enhanced by incorporat- ing distance atom features, or directly by improving the prediction of torsional angles itself. Our RNA-TorsionBERT method can nonetheless be used as a starting point for the reconstruction of RNA 3D struc- tures, with ab initio methods, for instance, that include mo- lecular dynamics to relax the structure. It could also be used as a feature in a bigger network to predict RNA 3D conformation. Supplementary data Supplementary data are available at Bioinformatics online. Conflict of interest: None declared. Funding This work was supported in part by UDOPIA-ANR-20- THIA-0013, Labex DigiCosme [project ANR11LABEX004 5DIGICOSME], performed using HPC resources from Figure 6. PCC and ES between five different scoring functions (RASP, \u03f5SCORE, DIFRE-RNA, rsRNASP, and our scoring function TB-MCQ) and the angle-based metric MCQ. Values are averaged over the three decoy test sets. RNA-TorsionBERT 7 Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025 GENCI/IDRIS [AD011014250], and operated by ANR as part of the program \u201cInvestissement d\u2019Avenir\u201d Idex ParisSaclay [ANR11IDEX000302]. Data availability The data underlying this article are freely available online at: https://evryrna.ibisc.univ-evry.fr/evryrna/RNA-TorsionBERT. References Abramson J, Adler J, Dunger J et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature 2024;636:E4. https://doi.org/10.1038/s41586-024-07487-w Akiyama M, Sakakibara Y. Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning. NAR Genom Bioinform 2022;4:lqac012. Antczak M, Zok T, Osowiecki M et al. RNAfitme: a webserver for modeling nucleobase and nucleoside residue conformation in fixed- backbone RNA structures. BMC Bioinformatics 2018;19:304. https://doi.org/10.1186/s12859-018-2317-9 Bernard C, Postic G, Ghannay S et al. Has AlphaFold 3 reached its suc- cess for RNAs? bioRxiv, https://doi.org/10.1101/2024.06.13. 598780, 2024a, preprint: not peer reviewed. Bernard C, Postic G, Ghannay S et al. RNAdvisor: a comprehensive benchmarking tool for the measure and prediction of RNA struc- tural model quality. Brief Bioinform 2024b;25:bbae064. Bernard C, Postic G, Ghannay S et al. State-of-the-RNArt: benchmarking current methods for RNA 3D structure prediction. NAR Genom Bioinform 2024c;6:lqae048. https://doi.org/10.1093/nargab/lqae048 Bottaro S, Di Palma F, Bussi G. The role of nucleobase interactions in RNA structure and dynamics. Nucleic Acids Res 2014; 42:13306\u201314. Capriotti E, Norambuena T, Marti-Renom MA et al. All-atom knowledge-based potential for RNA structure prediction and assess- ment. Bioinformatics 2011;27:1086\u201393. Cruz JA, Blanchet M-F, Boniecki M et al. RNA-Puzzles: a CASP-like evaluation of RNA three-dimensional structure prediction. RNA 2012;18:610\u201325. Das R, Kretsch R, Simpkin A et al. Assessment of three-dimensional RNA structure prediction in CASP15. Proteins 2023;91:1747\u201370. Devlin J, Chang M, Lee K et al. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018, preprint: not peer reviewed. Ji Y, Zhou Z, Liu H et al. DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in ge- nome. Bioinformatics 2021;37:2112\u201320. Jumper J, Evans R, Pritzel A et al. Highly accurate protein structure pre- diction with AlphaFold. Nature 2021;596:583\u20139. Kalvari I, Argasinska J, Quinones-Olvera N et al. Rfam 13.0: shifting to a genome-centric resource for non-coding RNA families. Nucleic Acids Res 2018;46:D335\u201342. Li J, Chen S-J. RNAJP: enhanced RNA 3D structure predictions with non-canonical interactions and global topology sampling. Nucleic Acids Res 2023;51:3341\u201356. Li J, Zhang S, Zhang D et al. Vfold-Pipeline: a web server for RNA 3D structure prediction from sequences. Bioinformatics 2022;38:4042\u20133. Magnus M, Antczak M, Zok T et al. RNA-Puzzles toolkit: a computa- tional resource of RNA 3D structure benchmark datasets, structure manipulation, and evaluation tools. Nucleic Acids Res 2020; 48:576\u201388. Mariani V, Biasini M, Barbato A et al. lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics 2013;29:2722\u20138. Miao Z, Adamiak RW, Antczak M et al. RNA-Puzzles round III: 3D RNA structure prediction of five riboswitches and one ribozyme. RNA 2017;23:655\u201372. https://doi.org/10.1261/rna.060368.116 Miao Z, Adamiak RW, Antczak M et al. RNA-Puzzles round IV: 3D structure predictions of four ribozymes and two aptamers. RNA 2020;26:982\u201395. https://doi.org/10.1261/rna.075341.120 Miao Z, Adamiak RW, Blanchet MF et al. RNA-Puzzles round II: as- sessment of RNA structure prediction programs applied to three large RNA structures. RNA 2015;21:1066\u201384. https://doi.org/10. 1261/rna.049502.114 Parisien M, Major F. The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data. Nature 2008;452:51\u20135. Parisien M, Cruz J, Westhof E et al. New metrics for comparing and assessing discrepancies between RNA 3D structures and models. RNA (New York, N.Y.) 2009;15:1875\u201385. Popenda M, Szachniuk M, Antczak M et al. Automated 3D structure composition for large RNAs. Nucleic Acids Res 2012;40:e112. Schneider B, Moravek Z, Berman HM. RNA conformational classes. Nucleic Acids Res 2004;32:1666\u201377. https://doi.org/10.1093/ nar/gkh333 Schneider B, Sweeney BA, Bateman A et al. When will RNA get its AlphaFold moment? Nucleic Acids Res 2023;51:9522\u201332. https:// doi.org/10.1093/nar/gkad726 Shen T, Hu Z, Peng Z et al. E2Efold-3D: end-to-end deep learning method for accurate de novo RNA 3D structure prediction. arXiv, arXiv:2207.01586, 2022, preprint: not peer reviewed. Singh J, Paliwal K, Singh J et al. RNA backbone torsion and pseudotor- sion angle prediction using dilated convolutional neural networks. J Chem Inf Model 2021;61:2610\u201322. Tan Y-L, Wang X, Shi Y-Z et al. rsRNASP: a residue-separation-based statistical potential for RNA 3D structure evaluation. Biophys J 2022;121:142\u201356. Townshend RJL, Eismann S, Watkins AM et al. Geometric deep learn- ing of RNA structure. Science 2021;373:1047\u201351. Wadley LM, Keating KS, Duarte CM et al. Evaluating and learning from RNA pseudotorsional space: quantitative validation of a re- duced representation for RNA structure. J Mol Biol 2007;372: 942\u201357. https://doi.org/10.1016/j.jmb.2007.06.058 Wang J, Wang J, Huang Y et al. 3dRNA v2.0: an updated web server for RNA 3D structure prediction. Int J Mol Sci 2019;20:4116\u20138. Wang W, Feng C, Han R et al. trRosettaRNA: automated prediction of RNA 3D structure with transformer network. Nat Commun 2023; 14:7266. Wiedemann J, Zok T, Milostan M et al. LCS-TA to identify similar frag- ments in RNA 3D structures. BMC Bioinformatics 2017;18:456. Zhang D, Li J, Chen S-J. IsRNA1: de novo prediction and blind screening of RNA 3D structures. J Chem Theory Comput 2021;17:1842\u201357. Zhang Y, Skolnick J. Scoring function for automated assessment of pro- tein structure template quality. Proteins 2004;57:702\u201310. Zhang Y, Lang M, Jiang J et al. Multiple sequence alignment-based RNA language model and its application to structural inference. Nucleic Acids Res 2024;52:e3. https://doi.org/10.1093/nar/gkad1031 Zok T, Popenda M, Szachniuk M. MCQ4Structures to compute similar- ity of molecule structures. Cent Eur J Oper Res 2014;22:457\u201373. Zok T, Antczak M, Riedel M et al. Building the library of rna 3D nucleotide conformations using the clustering approach. Int J Appl Math Comput Sci 2015;25:689\u2013700. https://doi.org/10.1515/amcs-2015-0050 Zok T, Antczak M, Zurkowski M et al. RNApdbee 2.0: multifunctional tool for RNA structure annotation. Nucleic Acids Res 2018;46: W30\u20135. https://doi.org/10.1093/nar/gky314 \u00a9 The Author(s) 2025. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Bioinformatics, 2025, 41, 1\u20138 https://doi.org/10.1093/bioinformatics/btaf004 Original Paper 8 Bernard et al. Downloaded from https://academic.oup.com/bioinformatics/article/41/1/btaf004/7945663 by Ripon College Library user on 17 March 2025RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction Cl\u00b4ement Bernard, Guillaume Postic, Sahar Ghannay, Fariza Tahi January 3, 2025 Datasets Figure S1 shows the distribution of the ribose sugar ring angles for the different datasets used. Their distributions seem quite close, which is also the case for the pseudrotation phase P angle. Figure S1: Polar distribution of the five ribose sugar ring angles (\u03bd0, \u03bd1, \u03bd2, \u03bd3 and \u03bd4) for the Training, Validation and Test datasets. For each angle, the logarithm of the normalized count is depicted Experimental protocol We have fine-tuned both DNABERT [1] and RNABERT [2] for the prediction of torsional and pseudo-torsional angles. For the two models, we used a batch size of 10, the Mean Average loss 1 with a learning rate of 1e-4 and a weight decay of 0.01. We used the AdamW [3] optimizer. All inputs were padded to have a fixed size of 512 for DNABERT and 440 for RNABERT (limited by the model), and we trained the models for a maximum of 20 epochs. As there is no RNA of sequence length between 440 and 512, we used the same datasets for both RNABERT and DNABERT. Performances Figure S2: MCQPT per window of 25nt (from 25nt to 200nt) for RNA-TorsionBERT, SPOT-RNA- 1D and AlphaFold 3 inferred angles for the Test set. 2 Figure S3: Structures with the associated MCQ for RNA-Puzzles (A) and CASP-RNA (B). In blue are reported examples of structures where RNA-TorsionBERT outperforms AlphaFold 3 and SPOT-RNA-1D. In red are examples of RNA structures where AlphaFold 3 outperforms RNA- TorsionBERT and SPOT-RNA-1D. 3 Table S1: MCQ per pseudo-torsional angle and MCQPT (MCQ computed for all the pseudo- torsional angles) over the Test set for RNA-TorsionBERT compared to SPOT-RNA-1D [4]. We also include inferred torsional angles from state-of-the-art methods that predict RNA 3D structures from State-of-the-RNArt [5]. Methods are sorted by MCQPT. Models MCQ(\u03b7) MCQ(\u03b8) MCQPT RNA-TorsionBERT 15.2 20.8 18.0 SPOT-RNA-1D [4] 17.0 21.3 19.1 AlphaFold3 [6] 13.8 17.4 15.6 IsRNA1 [7] 18.9 26.1 22.4 RNAJP [8] 20.3 25.5 22.8 Vfold-Pipeline [9] 21.0 27.6 24.2 RNAComposer [10] 21.0 28.1 24.5 3dRNA [11] 25.5 31.6 28.5 RhoFold [12] 28.1 31.6 29.8 MC-Sym [13] 28.5 32.9 30.6 trRosettaRNA [14] 26.0 36.9 31.3 Table S2: MCQPT for our method RNA-TorsionBERT, AlphaFold 3 [6] and SPOT-RNA-1D [4] on secondary motifs averaged on the Test set. Secondary motifs are extracted from RNApdbee [15] Motifs RNA-TorsionBERT AlphaFold 3 SPOT-RNA-1D Single-stranded 36.4 31.9 48.4 Loops 31.3 30 42.3 Stems 16.3 15.6 24.2 Table S3: MCQ per RNA family for the single-stranded structures from RNA-Puzzles [16\u201319] dataset. The number of times each model outperforms the others is described in parentheses. The models compared are RNA-TorsionBERT, AlphaFold 3 [6] and SPOT-RNA-1D [4]. Family RNA-TorsionBERT AlphaFold 3 SPOT-RNA-1D Aptamer 18.6 (0/3) 16.4 (2/3) 17.5 (1/3) Riboregulator 13.0 (1/1) 16.2 (0/1) 13.8 (0/1) Riboswitch 16 (1/11) 13.9 (9/11) 16.6 (1/11) Ribozyme 22.6 (3/4) 23.0 (1/4) 24.5 (0/4) Ricin loop 8.5 (0/1) 6.6 (1/1) 10.9 (0/1) Virus 13.1 (0/2) 10.3 (2/2) 16.5 (0/2) All 16.8 (5/22) 15.3 (15/22) 17.7 (1/22) 4 Model quality assessment based on torsional angles Figure S4: PCC (A) and ES (B) between five different scoring functions (RASP [20], \u03f5SCORE [21], DIFRE-RNA [22], rsRNASP [23] and our scoring functions TB-MCQ) and ten metrics (RMSD, INFall [24], DI [24], GDT-TS [25], CAD-score [26], \u03f5RMSD [21], TM-score [27, 28], lDDT [29], MCQ [30], and LCS-TA [31] (with a threshold of 10, 15, 20 and 25)). Values are averaged over the three decoy test sets. 5 References 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. Ji Y, Zhou Z, Liu H, et al. DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. Bioinformatics 15 2021;37:2112\u201320. Akiyama M and Sakakibara Y. Informative RNA base embedding for RNA structural align- ment and clustering by deep representation learning. NAR Genomics and Bioinformatics 2022;4:lqac012. Loshchilov I and Hutter F. Decoupled Weight Decay Regularization. arXiv 2019. Singh J, Paliwal K, Singh J, et al. RNA Backbone Torsion and Pseudotorsion Angle Prediction Using Dilated Convolutional Neural Networks. Journal of Chemical Information and Modeling 6 2021;61:2610\u201322. Bernard C, Postic G, Ghannay S, and Tahi F. State-of-the-RNArt: benchmarking current methods for RNA 3D structure prediction. NAR Genomics and Bioinformatics 2024;6:lqae048. Abramson J, Adler J, Dunger J, et al. Accurate structure prediction of biomolecular interac- tions with AlphaFold 3. Nature 2024. Zhang D, Li J, and Chen SJ. IsRNA1: De Novo Prediction and Blind Screening of RNA 3D Structures. Journal of Chemical Theory and Computation 3 2021;17:1842\u201357. Li J and Chen SJ. RNAJP: enhanced RNA 3D structure predictions with non-canonical in- teractions and global topology sampling. Nucleic Acids Research 7 2023;51:3341\u201356. Li J, Zhang S, Zhang D, et al. Vfold-Pipeline: a web server for RNA 3D structure prediction from sequences. Bioinformatics 2022;38:4042\u20133. Popenda M, Szachniuk M, Antczak M, et al. Automated 3D structure composition for large RNAs. Nucleic Acids Research 14 2012;40:e112\u2013e112. Wang J, Wang J, Huang Y, et al. 3dRNA v2.0: An Updated Web Server for RNA 3D Structure Prediction. International Journal of Molecular Sciences 17 2019;20:4116. Shen T, Hu Z, Peng Z, et al. E2Efold-3D: End-to-End Deep Learning Method for Accurate de Novo RNA 3D Structure Prediction. arXiv preprint arXiv:2207.01586 2022. Parisien M and Major F. The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data. Nature 7183 2008;452:51\u20135. Wang W, Feng C, Han R, et al. trRosettaRNA: automated prediction of RNA 3D structure with transformer network. Nat Commun 2023;14:7266. Zok T, Antczak M, Zurkowski M, et al. RNApdbee 2.0: multifunctional tool for RNA structure annotation. Nucleic Acids Research 2018;46:W30\u2013W35. Cruz JA, Blanchet MF, Boniecki M, et al. RNA-Puzzles : A CASP-like evaluation of RNA three-dimensional structure prediction. RNA 4 2012;18:610\u201325. Miao Z, Adamiak RW, Blanchet MF, et al. RNA-Puzzles Round II: assessment of RNA struc- ture prediction programs applied to three large RNA structures. RNA 2015;21:1066\u201384. Miao Z, Adamiak RW, Antczak M, et al. RNA-Puzzles Round III: 3D RNA structure predic- tion of five riboswitches and one ribozyme. RNA 5 2017;23:655\u201372. Miao Z, Adamiak RW, Antczak M, et al. RNA-Puzzles Round IV: 3D structure predictions of four ribozymes and two aptamers. RNA 8 2020;26:982\u201395. 6 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. Capriotti E, Norambuena T, Marti-Renom MA, et al. All-atom knowledge-based potential for RNA structure prediction and assessment. Bioinformatics 8 2011;27:1086\u201393. Bottaro S, Di Palma F, and Bussi G. The Role of Nucleobase Interactions in RNA Structure and Dynamics. Nucleic acids research 2014;42. Capriotti E, Norambuena T, Marti-Renom MA, et al. All-atom knowledge-based potential for RNA structure prediction and assessment. Bioinformatics 2011;27:1086\u201393. Tan YL, Wang X, Shi YZ, et al. rsRNASP: A residue-separation-based statistical potential for RNA 3D structure evaluation. Biophysical Journal 1 2022;121:142\u201356. Parisien M, Cruz J, Westhof E, et al. New metrics for comparing and assessing discrepancies between RNA 3D structures and models. RNA (New York, N.Y.) 2009;15:1875\u201385. Zemla A, Venclovas C, Moult J, et al. Processing and analysis of CASP3 protein structure predictions. Proteins: Structure, Function, and Bioinformatics 1999;37:22\u20139. Olechnovic K, Kulberkyte E, and Venclovas C. CAD-score: A new contact area difference- based function for evaluation of protein structural models. Proteins 2013;81. Zhang Y and Skolnick J. Scoring function for automated assessment of protein structure template quality. Proteins 2004;57:702\u201310. Gong S, Zhang C, and Zhang Y. RNA-align: quick and accurate alignment of RNA 3D struc- tures based on size-independent TM-scoreRNA. Bioinformatics 21 2019;35:4459\u201361. Mariani V, Biasini M, Barbato A, et al. lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics (Oxford, Eng- land) 2013;29:2722\u20138. Zok T, Popenda M, and Szachniuk M. MCQ4Structures to compute similarity of molecule structures. Central European Journal of Operations Research 2013;22. Wiedemann J, Zok T, Milostan M, et al. LCS-TA to identify similar fragments in RNA 3D structures. BMC Bioinformatics 2017;18:456. 7</p>"},{"location":"pipeline/stageB/torsionbert_code/","title":"Code","text":"<p>Directory Structure:</p> <p>\u2514\u2500\u2500 ./     \u251c\u2500\u2500 src     \u2502   \u251c\u2500\u2500 enums     \u2502   \u2502   \u251c\u2500\u2500 init.py     \u2502   \u2502   \u2514\u2500\u2500 atoms.py     \u2502   \u251c\u2500\u2500 helper     \u2502   \u2502   \u251c\u2500\u2500 init.py     \u2502   \u2502   \u251c\u2500\u2500 computation_helper.py     \u2502   \u2502   \u251c\u2500\u2500 extractor_helper.py     \u2502   \u2502   \u2514\u2500\u2500 rna_torsionBERT_helper.py     \u2502   \u251c\u2500\u2500 metrics     \u2502   \u2502   \u251c\u2500\u2500 init.py     \u2502   \u2502   \u2514\u2500\u2500 mcq.py     \u2502   \u251c\u2500\u2500 init.py     \u2502   \u251c\u2500\u2500 rna_torsionBERT_cli.py     \u2502   \u251c\u2500\u2500 tb_mcq_cli.py     \u2502   \u2514\u2500\u2500 utils.py     \u2514\u2500\u2500 README.md</p>"},{"location":"pipeline/stageB/torsionbert_code/#file-srcenumsinitpy","title":"File: /src/enums/init.py","text":""},{"location":"pipeline/stageB/torsionbert_code/#file-srcenumsatomspy","title":"File: /src/enums/atoms.py","text":"<p>import numpy as np import os</p> <p>ALL_ATOMS = [     \"P\",     \"OP1\",     \"OP2\",     \"O5'\",     \"C5'\",     \"C4'\",     \"O4'\",     \"C3'\",     \"O3'\",     \"C1'\",     \"C2'\",     \"N1\",     \"C2\",     \"N9\",     \"C4\", ]</p> <p>ANGLES = {     \"alpha\": {\"atoms\": [\"O3'\", \"P\", \"O5'\", \"C5'\"], \"index\": [-1, 0, 0, 0]},     \"beta\": {\"atoms\": [\"P\", \"O5'\", \"C5'\", \"C4'\"], \"index\": [0, 0, 0, 0]},     \"gamma\": {\"atoms\": [\"O5'\", \"C5'\", \"C4'\", \"C3'\"], \"index\": [0, 0, 0, 0]},     \"delta\": {\"atoms\": [\"C5'\", \"C4'\", \"C3'\", \"O3'\"], \"index\": [0, 0, 0, 0]},     \"epsilon\": {\"atoms\": [\"C4'\", \"C3'\", \"O3'\", \"P\"], \"index\": [0, 0, 0, 1]},     \"zeta\": {\"atoms\": [\"C3'\", \"O3'\", \"P\", \"O5'\"], \"index\": [0, 0, 1, 1]},     \"chi\": {\"atoms\": [\"O4'\", \"C1'\", \"N1\", \"C2\"], \"index\": [0, 0, 0, 0]},     \"eta\": {\"atoms\": [\"C4'\", \"P\", \"C4'\", \"P\"], \"index\": [-1, 0, 0, 1]},     \"theta\": {\"atoms\": [\"P\", \"C4'\", \"P\", \"C4'\"], \"index\": [0, 0, 1, 1]},     \"eta'\": {\"atoms\": [\"C1'\", \"P\", \"C1'\", \"P\"], \"index\": [-1, 0, 0, 1]},     \"theta'\": {\"atoms\": [\"P\", \"C1'\", \"P\", \"C1'\"], \"index\": [0, 0, 1, 1]},     \"v0\": {\"atoms\": [\"C4'\", \"O4'\", \"C1'\", \"C2'\"], \"index\": [0, 0, 0, 0]},     \"v1\": {\"atoms\": [\"O4'\", \"C1'\", \"C2'\", \"C3'\"], \"index\": [0, 0, 0, 0]},     \"v2\": {\"atoms\": [\"C1'\", \"C2'\", \"C3'\", \"C4'\"], \"index\": [0, 0, 0, 0]},     \"v3\": {\"atoms\": [\"C2'\", \"C3'\", \"C4'\", \"O4'\"], \"index\": [0, 0, 0, 0]},     \"v4\": {\"atoms\": [\"C3'\", \"C4'\", \"O4'\", \"C1'\"], \"index\": [0, 0, 0, 0]}, }</p>"},{"location":"pipeline/stageB/torsionbert_code/#file-srchelperinitpy","title":"File: /src/helper/init.py","text":""},{"location":"pipeline/stageB/torsionbert_code/#file-srchelpercomputation_helperpy","title":"File: /src/helper/computation_helper.py","text":"<p>import numpy as np from typing import List</p> <p>from src.enums.atoms import ANGLES, ALL_ATOMS from src.utils import compute_torsion_angle</p> <p>class ComputationHelper:     def init(self, matrix: np.ndarray, sequence: str):         self.matrix = matrix         self.sequence = sequence</p> <pre><code>def compute_angles(self, angle_name: str) -&gt; List:\n    \"\"\"\n    Compute all the angles for the given structure.\n    :param angle_name: the angle to compute values from\n    :return: a list with the angle values\n    \"\"\"\n    c_angle_dict = ANGLES.get(angle_name, {})\n    atoms = c_angle_dict.get(\"atoms\", [])\n    atoms_position = [ALL_ATOMS.index(atom) for atom in atoms]\n    indexes = c_angle_dict.get(\"index\", [])\n    angles_out = []\n    for i, c_atoms in enumerate(self.matrix):\n        if angle_name == \"chi\" and self.sequence[i] in [\"A\", \"G\"]:\n            atoms_position = [\n                ALL_ATOMS.index(atom) for atom in [\"O4'\", \"C1'\", \"N9\", \"C4\"]\n            ]\n        if angle_name == \"chi\" and self.sequence[i] in [\"C\", \"U\"]:\n            atoms_position = [\n                ALL_ATOMS.index(atom) for atom in [\"O4'\", \"C1'\", \"N1\", \"C2\"]\n            ]\n        specific_atoms = [\n            self.matrix[i + offset, atom_pos]\n            for offset, atom_pos in zip(indexes, atoms_position)\n            if i + offset &lt; len(self.matrix) and i + offset &gt;= 0\n        ]\n        angle = (\n            compute_torsion_angle(*specific_atoms)\n            if len(specific_atoms) == 4\n            else np.nan\n        )\n        angles_out.append(angle)\n    return angles_out\n</code></pre>"},{"location":"pipeline/stageB/torsionbert_code/#file-srchelperextractor_helperpy","title":"File: /src/helper/extractor_helper.py","text":"<p>from typing import Dict, List, Optional import numpy as np</p> <p>import pandas as pd</p> <p>from src.enums.atoms import ALL_ATOMS, ANGLES from src.helper.computation_helper import ComputationHelper from src.utils import read_all_atoms, get_sequence</p> <p>class ExtractorHelper:     def init(self, all_atoms: List = ALL_ATOMS):         self.all_atoms = all_atoms</p> <pre><code>def extract_all(\n    self, in_pdb: str, save_to_path: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract all the torsional angles and bond angles from the pdb file\n    :param in_pdb: path to a .pdb file\n    :param save_to_path: path where to save the output\n    :return: a .csv file with the torsional and bond angles.\n    \"\"\"\n    all_atoms = read_all_atoms(in_pdb)\n    matrix = self.convert_atoms_to_matrix(all_atoms)\n    sequence = [element for element in get_sequence(in_pdb)]\n    computation_helper = ComputationHelper(matrix, sequence)\n    torsion_angles = {\n        angle: computation_helper.compute_angles(angle) for angle in ANGLES\n    }\n    sequence = [element for element in get_sequence(in_pdb)]\n    df = pd.DataFrame(\n        {**{\"sequence\": sequence}, **torsion_angles},\n        index=range(1, len(sequence) + 1),\n    )\n    if save_to_path:\n        df.to_csv(save_to_path)\n    return df\n\ndef convert_atoms_to_matrix(self, all_atoms: Dict) -&gt; np.ndarray:\n    \"\"\"\n    Convert the different atoms into a matrix of size (L, N, 3) where:\n        L: the number of nucleotides\n        N: the number of atoms per nucleotide\n        3: the x,y,z coordinates\n    :param all_atoms: list of atoms with their coordinates\n    :return: a np.array matrix\n    \"\"\"\n    output = np.nan * np.ones((len(all_atoms), len(self.all_atoms), 3))\n    for index, atoms in enumerate(all_atoms):\n        for atom in atoms:\n            if atom in self.all_atoms:\n                output[index, self.all_atoms.index(atom)] = np.array(atoms[atom])\n    return output\n</code></pre>"},{"location":"pipeline/stageB/torsionbert_code/#file-srchelperrna_torsionbert_helperpy","title":"File: /src/helper/rna_torsionBERT_helper.py","text":"<p>import transformers from transformers import AutoModel, AutoTokenizer import numpy as np import pandas as pd from typing import Optional, Dict import os</p> <p>os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"</p> <p>transformers.logging.set_verbosity_error()</p> <p>BACKBONE = [     \"alpha\",     \"beta\",     \"gamma\",     \"delta\",     \"epsilon\",     \"zeta\",     \"chi\",     \"eta\",     \"theta\",     \"eta'\",     \"theta'\",     \"v0\",     \"v1\",     \"v2\",     \"v3\",     \"v4\", ]</p> <p>class RNATorsionBERTHelper:     def init(self):         self.model_name = \"sayby/rna_torsionbert\"         self.tokenizer = AutoTokenizer.from_pretrained(             self.model_name, trust_remote_code=True         )         self.params_tokenizer = {             \"return_tensors\": \"pt\",             \"padding\": \"max_length\",             \"max_length\": 512,             \"truncation\": True,         }         self.model = AutoModel.from_pretrained(self.model_name, trust_remote_code=True)</p> <pre><code>def predict(self, sequence: str):\n    sequence_tok = self.convert_raw_sequence_to_k_mers(sequence)\n    inputs = self.tokenizer(sequence_tok, **self.params_tokenizer)\n    outputs = self.model(inputs)[\"logits\"]\n    outputs = self.convert_sin_cos_to_angles(\n        outputs.cpu().detach().numpy(), inputs[\"input_ids\"]\n    )\n    output_angles = self.convert_logits_to_dict(\n        outputs[0, :], inputs[\"input_ids\"][0, :].cpu().detach().numpy()\n    )\n    output_angles.index = list(sequence)[:-2]  # Because of the 3-mer representation\n    return output_angles\n\ndef convert_raw_sequence_to_k_mers(self, sequence: str, k_mers: int = 3):\n    \"\"\"\n    Convert a raw RNA sequence into sequence readable for the tokenizer.\n    It converts the sequence into k-mers, and replace U by T\n    :return: input readable by the tokenizer\n    \"\"\"\n    sequence = sequence.upper().replace(\"U\", \"T\")\n    k_mers_sequence = [\n        sequence[i : i + k_mers]\n        for i in range(len(sequence))\n        if len(sequence[i : i + k_mers]) == k_mers\n    ]\n    return \" \".join(k_mers_sequence)\n\ndef convert_sin_cos_to_angles(\n    self, output: np.ndarray, input_ids: Optional[np.ndarray] = None\n):\n    \"\"\"\n    Convert the raw predictions of the RNA-TorsionBERT into angles.\n    It converts the cos and sinus into angles using:\n        alpha = arctan(sin(alpha)/cos(alpha))\n    :param output: Dictionary with the predictions of the RNA-TorsionBERT per angle\n    :param input_ids: the input_ids of the RNA-TorsionBERT. It allows to only select the of the sequence,\n        and not the special tokens.\n    :return: a np.ndarray with the angles for the sequence\n    \"\"\"\n    if input_ids is not None:\n        output[\n            (input_ids == 0)\n            | (input_ids == 2)\n            | (input_ids == 3)\n            | (input_ids == 4)\n        ] = np.nan\n    pair_indexes, impair_indexes = np.arange(0, output.shape[-1], 2), np.arange(\n        1, output.shape[-1], 2\n    )\n    sin, cos = output[:, :, impair_indexes], output[:, :, pair_indexes]\n    tan = np.arctan2(sin, cos)\n    angles = np.degrees(tan)\n    return angles\n\ndef convert_logits_to_dict(self, output: np.ndarray, input_ids: np.ndarray) -&gt; Dict:\n    \"\"\"\n    Convert the raw predictions into dictionary format.\n    It removes the special tokens and only keeps the predictions for the sequence.\n    :param output: predictions from the models in angles\n    :param input_ids: input ids from the tokenizer\n    :return: a dictionary with the predictions for each angle\n    \"\"\"\n    index_start, index_end = (\n        np.where(input_ids == 2)[0][0],\n        np.where(input_ids == 3)[0][0],\n    )\n    output_non_pad = output[index_start + 1 : index_end, :]\n    output_angles = {\n        angle: output_non_pad[:, angle_index]\n        for angle_index, angle in enumerate(BACKBONE)\n    }\n    out = pd.DataFrame(output_angles)\n    return out\n</code></pre>"},{"location":"pipeline/stageB/torsionbert_code/#file-srcmetricsinitpy","title":"File: /src/metrics/init.py","text":""},{"location":"pipeline/stageB/torsionbert_code/#file-srcmetricsmcqpy","title":"File: /src/metrics/mcq.py","text":"<p>from typing import Union, List import numpy as np import pandas as pd</p> <p>import warnings</p> <p>warnings.filterwarnings(\"ignore\")</p> <p>TORSION_TO_ANGLES = {     \"BACKBONE\": [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"chi\", \"phase\"],     \"PSEUDO\": [\"eta\", \"theta\"], }</p> <p>class MCQ:     \"\"\"     MCQ Helper. Reproduce the MCQ computation from https://github.com/tzok/mcq4structures.     \"\"\"</p> <pre><code>def mod(self, values: Union[np.ndarray, float]) -&gt; Union[np.ndarray, float]:\n    \"\"\"\n    Compute the mod(t) = t + 2pi % 2pi\n    Computation is done in degree\n    :param values:\n    :return:\n    \"\"\"\n    return (values + 360) % 360\n\ndef difference(self, x: float, y: float):\n    \"\"\"\n    Compute the distance between two angles\n    :param x: the true angle\n    :param x: the predicted atom\n    :return: the distance based on MCQ computation\n    \"\"\"\n    if np.isnan(x) and np.isnan(y):\n        return 0\n    elif np.isnan(x) or np.isnan(y):\n        return 180\n    else:\n        return min(\n            abs(self.mod(x) - self.mod(y)), 360 - abs(self.mod(x) - self.mod(y))\n        )\n\ndef get_phase(self, values: np.ndarray):\n    \"\"\"\n    Compute the phase P = arctan(v1 + v4 - v0 - v3, 2v2(sin 36 + sin72))\n    :param values:\n    :return:\n    \"\"\"\n    try:\n        riboses = np.radians(values[[\"v0\", \"v1\", \"v2\", \"v3\", \"v4\"]])\n        P = np.arctan(\n            riboses[\"v1\"] + riboses[\"v4\"] - riboses[\"v0\"] - riboses[\"v3\"],\n            2 * riboses[\"v2\"] * (np.sin(np.radians(36)) + np.sin(np.radians(72))),\n        )\n    except KeyError:\n        P = np.nan\n    return P\n\ndef compute_mcq(\n    self,\n    true_values: np.ndarray,\n    pred_values: np.ndarray,\n    torsion: str = \"BACKBONE\",\n):\n    \"\"\"\n    Compute the MCQ between two sets of angles.\n    :param true_values: experimental inferred angles from the native structure\n    :param pred_values: predicted angles\n    :param torsion: the type of angles to use. Default to BACKBONE.\n    :return:\n    \"\"\"\n    diff, angles = self._get_diff_angles(true_values, pred_values, torsion)\n    sin_mod, cos_mod = (\n        np.sin(np.radians(diff)).sum(),\n        np.cos(np.radians(diff)).sum(),\n    )\n    mcq = np.arctan2(sin_mod, cos_mod)\n    mcq = np.degrees(mcq)\n    return mcq\n\ndef get_current_angles(self, torsion: str, pred_cols: List):\n    \"\"\"\n    Get the current angles values from all the angles available.\n    \"\"\"\n    angles = []\n    for angle in TORSION_TO_ANGLES[torsion]:\n        if angle == \"phase\" and \"v0\" in pred_cols:\n            angles.append(angle)\n        elif angle in pred_cols:\n            angles.append(angle)\n    return angles\n\ndef _get_diff_angles(\n    self, true_values: pd.DataFrame, pred_values: pd.DataFrame, torsion: str\n):\n    \"\"\"\n    Compute the differences to be used for the MCQ computation\n    :return:\n    \"\"\"\n    if len(pred_values) &lt; len(true_values):\n        true_values = true_values[: len(pred_values)]\n    else:\n        pred_values = pred_values[: len(true_values)]\n    angles = self.get_current_angles(torsion, pred_values.columns)\n    if torsion == \"BACKBONE\":\n        phase_true = np.degrees(self.get_phase(true_values))\n        phase_pred = np.degrees(self.get_phase(pred_values))\n        true_values[\"phase\"] = phase_true\n        pred_values[\"phase\"] = phase_pred\n    true_angles = true_values[angles].values\n    pred_angles = pred_values[angles].values\n    diff_fn = np.vectorize(self.difference)\n    diff = diff_fn(true_angles, pred_angles)\n    return diff, angles\n\ndef compute_mcq_per_angle(self, true_values, pred_values, torsion: str):\n    \"\"\"\n    Compute the MCQ for a given angle.\n    :param true_values: experimental inferred angles from the native structure\n    :param pred_values: predicted angles\n    :param torsion: the type of angles to use. Default to BACKBONE.\n    :return: MCQ per angle\n    \"\"\"\n    diff, angles = self._get_diff_angles(true_values, pred_values, torsion)\n    sin_mod, cos_mod = np.sin(np.radians(diff)).sum(axis=0), np.cos(\n        np.radians(diff)\n    ).sum(axis=0)\n    mcq = np.arctan2(sin_mod, cos_mod)\n    mcq = np.degrees(mcq)\n    output = {angle: mcq[i] for i, angle in enumerate(angles)}\n    return output\n\ndef compute_mcq_per_sequence(self, true_values, pred_values, torsion: str):\n    \"\"\"\n    Compute the MCQ for a given position.\n    :return:\n    \"\"\"\n    diff, angles = self._get_diff_angles(true_values, pred_values, torsion)\n    sin_mod, cos_mod = np.sin(np.radians(diff)).sum(axis=1), np.cos(\n        np.radians(diff)\n    ).sum(axis=1)\n    mcq = np.arctan2(sin_mod, cos_mod)\n    mcq = np.degrees(mcq)\n    return mcq.tolist()\n</code></pre>"},{"location":"pipeline/stageB/torsionbert_code/#file-srcinitpy","title":"File: /src/init.py","text":""},{"location":"pipeline/stageB/torsionbert_code/#file-srcrna_torsionbert_clipy","title":"File: /src/rna_torsionBERT_cli.py","text":"<p>import argparse from typing import Optional</p> <p>from src.helper.rna_torsionBERT_helper import RNATorsionBERTHelper from src.utils import read_fasta from loguru import logger</p> <p>class RNATorsionBERTCLI:     def init(         self,         in_seq: Optional[str],         in_fasta: Optional[str],         out_path: Optional[str],         args,         *kwargs,     ):         self.sequence = self._init_inputs(in_seq, in_fasta)         self.out_path = out_path</p> <pre><code>def _init_inputs(self, in_seq: Optional[str], in_fasta: Optional[str]) -&gt; str:\n    \"\"\"\n    Initialise the inputs given the arguments\n    :return: the sequence\n    \"\"\"\n    if in_seq is None and in_fasta is None:\n        raise ValueError(\"You must provide either a sequence or a fasta file.\")\n    if in_seq is not None and in_fasta is not None:\n        raise ValueError(\n            \"Please provide only the sequence or the fasta file, not both.\"\n        )\n    if in_seq is not None:\n        sequence = in_seq\n    elif in_fasta is not None:\n        sequence = read_fasta(in_fasta)\n    return sequence\n\ndef run(self):\n    output = RNATorsionBERTHelper().predict(self.sequence)\n    if self.out_path is not None:\n        output.to_csv(self.out_path)\n        logger.info(f\"Saved the output to {self.out_path}\")\n    return output\n\n@staticmethod\ndef get_args():\n    parser = argparse.ArgumentParser(\n        description=\"Prediction of Torsional angles for RNA structures\"\n    )\n    # Add command line arguments\n    parser.add_argument(\n        \"--in_seq\",\n        dest=\"in_seq\",\n        type=str,\n        help=\"RNA Input sequence.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--in_fasta\",\n        dest=\"in_fasta\",\n        type=str,\n        help=\"Path to a fasta file.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--out_path\",\n        dest=\"out_path\",\n        type=str,\n        help=\"Path to a .csv file to save the prediction\",\n        default=None,\n    )\n    # Parse the command line arguments\n    args = parser.parse_args()\n    return args\n</code></pre> <p>if name == \"main\":     args = RNATorsionBERTCLI.get_args()     rna_torsionBERT_cli = RNATorsionBERTCLI(**vars(args))     rna_torsionBERT_cli.run()</p>"},{"location":"pipeline/stageB/torsionbert_code/#file-srctb_mcq_clipy","title":"File: /src/tb_mcq_cli.py","text":"<p>import argparse import os</p> <p>from loguru import logger import tqdm import pandas as pd from typing import Optional, List</p> <p>from src.helper.extractor_helper import ExtractorHelper from src.helper.rna_torsionBERT_helper import RNATorsionBERTHelper from src.metrics.mcq import MCQ</p> <p>class TBMCQCLI:     def init(self, in_pdb: str, out_path: Optional[str], args, *kwargs):         self.list_files = self._init_pdb(in_pdb)         self.out_path = out_path</p> <pre><code>def _init_pdb(self, in_pdb: Optional[str]) -&gt; List:\n    \"\"\"\n    Initialise the inputs structures.\n    :param in_pdb: a path to either a .pdb file or a directory of .pdb files\n    :return: a list of path to .pdb files\n    \"\"\"\n    if os.path.isdir(in_pdb):\n        list_files = os.listdir(in_pdb)\n        list_files = [os.path.join(in_pdb, file_) for file_ in list_files]\n    elif os.path.isfile(in_pdb):\n        list_files = [in_pdb]\n    else:\n        logger.info(f\"NO INPUTS FOUND FOR INPUT .PDB: {in_pdb}\")\n    return list_files\n\ndef run(self):\n    all_scores = {\"RNA\": [], \"TB-MCQ\": []}\n    for in_path in tqdm.tqdm(self.list_files):\n        score = self.compute_tb_mcq(in_path)\n        all_scores[\"RNA\"].append(os.path.basename(in_path))\n        all_scores[\"TB-MCQ\"].append(score)\n    all_scores = pd.DataFrame(all_scores, columns=[\"TB-MCQ\", \"RNA\"]).set_index(\n        \"RNA\"\n    )\n    if self.out_path is not None:\n        logger.info(f\"Saved the output to {self.out_path}\")\n        all_scores.to_csv(self.out_path, index=True)\n    return all_scores\n\ndef compute_tb_mcq(self, pred_path: str) -&gt; float:\n    \"\"\"\n    Compute the TB-MCQ with RNA-TorsionBERT model\n    :param pred_path: the path to the .pdb file of a prediction.\n            It could be a native or a predicted structure.\n    \"\"\"\n    experimental_angles = ExtractorHelper().extract_all(pred_path)\n    sequence = \"\".join(experimental_angles[\"sequence\"].values)\n    torsionBERT_helper = RNATorsionBERTHelper()\n    torsionBERT_output = torsionBERT_helper.predict(sequence)\n    mcq = MCQ().compute_mcq(experimental_angles, torsionBERT_output)\n    return mcq\n\n@staticmethod\ndef get_args():\n    parser = argparse.ArgumentParser(\n        description=\"Prediction of Torsional angles for RNA structures\"\n    )\n    # Add command line arguments\n    parser.add_argument(\n        \"--in_pdb\",\n        dest=\"in_pdb\",\n        type=str,\n        help=\"Path a .pdb file or a directory of .pdb files.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--out_path\",\n        dest=\"out_path\",\n        type=str,\n        help=\"Path to a .csv file to save the predictions.\",\n        default=None,\n    )\n    # Parse the command line arguments\n    args = parser.parse_args()\n    return args\n</code></pre> <p>if name == \"main\":     args = TBMCQCLI.get_args()     tb_mcq_cli = TBMCQCLI(**vars(args))     tb_mcq_cli.run()</p>"},{"location":"pipeline/stageB/torsionbert_code/#file-srcutilspy","title":"File: /src/utils.py","text":"<p>from typing import Any, List, Dict from Bio.PDB import Atom, Model, Chain, Residue, Structure, PDBIO import Bio import numpy as np from Bio.PDB import Atom, Residue, PDBParser import warnings</p> <p>warnings.filterwarnings(\"ignore\")</p> <p>def read_fasta(in_path: str) -&gt; str:     \"\"\"     Read a fasta file to get the sequence     :param in_path: path to a .fasta file     :return: the RNA sequence     \"\"\"     with open(in_path, \"r\") as f:         lines = f.readlines()     sequence = \"\".join([line.strip() for line in lines[1:]])     return sequence</p> <p>def read_all_atoms(in_pdb: str) -&gt; Any:     \"\"\"     Read and return the coordinates of all the  atoms from a pdb file     :param in_pdb: path to a .pdb file     \"\"\"     parser = PDBParser()     all_atoms = []     structure = parser.get_structure(\"\", in_pdb)     for model in structure:         for chain in model:             for residue in chain:                 res = residue.get_resname().replace(\" \", \"\")                 if res in [\"A\", \"C\", \"G\", \"U\"]:                     atoms = get_atoms_torsion(residue)                     c_atom = {                         atom.get_name(): atom.get_coord().tolist() for atom in atoms                     }                     all_atoms.append(c_atom)     return all_atoms</p> <p>def get_atoms_torsion(residue: Bio.PDB.Residue.Residue):     \"\"\"     Return the atoms coordinates for a given residue.     :param residue: the residue to get the atoms from     \"\"\"     atoms = []     for atom in residue:         atoms.append(atom)     return atoms</p> <p>def compute_torsion_angle(     atom1: np.ndarray, atom2: np.ndarray, atom3: np.ndarray, atom4: np.ndarray ) -&gt; float:     \"\"\"     Compute torsional angles between 4 atoms     :return: the torsional angles between the atoms     \"\"\"     v12 = atom1 - atom2     v23 = atom2 - atom3     v34 = atom3 - atom4     e1 = np.cross(v12, v23)     e2 = np.cross(v23, v34)     sign = +1 if np.dot(v23, np.cross(e1, e2)) &lt; 0 else -1     angle_in_radians = np.arccos(         np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2))     )     angle_in_degrees = sign * np.degrees(angle_in_radians)     return angle_in_degrees</p> <p>def get_sequence(in_pdb: str) -&gt; str:     \"\"\"     Return the RNA sequence from a .pdb file     :param in_pdb: path to a pdb file     :return: RNA sequence of nucleotides     \"\"\"     parser = PDBParser()     structure = parser.get_structure(\"structure\", in_pdb)     sequence = \"\"     for model in structure:         for chain in model:             for residue in chain:                 res = residue.get_resname().replace(\" \", \"\")                 if res in [\"A\", \"C\", \"G\", \"U\"]:                     sequence += res     return sequence</p>"},{"location":"pipeline/stageB/torsionbert_code/#file-readmemd","title":"File: /README.md","text":""},{"location":"pipeline/stageB/torsionbert_code/#rna-torsionbert","title":"RNA-TorsionBERT","text":"<p><code>RNA-TorsionBERT</code> is a 86.9 MB parameter BERT-based language model that predicts RNA torsional and pseudo-torsional angles from the sequence.</p> <p></p> <p><code>RNA-TorsionBERT</code> is a DNABERT model that was pre-trained on ~4200 RNA structures.</p> <p>It provides improvement of MCQ over the previous state-of-the-art models like  SPOT-RNA-1D or inferred angles from existing methods, on the Test Set (composed of RNA-Puzzles and CASP-RNA).</p>"},{"location":"pipeline/stageB/torsionbert_code/#installation","title":"Installation","text":"<p>To install RNA-TorsionBERT and it's dependencies following commands can be used in terminal:</p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"pipeline/stageB/torsionbert_code/#rna-torsionbert-usage","title":"RNA-TorsionBERT usage","text":"<p>To run the RNA-TorsionBERT, you can use the following command line: <pre><code>python -m src.rna_torsionBERT_cli [--seq_file] [--in_fasta] [--out_path]\n</code></pre></p> <p>The arguments are the following: - <code>--seq_file</code>: RNA Sequence.  - <code>--in_fasta</code>: Path to the input sequence fasta file.  - <code>--out_path</code>: Path to a <code>.csv</code> file where the output will be saved. </p> <p>You can also import in your python code the class <code>RNATorsionBERTCLI</code> from <code>src.rna_torsionBERT_cli</code>. </p>"},{"location":"pipeline/stageB/torsionbert_code/#tb-mcq","title":"TB-MCQ","text":"<p>TB-MCQ stands for TorsionBERT-MCQ, which is a scoring function to assess the quality of a predicted structure in torsional angle space. Given the inferred angles from the structures and the predicted angles from the model, TB-MCQ computes the quality of the predicted angles using  the MCQ (mean of circular quantities) metric.</p> <p></p> <p>To run the TB-MCQ scoring function, you can use the following command line: <pre><code>python -m src.rna_torsion_cli [--in_pdb] [--out_path]\n</code></pre> with:</p> <ul> <li><code>--in_pdb</code>: Path to the input PDB file.</li> <li><code>--out_path</code>: Path to a .csv file where the output will be saved.</li> </ul>"},{"location":"pipeline/stageB/torsionbert_code/#docker","title":"Docker","text":"<p>To run the code using <code>Docker</code>, you can use the following command line: <pre><code>docker build -t rna_torsionbert .\ndocker run -it rna_torsionbert \n</code></pre></p> <p>It will enter into a bash console where you could execute the previous commands with all the installations done. </p> <p>To have example of commands, you can look at the <code>Makefile</code>.</p>"},{"location":"pipeline/stageB/torsionbert_code/#citation","title":"Citation","text":"<pre><code>@article{rna_torsion_bert,\n    author = {Bernard, Cl\u00e9ment and Postic, Guillaume and Ghannay, Sahar and Tahi, Fariza},\n    title = {RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction},\n    journal = {Bioinformatics},\n    volume = {41},\n    number = {1},\n    pages = {btaf004},\n    year = {2025},\n    month = {01},\n    issn = {1367-4811},\n    doi = {10.1093/bioinformatics/btaf004},\n    url = {https://doi.org/10.1093/bioinformatics/btaf004},\n    eprint = {https://academic.oup.com/bioinformatics/article-pdf/41/1/btaf004/61381586/btaf004.pdf},\n}\n</code></pre> <p>====</p> <p>Hugging Face's logo Hugging Face</p> <p>Models Datasets Spaces Docs Enterprise Pricing</p> <p>sayby / rna_torsionBERT Token Classification Transformers PyTorch Safetensors rna_torsionbert feature-extraction biology RNA Torsional Angles custom_code Model card Files Community</p> <p>RNA-TorsionBERT Model Description</p> <p>RNA-TorsionBERT is a 86.9 MB parameter BERT-based language model that predicts RNA torsional and pseudo-torsional angles from the sequence.</p> <p>RNA-TorsionBERT is a DNABERT model that was pre-trained on ~4200 RNA structures.</p> <p>It provides improvement of MCQ over the previous state-of-the-art models like SPOT-RNA-1D or inferred angles from existing methods, on the Test Set (composed of RNA-Puzzles and CASP-RNA).</p> <p>Key Features</p> <pre><code>Torsional and Pseudo-torsional angles prediction\nPredict sequences up to 512 nucleotides\n</code></pre> <p>Usage</p> <p>Get started generating text with RNA-TorsionBERT by using the following code snippet:</p> <p>from transformers import AutoModel, AutoTokenizer</p> <p>tokenizer = AutoTokenizer.from_pretrained(\"sayby/rna_torsionbert\", trust_remote_code=True) model = AutoModel.from_pretrained(\"sayby/rna_torsionbert\", trust_remote_code=True)</p> <p>sequence = \"ACG CGG GGT GTT\" params_tokenizer = {     \"return_tensors\": \"pt\",     \"padding\": \"max_length\",     \"max_length\": 512,     \"truncation\": True, } inputs = tokenizer(sequence, **params_tokenizer) output = model(inputs)[\"logits\"]</p> <pre><code>Please note that it was fine-tuned from a DNABERT-3 model and therefore the tokenizer is the same as the one used for DNABERT. Nucleotide U should therefore be replaced by T in the input sequence.\nThe output is the sinus and the cosine for each angle. The angles are in the following order: alpha, beta,gamma,delta,epsilon,zeta,chi,eta,theta,eta',theta',v0,v1,v2,v3,v4.\n</code></pre> <p>To convert the predictions into angles, you can use the following code snippet:</p> <p>import transformers from transformers import AutoModel, AutoTokenizer import numpy as np import pandas as pd from typing import Optional, Dict import os</p> <p>os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"</p> <p>transformers.logging.set_verbosity_error()</p> <p>BACKBONE = [     \"alpha\",     \"beta\",     \"gamma\",     \"delta\",     \"epsilon\",     \"zeta\",     \"chi\",     \"eta\",     \"theta\",     \"eta'\",     \"theta'\",     \"v0\",     \"v1\",     \"v2\",     \"v3\",     \"v4\", ]</p> <p>class RNATorsionBERTHelper:     def init(self):         self.model_name = \"sayby/rna_torsionbert\"         self.tokenizer = AutoTokenizer.from_pretrained(             self.model_name, trust_remote_code=True         )         self.params_tokenizer = {             \"return_tensors\": \"pt\",             \"padding\": \"max_length\",             \"max_length\": 512,             \"truncation\": True,         }         self.model = AutoModel.from_pretrained(self.model_name, trust_remote_code=True)</p> <pre><code>def predict(self, sequence: str):\n    sequence_tok = self.convert_raw_sequence_to_k_mers(sequence)\n    inputs = self.tokenizer(sequence_tok, **self.params_tokenizer)\n    outputs = self.model(inputs)[\"logits\"]\n    outputs = self.convert_sin_cos_to_angles(\n        outputs.cpu().detach().numpy(), inputs[\"input_ids\"]\n    )\n    output_angles = self.convert_logits_to_dict(\n        outputs[0, :], inputs[\"input_ids\"][0, :].cpu().detach().numpy()\n    )\n    output_angles.index = list(sequence)[:-2]  # Because of the 3-mer representation\n    return output_angles\n\ndef convert_raw_sequence_to_k_mers(self, sequence: str, k_mers: int = 3):\n    \"\"\"\n    Convert a raw RNA sequence into sequence readable for the tokenizer.\n    It converts the sequence into k-mers, and replace U by T\n    :return: input readable by the tokenizer\n    \"\"\"\n    sequence = sequence.upper().replace(\"U\", \"T\")\n    k_mers_sequence = [\n        sequence[i : i + k_mers]\n        for i in range(len(sequence))\n        if len(sequence[i : i + k_mers]) == k_mers\n    ]\n    return \" \".join(k_mers_sequence)\n\ndef convert_sin_cos_to_angles(\n    self, output: np.ndarray, input_ids: Optional[np.ndarray] = None\n):\n    \"\"\"\n    Convert the raw predictions of the RNA-TorsionBERT into angles.\n    It converts the cos and sinus into angles using:\n        alpha = arctan(sin(alpha)/cos(alpha))\n    :param output: Dictionary with the predictions of the RNA-TorsionBERT per angle\n    :param input_ids: the input_ids of the RNA-TorsionBERT. It allows to only select the of the sequence,\n        and not the special tokens.\n    :return: a np.ndarray with the angles for the sequence\n    \"\"\"\n    if input_ids is not None:\n        output[\n            (input_ids == 0)\n            | (input_ids == 2)\n            | (input_ids == 3)\n            | (input_ids == 4)\n        ] = np.nan\n    pair_indexes, impair_indexes = np.arange(0, output.shape[-1], 2), np.arange(\n        1, output.shape[-1], 2\n    )\n    sin, cos = output[:, :, impair_indexes], output[:, :, pair_indexes]\n    tan = np.arctan2(sin, cos)\n    angles = np.degrees(tan)\n    return angles\n\ndef convert_logits_to_dict(self, output: np.ndarray, input_ids: np.ndarray) -&gt; Dict:\n    \"\"\"\n    Convert the raw predictions into dictionary format.\n    It removes the special tokens and only keeps the predictions for the sequence.\n    :param output: predictions from the models in angles\n    :param input_ids: input ids from the tokenizer\n    :return: a dictionary with the predictions for each angle\n    \"\"\"\n    index_start, index_end = (\n        np.where(input_ids == 2)[0][0],\n        np.where(input_ids == 3)[0][0],\n    )\n    output_non_pad = output[index_start + 1 : index_end, :]\n    output_angles = {\n        angle: output_non_pad[:, angle_index]\n        for angle_index, angle in enumerate(BACKBONE)\n    }\n    out = pd.DataFrame(output_angles)\n    return out\n</code></pre> <p>if name == \"main\":     sequence = \"AGGGCUUUAGUCUUUGGAG\"     rna_torsionbert_helper = RNATorsionBERTHelper()     output_angles = rna_torsionbert_helper.predict(sequence)     print(output_angles)</p> <p>Downloads last month     988 </p> <p>Safetensors Model size 86.9M params Tensor type F32 Inference Providers NEW Token Classification This model is not currently available via any of the supported Inference Providers. The model cannot be deployed to the HF Inference API: The HF Inference API does not support model that require custom code execution. Model tree for sayby/rna_torsionBERT</p> <p>Base model zhihan1996/DNA_bert_3 Finetuned (1) this model TOS Privacy About Jobs Models Datasets Spaces Pricing Docs</p>"},{"location":"pipeline/stageC/Integrated_RNA_Geometry_and_3D_Reconstruction/","title":"Geometry & Reconstruction","text":"<p>Below is a comprehensive, integrated guide that merges the strengths of all previous \u201cversions\u201d while addressing their weaknesses, aiming for a thorough, practical reference. It explains how to leverage DSSR/3DNA data and standard RNA covalent geometry to construct RNA 3D structures from torsion angles (e.g., an \u201cRNA mp-nerf\u201d), plus it provides extended reference tables, discussion of environmental variation, and a roadmap of relevant Python software. The result should be suitable for technical documentation\u2014both verbose and practical.</p> <p>\u2e3b</p> <ol> <li>Introduction and Scope</li> </ol> <p>Building RNA 3D coordinates from torsion angles requires two main ingredients:     1.  A library of standard RNA bond lengths, bond angles, and ring-closure constraints (i.e., the geometry of the phosphate\u2013ribose\u2013base).     2.  A forward-kinematics or NeRF (Natural Extension Reference Frame) style algorithm that places atoms in 3D given internal coordinates (bond length l, bond angle \\theta, and torsion \\chi).</p> <p>DSSR (Dissecting the Spatial Structure of RNA) from the 3DNA suite is analysis-focused: given a 3D RNA, it extracts torsions and structural annotations. While it does not natively do a \u201cpure torsion\u2192Cartesian rebuild,\u201d it encodes or references standard nucleic-acid geometry (in its \u201cfiber\u201d or \u201crebuild\u201d modules) that you can adopt. Meanwhile, mp-nerf (originally for proteins) can be extended from \u201camino acids\u201d to \u201cnucleotides\u201d by replacing the protein geometry knowledge base with RNA data.</p> <p>This document:     1.  Presents standard RNA bond lengths (Table 1) and bond angles (Table 2).     2.  Explains environmental factors that cause real-world deviations from these ideals.     3.  Enumerates Python-based tools that help measure, refine, and validate RNA geometry.     4.  Outlines how to adapt mp-nerf (or a NeRF pipeline) for an RNA \u201ctorsion \u2192 3D\u201d process, referencing DSSR/3DNA data as needed.</p> <p>\u2e3b</p> <ol> <li>Standard RNA Bond Lengths</li> </ol> <p>Below is a detailed table of average bond lengths (in \u00c5) for an RNA nucleotide. These values come from surveys of high-resolution crystal structures (e.g. in the Nucleic Acid Database (NDB), the Nucleic Acid Knowledge Base (NAKB), and small-molecule data from the Cambridge Structural Database (CSD)). Each has a small standard deviation, typically \u00b10.01\u20130.02\u202f\u00c5.</p> <p>Table 1: Approximate Bond Lengths in RNA</p> <p>Bond    Typical Value (\u00c5)   Comment / Notes Ribose Ring     5-membered ring: C1\u2032\u2013C2\u2032\u2013C3\u2032\u2013C4\u2032\u2013O4\u2032 C1\u2032\u2013C2\u2032 ~1.52   Sugar ring C\u2013C bond (ribose) C2\u2032\u2013C3\u2032 ~1.52   Sugar ring C\u2013C bond C3\u2032\u2013C4\u2032 ~1.52   Sugar ring C\u2013C bond C4\u2032\u2013O4\u2032 ~1.45   Sugar ring C\u2013O O4\u2032\u2013C1\u2032 ~1.41   Sugar ring O\u2013C Exocyclic Bonds    C5\u2032\u2013C4\u2032 ~1.51   Exocyclic from ring to the 5\u2032 carbon C3\u2032\u2013O3\u2032 ~1.42   3\u2032-oxygen exocyclic (bridging to phosphate) C2\u2032\u2013O2\u2032 ~1.41   2\u2032-hydroxyl in RNA (absent in DNA) Glycosidic Bond     Links base ring to the sugar C1\u2032\u2013N1 (pyrimidines) /C1\u2032\u2013N9 (purines)  ~1.47   \u201cN\u2013C1\u2032\u201d is the glycosidic bond; base identity affects small variations (\u00b10.01 \u00c5) Phosphate Linkages      Tetrahedral geometry at P P\u2013O5\u2032   ~1.59   Phosphate bridging to 5\u2032-O P\u2013O3\u2032   ~1.60   Phosphate bridging to 3\u2032-O P\u2013O(non-bridging)   ~1.48   The two non-bridging oxygens (OP1/OP2) Additional     O5\u2032\u2013C5\u2032 ~1.44   5\u2032-oxygen to 5\u2032-carbon (sugar\u2013phosphate junction) O3\u2032\u2013C3\u2032 ~1.43   3\u2032-oxygen to 3\u2032-carbon (sugar\u2013phosphate junction)</p> <p>Note: DNA is very similar except it lacks the 2\u2032-OH (C2\u2032\u2013O2\u2032 bond). The presence of 2\u2032-OH slightly shifts ring conformations and can alter bond lengths by ~0.005\u202f\u00c5 relative to deoxyribose.</p> <p>\u2e3b</p> <ol> <li>Standard RNA Bond Angles</li> </ol> <p>Bond angles around the sugar, exocyclic substituents, and phosphate typically reflect sp\u00b3-hybridization (\u2248109.5\u00b0) but deviate due to ring strain and resonance. The phosphate group has angles from ~105\u00b0 to ~111\u00b0 with bridging vs. non-bridging oxygens, and ~120\u00b0 between the two non-bridging oxygens.</p> <p>Table 2: Approximate Bond Angles in RNA</p> <p>Angle   Typical Value (\u00b0)   Description / Position Within Ribose Ring      5-membered ring angles often &lt; 109.5\u00b0 due to ring strain C1\u2032\u2013C2\u2032\u2013C3\u2032 ~101\u2013102    Interior angle at C2\u2032 C2\u2032\u2013C3\u2032\u2013C4\u2032 ~102\u2013103    Interior angle at C3\u2032 C3\u2032\u2013C4\u2032\u2013O4\u2032 ~105\u2013106    Ring angle at C4\u2032 C4\u2032\u2013O4\u2032\u2013C1\u2032 ~109\u2013110    Ring angle at O4\u2032 O4\u2032\u2013C1\u2032\u2013C2\u2032 ~106    Angle at C1\u2032 Exocyclic (Sugar)      C5\u2032\u2013C4\u2032\u2013C3\u2032 ~115    5\u2032 exocyclic angle at C4\u2032 Phosphate Region        Tetrahedral-like angles around P O5\u2032\u2013P\u2013O3\u2032   ~105\u2013110    Bridging O vs bridging O angle (~109 typical) O5\u2032\u2013P\u2013O(non-bridging)   105\u2013111 / 107\u2013110   Variation for bridging vs non-bridging O O1P\u2013P\u2013O2P (non-bridging)    ~120    Angle between the two non-bridging oxygens C3\u2032\u2013O3\u2032\u2013P   ~118\u2013120    Exocyclic angle bridging sugar to phosphate Glycosidic Region       The angle (\u03c7) is typically measured as a dihedral, but local bond angles are ~110\u2013120 O4\u2032\u2013C1\u2032\u2013N (glycosidic)  ~108\u2013112    e.g., O4\u2032\u2013C1\u2032\u2013N1 for pyrimidines, O4\u2032\u2013C1\u2032\u2013N9 for purines</p> <p>Sugar pucker modifies these angles. RNA frequently has C3\u2032-endo pucker (A-form), but local C2\u2032-endo or mixed puckers cause slight shifts of ~1\u20132\u00b0 or ~0.01\u20130.02\u202f\u00c5 in ring bond lengths.</p> <p>\u2e3b</p> <ol> <li>Real-World Variation and Influences</li> </ol> <p>Though \u201cstandard\u201d values are accurate references, real RNA structures deviate due to:     1.  Sugar Pucker Changes     \u2022   RNA is often in C3\u2032-endo (A-form). However, local C2\u2032-endo or \u201cpseudorotation\u201d angles can cause ring angles to shift by a few degrees.     2.  Base Pairing &amp; Stacking     \u2022   Hydrogen bonds (e.g. Watson\u2013Crick, noncanonical pairs) can slightly elongate carbonyl or exocyclic N\u2013C bonds by ~0.003\u20130.01\u202f\u00c5.     3.  Metal Ion Coordination     \u2022   Mg\u00b2\u207a (common in functional RNAs) can compress or slightly distort phosphate O\u2013P\u2013O angles.     4.  Thermal Fluctuations in MD     \u2022   Molecular dynamics simulations at room temperature show bond length fluctuations of ~0.01\u20130.02\u202f\u00c5 from their equilibrium.     5.  Chemical Modification     \u2022   Methylations or protonation of bases can shift bond orders, altering standard lengths by a few hundredths of an \u00c5.</p> <p>If you see discrepancies &gt; ~0.03\u202f\u00c5 or &gt; ~3\u20135\u00b0 from these standard references, it might indicate either unusual strain, special modifications, or experimental error (like poor crystallographic resolution).</p> <p>\u2e3b</p> <ol> <li>Python-Based Tools and Resources</li> </ol> <p>Below is a broader set of tools to measure, refine, or even build RNA with standard geometry.     1.  Barnaba     \u2022   Python library for analyzing nucleic-acid structures (PDB or MD trajectories).     \u2022   Computes base-pair interactions, torsion angles, sugar puckers, etc.     \u2022   Good for validating or comparing your newly built RNA structure\u2019s angles/lengths to real data.     2.  MDAnalysis / MDTraj     \u2022   Popular Python libraries for reading/writing MD trajectories.     \u2022   Provide distance, angle, dihedral computations.     \u2022   They do not themselves enforce standard geometry but help confirm it.     3.  3DNA / DSSR     \u2022   Written in C/C++. x3dna-dssr is typically used via CLI, but you can script calls from Python.     \u2022   DSSR extracts RNA structural parameters: base pairs, backbone torsions (\\alpha,\\beta,\\gamma,\\dots), sugar puckers, etc.     \u2022   The older 3DNA \u201crebuild\u201d or \u201cfiber\u201d subprogram can generate ideal duplex forms (A-form) or do partial reconstruction from base-pair parameters.     \u2022   Not purely torsion\u2192Cartesian for single-stranded loops (that is precisely where a NeRF approach can come in).     4.  PyMOL     \u2022   Molecular viewer with a Python API.     \u2022   You can script measuring bond lengths/angles or visualize geometry outliers.     5.  UCSF ChimeraX     \u2022   Another advanced viewer with Python-based commands.     \u2022   Good for interactive geometry checks and building small linkers.     6.  Rosetta FARFAR2     \u2022   Rosetta\u2019s RNA de novo folding approach, used for large-scale tertiary structure prediction.     \u2022   Enforces standard geometry constraints, but it\u2019s more \u201cheavyweight\u201d for folding tasks.     7.  PHENIX / MolProbity     \u2022   Typically used for crystallographic refinement and validation.     \u2022   Checks geometry outliers vs. standard bond lengths/angles.     \u2022   Scripting possible in PHENIX (Python-based).</p> <p>\u2e3b</p> <ol> <li>Converting RNA Torsions to 3D: Adapting mp-nerf or a NeRF Pipeline</li> </ol> <p>mp-nerf was designed to convert protein internal coordinates (bond length l, bond angle \\theta, dihedral \\phi) into 3D in a parallel manner. The concept:     1.  Protein knowledge base: lists standard bond lengths/angles for N\u2013CA\u2013C, plus sidechain geometry.     2.  Algorithm: uses the NeRF method to place each new atom given three references and (l, \\theta, \\phi).</p> <p>For RNA:     \u2022   You\u2019d replace \u201camino acid residue geometry\u201d with nucleotide residue geometry.     \u2022   The \u201cmassively parallel\u201d part remains the same: each residue can be built in a parallel block, then unified via rotation\u2013translation.</p> <p>6.1. Implementation Outline     1.  Create an RNA knowledge base (kb_rna.py)     \u2022   List bond lengths for P\u2013O5\u2032, O5\u2032\u2013C5\u2032, C5\u2032\u2013C4\u2032, etc.     \u2022   List bond angles at each pivot (O5\u2032\u2013C5\u2032\u2013C4\u2032, C4\u2032\u2013C3\u2032\u2013O3\u2032, O3\u2032\u2013P\u2013O5\u2032, etc.).     \u2022   For the sugar ring, define either:     1.  A standard set of ring atoms if you want a \u201cfrozen\u201d pucker (e.g., always C3\u2032-endo), or     2.  Torsions for ring closure if you want flexible ring geometry (\\nu_0,\\nu_1,\\ldots).     2.  Rewrite or Add a \u2018rna_fold(\u2026)\u2019 function     \u2022   Instead of referencing \u201cN, CA, C\u201d from the protein data, your code references \u201cP, O5\u2032, C5\u2032, C4\u2032, C3\u2032, O3\u2032, C2\u2032, C1\u2032, base atoms.\u201d     \u2022   If mp-nerf\u2019s approach for sidechains is a separate block, then \u201cthe base\u201d can be treated like a sidechain (i.e., once the sugar is built, place the glycosidic bond and base ring).     3.  Handling the Sugar Ring     \u2022   5-membered ring closure is trickier than a linear chain. Two main strategies:     1.  Fixed ring approach: Hard-code a standard ribose geometry with a chosen pucker (C3\u2032-endo). Then you only define the backbone torsions \\alpha,\\beta,\\gamma,\\delta,\\epsilon,\\zeta plus the glycosidic angle \\chi.     2.  Full ring torsions: If you want the ring to be flexible, define ring torsion angles \\nu_0!\\dots\\nu_4. Then do a small ring-closure routine. This is somewhat like building a proline ring in a protein context.     4.  Bridging Residues     \u2022   Each residue is \u201cP\u2013O5\u2032\u2013C5\u2032\u2013C4\u2032\u2013C3\u2032\u2013O3\u2032\u2013P next residue.\u201d You can build them in small parallel blocks and then unify them, the same way mp-nerf does with protein backbone segments.     5.  Use DSSR or 3DNA for verifying parameters     \u2022   If you have an existing RNA structure, run x3dna-dssr -i=struct.pdb --json to see the real bond lengths/angles.     \u2022   Confirm they match your \u201cknowledge base\u201d or adapt to small differences.     6.  Validation     \u2022   Reconstruct a known RNA motif from its known torsions. Then measure RMSD vs. the actual 3D structure to confirm accuracy.</p> <p>6.2. Practical Example Steps</p> <p>Suppose you have a short RNA 5\u2032-GCAA-3\u2032 in a known hairpin. You do:     1.  Parse the torsions {\\alpha,\\beta,\\gamma,\\delta,\\epsilon,\\zeta,\\chi} for each residue from DSSR.     2.  Initialize the first phosphate in a reference frame (origin).     3.  For each residue in parallel (like mp-nerf):     \u2022   Place P, O5\u2032, C5\u2032 (using length + angle + torsion).     \u2022   Place C4\u2032, C3\u2032 in the same manner.     \u2022   Possibly fix C2\u2032, O2\u2032, ring closure around O4\u2032.     \u2022   Attach the base using the glycosidic angle \\chi and known base geometry.     4.  Assemble them into a single chain (rotation\u2013translation if you do them in separate blocks).     5.  Compare final 3D coordinates with the original hairpin\u2019s PDB. Ideally, RMSD is small (0.5\u20131.0\u202f\u00c5 or better).</p> <p>\u2e3b</p> <ol> <li>Putting It All Together</li> </ol> <p>DSSR (and 3DNA) alone does not do a direct \u201ctorsion\u21923D for single-stranded loops,\u201d but it has partial rebuild logic for base-pair steps or standard fiber models. Meanwhile, mp-nerf is excellent for converting internal coordinates to Cartesian quickly but is protein-specific out of the box.</p> <p>By merging:     \u2022   The standard RNA covalent geometry (Tables 1 &amp; 2 above) plus     \u2022   A NeRF algorithm (like mp-nerf\u2019s parallel approach),</p> <p>you get a pipeline that can read torsion angles for each residue and produce a valid 3D RNA structure. The approach:     1.  Provides a backbone that matches known distances and angles.     2.  Correctly closes or fixes the sugar ring.     3.  Places each nucleobase with the correct glycosidic bond length and angle (\\chi).     4.  Allows you to integrate with Python-based MD or analysis (e.g., Barnaba, MDAnalysis) for further validation or refinement.</p> <p>\u2e3b</p> <ol> <li>References and Further Reading<ol> <li>Nucleic Acid Knowledge Base or NDB for standard bond-length/angle data: \u2022   http://nucleicacidknowledgebase.org/ \u2022   https://ndbserver.rutgers.edu/</li> <li>DSSR &amp; 3DNA: \u2022   Homepage: http://x3dna.org/ \u2022   DSSR paper: Lu et al. (2015) \u201cDSSR: an integrated software tool\u2026\u201d Nucleic Acids Res 43:e142.</li> <li>mp-nerf: \u2022   Often described in a protein context. E.g., see AlQuraishi\u2019s pNeRF approach or the repository for \u201cmassively parallel\u201d transformations.</li> <li>Barnaba: \u2022   Bottaro &amp; Bussi (2019). Nucleic Acids Res. 47(11):e56, also on GitHub.</li> <li>Rosetta FARFAR2: \u2022   For RNA folding and structure generation with strong geometric constraints.</li> </ol> </li> </ol> <p>Advanced: If ring closure is needed, consult literature on five-membered ring building approaches (like proline ring closure in proteins or cycpeptidic building in general). Tools like OpenBabel\u2019s \u201cGenerate 3D\u201d or RDKit can also handle smaller ring conformations, if integrated carefully with your code.</p> <p>\u2e3b</p> <ol> <li>Conclusion</li> </ol> <p>Yes, you can thoroughly adapt mp-nerf (or any standard NeRF code) for RNA by:     \u2022   Substituting a new knowledge base with RNA bond lengths and angles (as tabulated above).     \u2022   Implementing minimal or full ring closure for the ribose.     \u2022   Possibly labeling the base as a \u201csidechain,\u201d using standard geometry for A/U/G/C and the glycosidic angle \\chi.     \u2022   Optionally referencing DSSR/3DNA to confirm actual parameter values from real PDB structures or to do partial fiber-like rebuilds.</p> <p>Simultaneously, you can harness Python libraries (Barnaba, MDAnalysis, PyMOL/ChimeraX scripts) to validate that your final geometry remains near standard references. Through this synergy, you get a robust pipeline to convert predicted torsion angles (\\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi, plus sugar pucker if desired) into 3D atomic coordinates for RNA\u2014fully leveraging the high-quality geometry data gleaned from DSSR/3DNA or the Nucleic Acid Knowledge Base.</p> <p>If you need a deeper dive into code specifics (e.g., rewriting mp-nerf\u2019s kb_proteins.py into kb_rna.py), you can replicate the protein code structure, enumerating each \u201cRNA atom chain\u201d in the correct order, specifying for each new bond:     \u2022   bond_length = \u2026,     \u2022   bond_angle = \u2026,     \u2022   torsion = (the user-supplied or predicted value), plus ring constraints. Then the parallel approach remains the same.</p>"},{"location":"pipeline/stageC/Integrated_RNA_Geometry_and_3D_Reconstruction/#end-of-document","title":"End of Document.","text":"<p>Comprehensive Technical Documentation on Standard RNA Geometry Sources and Their Applicability to mp_nerf</p> <p>\u2e3b</p> <ol> <li>Introduction</li> </ol> <p>When generating three-dimensional coordinates for RNA nucleotides from torsion angles (as in the mp_nerf algorithm), it is essential to use well-established \u201cideal\u201d bond lengths and bond angles for the sugar\u2013phosphate backbone and the bases. Over the past several decades, multiple groups have collected or refined these geometric parameters through crystallographic surveys or small-molecule databases. Despite incremental updates, the consensus across the community remains that the 1990s work by Parkinson, Gelbin, Clowney, and Berman (often labeled collectively as \u201cParkinson et al. 1996\u201d or the \u201cBerman group 1996\u201d data) is the primary standard for RNA geometry.</p> <p>However, other references sometimes present small numerical refinements or clarify specific subsets (e.g., base ring geometry, definitions of torsions, partial charges). Below is a thorough comparison of these references, emphasizing (1) how they relate to each other, (2) their strengths and weaknesses, and (3) final recommended \u201cbest practices\u201d for choosing a geometry dataset suitable for tools like mp_nerf. This document also addresses critiques or shortcomings of each source, ensuring you have the most balanced and up-to-date guidance possible.</p> <p>\u2e3b</p> <ol> <li>Overview of Key References</li> </ol> <p>This section briefly introduces the major references frequently cited for RNA geometry. Each subsequent section then discusses them in more detail, noting how each might (or might not) be relevant for building a numeric constants file.     1.  Parkinson et al. (1996) and Related \u201cBerman Group\u201d Publications     \u2022   Gold standard crystallographic survey from the mid-1990s; these numbers are at the heart of many refinement programs.     2.  IUPAC\u2013IUB Joint Commission Recommendations (1982/1983)     \u2022   Authoritative for nomenclature (torsion angles \u03b1\u2013\u03b6, ring numbering), but not for updated numeric geometry.     3.  Gilski et al. (2019)     \u2022   Recent re-analysis of nucleobase geometry (especially Watson\u2013Crick base pairing) using small-molecule crystal structures in the Cambridge Structural Database (CSD).     4.  Cambridge Structural Database (CSD)     \u2022   The world\u2019s largest repository of small-molecule crystallography data. Not a single \u201ctable,\u201d but the raw data underpin many published geometric surveys.     5.  Nucleic Acid Database (NDB)     \u2022   A curated database for nucleic acids. Often mirrors Parkinson et al. (1996), with minor incremental updates.     6.  Aduri (2007) \u2013 AMBER Force Field for Modified Nucleotides     \u2022   Provides partial charges and dihedral parameters for modified bases but defers to the 1996 references for standard bond/angle geometry.     7.  PDB Validation / MolProbity Documentation     \u2022   Modern structure validation pipelines (wwPDB, MolProbity) rely on Parkinson (1996) as the reference for nucleic acid geometry.</p> <p>\u2e3b</p> <ol> <li>Detailed Examination of Each Reference</li> </ol> <p>3.1 Parkinson et al. (1996) / Gelbin, Clowney, Berman (1996)     \u2022   Key Publications     \u2022   Parkinson G, Vojtechovsky J, Clowney L, Brunger AT, Berman HM (1996) Acta Cryst. D52, 57\u201364.     \u2022   Gelbin A, Schneider B, Clowney L, Hsieh S-H, Olson WK, Berman HM (1996) J. Am. Chem. Soc. 118, 519\u2013529.     \u2022   Clowney L, Jain SC, Srinivasan AR, Westbrook J, Olson WK, Berman HM (1996) J. Am. Chem. Soc. 118, 509\u2013518.     \u2022   What These Papers Provide These are systematic surveys of high-resolution nucleic acid structures (both RNA and DNA) that extracted empirical average bond lengths, angles, and torsions for sugar moieties, phosphate groups, and bases. Their data form the \u201cideal geometry\u201d often used in standard refinement libraries for crystallography and NMR structure determination.     \u2022   Strengths     1.  Comprehensive and Data-Driven: Among the first large-scale attempts to extract robust average values from crystal structures.     2.  Widely Adopted: Many structural biology pipelines (e.g., X-PLOR, REFMAC, Phenix, MolProbity) reference these numbers, ensuring consistency with PDB validations.     3.  Inclusion of Sugar\u2013Phosphate: Crucially, they detail phosphate bridging angles, sugar ring puckers, glycosidic bond angles, and more, making them a one-stop source for backbone geometry.     \u2022   Weaknesses     1.  Date of Publication: Now ~25+ years old. Some modest refinements have trickled in from expanded data sets.     2.  Incremental Changes: Later references have observed minor shifts (0.01\u20130.02\u202f\u00c5 or a few degrees), especially in base rings.     \u2022   Criticisms Addressed     \u2022   Age: While often labeled \u201cold,\u201d no major contradictory study has replaced these data; updates typically confirm only minor numerical adjustments.     \u2022   Sparsity of Very High-Resolution RNA Structures in the 90s: Over time, more structures have been added to the NDB, but the fundamental backbone geometry has remained largely unchanged.     \u2022   Relevance to mp_nerf     \u2022   Primary Source: If mp_nerf needs a canonical set of bond lengths and angles, Parkinson 1996 remains the top choice\u2014particularly for sugar and phosphate geometry.</p> <p>\u2e3b</p> <p>3.2 IUPAC\u2013IUB Joint Commission (Recommendations 1982/1983)     \u2022   What These Recommendations Cover They formalize the nomenclature for polynucleotide chains, defining how to label backbone torsions (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7), how to number the sugar ring atoms, the 5\u2032\u21923\u2032 chain direction, etc.     \u2022   Strengths     \u2022   Authoritative Definitions: Whenever you see \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6 in any RNA or DNA context, these definitions usually trace back to the IUPAC\u2013IUB documents.     \u2022   Nomenclature Consistency: Minimizes confusion about how angles are measured.     \u2022   Weaknesses     \u2022   Minimal Numeric Data: Not intended to define \u201cideal\u201d bond lengths or angles.     \u2022   Somewhat Outdated for Fine Geometric Details: These are from the early 1980s, focusing more on naming conventions than empirical geometry.     \u2022   Relevance to mp_nerf     \u2022   Essential for Torsion Labeling: Great if you want standard naming for angles that mp_nerf might read or produce.     \u2022   Not a Source for Bond Lengths/Angles: You still need the 1996 references for numeric geometry.</p> <p>\u2e3b</p> <p>3.3 Gilski et al. (2019) \u2013 \u201cAccurate Geometrical Restraints for W\u2013C Base Pairs\u201d     \u2022   Focus of This Study Published in Acta Cryst. B, Gilski and co-workers re-examined small-molecule crystal structures deposited in the CSD, particularly for nucleobase rings and Watson\u2013Crick hydrogen-bond interactions.     \u2022   Strengths     1.  Recent Analysis: Incorporates more modern, higher-resolution small-molecule data.     2.  Refined Base Geometry: Bond lengths and angles in the heterocyclic rings might be slightly more precise than the older references.     \u2022   Weaknesses     1.  Limited to Bases: Does not significantly alter sugar\u2013phosphate geometry.     2.  Incremental Adjustments: The reported updates over Parkinson (1996) are typically around 0.01\u20130.02\u202f\u00c5 or a few degrees\u2014useful for very high precision, but not a wholesale revision.     \u2022   Criticisms Addressed     \u2022   Practical Impact: The changes are small enough that many pipeline validations and geometry libraries have not urgently adopted them.     \u2022   Relevance to mp_nerf     \u2022   Optional Refinement for Bases: If you want state-of-the-art base ring geometry, you can fold Gilski\u2019s numbers into your mp_nerf constants.     \u2022   Backbone Unchanged: For phosphate and sugar, the older references are still the standard.</p> <p>\u2e3b</p> <p>3.4 Cambridge Structural Database (CSD)     \u2022   What It Is A massive compilation of small-molecule crystal structures (not just nucleic acids, but all organic and metal-organic compounds).     \u2022   Strengths     \u2022   Highly Robust: Contains tens of thousands of structures that can be used to statistically derive average geometries for any piece of RNA (ribose, bases).     \u2022   Independently Curated: Data from small-molecule crystals often reach higher resolution than large biomacromolecular crystals, giving precise bond lengths.     \u2022   Weaknesses     \u2022   Requires DIY Data Mining: No built-in universal \u201cRNA geometry table\u201d is provided.     \u2022   Time-Consuming: One must carefully filter for relevant structures (e.g., high resolution, minimal disorder, correct protonation state).     \u2022   Criticisms Addressed     \u2022   Potential Overlap with Gilski: Gilski (2019) is essentially a curated re-analysis of nucleobases in the CSD. Doing it yourself would presumably yield similar results if you follow rigorous filtering criteria.     \u2022   Relevance to mp_nerf     \u2022   Indirect: You can rely on published analyses (Parkinson, Gilski) that already tapped into the CSD. No urgent need to re-derive these numbers unless you have a specialized reason.</p> <p>\u2e3b</p> <p>3.5 Nucleic Acid Database (NDB)     \u2022   What It Is A specialized repository for nucleic acid structures (X-ray, NMR, cryo-EM), curated by the same Rutgers group historically responsible for the 1996 surveys.     \u2022   Strengths     \u2022   Direct Tabulation of Geometric Values: The NDB often provides updated sugar\u2013phosphate bond lengths and angles, close to or slightly refining the 1996 numbers.     \u2022   Community-Trusted: Maintained by experts in nucleic acid structure.     \u2022   Weaknesses     \u2022   Minimal Differences: The data are usually only slightly updated from Parkinson (1996), with typical changes \u2264 0.02\u202f\u00c5 or a couple of degrees.     \u2022   Not a Drastic Overhaul: The core standard remains effectively the same.     \u2022   Criticisms Addressed     \u2022   \u201cSame as 1996\u201d: Some worry it is just re-publishing the old data. However, any expansions typically confirm those older values were robust.     \u2022   Relevance to mp_nerf     \u2022   Equivalent Choice: Using NDB\u2019s geometry tables is effectively the same as using Parkinson (1996). If you want the convenience of a modern website, it is a perfectly valid source.</p> <p>\u2e3b</p> <p>3.6 Aduri (2007) \u2013 AMBER Force Field for Modified Nucleotides     \u2022   Main Goal This study addresses partial charges, torsion parameters, and force-field updates for modified or unusual bases in RNA (e.g., pseudouridine, 2\u2032-O-methylated nucleotides).     \u2022   Strengths     \u2022   Invaluable for Modifications: If your mp_nerf or modeling workflow must handle a large variety of non-standard bases or sugar modifications.     \u2022   Force Field Integration: Aligns with well-known AMBER force fields (Cornell et al., parm99, GAFF).     \u2022   Weaknesses     \u2022   Not a New Source for Standard Geometry: The authors themselves typically reference the same 1996 Berman/Olson data or the Cornell parameters for standard nucleotides.     \u2022   Focused on Partial Charges: The impetus is electrostatic and dihedral parameters, not bond length or angle refinement.     \u2022   Relevance to mp_nerf     \u2022   Only If Handling Modified Residues: Great if you need geometry or charges for unusual groups. For standard A, U, G, C nucleotides, it offers no new geometry.</p> <p>\u2e3b</p> <p>3.7 PDB Validation / MolProbity     \u2022   What It Is Structural validation tools and guidelines used by the wwPDB, relying on known \u201cideal\u201d values to detect outliers.     \u2022   Strengths     \u2022   Official Confirmation: Reiterates that the PDB strongly references Parkinson (1996) or Engh &amp; Huber (for proteins).     \u2022   Community Standard: If your deposited RNA structure has bond lengths/angles far from these references, you\u2019ll get flagged.     \u2022   Weaknesses     \u2022   No New Data: MolProbity and wwPDB do not propose alternative geometry sets; they consume the standard references.     \u2022   Relevance to mp_nerf     \u2022   Consistency: If you adopt the same geometry, your generated coordinates will be consistent with validation norms.</p> <p>\u2e3b</p> <ol> <li>Comparing These References and Their Numeric Discrepancies</li> </ol> <p>A recurring theme is that nearly all sources agree on the numeric values within very small margins (~0.02\u202f\u00c5, 1\u20132\u00b0). The 1996 references remain the de facto standard, while Gilski (2019) offers incremental updates for base rings. Below is a concise summary of how each compares:     \u2022   Parkinson (1996) vs. NDB: The NDB is essentially an evolution of the same dataset. Differences are typically minuscule.     \u2022   Parkinson (1996) vs. Gilski (2019): Gilski improves base ring geometry slightly. The sugar\u2013phosphate geometry remains nearly identical. The changes rarely exceed 0.01\u20130.02\u202f\u00c5.     \u2022   IUPAC: Provides definitions, not numeric tables, so no numerical conflict.     \u2022   Aduri (2007): Defers to earlier references for unmodified nucleotides. Again, no conflict.     \u2022   CSD (Raw): The underlying source for many of these surveys. You could re-derive the same means if you replicate Gilski\u2019s or Berman\u2019s methods.</p> <p>Hence, there is no major contradiction among references; it is more a matter of how precise you want to be (i.e., whether you incorporate Gilski\u2019s minor updates for bases).</p> <p>\u2e3b</p> <ol> <li>Strengths and Weaknesses: A Consolidated Table</li> </ol> <p>Reference   Strengths   Weaknesses Parkinson / Gelbin / Clowney (1996) Berman Group    - Empirically derived from high-res structures  - Backbone + base geometry covered  - Still widely used in refinement  - \u201cGold standard\u201d for sugar\u2013phosphate    - Older dataset (1990s)  - Minor numerical updates exist in newer references IUPAC\u2013IUB (1982/83) - Authoritative naming of torsions (\u03b1, \u03b2, \u03b3, \u03b4, etc.)  - Official polynucleotide nomenclature   - Not an updated numeric source  - Dated in terms of modern structural knowledge Gilski et al. (2019)    - Latest small-molecule data for base rings  - Focuses on W\u2013C pair geometry  - Potentially more precise nucleobase angles/lenths    - Limited to base geometry  - Differences from 1996 sets are small (~0.01\u20130.02\u202f\u00c5) Cambridge Structural Database (CSD) - Vast repository of small-molecule crystals  - Can yield highly precise bond lengths  - The foundation for Gilski\u2019s approach   - No single \u201cstandard table\u201d provided  - Must do extensive data mining &amp; filtering Nucleic Acid Database (NDB) - Modern aggregator of NA structures  - Publishes geometry tables similar to Parkinson (1996)  - Occasionally updated   - Differences from 1996 are minor  - Not a fundamentally \u201cnew\u201d dataset Aduri (2007)    - AMBER force field parameters for modified nucleotides  - Great for partial charges &amp; unusual bases    - Not an independent geometry source  - Reuses standard references for unmodified nucleotides PDB Validation / MolProbity - Official pipeline referencing Parkinson (1996)  - Reinforces community standard for geometry  - No new data  - Merely restates established norms</p> <p>\u2e3b</p> <ol> <li>Recommendations for mp_nerf Implementation</li> </ol> <p>Given the convergence among these sources, the path forward for a reliable \u201cconstants file\u201d in mp_nerf is fairly straightforward:     1.  Default to Parkinson et al. (1996)     \u2022   Use these bond lengths and angles for the sugar\u2013phosphate backbone.     \u2022   These values are robust, widely accepted, and ensure consistency with PDB validation.     2.  Optionally Incorporate Gilski (2019) for Bases     \u2022   If mp_nerf idealizes base ring structures in a highly precise manner (e.g., if your application is sensitive to subtle ring metrics), you can overwrite the base\u2010specific parameters with Gilski\u2019s updates.     \u2022   The backbone geometry can remain from the 1996 data.     3.  Leverage NDB as a Backup     \u2022   If it is more convenient to pull from a modern online database, the NDB\u2019s valence geometry tables effectively replicate the 1996 numbers (sometimes with trivially small refinements).     \u2022   This ensures you are still aligned with standard practice.     4.  Retain IUPAC\u2013IUB for Torsion Naming     \u2022   For clarity and consistency, use the IUPAC scheme to label \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, and \u03c7 angles.     \u2022   This step avoids confusion in angle definitions.     5.  Add Specialized Data for Modified Nucleotides (If Needed)     \u2022   If mp_nerf must handle modifications such as pseudouridine or 2\u2032-O-methyl groups, consult Aduri (2007) or relevant AMBER force-field parameter sets for partial charges and specialized geometry.     \u2022   The standard backbone bond lengths typically remain the same, but ring or substituent parameters may differ.</p> <p>By following this scheme, you ensure your geometry is in line with the broad consensus of structural biology tools and references.</p> <p>\u2e3b</p> <ol> <li>Frequently Asked Questions (FAQ)<ol> <li>Why not use Gilski (2019) for everything? Gilski\u2019s publication focuses heavily on base rings and Watson\u2013Crick hydrogen bonds. It does not supersede the backbone geometry from 1996. The numeric changes for the bases themselves are small enough (&lt;0.02\u202f\u00c5) that for most routine modeling or structure building, the older 1996 numbers are perfectly acceptable.</li> <li>Are the 1996 data too old? Despite their age, these data have withstood the test of time. Subsequent expansions of the dataset typically confirm only minimal numerical shifts. No major contradictions or overhauls have emerged in more recent literature.</li> <li>Do I really need partial charges or dihedral parameters from Aduri (2007)? Only if you plan to do molecular dynamics or energy calculations involving non-standard nucleotides. If mp_nerf is strictly for coordinate generation of standard A, U, G, C residues, the 1996 bond lengths/angles are sufficient.</li> <li>Could I do my own CSD-based analysis? In theory, yes\u2014many researchers do. But it is time-consuming, and the published results from Parkinson, Gilski, and the NDB already represent thorough analyses of that same data. You would likely find near-identical numbers unless you include new structures or different filtering.</li> <li>What about naming conventions beyond \u03b1\u2013\u03b6, like sugar ring puckers? The IUPAC\u2013IUB recommendations remain the default for naming the sugar ring atoms, while standard pucker notation (C3\u2032-endo, C2\u2032-endo, etc.) is widely accepted. The numeric definitions of ring conformation angles can be found in the references from the Berman group and in typical computational chemistry software manuals (e.g., AMBER docs).</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Conclusion</li> </ol> <p>In summary, Parkinson et al. (1996) (also collectively described as the \u201cBerman group 1996\u201d data) remains the central, most authoritative source for standard RNA geometry. Although Gilski et al. (2019) provides slight improvements for nucleobase rings, the sugar\u2013phosphate parameters remain effectively unchanged. Alternative sources like the NDB or older IUPAC documentation typically echo these fundamental 1996 values; the CSD underlies them with raw small-molecule data, and the PDB validation pipeline further cements these numbers as the community standard.</p> <p>For a tool like mp_nerf\u2014which reconstructs 3D coordinates from torsion angles\u2014the best practice is to adopt the Parkinson (1996) data for sugar\u2013phosphate bond lengths and angles, optionally layering in Gilski (2019) for nucleobase ring geometry if you want the most current refinements. All other references are either duplicative, focus on nomenclature, or address more specialized topics (modified nucleotides, partial charges).</p> <p>By heeding these guidelines, your geometry constants file will be both authoritative and consistent with widely used structural biology protocols\u2014ensuring that the final result is better than the sum of its parts and firmly aligned with the established consensus in RNA structural science.</p> <p>\u2e3b</p> <p>References and Suggested Reading     1.  Parkinson G, Vojtechovsky J, Clowney L, Brunger AT, Berman HM (1996). Acta Cryst. D52, 57\u201364.     2.  Gelbin A, Schneider B, Clowney L, Hsieh S-H, Olson WK, Berman HM (1996). J. Am. Chem. Soc. 118, 519\u2013529.     3.  Clowney L, Jain SC, Srinivasan AR, Westbrook J, Olson WK, Berman HM (1996). J. Am. Chem. Soc. 118, 509\u2013518.     4.  IUPAC\u2013IUB Joint Commission on Biochemical Nomenclature (1983). Eur. J. Biochem., 131, 9\u201315.     5.  Gilski M, et al. (2019). Acta Cryst. B75, 235\u2013254.     6.  Cambridge Structural Database (CSD): https://www.ccdc.cam.ac.uk/     7.  Nucleic Acid Database (NDB): http://ndbserver.rutgers.edu     8.  Aduri R, et al. (2007). J. Chem. Theory Comput. 3(4), 1464\u20131475.     9.  MolProbity / PDB Validation: http://molprobity.biochem.duke.edu</p> <p>\u2e3b</p> <p>End of Document</p>"},{"location":"pipeline/stageC/Stage_C/","title":"Overview","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f Integrated Stage C: Forward Kinematics Comprehensive Guide (RNA Pipeline) \ud83d\ude80</p>"},{"location":"pipeline/stageC/Stage_C/#motivation-and-key-concepts","title":"\ud83e\uddec Motivation and Key Concepts","text":"<p>Why Forward Kinematics?</p> <ul> <li>RNA structure is best represented using internal coordinates: bond lengths, bond angles, and torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, sugar pucker).</li> <li>Predicting torsion angles (Stage B) is more straightforward and biologically relevant compared to directly predicting Cartesian coordinates.</li> <li>Forward Kinematics (FK) translates these angles into accurate 3D atom positions, ensuring physically consistent and valid structures.</li> </ul> <p>\ud83d\udd04 Invariance &amp; Efficiency</p> <ul> <li>Torsion angles are rotation/translation-invariant, thus RNA\u2019s intrinsic \u201cfolding instructions.\u201d</li> <li>FK efficiently manages geometric constraints through sequential rotations around bonds with fixed reference geometry.</li> </ul> <p>\ud83d\udccc Core Steps</p> <ol> <li>Initialize the first residue in a canonical orientation.</li> <li>For each subsequent residue, apply torsion angles (from Stage B) using known bond geometry.</li> <li>Position atoms through rotations around bond axes.</li> <li>Explicitly manage sugar puckering variations (C3\u2032-endo, pseudorotation).</li> <li>Output robust 3D coordinates for each residue\u2019s heavy atoms (optionally including base atoms).</li> </ol>"},{"location":"pipeline/stageC/Stage_C/#extended-theoretical-foundations","title":"\ud83d\udcda Extended Theoretical Foundations","text":""},{"location":"pipeline/stageC/Stage_C/#kinematic-analogy-robotics-rna","title":"\ud83d\udee0\ufe0f Kinematic Analogy (Robotics &amp; RNA)","text":"<ul> <li>RNA backbone analogous to robotic joint chains: torsion angles represent joints.</li> <li>Sequential rotations reconstruct RNA's 3D backbone using known bond lengths and angles.</li> </ul>"},{"location":"pipeline/stageC/Stage_C/#mathematics-rotation-matrices-homogeneous-transformations","title":"\ud83d\udcd0 Mathematics: Rotation Matrices &amp; Homogeneous Transformations","text":"<p>Rotation Matrices: (SO(3), orthonormal)</p> \\[ R_z(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>Homogeneous Transformations (4\u00d74): Efficient combination of rotations and translations.</p> \\[ T = \\begin{bmatrix} R &amp; t \\\\ 0 &amp; 1 \\end{bmatrix} \\] <ul> <li>Enables chained transformations along the RNA backbone (similar to Denavit\u2013Hartenberg method in robotics).</li> </ul>"},{"location":"pipeline/stageC/Stage_C/#numerical-stability-reference-geometry","title":"\ud83d\udccf Numerical Stability &amp; Reference Geometry","text":"<ul> <li>Small numerical errors accumulate through sequential transformations. Regular orthonormalization recommended.</li> <li>Natural Extension Reference Frame (NeRF) method strongly recommended for numerical stability and efficiency.</li> <li>Standard RNA geometry from parameter sets (AMBER, 3DNA/DSSR).</li> <li>Flexible modeling of sugar puckers (C3\u2032-endo, predicted pseudorotation, or refined via minimization).</li> </ul> <p>Key References: - Richardson et al. (2008), Murray et al. (2003), 3DNA/DSSR documentation.</p>"},{"location":"pipeline/stageC/Stage_C/#data-flow-in-rna-multi-stage-pipeline","title":"\ud83d\udcc8 Data Flow in RNA Multi-Stage Pipeline","text":"<ol> <li>Stage\u202fA: Extract 2D adjacency/base pairs from raw sequence.</li> <li>Stage\u202fB: Predict torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7).</li> <li>Stage\u202fC (Current):</li> <li>Input: Predicted torsion angles, reference geometry.</li> <li>Output: 3D atom coordinates (heavy atoms).</li> <li>Stage\u202fD (Optional): AF3-like or diffusion-based refinement in Cartesian or angle space.</li> </ol>"},{"location":"pipeline/stageC/Stage_C/#detailed-pseudocode-implementation","title":"\ud83d\udcbb Detailed Pseudocode Implementation","text":"<pre><code>def forward_kinematics(torsion_angles, sequence, reference_geometry, ring_pucker_model=None):\n    N = len(sequence)\n    coords = alloc_coord_array(N)\n\n    coords[0] = place_first_residue(torsion_angles[0], sequence[0], reference_geometry)\n\n    for i in range(1, N):\n        anchor_positions = get_anchor_positions(coords[i-1], sequence[i-1])\n        alpha, beta, gamma, delta, epsilon, zeta, chi = torsion_angles[i]\n\n        coords[i] = build_residue(\n            anchor_positions,\n            (alpha, beta, gamma, delta, epsilon, zeta, chi),\n            sequence[i],\n            reference_geometry\n        )\n\n        if ring_pucker_model is not None:\n            coords[i] = refine_sugar_pucker(coords[i], ring_pucker_model[i])\n\n    coords = final_refinement(coords)\n    return coords\n</code></pre>"},{"location":"pipeline/stageC/Stage_C/#step-by-step-local-frame-construction-detailed","title":"\ud83d\udd0e Step-by-Step Local Frame Construction (Detailed)","text":"<ul> <li>Local Frame Construction:<ul> <li>Identify three previously placed atoms (A, B, C).</li> <li>Define local axes at atom C:     <pre><code>x_axis = normalize(B - C)\ntemp   = normalize(A - B)\nz_axis = normalize(cross(x_axis, temp))\ny_axis = cross(z_axis, x_axis)\n</code></pre></li> <li>Place atom D in this local frame using bond length, angle, and torsion angle:     <pre><code>D_local = (d * cos(theta), d * sin(theta), 0)\nrotate around x_axis by torsion phi\n</code></pre></li> <li>Transform back to global coordinates.</li> </ul> </li> </ul>"},{"location":"pipeline/stageC/Stage_C/#implementation-notes","title":"\u2699\ufe0f Implementation Notes","text":"<ul> <li>Anchor Atoms: Usually O3\u2032(i-1); consistency with Stage B indexing critical.</li> <li>Applying Torsions: Each torsion is a rotation around a local bond axis. NeRF preferred.</li> <li>Sugar Ring Closure: Use ideal geometry or refine ring via minimization if flexible puckering.</li> <li>Complexity: Linear (O(N)) complexity with nucleotide count.</li> </ul>"},{"location":"pipeline/stageC/Stage_C/#validation-error-metrics-and-constraints","title":"\ud83e\uddea Validation, Error Metrics, and Constraints","text":"<ul> <li>RMSD Validation:<ul> <li>Build RNA structure from known torsions (PDB).</li> <li>Accept RMSD &lt;0.5 \u00c5.</li> </ul> </li> <li>Geometric Checks: Verify bond lengths/angles (use MolProbity Suite).</li> <li>Steric Clash Checks: Validate absence of severe clashes; possibly minimize (MD engines).</li> <li>Ring Closure &amp; Constraints: Use mini inverse-kinematics for sugar ring closure.</li> <li>Numerical Stability: Regularly recompute internal coordinates to verify against input torsion angles for drift checks.</li> <li>Energy Scoring: Optional short minimization to relieve minor steric strain.</li> </ul>"},{"location":"pipeline/stageC/Stage_C/#configuration-execution-hydra","title":"\ud83d\udd27 Configuration &amp; Execution (Hydra)","text":"<p>Stage C (reconstruction) is configured using Hydra. Parameters are defined in YAML files and can be overridden via the command line when running the stage's entry point.</p> <p>Entry Point: <code>rna_predict.pipeline.stageC.stage_c_reconstruction</code></p> <p>Configuration Files:</p> <ul> <li>Main Stage C settings: <code>rna_predict/conf/model/stageC.yaml</code> (controls method selection, device, reconstruction options)</li> <li>(Optional) MP-NeRF Model settings: Referenced within <code>stageC.yaml</code> (e.g., <code>defaults: - mp_nerf_model: default_rna</code>). See <code>rna_predict/conf/mp_nerf_model/</code> for specific model variants.</li> </ul> <p>These are loaded via the main <code>rna_predict/conf/default.yaml</code>.</p>"},{"location":"pipeline/stageC/Stage_C/#key-configuration-parameters-stagecyaml","title":"Key Configuration Parameters (<code>stageC.yaml</code>)","text":"<pre><code># rna_predict/conf/model/stageC.yaml\ndefaults:\n  - mp_nerf_model: default_rna # Selects which MP-NeRF config to load from mp_nerf_model/\n  - _self_\n\nstageC:\n  method: \"mp_nerf\"            # \"mp_nerf\" or \"legacy\" (fallback)\n  device: \"cpu\"                # \"cpu\" or \"cuda\"\n  angle_representation: \"cartesian\" # Expected input format ('cartesian' or 'dihedral')\n  do_ring_closure: false       # Apply ring closure constraints\n  place_bases: true            # Reconstruct base atoms\n  sugar_pucker: \"C3'-endo\"     # Default sugar pucker conformation\n  # ... add other stageC specific parameters ...\n</code></pre> <p>Note: MP-NeRF specific hyperparameters (layers, dimensions) are typically defined in files within <code>rna_predict/conf/mp_nerf_model/</code>, selected by the <code>defaults</code> list above.</p>"},{"location":"pipeline/stageC/Stage_C/#command-line-overrides","title":"Command-Line Overrides","text":"<p>Override parameters using dot notation:</p> <ul> <li>Use the legacy reconstruction method:     <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction stageC.method=legacy\n</code></pre></li> <li>Run on CUDA:     <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction stageC.device=cuda\n</code></pre></li> <li>Enable ring closure:     <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction stageC.do_ring_closure=true\n</code></pre></li> <li>Disable base placement:     <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction stageC.place_bases=false\n</code></pre></li> <li>Select a different MP-NeRF model configuration (assuming <code>rna_predict/conf/mp_nerf_model/fast_model.yaml</code> exists):     <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction mp_nerf_model=fast_model\n</code></pre></li> </ul>"},{"location":"pipeline/stageC/Stage_C/#hpc-execution","title":"HPC Execution","text":"<p>For High Performance Computing (HPC) environments, see the HPC Integration Guide for SLURM and GridEngine examples.</p> <p>Basic HPC Example: <pre><code>python -m rna_predict.pipeline.stageC.stage_c_reconstruction \\\n    stageC.device=cuda \\\n    stageC.method=mp_nerf \\\n    +hpc_cluster=slurm \\\n    hydra.launcher.gpus=2\n</code></pre></p>"},{"location":"pipeline/stageC/Stage_C/#typed-configuration-optional","title":"Typed Configuration (Optional)","text":"<p>Check <code>rna_predict/conf/config_schema.py</code> for potential typed dataclasses (e.g., <code>StageCConfig</code>) that provide structure and validation for the configuration.</p>"},{"location":"pipeline/stageC/Stage_C/#detailed-references-acknowledgments","title":"\ud83d\udcd6 Detailed References &amp; Acknowledgments","text":"<ul> <li>Murray et al. (2003): RNA backbone is rotameric (PNAS).</li> <li>Richardson et al. (2008): RNA backbone suite nomenclature (RNA).</li> <li>3DNA/DSSR: Standard RNA geometry tools (x3dna.org).</li> <li>MolProbity Suite (Suitename): RNA rotamer and sugar pucker validation.</li> <li>User documentation: <code>Multi_Stage_RNA3D_Pipeline_Technical_Architecture&amp;Implementation_Plan.md</code>, <code>torsion_angles.md</code>.</li> </ul>"},{"location":"pipeline/stageC/Stage_C/#comprehensive-conclusion","title":"\ud83c\udfaf Comprehensive Conclusion","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f Stage C provides a mathematically rigorous and computationally efficient approach to converting predicted torsion angles into accurate RNA 3D structures. By leveraging rotation matrices, homogeneous transformations, and the NeRF method, FK ensures physically consistent and biologically valid atomic coordinates.</p> <p>\u2705 Recommended Next Steps:</p> <ul> <li> <p>Implement and rigorously test the provided <code>forward_kinematics</code> pseudocode.</p> </li> <li> <p>Validate against PDB structures.</p> </li> <li> <p>Integrate the validated method into Stage\u202fB for a complete RNA structural pipeline.</p> </li> </ul> <p>\u2728 Additional Enhancements:</p> <ul> <li> <p>Include visual diagrams illustrating rotations and local reference frames.</p> </li> <li> <p>Incorporate MkDocs admonitions (<code>!!! note</code>) for clarity.</p> </li> <li> <p>Auto-generate Table of Contents for enhanced readability.</p> </li> </ul>"},{"location":"pipeline/stageC/Unified%2C%20Comprehensive%20Plan%20for%20Integrating%20MP-NeRF%20into%20Stage%E2%80%AFC/","title":"Unified, Comprehensive Plan for Integrating MP NeRF into Stage\u202fC","text":"<p>Below is a comprehensive, consolidated technical document combining the best features of all four proposed implementation plans (V1\u2013V4) for integrating mp-nerf into your RNA Stage\u202fC. It is verbose, covers all relevant details, and addresses previous criticisms or omissions in the earlier versions. The final result is designed to be more robust and better than any single earlier plan.</p> <p>\u2e3b</p> <p>Unified, Comprehensive Plan for Integrating MP-NeRF into Stage\u202fC</p> <ol> <li>Purpose and Scope</li> </ol> <p>In an RNA (or protein) structure prediction pipeline, Stage\u202fC converts predicted internal coordinates (e.g., torsion angles, bond lengths, ring pucker angles) into final 3D Cartesian coordinates. Typically, one might implement a naive forward-kinematics routine, which can be slow for large molecules.</p> <p>By adopting MP-NeRF (the Massively Parallel Natural Extension of Reference Frame), you can build these 3D coordinates in a fraction of the time\u2014often with 400\u20131200\u00d7 speedups\u2014by splitting the polymer into subunits, building them in parallel, and then linking them with local transformations.</p> <p>High-Level Integration Goals     1.  Use or adapt your existing RNA geometry data (e.g. final_kb_rna.py) and replicate the pattern that MP-NeRF uses for proteins in kb_proteins.py and proteins.py.     2.  Hook these references into your pipeline\u2019s Stage\u202fC code\u2014often located in rna_predict/pipeline/stageC/...\u2014so that predicted torsions from Stage\u202fB feed seamlessly into MP-NeRF.     3.  Handle RNA-specific features like:     \u2022   Sugar\u2013phosphate backbone (P\u2013O5\u2032\u2013C5\u2032\u2013C4\u2032\u2013O4\u2032\u2013C3\u2032\u2013O3\u2032).     \u2022   Sugar ring pucker (C3\u2032-endo, possibly flexible ring closure).     \u2022   Bases (A, U, G, C) as \u201csidechains,\u201d or combined in the main residue definition.     4.  Optionally keep a fallback to your older forward-kinematics code for debugging or partial usage.     5.  Preserve differentiability if you want to do end-to-end training.</p> <p>This plan merges the file-level organization approach from V1, the conceptual and parallel expansions from V2, the succinct data-flow bridging from V3, and the RNA-oriented clarifications from V4\u2014thus forming a single, robust solution.</p> <p>\u2e3b</p> <ol> <li>Directory &amp; File Organization</li> </ol> <p>Below is a recommended layout. You can adapt as needed.</p> <p>/your_project \u251c\u2500\u2500 mp_nerf \u2502   \u251c\u2500\u2500 init.py \u2502   \u251c\u2500\u2500 kb_proteins.py         # Existing protein knowledge base \u2502   \u251c\u2500\u2500 massive_pnerf.py       # The core mp_nerf_torch(...) logic \u2502   \u251c\u2500\u2500 ml_utils.py \u2502   \u251c\u2500\u2500 proteins.py            # High-level \"protein_fold(...)\" for proteins \u2502   \u251c\u2500\u2500 utils.py \u2502   \u251c\u2500\u2500 kb_rna.py              # (NEW) store RNA geometry references \u2502   \u251c\u2500\u2500 rna.py                 # (NEW) \"rna_fold(...)\" or \"build_scaffolds_rna(...)\" \u2502   \u2514\u2500\u2500 ... \u251c\u2500\u2500 rna_predict \u2502   \u251c\u2500\u2500 pipeline \u2502   \u2502   \u251c\u2500\u2500 stageC \u2502   \u2502   \u2502   \u251c\u2500\u2500 stage_c_reconstruction.py   # Where you integrate mp-nerf calls \u2502   \u2502   \u2502   \u251c\u2500\u2500 forward_kinematics.py       # (Optional older approach) \u2502   \u2502   \u2514\u2500\u2500 ... \u2502   \u2514\u2500\u2500 ... \u251c\u2500\u2500 final_kb_rna.py            # Your existing RNA geometry data \u2514\u2500\u2500 ...</p> <p>Why this structure?     1.  You keep mp_nerf code in one place, and put your new RNA logic (files kb_rna.py, rna.py) next to the existing protein code (kb_proteins.py, proteins.py).     2.  You can either import from final_kb_rna.py or replicate key values in kb_rna.py directly. Some people prefer an intermediate file, e.g. kb_rna_bridge.py, to convert existing data structures into mp-nef\u2019s format.     3.  In stageC, you add or modify stage_c_reconstruction.py to call your new rna_fold(...) function, ensuring synergy with the rest of the pipeline.</p> <p>\u2e3b</p> <ol> <li>Creating an RNA Knowledge Base</li> </ol> <p>3.1. kb_rna.py: Storing Standard Values</p> <p>MP-NeRF is built for proteins in kb_proteins.py (with dictionaries like SC_BUILD_INFO, BB_BUILD_INFO). For RNA, you want an analogous dictionary for each base type (A, U, G, C):</p>"},{"location":"pipeline/stageC/Unified%2C%20Comprehensive%20Plan%20for%20Integrating%20MP-NeRF%20into%20Stage%E2%80%AFC/#mp_nerfkb_rnapy","title":"mp_nerf/kb_rna.py","text":"<p>RNA_BUILD_INFO = {   \"A\": {      # For the backbone      \"backbone_atoms\": [\"P\",\"O5'\",\"C5'\",\"C4'\",\"O4'\",\"C3'\",\"O3'\"],      \"bond_lengths\": [...],  # e.g. from final_kb_rna.py (P\u2013O5' ~1.59\u00c5, etc.)      \"bond_angles\": [...],      \"torsions\": [...],      # e.g. alpha, beta, gamma, delta, epsilon, zeta      # For the base ring (treated like sidechain)      \"base_atoms\": [\"N9\",\"C8\",\"N7\",\"C5\", ...],      \"base_bond_lengths\": [...],      \"base_bond_angles\": [...],      \"base_torsions\": [...]   },   \"U\": {...},   \"G\": {...},   \"C\": {...} }</p> <p>Data Source:     \u2022   You can parse from your final_kb_rna.py which might define RNA_BOND_LENGTHS_C3_ENDO, RNA_BACKBONE_TORSIONS_AFORM, etc.     \u2022   If the user wants a flexible sugar ring, define ring dihedrals or partial ring constraints instead of a single \u201cC3\u2032-endo.\u201d</p> <p>3.2. Handling the Sugar Ring</p> <p>Fixed Pucker     \u2022   Set your ring angles to typical A-form or C3\u2032-endo. Hard-code them so that the ring is \u201cfrozen.\u201d Flexible Pucker     \u2022   If your pipeline\u2019s Stage\u202fB predicts ring torsions (\u03bd\u2080..\u03bd\u2084), include them in your dictionary.     \u2022   Optionally do a ring-closure routine, or treat each ring atom as a local sub-step. This is more advanced.</p> <p>\u2e3b</p> <ol> <li>Adapting mp_nerf\u2019s \u201cfold\u201d Logic for RNA</li> </ol> <p>4.1. A New File: rna.py</p> <p>In proteins.py, you\u2019ll find high-level folding routines like protein_fold(...) or build_scaffolds_from_scn_angles(...). For RNA, do something similar:</p>"},{"location":"pipeline/stageC/Unified%2C%20Comprehensive%20Plan%20for%20Integrating%20MP-NeRF%20into%20Stage%E2%80%AFC/#mp_nerfrnapy","title":"mp_nerf/rna.py","text":"<p>import torch from mp_nerf.massive_pnerf import mp_nerf_torch from mp_nerf.kb_rna import RNA_BUILD_INFO</p> <p>def build_scaffolds_rna_from_torsions(seq, torsions, device=\"cpu\"):     \"\"\"     Convert Stage B\u2019s predicted torsions for each residue into     mp-nerf-friendly dictionaries: cloud_mask, point_ref_mask,     angles_mask, bond_mask, etc.     \"\"\"     # 1) For each nucleotide i in seq, gather standard geometry from RNA_BUILD_INFO[seq[i]].     # 2) Overwrite the \u201cdihedrals\u201d from your 'torsions' array. For example, alpha=torsions[i,0], beta=torsions[i,1], ...     # 3) Possibly handle sugar ring or base as separate expansions.</p> <pre><code>scaffolds = {\n    \"cloud_mask\": ...,\n    \"point_ref_mask\": ...,\n    \"angles_mask\": ...,\n    \"bond_mask\": ...\n}\nreturn scaffolds\n</code></pre> <p>def rna_fold(scaffolds, device=\"cpu\"):     \"\"\"     The main parallel fold routine, analogous to protein_fold(...).     1) Place the backbone for each residue in parallel using mp_nerf_torch(...)     2) Link the subunits from 5' end to 3' end.     3) Build base sidechains in parallel, referencing glycosidic angles.     Returns: final coords shape [len(seq), #atoms_per_res, 3]     \"\"\"     # Implementation:     #  - Place the first residue\u2019s P at (0,0,0), etc.     #  - For subsequent residues, do a partial rotation/translation pass     #  - If needed, do a sidechain-like pass for the base ring     coords = ...     return coords</p> <p>4.2. Minimizing Changes to massive_pnerf.py and proteins.py     \u2022   The core function mp_nerf_torch(a, b, c, l, theta, chi) is universal. You typically do not need to modify it.     \u2022   If protein_fold(...) references \u201cN, CA, C\u201d and a 3-atom backbone, you can replicate that logic for \u201cP, O5\u2032, C5\u2032, \u2026\u201d in your new rna_fold(...).     \u2022   Keep large structural changes in rna.py. Let the existing protein code remain intact.</p> <p>\u2e3b</p> <ol> <li>Stage\u202fC Integration</li> </ol> <p>5.1. The Single Wrapper Function</p> <p>A concise approach from Version\u202f3 is to define a single \u201cStage\u202fC\u201d method that calls your new RNA build logic:</p>"},{"location":"pipeline/stageC/Unified%2C%20Comprehensive%20Plan%20for%20Integrating%20MP-NeRF%20into%20Stage%E2%80%AFC/#rna_predictpipelinestagecstage_c_reconstructionpy","title":"rna_predict/pipeline/stageC/stage_c_reconstruction.py","text":"<p>def run_stageC_rna_mpnerf(seq, predicted_torsions, device=\"cpu\"):     \"\"\"     Convert predicted RNA torsions into 3D coordinates using mp-nerf/rna logic.     \"\"\"     from mp_nerf.rna import build_scaffolds_rna_from_torsions, rna_fold     # 1) Create the scaffolds from the predicted angles     scaffolds = build_scaffolds_rna_from_torsions(seq, predicted_torsions, device=device)     # 2) Build the coordinates     coords = rna_fold(scaffolds, device=device)     return coords</p> <p>Call this in your pipeline\u2019s Stage\u202fC code:</p> <p>coords = run_stageC_rna_mpnerf(sequence, torsion_angles, device=\"cuda\")</p> <p>5.2. Optional Fallback</p> <p>If you still want your old forward-kinematics approach:</p> <p>def run_stageC(sequence, torsion_angles, method=\"mp_nerf\", device=\"cpu\"):     if method == \"mp_nerf\":         return run_stageC_rna_mpnerf(sequence, torsion_angles, device=device)     else:         return old_forward_kinematics(sequence, torsion_angles)</p> <p>This \u201ctoggle\u201d was a highlight from Version\u202f4\u2014you can keep a config-based approach in pipeline/config.py for method selection.</p> <p>\u2e3b</p> <ol> <li>Data Flow from Stage\u202fB</li> </ol> <p>6.1. Aligning Torsion Outputs to mp-nerf</p> <p>Your Stage\u202fB might yield angles in an array [alpha, beta, gamma, delta, epsilon, zeta, chi] for each residue. mp-nerf typically wants:     \u2022   A \u201cbond angle\u201d + \u201cdihedral\u201d for each new atom placed.     \u2022   Possibly an \u201cangles_mask\u201d shaped [2, L, #atoms], where angles_mask[0] is bond angles, angles_mask[1] is dihedrals.     \u2022   You can fill:     \u2022   angles_mask[1, i, 0] = alpha     \u2022   angles_mask[1, i, 1] = beta     \u2022   \u2026     \u2022   angles_mask[1, i, 6] = chi     \u2022   The \u201cbond angle\u201d portion might store typical RNA references or partial updates from your pipeline if it also predicts bond angles.</p> <p>6.2. Sugar Ring Torsions</p> <p>If Stage\u202fB also predicts sugar ring angles (\u03bd0..\u03bd4), you must incorporate them. You can treat them like an internal mini sidechain. Alternatively, you can skip ring closure and do a single \u201cfrozen ring.\u201d The user can decide how advanced the ring modeling is.</p> <p>6.3. Minimal or Detailed?     \u2022   Minimal: Only pass the backbone (\u03b1..\u03b6) plus a fixed ring and base geometry.     \u2022   Detailed: Fully pass ring torsions + base torsions. Each additional angle requires a spot in the \u201cscaffolds\u201d dictionary for mp-nerf to place them.</p> <p>\u2e3b</p> <ol> <li>Handling the Sugar Ring Closure</li> </ol> <p>7.1. Fixed Pucker (Straightforward)     \u2022   Hard-code bond angles for C2\u2032 or C3\u2032 endo, ignoring ring closure loops.     \u2022   This is akin to a \u201cpartial sidechain\u201d that\u2019s 100% static.</p> <p>7.2. Full or Partial Ring Closure</p> <p>If you want a fully flexible ring:     1.  Define the ring as a chain of 5 atoms (C1\u2032\u2013C2\u2032\u2013C3\u2032\u2013C4\u2032\u2013O4\u2032).     2.  Assign each bond length, bond angle, and dihedral from Stage\u202fB or from standard references.     3.  mp-nerf can place them in a linear chain.     4.  Then you either:     \u2022   Accept a small mismatch at the final bond if you do not strictly close the ring.     \u2022   or Iterate an extra step to close the final bond, possibly adjusting a single angle. (This is more advanced and not fully addressed by standard mp-nerf.)</p> <p>In practice, many choose the simpler approach.</p> <p>\u2e3b</p> <ol> <li>Base as Sidechain</li> </ol> <p>8.1. Parallel to \u201csidechain_fold(\u2026)\u201d</p> <p>In proteins.py, sidechains are built after the backbone is placed. For RNA, you can treat each base as a sidechain anchored at C1\u2032:     1.  Backbone: P\u2013O5\u2032\u2013C5\u2032\u2013C4\u2032\u2013\u2026\u2013C1\u2032 is placed first.     2.  Base: A set of ring atoms N9/C8/\u2026 is placed in parallel referencing the glycosidic angle (\u03c7).     3.  Connectivity: If the base ring is 6 or more atoms, you can either do a small ring closure or treat it as a single ring with partial constraints. Typically, we place the ring in a standard conformation or we rely on partial torsions from Stage B.</p> <p>\u2e3b</p> <ol> <li>Testing &amp; Validation</li> </ol> <p>9.1. Unit Tests</p> <p>Create a new test file, e.g., tests/test_rna.py:</p> <p>def test_rna_fold_basic():     from mp_nerf.rna import build_scaffolds_rna_from_torsions, rna_fold     seq = [\"A\",\"U\",\"G\"]     torsions = torch.tensor([...])  # shape [3, #torsions]     scaffolds = build_scaffolds_rna_from_torsions(seq, torsions)     coords = rna_fold(scaffolds)     assert coords.shape[0] == 3, \"Should have 3 residues\"     # Optionally check RMSD vs. a known small motif</p> <p>9.2. Round-Trip Check     1.  Take a small PDB with 3\u20135 nucleotides.     2.  Extract torsions using 3DNA or DSSR.     3.  Feed them to run_stageC_rna_mpnerf(...).     4.  Compare resulting 3D with the original using RMSD.</p> <p>9.3. Performance Bench     \u2022   For large RNAs (100\u20131000 residues), measure the time for 100 or 1000 folds. Compare with your older forward-kinematics approach. Expect big speed gains on CPU or GPU.</p> <p>\u2e3b</p> <ol> <li>Architectural Decisions &amp; Best Practices</li> </ol> <p>10.1. Differentiability</p> <p>If your pipeline does end-to-end training, ensure:     \u2022   All transformations remain in PyTorch (avoid Numpy calls that break autograd).     \u2022   mp_nerf_torch(...) is already differentiable.</p> <p>10.2. Adjacency or Pairing</p> <p>mp-nerf does not automatically account for base pairs or \u201clong-range constraints.\u201d If your pipeline needs explicit base-pair constraints, you might do a subsequent refinement or keep the adjacency in Stage B only.</p> <p>10.3. Potential Fallback or Hybrid</p> <p>As mentioned, you can keep your older kinematics approach or do partial usage. For instance, you can do a naive build for the sugar ring but use mp-nerf for the rest.</p> <p>\u2e3b</p> <ol> <li>Combining All Versions\u2019 Strengths</li> </ol> <p>Below is a short recap of how we merged each version\u2019s best elements into one design:     1.  Version\u202f1 (File-Level Organization &amp; Bridging)     \u2022   We adopt a bridging style of new modules (kb_rna.py + rna.py) to keep changes minimal in massive_pnerf.py.     \u2022   Encourages a clean modular approach.     2.  Version\u202f2 (Conceptual Explanation &amp; Data Transformation)     \u2022   We incorporate the advice on sugar ring approaches (fixed vs. ring closure) and the parallel subunit logic.     \u2022   Clarifies how your torsions flow into mp-nerf\u2019s scaffolding arrays.     3.  Version\u202f3 (Single Wrapper Function &amp; Minimal Intrusion)     \u2022   We use a single \u201crun_stageC_rna_mpnerf(...)\u201d function to unify building scaffolds and calling the fold.     \u2022   This keeps your pipeline code simpler.     4.  Version\u202f4 (RNA-Focused &amp; Optional Fallback)     \u2022   We specifically address building \u201cnucleotide + base sidechain,\u201d toggling old vs. new approaches, and handling partial ring logic.     \u2022   Maintains synergy with the existing pipeline config.</p> <p>Hence, the final design is more robust and comprehensive than any single earlier version.</p> <p>\u2e3b</p> <ol> <li>Conclusion</li> </ol> <p>By following this unified plan, you can adapt mp-nerf for RNA Stage\u202fC with minimal code disruption, high performance, and the flexibility to handle sugar ring angles, base sidechains, and partial or full ring closure. Key steps include:     1.  Creating an mp_nerf/kb_rna.py with your standard bond lengths/angles or bridging from final_kb_rna.py.     2.  Defining an rna_fold(...) function in mp_nerf/rna.py analogous to protein_fold(...).     3.  Adding a wrapper in stage_c_reconstruction.py (or similar) to call your new code from Stage\u202fB\u2019s torsion outputs.     4.  Testing with known short RNA motifs, checking RMSD or coordinate correctness.     5.  Optional fallback toggles or advanced ring closure routines as needed.</p> <p>With these changes, your Stage\u202fC pipeline harnesses mp-nerf\u2019s parallel efficiency, dramatically speeding up 3D coordinate generation for large RNAs while retaining a clean, modular code structure and the capacity to incorporate specialized RNA geometry references.</p>"},{"location":"pipeline/stageC/mp_nerf/","title":"MP-NeRF Details","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: You want to know if MP-NeRF (the massively parallel NeRF-based conversion) can be used inside a neural network and allow gradient-based backpropagation. [emoji]: The short answer is yes \u2013 MP-NeRF is designed to be differentiable and can be integrated as a layer in a deep learning pipeline. Below is an overview of how that works and how you could implement it:</p> <p>\u2e3b</p> <ol> <li>Why MP-NeRF is Differentiable     \u2022   The key step is that MP-NeRF computes Cartesian coordinates from internal coordinates by applying a series of transformations (bond lengths, bond angles, dihedral angles \u2192 3D xyz).     \u2022   These transformations are ultimately matrix multiplications, translations, and trigonometric operations on the angles. Because they are all differentiable functions with respect to the internal coordinates, modern automatic differentiation engines (e.g. PyTorch, JAX, TensorFlow) can track the gradients through these steps.     \u2022   In the MP-NeRF paper, they explicitly mention that the library is differentiable, so that \u201cone can train end-to-end deep learning models with it.\u201d</li> </ol> <p>\u2e3b</p> <ol> <li>Practical Steps for Integration<ol> <li>Import the MP-NeRF library \u2022   It\u2019s open-sourced at github.com/EleutherAI/mp_nerf. Clone or install it as a Python package.</li> <li>Use the provided functions as a layer \u2022   If it\u2019s PyTorch-based, you can treat the NeRF conversion function as part of your forward pass. For example:</li> </ol> </li> </ol> <p>import torch import mp_nerf  # Hypothetical import</p> <p>class MyProteinModel(torch.nn.Module):     def init(self, ...):         super().init()         # define your internal-coordinate predictor or anything else</p> <pre><code>def forward(self, internal_coords):\n    # Possibly shape: (batch_size, protein_length, 3) for bond-lengths/angles\n    # 1) Any neural net layers that predict or refine dihedrals\n    # ...\n    # 2) Convert predicted internal coords to Cartesian\n    xyz_coords = mp_nerf.nerf(internal_coords)  \n    # 3) Loss or subsequent modules that operate on xyz_coords\n    return xyz_coords\n\n\n\u2022   During backprop, PyTorch will compute partial derivatives of your loss w.r.t. the internal angles automatically (since matrix ops + trig ops are differentiable).\n\n3.  Attach a suitable loss function\n\u2022   For example, if you want to compare predicted Cartesian coordinates to experimental structures, you could use an RMSD or coordinate-based distance measure as your loss. That means:\n</code></pre> <p>\\mathcal{L} = \\mathrm{RMSD}\\bigl(\\hat{x}{\\mathrm{pred}}, x{\\mathrm{true}}\\bigr)     \u2022   Once the gradient is computed, it flows \u201cbackwards\u201d through the MP-NeRF step to your internal-coordinate predictor.</p> <p>\u2e3b</p> <ol> <li>Points to Note for Performance     \u2022   Batch Size Parallelizing across multiple protein chains or across multiple subunits can significantly reduce runtime on modern CPUs or GPUs.     \u2022   Data Transfer Overheads If you use a GPU, watch for overhead in transferring data between CPU \u2194 GPU. For shorter protein lengths, the overhead can dominate.     \u2022   Tradeoffs     \u2022   The authors mention that the Python/NumPy-based implementation is marginally slower than a pure C++ version might be, but it retains flexibility.     \u2022   If you need maximum single-core speed, you could re-implement the core routines in C++ or compile to GPU kernels. That said, they already see big speedups by leveraging matrix ops in parallel.</li> </ol> <p>\u2e3b</p> <ol> <li>Example Use Case</li> </ol> <p>Protein Folding Model     \u2022   You have a neural network that predicts dihedral angles \\phi, \\psi, \\omega per residue.     \u2022   Instead of stopping there, you transform those angles into 3D xyz coordinates using MP-NeRF.     \u2022   You compare the 3D structure to known coordinates from a PDB entry.     \u2022   The difference (e.g., L1 or RMSD) is your loss.     \u2022   You backprop through the entire pipeline, letting the network learn angle predictions that reduce 3D error.</p> <p>\u2e3b</p> <p>Answer Recap: Yes, MP-NeRF is directly usable in a neural network context, and you can backpropagate through it because all the coordinate transformations are differentiable. If you install and import it in PyTorch (or another AD framework), you can wrap it in a module, call the mp_nerf function during forward, and connect it to a loss on Cartesian coordinates.</p> <p>Would you like more details on specific code samples or on how to compile and optimize this library further?</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: [Aligning on your goal of creating a concise yet comprehensive set of notes on the provided text, capturing fundamental concepts and code organization.]</p> <p>[emoji]: Below is a set of structured notes that distill the key ideas and implementation details. The text revolves around MP-NeRF (Massively Parallel Natural Extension of Reference Frame), a Python package designed to convert between internal and Cartesian coordinates (primarily for proteins), with an emphasis on parallelization and speed.</p> <p>\u2e3b</p> <ol> <li>Directory and File Structure</li> </ol> <p>./ \u251c\u2500\u2500 mp_nerf/ \u2502   \u251c\u2500\u2500 init.py \u2502   \u251c\u2500\u2500 kb_proteins.py \u2502   \u251c\u2500\u2500 massive_pnerf.py \u2502   \u251c\u2500\u2500 ml_utils.py \u2502   \u251c\u2500\u2500 proteins.py \u2502   \u2514\u2500\u2500 utils.py \u251c\u2500\u2500 notebooks/ \u2502   \u251c\u2500\u2500 integrated_alanines.py \u2502   \u2514\u2500\u2500 integrated_test.py \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 test_main.py \u2502   \u2514\u2500\u2500 test_ml_utils.py \u251c\u2500\u2500 README.md \u2514\u2500\u2500 setup.py</p> <pre><code>\u2022   mp_nerf/: Main Python package implementing parallel NeRF functionality.\n\u2022   __init__.py: Exports key classes/functions from massive_pnerf.py and proteins.py.\n\u2022   kb_proteins.py: Knowledge base for protein residues, including sidechain definitions, bond lengths, angles, torsions, and indexes. A large dictionary structure (SC_BUILD_INFO, BB_BUILD_INFO, etc.) holds data for each amino acid.\n\u2022   massive_pnerf.py: Core parallel NeRF routines (e.g., mp_nerf_torch()) used to place a new atom given three references, bond length, bond angle, and dihedral.\n\u2022   ml_utils.py: Utilities for ML workflows, including functions to rename symmetric atoms, create embeddings, compute losses (e.g., torsion angle loss, FAPE loss).\n\u2022   proteins.py: High-level methods for constructing protein coordinates from angles, sidechain folding, building scaffolds from sequences. Integrates sidechain data with core geometry methods.\n\u2022   utils.py: General helper functions such as angle/dihedral computations, Kabsch alignment, reformatting coordinate shapes, plus additional I/O and data structures.\n\u2022   notebooks/: Contains example scripts/notebooks (integrated_alanines.py and integrated_test.py) showcasing usage, performance testing, and integration with other libraries like SidechainNet.\n\u2022   tests/: Basic test scripts (test_main.py, test_ml_utils.py) covering essential functionality and integration tests.\n\u2022   README.md: Repository overview, installation instructions, performance benchmarks, references, and usage details.\n\u2022   setup.py: Standard Python packaging script.\n</code></pre> <p>\u2e3b</p> <ol> <li>High-Level Functionality</li> </ol> <p>2.1. Natural Extension of Reference Frame (NeRF)     \u2022   NeRF calculates positions of atoms using bond lengths, bond angles, and dihedral angles (internal coordinates).     \u2022   Core function mp_nerf_torch(a, b, c, l, theta, chi) places a new point d based on reference points a, b, c, length l, bond angle \\theta, and dihedral \\chi.</p> <p>2.2. Parallelization Approach (MP-NeRF)     \u2022   Parallel Composition of Backbone:     1.  Each backbone subunit is built in parallel near the origin.     2.  A small sequence of NeRF steps for each subunit\u2019s minimal repeated structure (N, CA, C, etc.).     \u2022   Assembly of Backbone Subunits:     1.  Uses rotation-translation matrices derived from referencing adjacent subunits.     2.  Performs a cumulative sequential multiplication of these rotation matrices (still fewer steps than if the entire chain was done atom-by-atom).     \u2022   Parallel Sidechain Elongation:     1.  Once the backbone is in place, sidechain building can happen in parallel for each residue.     2.  Each sidechain is placed according to the same NeRF logic, referencing the known backbone coordinates.</p> <p>2.3. Protein Representation     \u2022   SidechainNet Format ((L, 14, 3)):     \u2022   L = length of the protein (number of residues).     \u2022   14 = up to 14 atoms per residue in the data scheme (N, CA, C, O, sidechain atoms).     \u2022   3 = Cartesian coordinates (x, y, z).     \u2022   kb_proteins.py:     \u2022   Dictionaries like SC_BUILD_INFO, BB_BUILD_INFO, SCN_CONNECT store default bond lengths, angles, and torsion patterns for each standard amino acid.     \u2022   Contains lookups for ambiguous sidechain atoms (e.g., Asp, Glu, ring flips).</p> <p>\u2e3b</p> <ol> <li>Key Modules and Their Roles<ol> <li>kb_proteins.py \u2022   Houses residue-specific data (bond lengths, angles, sidechain definitions). \u2022   BLOSUM substitution matrix included (for any AA scoring). \u2022   Routines for generating \u201ccloud masks\u201d (identifying valid atoms in each residue) and indexing sidechain groups.</li> <li>massive_pnerf.py \u2022   Provides the fundamental parallelized NeRF method mp_nerf_torch. \u2022   Contains helper for orthonormal basis (get_axis_matrix) to create rotation frames.</li> <li>ml_utils.py \u2022   ML-oriented functions: \u2022   scn_atom_embedd(seq_list): Embeds sequences to token IDs. \u2022   rename_symmetric_atoms(...): Disambiguates ring flips or symmetrical sidechains. \u2022   torsion_angle_loss(...): Computes loss between predicted vs. true torsion angles. \u2022   fape_torch(...): Frame-Aligned Point Error (FAPE) computation. \u2022   Additional noise injection or chain manipulation utilities.</li> <li>proteins.py \u2022   High-level assembly: \u2022   scn_cloud_mask(seq), scn_bond_mask(seq), scn_angle_mask(seq, angles): Generate coordinate placeholders and standard geometry for each residue. \u2022   build_scaffolds_from_scn_angles(...): Creates scaffold data from angles to feed the folding routine. \u2022   protein_fold(...) and sidechain_fold(...): Main workflows for building entire proteins from internal coordinates.</li> <li>utils.py \u2022   Low-level geometry: get_angle(...), get_dihedral(...), Kabsch alignment (kabsch_torch(X, Y)), RMSD calculations, plus convenience transformations for angles (to_pi_minus_pi, etc.). \u2022   Additional I/O or reformatting utilities.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Key Observations in the README     \u2022   Performance Gains: Claims 400\u20131200\u00d7 speedup vs. older pNeRF-style solutions. CPU-based approach can outperform certain GPU approaches due to overhead and data-transfer costs.     \u2022   Design Choices:     \u2022   Implementation in Python for readability/differentiability.     \u2022   Could be optimized further with compiled languages but aims for broad usability.     \u2022   Use Cases:     \u2022   Protein structure building from predicted angles in deep learning.     \u2022   Molecular dynamics or coordinate transformations.     \u2022   Comparison:     \u2022   Summaries of prior state-of-the-art approaches (Parsons, pNeRF, etc.).     \u2022   Benchmarked on chains of different lengths (100\u20131000 residues).</li> </ol> <p>\u2e3b</p> <ol> <li>Testing, Examples, and Integration     \u2022   Testing:     \u2022   tests/test_main.py &amp; tests/test_ml_utils.py: Basic usage checks, verifying angle/distance correctness, shape validations, etc.     \u2022   Notebooks:<ol> <li>integrated_alanines.py: Example pipeline + logging, timeit-based performance checks, usage of ProDy/SidechainNet.</li> <li>integrated_test.py: Similar performance analysis, focusing on computational speed for various protein lengths. \u2022   Integration: \u2022   Compatible with SidechainNet data loaders. \u2022   Could integrate with PyTorch geometry, standard PDB tools, or other ML frameworks requiring differentiable coordinate transformations.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Fundamental Concepts to Remember<ol> <li>NeRF Algorithm: Uses bond length l, bond angle \\theta, and torsion \\chi plus three reference points (a, b, c) to find point d in 3D space.</li> <li>Parallel Strategy: Instead of strictly sequential addition of one residue at a time, split the protein into subunits (often one subunit per residue), build them in parallel, then join them through a rotation-translation approach, and handle sidechains in parallel as well.</li> <li>Data Structures: Knowledge-base dictionaries define standard geometry for each amino acid so angles can be easily looked up.</li> <li>Differentiability: Implementation in PyTorch to allow end-to-end training (derivatives of geometry steps are possible).</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>References (Brief)     \u2022   Primary:     \u2022   Parsons et al. (2005) introduced NeRF for practical Cartesian conversions.     \u2022   AlQuraishi (2019) introduced pNeRF with partial parallelization for the backbone only.     \u2022   Bayati et al. (2020) high-performance transformations.     \u2022   MP-NeRF Paper:     \u2022   Preprint on bioRxiv (doi: 10.1101/2021.06.08.446214).     \u2022   Further details on theoretical background, benchmarks, and code structure.</li> </ol> <p>\u2e3b</p> <p>Would you like to explore any specific code module or concept in more depth?</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: [Aligning on your question: \u201cCan MP-NeRF be adapted for Stage C in an RNA pipeline, i.e., going from RNA torsion angles to 3D coordinates?\u201d]</p> <p>[emoji]: Short Answer: Yes, you can use MP-NeRF\u2019s core ideas (or the NeRF-style approach it implements) for RNA 3D reconstruction from torsion angles, but you\u2019d have to adapt the code to handle RNA-specific geometry (sugar\u2013phosphate backbone, sugar pucker, etc.). Out of the box, MP-NeRF is tailored to proteins, so you would need to:     1.  Replace or extend the protein \u201cknowledge base\u201d in kb_proteins.py with the bond lengths, angles, and torsion definitions for nucleotides (including the sugar ring, phosphate group, glycosidic bond, etc.).     2.  Adjust or omit sidechain code (the library is protein-centric) and swap in the correct set of RNA backbone angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6), the glycosidic angle (\u03c7), and sugar-pucker parameters.     3.  Rewrite the build logic in proteins.py (or add an analogous rna.py) so that the \u201cforward kinematics\u201d steps align with RNA\u2019s connectivity (P\u2013O5\u2032\u2013C5\u2032\u2013C4\u2032\u2013C3\u2032\u2013O3\u2032, etc.) instead of the protein N\u2013CA\u2013C backbone.</p> <p>Below is a more detailed explanation:</p> <p>\u2e3b</p> <ol> <li>What MP-NeRF Does and Why It Applies to RNA     \u2022   NeRF Algorithm: NeRF (Natural extension of Reference Frame) is a systematic way of placing atoms in 3D given internal coordinates (bond length, bond angle, torsion).     \u2022   MP-NeRF: A \u201cmassively parallel\u201d version that speeds up the standard sequential NeRF by grouping backbone fragments and sidechains in parallel. For proteins, it predefines standard bond lengths/angles for each residue, then uses dihedral angles to place sidechain atoms.</li> </ol> <p>Relevance to RNA     \u2022   RNA also has a repeat unit (nucleotide) with known local geometry. If you have torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, plus sugar puckers), you can systematically place each residue in 3D using the same fundamental geometry transformations that MP-NeRF uses for proteins.</p> <p>\u2e3b</p> <ol> <li>Key Adaptations for RNA vs. Protein<ol> <li>Different \u201cKnowledge Base\u201d \u2022   MP-NeRF\u2019s kb_proteins.py has huge dictionaries of standard bond lengths and angles for amino acids. You would replace or supplement these with RNA\u2019s known geometry: \u2022   Backbone bonds (P\u2013O5\u2032, O5\u2032\u2013C5\u2032, etc.). \u2022   Sugar ring bond angles and ring closure constraints (C3\u2032-endo, C2\u2032-endo). \u2022   Glycosidic link to the base (\u03c7 angle). \u2022   Possibly standard starting values for bond lengths (e.g., ~1.59 \u00c5 for P\u2013O, ~1.42 \u00c5 for C\u2013O in the phosphate).</li> <li>Rewrite Routines for \u201cResidue Build\u201d \u2022   In proteins.py (or massive_pnerf.py), the code expects a residue to have N, CA, C, O, etc. For RNA, the backbone atoms are P, O5\u2032, C5\u2032, C4\u2032, C3\u2032, O3\u2032 (plus the sugar ring). \u2022   You\u2019d either define an RNA-build function parallel to the protein-build approach or swap out code for each step of the backbone extension. The concept is the same: you have dihedral(\u03b1) around P\u2013O5\u2032, etc.</li> <li>Handle the Sugar Pucker \u2022   The sugar ring is a five-membered ring, typically summarized by pseudorotation angles or explicit ring torsions (\u03bd\u2080\u2026\u03bd\u2084). In practice, you might: \u2022   Fix the ring in C3\u2032-endo or whichever pucker you need, or \u2022   Add a small subroutine that sets the sugar ring conformation if the user provides a predicted pucker angle. \u2022   This is conceptually similar to \u201csidechain building\u201d for proteins. MP-NeRF does sidechain rotations after placing the backbone. For RNA, you do \u201csugar ring closure + base placement\u201d after placing the phosphate\u2013ribose backbone skeleton.</li> <li>Base Placement \u2022   Proteins have sidechains enumerated in dictionaries; for RNA, you have four possible \u201cbases\u201d that differ in ring structure and glycosidic bond orientation (syn vs. anti). \u2022   The library\u2019s parallel sidechain logic can be turned into \u201cparallel base placement.\u201d You\u2019d define standard ring geometry for each base (A, U, G, C) and apply the torsion angle \u03c7 for the glycosidic bond.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Practical Steps to Implement<ol> <li>Create kb_rna.py (analogous to kb_proteins.py) \u2022   Collect standard bond lengths/angles for RNA. For example:</li> </ol> </li> </ol> <p>RNA_BUILD_INFO = {   'A': { ... standard geometry for A ... },   'U': { ... },   'G': { ... },   'C': { ... }, }</p> <pre><code>\u2022   For the backbone, define the partial dictionary that\u2019s repeated for each residue: P\u2013O5\u2032 bond length, O5\u2032\u2013C5\u2032 bond length, angles at O5\u2032\u2013C5\u2032\u2013C4\u2032, etc.\n\n2.  Add or Replace \u201cprotein_fold\u201d with \u201crna_fold\u201d\n\u2022   In proteins.py, you see protein_fold(...) or sidechain_fold(...). Create your version, e.g. rna_fold(...), that:\n\u2022   Places the first residue\u2019s atoms in a reference orientation (like setting P at the origin, O5\u2032 on x-axis, etc.).\n\u2022   Iterates across nucleotides. For each one, read the torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7).\n\u2022   Use the same \u201cmp_nerf_torch(\u2026)\u201d function that does the coordinate transformation, but pass in your references for RNA bond lengths and angles.\n3.  Handle the Sugar\n\u2022   Either:\n\u2022   Hard-code C3\u2032-endo or C2\u2032-endo geometry, or\n\u2022   Let the user pass in ring torsion angles (\u03bd\u2080\u2013\u03bd\u2084) and close the ring. This is more complex but yields more accurate sugar conformation.\n\u2022   Possibly adapt MP-NeRF\u2019s \u201csidechain building\u201d method to do the ring closure in parallel if you want maximum speed.\n4.  Test &amp; Validate\n\u2022   Start with a known small RNA (e.g. a short helix from the PDB) for which you know the torsion angles.\n\u2022   Convert them to 3D, measure RMSD vs. the real structure.\n\u2022   If everything lines up, the approach is working.\n</code></pre> <p>\u2e3b</p> <ol> <li>Summary: Yes, But Expect Some Work     \u2022   MP-NeRF is a generalizable NeRF approach with efficient parallel steps.     \u2022   For Stage C in an RNA pipeline (torsion \u2192 3D), you absolutely can adapt MP-NeRF, reusing its matrix-based transformations and parallel \u201cbatch\u201d logic.     \u2022   Main Task: Provide RNA geometry definitions and remove or replace the protein-specific dictionary calls.     \u2022   Outcome: You\u2019d have a fast, parallelized \u201cRNA NeRF\u201d solution well-suited to large RNAs.</li> </ol> <p>\u2e3b</p> <p>Recommended Next Steps     1.  Clone the MP-NeRF repo.     2.  Study kb_proteins.py and proteins.py to see how they store bond geometry and do sequential chain building.     3.  Create an RNA geometry module (bond lengths/angles).     4.  Implement or modify the folding routine to handle P\u2013O5\u2032\u2013C5\u2032\u2013C4\u2032\u2013C3\u2032\u2013O3\u2032 plus sugar/base attachments.     5.  Validate with a known short RNA.     6.  Optional: If you want more advanced ring flexibility, you might define partial dihedrals for sugar ring or do a ring-closure sub-step.</p> <p>Once done, you can feed your Stage B torsion predictions (\u03b1\u2026\u03b6, \u03c7, etc.) into this adapted routine to generate 3D coordinates for the entire RNA chain.</p> <p>\u2e3b</p> <p>Conclusion: Adapting MP-NeRF to RNA is fully feasible\u2014its core parallels remain the same (NeRF). You just have to supply RNA\u2019s geometry \u201cknowledge base\u201d and override the residue-building routines. Good luck with your Stage C RNA pipeline!</p>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/","title":"\ud83c\udf00 Stage D: Diffusion-Based RNA Structure Refinement","text":""},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#overview","title":"Overview","text":"<p>Stage D performs final refinement of RNA 3D structures using diffusion models. This stage takes the output from Stage C and further optimizes the structure through iterative refinement.</p>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#key-features","title":"Key Features","text":"<ul> <li>Diffusion-based refinement of RNA coordinates</li> <li>Energy minimization integration</li> <li>Steric clash resolution</li> <li>Optional geometric constraints</li> </ul>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#configuration-hydra","title":"Configuration (Hydra)","text":"<p>Stage D is configured using Hydra with parameters defined in:</p> <ul> <li>Main settings: <code>rna_predict/conf/model/stageD.yaml</code></li> <li>Diffusion model settings: <code>rna_predict/conf/diffusion_model/</code></li> </ul>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#key-configuration-parameters","title":"Key Configuration Parameters","text":"<pre><code># rna_predict/conf/model/stageD.yaml\nstageD:\n  method: \"diffusion\"       # \"diffusion\" or \"classical\"\n  device: \"cuda\"           # \"cpu\" or \"cuda\"\n  steps: 1000              # Diffusion steps\n  guidance_scale: 7.5      # Controls refinement strength\n  apply_constraints: true  # Apply geometric constraints\n  # ... other parameters ...\n</code></pre>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#command-line-overrides","title":"Command-Line Overrides","text":"<p>Override parameters using dot notation:</p> <ul> <li> <p>Change diffusion steps:     <pre><code>python -m rna_predict.pipeline.stageD.main stageD.steps=500\n</code></pre></p> </li> <li> <p>Run on CPU:     <pre><code>python -m rna_predict.pipeline.stageD.main stageD.device=cpu\n</code></pre></p> </li> <li> <p>Disable constraints:     <pre><code>python -m rna_predict.pipeline.stageD.main stageD.apply_constraints=false\n</code></pre></p> </li> </ul>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#hpc-execution","title":"HPC Execution","text":"<p>For High Performance Computing (HPC) environments, see the HPC Integration Guide for SLURM and GridEngine examples.</p> <p>Basic HPC Example: <pre><code>python -m rna_predict.pipeline.stageD.main \\\n    stageD.device=cuda \\\n    stageD.steps=1000 \\\n    +hpc_cluster=slurm \\\n    hydra.launcher.gpus=1\n</code></pre></p>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>GPU Memory: Requires significant GPU memory (16GB+ recommended)</li> <li>Runtime: Scales linearly with number of diffusion steps</li> <li>Multi-GPU: Supports data parallel distribution</li> </ul>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#validation-metrics","title":"Validation Metrics","text":"<ul> <li>RMSD improvement over input structure</li> <li>Steric clash reduction</li> <li>Energy score improvement</li> <li>Geometric constraint satisfaction</li> </ul>"},{"location":"pipeline/stageD/StageD_Diffusion_Refinement/#references","title":"References","text":"<ul> <li>Diffusion models for molecular refinement</li> <li>RNA-specific energy functions</li> <li>Geometric constraint methods</li> </ul>"},{"location":"pipeline/unified_latent/Perceiver_IO/","title":"Perceiver IO","text":"<p>\u2e3b</p> <p>Perceiver IO: A General Architecture for Structured Inputs and Outputs</p> <ol> <li>Introduction and Core Motivation</li> </ol> <p>Perceiver IO (\\text{Jaegle et al., ICLR 2022}) is a general-purpose neural network architecture designed to handle:     1.  Arbitrary input modalities\u2014from images, text, and audio waveforms to multimodal combinations (video+audio, symbolic sets, etc.).     2.  Arbitrary output structures, which can be as simple as a single class label or as complex as dense 2D/3D fields (optical flow, segmentation), multiscale audio waveforms, or sets of discrete symbolic tokens.</p> <p>1.1 Why Another Architecture?</p> <p>Many existing deep-learning systems are:     \u2022   Specialized: Different modules or entire networks are used for different tasks or data types (e.g., language vs. image).     \u2022   Scaling Bottlenecks: Transformers, while highly effective in language, typically scale quadratically (\\mathcal{O}(T^2)) with the input length T. This becomes prohibitive for large images or raw audio waveforms without heavy preprocessing.     \u2022   Limited Output Flexibility: Even successful cross-domain architectures (like the original Perceiver) could only straightforwardly produce simple outputs (e.g., a single classification). More structured outputs (optical flow, entire waveforms) required specialized heads or specific design heuristics.</p> <p>Goal: Perceiver IO aims to unify large-scale input handling (as in the original Perceiver) with the ability to generate large, structured outputs (IO stands for \u201cInput-Output\u201d), enabling a single model to do \u201cread-process-write\u201d for diverse tasks.</p> <p>\u2e3b</p> <ol> <li>Architectural Overview</li> </ol> <p>Perceiver IO builds on the original Perceiver (\\text{Jaegle et al., 2021}) by introducing a flexible decoding mechanism for any output shape. Let us break down the three-stage process:     1.  Encode (Cross-Attention Encoder)     \u2022   Inputs: We have an input array \\mathbf{x} of shape M \\times C. This might be (a) images converted into patches or RGB pixel embeddings; (b) text tokens or raw bytes; (c) audio waveforms, etc.     \u2022   Cross-Attention: The Perceiver IO maps these M inputs into a fixed-size latent array \\mathbf{z} \\in \\mathbb{R}^{N \\times D} via a cross-attention module. Crucially, N can be much smaller than M. This step effectively compresses large inputs into a manageable latent space.     2.  Process (Latent Transformer / Latent Self-Attention)     \u2022   Having a fixed-size latent allows the network to perform multiple layers of Transformer-style self-attention with complexity \\mathcal{O}(N^2), independent of the original input size M.     \u2022   This \u201clatent bottleneck\u201d is the heart of Perceiver-like approaches, as it decouples deep processing from input length.     3.  Decode (Cross-Attention Decoder + Output Queries)     \u2022   The new Perceiver IO decoding mechanism uses output queries. For an output array of size O \\times E, it constructs a set of O query vectors \\mathbf{q}_1, \\dots, \\mathbf{q}_O.     \u2022   Each query attends to the latent array \\mathbf{z} (via cross-attention again) to produce the output element \\mathbf{y}_i.     \u2022   Why Queries? This design is highly flexible: you can produce a wide variety of outputs\u2014dense pixel maps, sequences of tokens, sets of symbolic units, or a single classification\u2014all from the same latent space.</p> <p>2.1 Internal Attention Modules</p> <p>Each module follows a QKV (query-key-value) attention pattern plus a feed-forward MLP:     \u2022   Encoder: Key-Value (KV) inputs come from the high-dimensional input \\mathbf{x}; queries (Q) come from the latent array.     \u2022   Processor: Self-attention among latent elements only (latent as both Q and KV).     \u2022   Decoder: Key-Value (KV) inputs come from the latent array; queries (Q) come from the learnable output queries.</p> <p>2.2 Complexity and Scalability</p> <p>A standard Transformer suffers \\mathcal{O}(T^2) per layer. In Perceiver IO:     \u2022   Encode: \\mathcal{O}(M \\times N)     \u2022   Latent Self-Attention: \\mathcal{O}(N^2 \\times L) for L layers     \u2022   Decode: \\mathcal{O}(O \\times N)</p> <p>Hence total \\approx \\mathcal{O}\\bigl( (M + O) \\times N + L \\times N^2 \\bigr), which is linear in input M and output O if N is chosen to be relatively small. This allows the model to handle very large inputs (e.g. raw text bytes up to length 2048) and large outputs (dense images, waveforms) with feasible memory and compute.</p> <p>\u2e3b</p> <ol> <li>Methodological Highlights</li> </ol> <p>3.1 Constructing Input Embeddings     \u2022   Raw Bytes (Text): Directly embed UTF-8 bytes (plus a few special tokens). Eliminates tokenization overhead and engineering of subword vocabularies.     \u2022   Patches or Convolution (Images, Video): For large images, one can patch or do a light convolution+pool to reduce dimensionality. Fourier features or learnable positional embeddings can be concatenated for positional cues.     \u2022   Multimodal Tagging: For tasks with multiple modalities\u2014e.g. video + audio\u2014one can prepend or concatenate a small \u201cmodality token\u201d so the model differentiates them.</p> <p>3.2 Output Query Construction</p> <p>A crucial novelty is how Perceiver IO decodes:     \u2022   Single-Query Classification: For classification tasks like ImageNet, we can have a single query embedding that attends to the latent and returns a single logit vector.     \u2022   Dense Queries: Optical flow or segmentation tasks can assign a query to each pixel or spatial location, typically encoding (x,y) coordinates. Each query then attends to the latent to produce the flow/label for that pixel.     \u2022   Multi-Task/Multimodal: Kinetics autoencoding might combine position embeddings (for frames or audio samples) plus a \u201cmodality embedding\u201d that indicates whether we decode video, audio, or label.</p> <p>3.3 Training and Subsampling of Outputs</p> <p>When output dimension O is huge (e.g., hundreds of thousands of positions for high-resolution video+audio):     \u2022   Subsample the output queries during training: sample a subset of pixel/voxel locations or audio time steps each minibatch.     \u2022   Full Decoding can be done at inference, possibly in mini-batches of queries if memory is a concern.</p> <p>\u2e3b</p> <ol> <li>Experimental Evaluation</li> </ol> <p>Perceiver IO was tested across multiple domains, showcasing its generality:</p> <p>4.1 Language (Masked Language Modeling and GLUE)     \u2022   Task: Train on a large text corpus (C4 + Wikipedia) with masked language modeling (MLM). Then fine-tune on GLUE (a standard NLP benchmark).     \u2022   Findings:     \u2022   UTF-8 Bytes: Perceiver IO can match or exceed BERT-like models on GLUE while directly processing raw bytes (no subword tokenization).     \u2022   Efficiency: Under the same FLOPs budget, the byte-level Perceiver IO outperforms a similarly \u201cde-tokenized\u201d BERT baseline by a substantial margin.     \u2022   Multitask Queries: A single Perceiver IO can handle multiple GLUE tasks by adopting separate query embeddings for each task, effectively replacing BERT\u2019s [CLS] token approach.</p> <p>4.2 Optical Flow (Sintel, KITTI)     \u2022   Traditional Challenge: Optical flow typically relies on cost volumes or correlation for capturing large motions.     \u2022   Perceiver IO Approach:     1.  Concatenate two consecutive frames along the channel dimension, possibly with 3\u00d73 patches around each pixel plus (x,y) positional features.     2.  Encode \u2192 Process in latent \u2192 Decode a flow vector for each pixel\u2019s query.     \u2022   Results:     \u2022   SOTA Performance: Achieves near or better than state-of-the-art, outperforming RAFT (Teed &amp; Deng, 2020) and PWC-Net (Sun et al., 2018) on some benchmarks (e.g., Sintel.final).     \u2022   Surprising Generality: Succeeds without explicit multi-scale or correlation-volume modules, purely from learned cross-attention.</p> <p>4.3 Multimodal Autoencoding (Kinetics-700)     \u2022   Setup: Input is raw video frames (16 frames at 224\u00d7224) + raw audio (48kHz) + 700-class label. This is huge: ~800k input points if fully unrolled.     \u2022   Model:     \u2022   A single Perceiver IO compresses everything into the latent.     \u2022   Queries are built for each output position: e.g., video pixel positions, audio sample indices, class label queries.     \u2022   Findings:     \u2022   Can reconstruct (autoencode) the video, audio, and label from the latent representation.     \u2022   Showcases how \u201cmodality tokens\u201d plus coordinate embeddings allow flexible bridging of multiple data streams.</p> <p>4.4 Image Classification (ImageNet)     \u2022   Motivation: Validate that Perceiver IO is also effective for standard image classification.     \u2022   Performance:     \u2022   Reaches &gt;80% top-1 accuracy on ImageNet even without 2D convolutions or patch embedding, showing that the cross-attend decoding outperforms the older \u201caverage+project\u201d approach.     \u2022   After large-scale pretraining (e.g., on JFT), the model surpasses 84% top-1.</p> <p>4.5 Symbolic Outputs (StarCraft II via AlphaStar)     \u2022   AlphaStar: A high-profile RL system for StarCraft II uses a Transformer to encode sets of \u201centities\u201d (e.g., units, buildings).     \u2022   Replacing the Transformer: Perceiver IO can directly substitute the entity encoder with minimal tuning, preserving the ~87% elite-bot win rate while reducing FLOPs by about 3\u00d7.</p> <p>4.6 AudioSet Classification     \u2022   Task: Classify 10s audio-video clips among 527 labels.     \u2022   Results: Perceiver IO slightly outperforms the original Perceiver\u2019s average+project decoder, demonstrating that the attention-based decoder is beneficial even for \u201csimple\u201d classification tasks.</p> <p>\u2e3b</p> <ol> <li>Performance, Efficiency, and Complexity</li> </ol> <p>Key claim: Perceiver IO decouples the input and output dimensionalities from the deep processing. Once the data is in the latent, additional layers (depth L) only scale with N, the latent dimension. As a result:     \u2022   Large inputs (raw waveforms, 4K images) can be scaled more gracefully.     \u2022   Large or structured outputs (entire flow fields, entire waveforms) remain feasible: decoding is \\mathcal{O}(O \\times N), rather than \\mathcal{O}(O^2).</p> <p>5.1 Latent Size N Tuning</p> <p>One important hyperparameter is the latent index dimension N.     \u2022   Trade-Off: Larger N can capture more detail in the latent representation but increases the cost of each self-attention layer.     \u2022   Practice: The authors typically choose moderate values like N=256 or N=512 in language tasks. For vision tasks, they sometimes go higher, e.g. N=1024, depending on hardware constraints.</p> <p>5.2 Hardware Considerations     \u2022   TPU vs. GPU: Some experiments (like optical flow) show that Perceiver IO can be faster on TPUs than specialized methods, even if it may be slower on standard GPUs due to memory layouts in attention vs. specialized operations.</p> <p>\u2e3b</p> <ol> <li>Limitations and Considerations</li> </ol> <p>Despite its strengths, Perceiver IO has limitations worth keeping in mind:     1.  Memory for Extremely Large Inputs     \u2022   While the complexity is linear in input size, you still need enough memory to hold \\mathbf{x} in a single pass unless chunking or patch sampling is done. This can be challenging if M is extremely large (e.g., unrolled 4K video frames).     2.  Output Subsampling for Training     \u2022   For tasks with massive output arrays (e.g., video reconstruction), the approach often subsamples the output queries during training. Full decoding is still linear but can become large in practice. This can complicate training or slow down inference for extremely dense tasks.     3.  Latent Dimension Tuning     \u2022   The choice of N is a critical hyperparameter balancing representational capacity vs. compute. The correct value is somewhat task-dependent, so it may require iterative experimentation.     4.  Domain-Specific Preprocessing     \u2022   In principle, Perceiver IO can handle raw signals. However, some tasks (especially large images or raw audio) still benefit from mild domain-aware steps (e.g. patching, short convolutions) to reduce the raw dimensionality or capture local structure before cross-attention.     5.  Query Construction     \u2022   Designing robust query embeddings for complex tasks (especially multi-task or multimodal outputs) can require careful engineering or domain knowledge (e.g., coordinate embeddings, learned vs. Fourier positional encodings).     6.  Model Size and FLOPs     \u2022   While the model can surpass specialized systems, it might have higher parameter counts or FLOPs if not carefully tuned. The theoretical linear efficiency is an advantage, but the constant factors in attention can still be large.</p> <p>\u2e3b</p> <ol> <li>Conclusion and Key Takeaways</li> </ol> <p>Perceiver IO is a scalable, flexible, and domain-agnostic neural architecture that:     1.  Reads massive inputs (images, bytes, waveforms) into a modest latent bottleneck via cross-attention.     2.  Processes the latent array using repeated self-attention that is independent of input and output sizes.     3.  Writes arbitrary structured outputs via a powerful query-based decoding mechanism.</p> <p>Empirical Results:     \u2022   Matches or exceeds specialized baselines on language (comparable to or better than BERT), optical flow (near or state-of-the-art on Sintel), ImageNet classification (&gt;80% top-1 without 2D assumptions), multi-modal tasks (audio+video), and discrete sets (StarCraft II).</p> <p>Why It Matters:     \u2022   Unified Architecture: Reduces or removes the need for domain-specific trunk engineering.     \u2022   Linear Scaling: More friendly to large input/output tasks, both unimodal and multimodal.     \u2022   Structured Output Decoding: A powerful query-based approach that can unify tasks from classification to dense predictions.</p> <p>Looking Ahead:     \u2022   Continued exploration of how to best handle extremely large inputs and outputs, including improved chunking strategies and dynamic query design.     \u2022   Enhanced theoretical understanding of how the latent bottleneck learns to represent diverse signals.     \u2022   Potential synergy with other \u201ctoken-free\u201d or \u201cdomain-free\u201d methods (e.g. raw audio-linguistic training, 3D spatiotemporal tasks).</p> <p>\u2e3b</p> <p>References to Key Points     \u2022   Jaegle et al. (2021): Original Perceiver approach.     \u2022   Transformers: Vaswani et al. (2017), BERT (Devlin et al., 2019).     \u2022   Comparable SOTA: Optical Flow (RAFT, PWCNet), ImageNet (ResNet, ViT), Language (BERT, Byte-based models).     \u2022   Multimodal: Kinetics, AudioSet, StarCraft II entity sets.</p> <p>\u2e3b</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: This final document should provide a thorough technical understanding of the Perceiver IO framework. It integrates the advantages of detailed, step-by-step logic (like in V2), the extended coverage of specialized tasks and limitations (V1), and the concise structuring of major concepts (V3)\u2014all while addressing the criticisms regarding clarity, depth, and organization.</p>"},{"location":"pipeline/unified_latent/Perceiver_IO/#emoji-if-you-have-further-questions-on-any-specific-domain-experiment-eg-details-on-optical-flow-ablations-or-the-precise-query-embeddings-in-kinetics-autoencoding-feel-free-to-ask-for-an-expanded-section","title":"[emoji]: If you have further questions on any specific domain experiment (e.g., details on optical flow ablations or the precise query embeddings in Kinetics autoencoding), feel free to ask for an expanded section.","text":"<p>Below is a comprehensive, \u201cbest\u2010of\u2010all\u2010worlds\u201d architectural design document that fuses the strengths of earlier versions (V1, V2, V3, and the previous \u201cFinal\u201d references), while directly addressing the criticisms raised\u2014particularly about:     \u2022   Ensuring synergy between local (torsion\u2010based) and global (pairwise) modules,     \u2022   Clarifying mandatory vs. optional pipeline components,     \u2022   Explaining how Perceiver\u202fIO (or a simpler \u201cmini\u2010Transformer/MLP\u201d) can serve as the \u201cUnified Latent Merger,\u201d     \u2022   Incorporating LoRA/QLoRA to keep memory usage manageable for large pretrained modules (TorsionBERT, Pairformer),     \u2022   Providing a robust end\u2010to\u2010end strategy (including forward kinematics, energy minimization, multi\u2010loss training, etc.).</p> <p>The result is a verbose, implementation\u2010oriented piece of technical documentation that should surpass the sum of its parts in clarity, depth, and synergy.</p> <p>\u2e3b</p> <ol> <li>High-Level Pipeline &amp; Goals</li> </ol> <p>1.1 Overall Objective</p> <p>We want to predict RNA 3D coordinates from sequence data. We do so by:     1.  Local Torsion Pipeline: TorsionBERT (or a similar BERT-like model) that outputs torsion angles for each residue, guided by adjacency (2D structure).     2.  Global Pairwise Trunk: An AlphaFold\u202f3\u2013style Pairformer that ingests MSA or single-sequence input and adjacency signals to produce pair embeddings z_{ij} + single embeddings s_{i}.     3.  Unified Latent Merger (ULM): Merges local angles + adjacency with global pair embeddings to yield a single \u201cconditioning latent.\u201d This is where we can use a small Transformer/MLP or a more advanced Perceiver\u202fIO approach.     4.  Diffusion Module: Converts random/noisy coordinates (optionally partial from forward kinematics) into final 3D structure(s) using the merged latent.     5.  (Optional) Forward Kinematics: If we want partial 3D \u201cwarm starts\u201d from the torsion angles.     6.  (Optional) Energy Minimization: A short post\u2010inference pass (e.g., local MD) to fix minor sterics or bond angles.     7.  Multi\u2010Loss: Typically a final 3D RMSD/lDDT or distance\u2010based loss for the Diffusion, plus an angle loss for TorsionBERT if you have torsion labels.</p> <p>1.2 Why This Combined Architecture?     \u2022   Synergy: We don\u2019t want to lose adjacency or pair embeddings. Torsion angles alone are local, so we incorporate global pair constraints from the Pairformer.     \u2022   Flexibility: If N is large, the number of pair embeddings can be N^2. We must unify them efficiently. That\u2019s where an advanced \u201cULM,\u201d possibly Perceiver\u202fIO, helps.     \u2022   Memory Constraints: We partial\u2010finetune only small LoRA adapters in TorsionBERT/Pairformer to keep GPU usage feasible.     \u2022   Accuracy: By combining local + global constraints in one final diffusion pass, we typically see improved 3D predictions over separate or \u201coptional\u201d merges.</p> <p>\u2e3b</p> <ol> <li>Mandatory vs. Optional Steps</li> </ol> <p>A key criticism of earlier \u201cversioned\u201d designs was the confusion around how many steps are truly needed vs. \u201cnice to have.\u201d Let\u2019s clarify:     1.  Mandatory     \u2022   Torsion Pipeline (TorsionBERT + adjacency): We need local angles for synergy.     \u2022   Pairformer (AF3-like trunk): We need global pair constraints.     \u2022   Unified Latent: So the final 3D generator (Diffusion) sees both local + global embeddings.     \u2022   Diffusion: The main generative step for final 3D.     2.  Strongly Recommended     \u2022   Energy Minimization: Even a short minimization helps fix steric or bond\u2010length problems.     \u2022   Adjacency: TorsionBERT heavily relies on adjacency. If we skip adjacency, torsion predictions degrade.     3.  Truly Optional     \u2022   Forward Kinematics: You can do partial 3D from angles (via MP-NeRF) if you want an initial conformation. If the torsion predictions are poor or if time is short, let the Diffusion handle from random noise.     \u2022   MSA: If multiple sequences exist, the Pairformer\u2019s performance is improved. Otherwise, single\u2010sequence mode is an option.     \u2022   Template: Some advanced workflows might feed partial 3D from external templates. Not mandatory.</p> <p>By labeling these carefully, we ensure synergy isn\u2019t lost: local angles and global pair embeddings are always merged for the final 3D generation.</p> <p>\u2e3b</p> <ol> <li>Step-by-Step Technical Diagram</li> </ol> <p>Inputs &amp; Setup (sequence, adjacency, MSA, optional partial coords)         \u2502         v (1) TorsionBERT (LoRA) \u2500\u2500&gt; (angles)         \u2502         \u251c\u2500(Optional) Forward Kinematics (partial 3D)         \u2502         \u2514\u2500\u2500&gt; (angles + adjacency + partial coords) ----                                                         \\                                (2) Pairformer (LoRA) ---&gt; (z\u1d62\u2c7c, s\u1d62)                                                          /                                                         \u2193                 (3) Unified Latent Merger (could be Perceiver IO or smaller subnetwork)                                                         \u2193                          (4) Diffusion (LoRA optional) \u2192 final 3D coords                                                         \u2193                   (5) Energy Minimization (Short MD) \u2192 polished final 3D</p> <p>\u2e3b</p> <ol> <li>Detailed Modules &amp; Where Perceiver\u202fIO Fits</li> </ol> <p>4.1 TorsionBERT + Adjacency     1.  Input: RNA sequence (length N), plus adjacency from a 2D method (RFold, etc.).     2.  Output: Torsion angles (\\alpha, \\beta, \\ldots, \\chi) for each residue, possibly sugar pucker.     3.  LoRA: We freeze the large pretrained \u201cBERT\u201d backbone and add rank\u2010limited LoRA adapters in its attention or feed\u2010forward layers. This drastically reduces trainable parameters.</p> <p>Indexing: Keep a consistent residue list from 0..N\u22121. If adjacency includes base pairs, we store them in a matrix or dictionary. The TorsionBERT final heads produce angles in the correct order.</p> <p>4.2 Pairformer (AlphaFold\u202f3\u2013Style)     1.  Input:     \u2022   Possibly an MSA, or a single sequence if MSA is unavailable.     \u2022   Optional adjacency as a bias (like a logit shift or an embedding factor).     2.  Trunk: ~48 blocks of triangular attention, pair updates, etc.     3.  Output: A pair embedding \\mathbf{z}_{ij} (dimension pair_dim) for each residue pair (i,j), plus single embeddings \\mathbf{s}_i.     4.  LoRA: Freeze the main trunk and insert LoRA. This partial finetuning approach keeps memory usage feasible.</p> <p>4.3 (Optional) Forward Kinematics     \u2022   If used, we feed the TorsionBERT angles into a differentiable NeRF approach to get partial 3D.     \u2022   This partial conformation can help the Diffusion start from something less random.     \u2022   If angles are inaccurate, it might hamper the pipeline, so we can skip it and let Diffusion do the entire 3D from scratch.</p> <p>4.4 Unified Latent Merger (ULM)</p> <p>Core Step: merges local angles + adjacency + partial coords with global pair embeddings.     \u2022   Standard Approach:     \u2022   A small MLP or mini\u2010Transformer that ingests node\u2010level angles, adjacency info, plus pair\u2010level \\mathbf{z}{ij}. Output: a single \u201clatent array\u201d or \u201cconditioning vector.\u201d     \u2022   Advanced Approach: Perceiver\u202fIO     \u2022   If \\mathbf{z}(N^4)).     \u2022   Perceiver\u202fIO uses cross\u2010attention to read many tokens (angles, adjacency, pair embeddings) into a smaller latent dimension N{\\prime}. Then repeated self\u2010attention is only \\mathcal{O}(N{\\prime}^2). Finally, decode (O queries) to produce the final synergy vector.     \u2022   Pro: Great for scaling to large RNA or complex embeddings, easily merges multiple modalities.     \u2022   Con: More code complexity than a small MLP. Overkill for very small N.} is large (like N^2 for big RNAs), a naive Transformer might blow up in memory (\\mathcal{O</p> <p>Hence, if your pipeline must unify large pair embeddings or you anticipate adding new constraints (like partial templates, more adjacency data), Perceiver\u202fIO is strongly recommended for synergy.</p> <p>4.5 Diffusion     1.  Goal: Denoise random/noisy coords (or partial coords) into final 3D.     2.  Conditioning: The \u201cunified latent\u201d from step (4.4). Possibly fed at each diffusion step or used as an initial \u201ccontext.\u201d     3.  LoRA: If the Diffusion model is large (like a 3D U\u2010Net or Transformer), freeze base weights, add LoRA. If it\u2019s moderate sized, you can train fully.     4.  Output: final 3D coordinates. Because it\u2019s generative, we can sample multiple times for an ensemble.</p> <p>4.6 Energy Minimization     1.  Implementation: short local MD or partial minimization (Amber, CHARMM, etc.).     2.  No gradient: Typically outside the end\u2010to\u2010end backprop.     3.  Ensemble: Evaluate ~5\u201310 diffusion samples. Minimization might fix small sterics. Choose the top structure(s) by geometry score or some model confidence metric.</p> <p>\u2e3b</p> <ol> <li>Multi\u2010Loss Training &amp; Backprop</li> </ol> <p>Because we have multiple sub\u2010modules, each with partial or full finetuning, we define multi\u2010objective losses:     1.  Angle Loss \\mathcal{L}{\\mathrm{angle}}:     \u2022   If you have ground\u2010truth angles, you can match TorsionBERT\u2019s outputs to those angles (circular MSE, for example).     \u2022   Directly updates TorsionBERT LoRA parameters.     2.  3D Loss \\mathcal{L}:     \u2022   Compare final 3D from Diffusion to known 3D structure. RMSD, lDDT, or FAPE are common.     \u2022   Grad flows through the Diffusion \u2192 Unified Merger \u2192 Pairformer (LoRA) + TorsionBERT (LoRA).     3.  (Optional) Pair Distogram Loss \\mathcal{L}_{\\mathrm{pair}}:     \u2022   If you have distance or contact data, you can partially train the Pairformer trunk. Only LoRA layers are updated.</p> <p>Final Weighted Loss: \\mathcal{L}{\\text{total}} = \\lambda{3D}\\,\\mathcal{L}{3D}     \u2022   \\lambda}}\\,\\mathcal{L{\\mathrm{angle}}     \u2022   \\lambda     \u2022   \\dots}}\\,\\mathcal{L}_{\\mathrm{pair}</p> <p>Validation:     \u2022   Angle metrics: average angle error, sugar pucker accuracy.     \u2022   Pair metrics: contact precision, distogram KL, etc.     \u2022   Final 3D metrics: RMSD, GDT, lDDT, or specialized RNA geometry checks (like base\u2010pair RMSD).</p> <p>\u2e3b</p> <ol> <li>LoRA / QLoRA for Partial Finetuning</li> </ol> <p>Key to memory feasibility: TorsionBERT or Pairformer can each have \\sim!!10^8 parameters. We do:     1.  Load Pretrained base model (frozen).     2.  Wrap with LoRA: Insert low\u2010rank adapter matrices in the attention or feed\u2010forward layers (e.g., HF PEFT library).     3.  Train only LoRA adapter weights + newly introduced heads (like angle heads in TorsionBERT).</p> <p>Implementation:</p> <p>from peft import LoraConfig, get_peft_model</p> <p>lora_cfg = LoraConfig(     r=8,     lora_alpha=32,     target_modules=[\"q_proj\", \"v_proj\", \"fc1\", \"fc2\"], # example     ... ) torsion_bert_lora = get_peft_model(pretrained_torsionBert, lora_cfg)</p> <p>Then only these adapter parameters get requires_grad=True. The rest remain frozen, drastically cutting memory usage.     \u2022   QLoRA: If extremely large, quantize the base to 8\u2010bit or 4\u2010bit, keep LoRA in higher precision.</p> <p>Which Modules:     \u2022   TorsionBERT: Typically we adapt a few layers, or the entire self\u2010attention stack (with rank=8 or so).     \u2022   Pairformer: Similarly, insert LoRA in triangular attention blocks.     \u2022   Diffusion: Optional if the diffusion network is large or if you have a partial pretrained model.</p> <p>Result: an end\u2010to\u2010end differentiable pipeline, but only a small fraction of total weights is updated.</p> <p>\u2e3b</p> <ol> <li>Putting It All Together: Implementation Roadmap</li> </ol> <p>Below is a unified approach that merges the synergy arguments from earlier versions (V1, V2) with the memory/LoRA details (V3) and clarifications from the final pipeline descriptions:</p> <p>7.1 Data Preprocessing     1.  Obtain Adjacency (2D) from a method like RFold.     2.  Create MSA if you have multiple sequences. If not, single sequence is okay.     3.  Residue Index: define a stable 0..N\u22121 labeling so TorsionBERT and Pairformer see the same residue ordering.</p> <p>7.2 Torsion Pipeline (TorsionBERT + LoRA)</p>"},{"location":"pipeline/unified_latent/Perceiver_IO/#pseudocode","title":"Pseudocode","text":"<p>torsion_bert_base = load_pretrained_torsion_bert(...) torsion_bert_lora = wrap_with_LoRA(torsion_bert_base, config)</p> <pre><code>\u2022   Forward: angles = torsion_bert_lora(sequence, adjacency).\n\u2022   Possibly define angle_loss = circular_mse(angles, angles_gt) if we have angle data.\n</code></pre> <p>7.3 (Optional) Forward Kinematics (MP-NeRF)</p> <p>if use_fk:     partial_coords = mp_nerf(angles) else:     partial_coords = None</p> <pre><code>\u2022   If used, partial_coords is a differentiable function of angles.\n</code></pre> <p>7.4 Pairformer (AlphaFold\u202f3\u2013Style + LoRA)</p> <p>pairformer_base = load_af3_like_trunk(...) pairformer_lora = wrap_with_LoRA(pairformer_base, config)</p> <p>z_ij, s_i = pairformer_lora(MSA or single_seq, adjacency=adjacency?)</p> <pre><code>\u2022   Grad from final 3D or pair constraints can update only LoRA weights.\n</code></pre> <p>7.5 Unified Latent Merger</p> <p>Option A: Small Transformer or MLP merges {\\text{angles}, \\text{adjacency}, \\partial\\text{coords}} with {z_{ij}, s_i}.</p> <p>Option B: Perceiver\u202fIO for large data:     1.  Flatten \\mathbf{z}_{ij} + angles + partial coords into M tokens, each tagged with type embeddings or \u201c(i,j)\u201d coordinate embeddings.     2.  Cross\u2010attend them once to a smaller latent dimension N{\\prime}.     3.  Self\u2010attention for L layers on that latent.     4.  Cross\u2010attend from O queries to produce the final synergy vector.</p> <p>In either approach, we get a final \u201cmerged latent\u201d that the diffusion sees.</p> <p>7.6 Diffusion (LoRA optional)</p> <p>diffusion_net = load_diffusion_model(...) # could also do from scratch if large:     diffusion_lora = wrap_with_LoRA(diffusion_net, config)</p> <p>final_3D = diffusion_lora(noisy_init, merged_latent)</p> <pre><code>\u2022   We do a standard diffusion loss or direct RMSD at the final step.\n</code></pre> <p>7.7 Energy Minimization</p> <p>For each final 3D structure from diffusion:     1.  Run a short local MD or partial minimization.     2.  Evaluate geometry or an internal rank metric.     3.  Keep top structure(s).</p> <p>\u2e3b</p> <ol> <li>Example Training Loop (End\u2010to\u2010End)</li> </ol> <p>def forward_pipeline(seq, adjacency, MSA, coords_gt=None, angles_gt=None):     # 1) Torsion angles     torsion_angles = torsion_bert_lora(seq, adjacency)</p> <pre><code># Possibly partial coords\npartial_coords = mp_nerf(torsion_angles) if use_fk else None\n\n# 2) Pair embeddings\nz_ij, s_i = pairformer_lora(MSA or seq, adjacency=adjacency)\n\n# 3) Merge\nunified_latent = unify_latent(torsion_angles, adjacency, partial_coords, z_ij, s_i)\n\n# 4) Diffusion\nfinal_3D = diffusion_model(unified_latent)\n\n# Compute losses\nlosses = {}\nif angles_gt is not None:\n    losses[\"angle_loss\"] = angle_loss_fn(torsion_angles, angles_gt)\nif coords_gt is not None:\n    losses[\"3D_loss\"] = coordinate_loss(final_3D, coords_gt)\nreturn final_3D, losses\n</code></pre> <p>optimizer = ... for batch in dataloader:     seq, adjacency, coords_gt, angles_gt, MSA = batch     final_coords, loss_dict = forward_pipeline(seq, adjacency, MSA, coords_gt, angles_gt)</p> <pre><code>total_loss = (lambda_angles * loss_dict.get(\"angle_loss\", 0.0)\n             + lambda_3D * loss_dict.get(\"3D_loss\", 0.0))\ntotal_loss.backward()\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> <p>Memory:     \u2022   Because TorsionBERT, Pairformer, (optionally) Diffusion are mostly frozen with small LoRA adapters, we only store gradient states for those low\u2010rank parameters, drastically reducing GPU usage.</p> <p>\u2e3b</p> <ol> <li>Addressing Criticisms &amp; Strengths Over Previous Versions<ol> <li>No \u201clost synergy\u201d: \u2022   Torsion angles + Pair embeddings are mandatory. We do not allow skipping either. They unify in a single \u201clatent.\u201d</li> <li>Clarity on optional: \u2022   We label forward kin and MSA as truly optional, so it\u2019s not a confusion of \u201csome synergy might be lost.\u201d</li> <li>Improved Merging: \u2022   We mention a purposeful \u201cUnified Latent Merger\u201d that can be Perceiver\u202fIO if data is large or a simpler subnetwork if data is smaller.</li> <li>LoRA: \u2022   We detail how partial finetuning is inserted into TorsionBERT + Pairformer (and possibly Diffusion), addressing memory constraints.</li> <li>Energy Minimization: \u2022   Shown as recommended, clarifying it\u2019s a final, non\u2010differentiable step for geometry polishing.</li> </ol> </li> </ol> <p>Overall, we unify the synergy arguments and the partial finetuning approach into a single pipeline, ensuring final 3D coordinate generation truly leverages local angles and global pair constraints.</p> <p>\u2e3b</p> <ol> <li>Conclusion &amp; Best Practices<ol> <li>End\u2010to\u2010End Flow: \u2022   Start from sequence + adjacency \u2192 TorsionBERT angles (LoRA) \u2192 Pairformer embeddings (LoRA) \u2192 merge them \u2192 final Diffusion (LoRA) \u2192 optional minimization.</li> <li>LoRA: \u2022   Paramount for large pretrained modules. Freed memory can be used for bigger batch sizes or deeper merges (like Perceiver IO).</li> <li>Perceiver\u202fIO in the Merger**: \u2022   Ideal if you have large N or you want a single domain\u2010agnostic architecture to unify angles, adjacency, partial coords, pair embeddings. \u2022   Flatten everything, cross\u2010attend once, process in a small latent dimension, decode final synergy vector. \u2022   Implementation overhead is higher; for smaller N or simpler merges, a small MLP might suffice.</li> <li>Loss Weights: \u2022   Typically emphasize \\mathcal{L}{3D}. If you have good angle supervision, add \\mathcal{L}{\\text{angle}}. Possibly incorporate contact constraints.</li> <li>Energy Minimization &amp; Ensemble: \u2022   Running a short local minimization for each predicted structure can fix tiny geometry issues. Then you can rank multiple final structures to pick the best.</li> </ol> </li> </ol> <p>Final Word</p> <p>By following this comprehensive design, you harness both local angle constraints (TorsionBERT) and global pair embeddings (Pairformer) in an end\u2010to\u2010end trainable framework\u2014kept memory\u2010efficient via LoRA. The Unified Latent Merger step ensures synergy; if the embeddings are large, Perceiver\u202fIO is an excellent advanced approach to unify them. The pipeline concludes with a Diffusion generator for final 3D and an Energy Minimization pass, typically producing high\u2010fidelity, physically consistent RNA structures that significantly improve over smaller or partial \u201cversioned\u201d designs.</p> <p>This final document:     \u2022   Builds on the synergy arguments of V1/V2,     \u2022   Includes the LoRA/QLoRA partial finetuning details from V3,     \u2022   Clarifies optional vs. mandatory steps (a major critique in earlier versions),     \u2022   Incorporates the robust \u201cfinal design\u201d pipeline from the prior \u201cV4\u201d references,     \u2022   And more explicitly enumerates how or why to adopt Perceiver\u202fIO if data is large, fulfilling the \u201cbest-of\u2010all\u2010worlds\u201d criteria while surpassing the partial designs in both thoroughness and clarity.</p>"},{"location":"reference/advanced_methods/lora/","title":"Lora","text":"<p>Great. I\u2019ll look into the most up-to-date libraries and practices for implementing LoRA (Low-Rank Adaptation) with transformer models in PyTorch. This will include training and inference workflows, CPU/GPU support, and integration with widely used libraries such as Hugging Face and PEFT.</p> <p>I\u2019ll get back to you shortly with a breakdown of the foundational concepts, recommended libraries, code examples, and best practices.</p>"},{"location":"reference/advanced_methods/lora/#low-rank-adaptation-lora-for-transformers-in-pytorch","title":"Low-Rank Adaptation (LoRA) for Transformers in PyTorch","text":""},{"location":"reference/advanced_methods/lora/#what-is-lora-and-why-use-it","title":"What is LoRA and Why Use It?","text":"<p>Low-Rank Adaptation (LoRA) is a technique for efficiently fine-tuning large neural networks \u2013 especially Transformer models \u2013 by introducing a small number of trainable parameters instead of updating the entire model. In the standard fine-tuning of a transformer like GPT-3 or BERT, all model weights (potentially billions of parameters) are adjusted, which is memory-intensive and slow. LoRA addresses this by freezing the original pretrained weights and injecting small low-rank weight matrices (adapters) into each layer of the Transformer ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models) (What is LoRA (Low-Rank Adaption)? | IBM). These adapters represent the weight updates in a compressed form. During training, only the low-rank adapter weights are updated, which drastically reduces the number of trainable parameters and required GPU memory:</p> <ul> <li>Dramatic Reduction in Trainable Parameters: For example, applying LoRA to GPT-3 (175B parameters) can reduce the number of trainable parameters by up to 10,000\u00d7 (from 175 billion to only millions) and cut memory usage by ~3\u00d7 ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models). A full fine-tune of GPT-3 would adjust all 175B parameters, but LoRA can tune roughly 18M parameters (rank-dependent) for an equivalent adaptation (What is LoRA (Low-Rank Adaption)? | IBM). This enables fine-tuning huge models on much smaller hardware.</li> <li>Minimal Impact on Performance: Despite updating far fewer parameters, LoRA-based fine-tuning achieves on-par (or even better) model quality compared to full fine-tuning on a variety of models (RoBERTa, GPT-2, GPT-3, etc.) ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models). It retains model accuracy while being more efficient.</li> <li>No Added Inference Latency: Unlike some other adapter methods that add extra network layers, LoRA doesn\u2019t slow down inference. The low-rank updates can be merged into the original weights for deployment, so the model runs just as fast as the fully fine-tuned model ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models) (What is LoRA (Low-Rank Adaption)? | IBM). By design, LoRA\u2019s adjustments are linear and can be integrated into the existing layers without additional computational steps at inference time.</li> <li>Reusable Base Models &amp; Modular Adaptation: A single frozen pretrained model can support many LoRA adapters for different tasks. You can keep the original model weights fixed and swap in different LoRA weight modules for each new task or domain (What is LoRA (Low-Rank Adaption)? | IBM). This means you don\u2019t need to maintain multiple full copies of a model for different fine-tuned variants \u2013 you just maintain small adapter files (often only tens of megabytes each) for each task, which is far more storage-efficient (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.).</li> </ul> <p>In summary, LoRA is a form of parameter-efficient fine-tuning that injects a few trainable parameters per layer to achieve results comparable to traditional fine-tuning, with massive savings in compute and memory ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models) (PEFT). This method has become popular for adapting large language models (LLMs) and other transformer-based models on consumer-grade hardware.</p>"},{"location":"reference/advanced_methods/lora/#how-lora-works-low-rank-adaptation-mechanism","title":"How LoRA Works (Low-Rank Adaptation Mechanism)","text":"<p>LoRA is based on the idea that the changes needed to fine-tune a large model lie in a low-dimensional subspace. In practical terms, LoRA represents the weight updates for a given layer as a low-rank decomposition. Consider a weight matrix \\(W\\) (from the pretrained model) of size \\(d \\times k\\) in a Transformer layer. Standard fine-tuning would compute an update matrix \u0394\\(W\\) of the same size. LoRA instead assumes \u0394\\(W = A \\times B\\), where \\(A\\) is a \\(d \\times r\\) matrix and \\(B\\) is an \\(r \\times k\\) matrix with \\(r \\ll \\min(d,k)\\) (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)) (What is LoRA (Low-Rank Adaption)? | IBM). The rank \\(r\\) is a tunable hyperparameter (e.g. 4, 8, 16) that controls the dimensionality of the adaptation. During training, \\(A\\) and \\(B\\) are the only weights that get updated (while the original \\(W\\) remains frozen).</p> <p>(Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)) Visualization of weight updates in regular fine-tuning vs. LoRA. In LoRA (right), the full weight update matrix \u0394W is approximated by the product of two low-rank matrices \\(B\\) and \\(A\\) (of inner dimension \\(r\\)). These low-rank matrices are learned during fine-tuning instead of modifying the large pretrained weight \\(W\\) (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)).</p> <p>At inference time, the pretrained weight \\(W\\) and the learned LoRA matrices \\(A\\), \\(B\\) are combined to get the adapted weight \\(W_{\\text{adapted}} = W + \\Delta W = W + A B\\). In practice, the model\u2019s forward pass for a LoRA-augmented layer computes:</p> \\[\\text{output} = W \\cdot x + (A B) \\cdot x,\\] <p>where \\(x\\) is the input to that layer. Because \\(A B x = A (B x)\\), this can be implemented efficiently by first projecting \\(x\\) into a smaller \\(r\\)-dimensional space (with \\(B\\)) and then back to \\(d\\) (with \\(A\\)), and adding it to the original \\(W x\\) output. The low-rank bottleneck (dimension \\(r\\)) ensures the update has far fewer parameters than \\(W\\) itself. In effect, LoRA adds a small trainable \u201cbranch\u201d to each weight matrix's output that nudges the layer's behavior, instead of changing the main weight directly (What is LoRA (Low-Rank Adaption)? | IBM) (What is LoRA (Low-Rank Adaption)? | IBM). Key points about how LoRA works:</p> <ul> <li>LoRA injection points: LoRA adapters are typically applied to the linear projection layers inside transformer architectures. For example, in a self-attention block, one might add LoRA adapters to the query and value projection matrices (as was common in some LoRA fine-tuning setups) or to all attention and feed-forward linear layers. The choice of which modules to target (often specified by name, e.g. all <code>\"q_proj\"</code> and <code>\"v_proj\"</code> layers in the model) is a hyperparameter (What is LoRA (Low-Rank Adaption)? | IBM). Targeting more layers can improve adaptability at the cost of more trainable parameters. (Originally, LoRA was demonstrated mainly on attention layers, but recent practice suggests applying LoRA broadly across many or all transformer layers yields the best performance (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)).)</li> <li>Rank (\\(r\\)) and Scaling (\\(\\alpha\\)): The rank \\(r\\) controls the size of the LoRA adapters. A higher \\(r\\) means more capacity to learn complex adaptations, but also more parameters to train. A common setting is \\(r=8\\) or \\(r=16\\) for large models, though it can vary. LoRA also often uses a scaling factor \\(\\alpha\\) (sometimes called <code>lora_alpha</code> in code) to adjust the magnitude of the LoRA update. In many implementations, the actual update added is scaled as \\((\\alpha/r) \\cdot (A B x)\\) \u2013 effectively, \\(\\alpha\\) controls an initial weight for the LoRA branch. A heuristic is to set \\(\\alpha\\) to about twice the rank (e.g. if \\(r=8\\), \\(\\alpha=16\\)) (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)), although this can be tuned. This scaling helps ensure the LoRA update is initially small and grows to the right scale during training.</li> <li>LoRA Dropout: Some implementations include a dropout on the LoRA branch (e.g. <code>lora_dropout</code> probability) (Efficient Large Language Model training with LoRA and Hugging Face). This means during training, the LoRA adapter is sometimes stochastically disabled for regularization. LoRA dropout can help prevent overfitting of the small adapter, especially if \\(r\\) is relatively large or the fine-tuning dataset is small.</li> <li>Merging and Unmerging: Because \\(W\\) remains unchanged and all adjustments are in \\(A\\) and \\(B\\), one can choose after training to merge the LoRA weights into \\(W\\) (simply by computing \\(W \\leftarrow W + A B\\) once) for deployment, or keep them separate. Merging produces a standalone model identical in inference behavior to using LoRA, whereas keeping them separate allows flexibility to swap adapters. Merging is straightforward since it\u2019s just matrix addition \u2013 and doing so results in a model that is equivalent to fully fine-tuned weights (What is LoRA (Low-Rank Adaption)? | IBM) (What is LoRA (Low-Rank Adaption)? | IBM).</li> </ul> <p>LoRA builds on the insight that large neural networks are often highly over-parameterized (many weights are redundant). By training only a small low-rank subset of weights, we exploit the model\u2019s inherent low-dimensional structure (What is LoRA (Low-Rank Adaption)? | IBM) (What is LoRA (Low-Rank Adaption)? | IBM). This not only saves memory and compute, but can even act as a form of regularization \u2013 the model can\u2019t overfit too badly when it only has a few million parameters to tweak, though one must still be mindful of overfitting on small data (e.g., multiple epochs on a small dataset can still overfit even with LoRA). The rank \\(r\\) may need to be adjusted depending on task complexity: too low a rank might under-fit (losing some information by oversimplifying the weight update), while too high a rank diminishes the efficiency gains and could overfit (What is LoRA (Low-Rank Adaption)? | IBM). In practice, values in the single digits to a few tens are common, and one can start with a moderate rank (like 8 or 16) and tune from there.</p>"},{"location":"reference/advanced_methods/lora/#popular-libraries-for-lora-in-pytorch-training-inference","title":"Popular Libraries for LoRA in PyTorch (Training &amp; Inference)","text":"<p>LoRA was first introduced in 2021 alongside an open-source package called <code>loralib</code> (by Microsoft) that implemented LoRA layers for PyTorch (GitHub - microsoft/LoRA: Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"). However, the most widely adopted tools for using LoRA today are part of the Hugging Face ecosystem, which provides high-level integration for transformers models. Below are the main libraries and frameworks used for LoRA with PyTorch:</p> <ul> <li>Hugging Face Transformers + PEFT: Hugging Face\u2019s Transformers library (for model architectures and weights) combined with the PEFT library (<code>peft</code> \u2013 Parameter-Efficient Fine-Tuning) is the de facto standard for using LoRA in 2024/2025. The PEFT library includes LoRA as one of its methods, along with other techniques like prefix-tuning and prompt tuning (Efficient Large Language Model training with LoRA and Hugging Face). Using PEFT, applying LoRA to a model is very straightforward \u2013 you load a pretrained transformer model with \ud83e\udd17 Transformers, then wrap it with a LoRA configuration via PEFT. For example, with PEFT one can do:</li> </ul> <pre><code>from transformers import AutoModelForSeq2SeqLM\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=32, lora_dropout=0.1)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n</code></pre> <p>This would add LoRA adapters of rank 8 to the seq2seq model and freeze the rest. The <code>print_trainable_parameters()</code> method would show that only a tiny fraction of parameters (on the order of 0.1\u20130.2%) are now trainabl (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011. For instance, wrapping a 1.2 billion parameter MT0 model with LoRA (r=8) results in only ~2.36 million trainable params (~0.19% of the model (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011. Hugging Face\u2019s implementation automatically handles freezing the original weights and marking the LoRA parameters as trainable.</p> <p>Hugging Face Transformers also has built-in support for loading and merging LoRA adapters at inference. Thanks to the integration of PEFT with Transformers, if you save a LoRA adapter, you can load it by itself or along with the base model. For example, <code>AutoPeftModelForCausalLM.from_pretrained(\"username/model-lora\")</code> will download the LoRA adapter from the Hub and automatically load the base model and apply the adapter weights (the adapter\u2019s config file contains a reference to the <code>base_model_name</code> (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums)\u3011. This makes using LoRA in pipelines or deployment very convenient \u2013 the library will handle the composition of base weights + LoRA weights under the hood. In practice, you can also manually load a base model and then attach a LoRA adapter via <code>PeftModel.from_pretrained</code>: for example, load the base <code>facebook/opt-350m</code> and then apply a LoRA adapter on to (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011. Both training and inference are supported on CPU and GPU \u2013 the <code>.to(\"cuda\")</code> call can move the combined model to GPU for fast inference, or you can keep it on CPU for smaller models or testing. (Large LoRA-adapted models typically require a GPU to run efficiently, but the adapters themselves don\u2019t change the device requirements beyond what the base model needs.)</p> <ul> <li> <p>Original <code>loralib</code> (Microsoft): The initial implementation of LoRA was released as an open-source PyTorch library (<code>pip install loralib</code>). It provides LoRA-enabled layers (e.g., a custom <code>lora.Linear</code> module) that replace or augment <code>nn.Linear</code> layers in a mode (GitHub - microsoft/LoRA: Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\")\u3011. Under the hood, these layers perform the \\(W x + BA x\\) computation (with \\(A\\), \\(B\\) as the low-rank matrices). The <code>loralib</code> package supports Linear, Embedding, and Conv2D layers, and even a merged Linear for cases where multiple weight matrices are packed together (as in some Transformer implementations (GitHub - microsoft/LoRA: Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\")\u3011. While one can still use <code>loralib</code> to manually modify a model, it\u2019s somewhat low-level. In practice, the Hugging Face PEFT approach has largely supplanted direct use of <code>loralib</code> by offering a higher-level, easier API (and support for saving/loading adapters, etc.). However, <code>loralib</code> was instrumental in demonstrating integration of LoRA with Hugging Face models early o (GitHub - microsoft/LoRA: Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\")\u3011, and it remains a lightweight alternative if needed.</p> </li> <li> <p>PyTorch Lightning and Others: For users of PyTorch Lightning or other training frameworks, LoRA can also be integrated. Lightning, for example, has guides on how to replace a model\u2019s linear layers with LoRA layers (using either <code>loralib</code> or custom modules) to allow training with LoRA while still using the Lightning Trainer for the loo (LoRA finetuning Lightning AI - Docs)\u3011. There are also community projects like Lit-LLaMA (Lit-GPT) and others that have LoRA support built-in, since LoRA is vital for fine-tuning large models without enormous resources. Additionally, minimal reimplementations (such as <code>minLoRA</code>, ~100 lines of code) exist to illustrate LoRA\u2019s simplicit (minLoRA: a minimal PyTorch library that allows you to apply LoRA ...)\u3011. These alternatives can be useful for understanding or customizing LoRA, but when it comes to \u201cwidely adopted\u201d solutions, Hugging Face\u2019s Transformer+PEFT combo is by far the most popular and well-maintained route. It benefits from community support and continued development (e.g., Hugging Face has extended PEFT with variants like AdaLoRA, and even domain-specific LoRA for diffusion models, though those are beyond our scope).</p> </li> </ul> <p>Hardware support (CPU/GPU): All the libraries above are built on PyTorch, which means LoRA-enhanced models can run on CPU or GPU. In practice, training large models with LoRA is typically done on GPUs for speed, but because you\u2019re training far fewer parameters, the memory footprint is greatly reduced. This can allow training on a single GPU what would normally require multiple GPUs or not be feasible at all. For example, with LoRA, a 12-billion-parameter model that would normally OOM on an 80GB GPU could be fine-tuned in 22GB of GPU memory (with some offloading (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011. Phil Schmid demonstrated fine-tuning an 11B parameter T5 model (FLAN-T5-XXL) on a single 24GB GPU by using LoRA with 8-bit quantization, whereas full fine-tuning the same model required 8\u00d7 40GB GPU (Efficient Large Language Model training with LoRA and Hugging Face)\u3011. Inference can similarly be done on GPU for speed or on CPU if needed (for smaller models or where real-time is not required). The model + LoRA weights combined take only slightly more RAM than the base model alone (since the LoRA matrices are tiny). Therefore, if a base model fits in CPU RAM, the LoRA-adapted model will as well. The PEFT/Transformers integration handles device placement \u2013 you can move the model to CPU or GPU with standard PyTorch <code>.to()</code> calls or use Hugging Face Accelerate for more complex multi-device setups. Additionally, Hugging Face\u2019s inference tooling (like Text Generation Inference server) has added explicit support for LoRA adapters, including the ability to serve multiple LoRA adapters on one base model concurrently (multi-adapter serving (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums)\u3011. This shows the ecosystem is evolving to make deploying LoRA-modified models as convenient as possible.</p>"},{"location":"reference/advanced_methods/lora/#training-workflow-with-lora","title":"Training Workflow with LoRA","text":"<p>Using LoRA for training a Transformer model typically follows these steps:</p> <ol> <li> <p>Load a Pretrained Base Model: Start with a pretrained Transformer (e.g., a Hugging Face model checkpoint for BERT, GPT-2, T5, GPT-NeoX, etc.). This will be your base model that provides all the knowledge that LoRA will adapt. For example: <code>model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")</code>. It\u2019s often useful to use half-precision (<code>torch.float16</code>) or 8-bit loading for very large models to reduce memory, but this is optional. Ensure you have the tokenizer and model config as well if needed.</p> </li> <li> <p>Configure LoRA: Decide which parts of the model to adapt and set the LoRA hyperparameters. Using the PEFT library, you create a <code>LoraConfig</code> specifying:</p> </li> <li><code>r</code>: the rank of the LoRA matrices (small integer, e.g. 4, 8, 16 (What is LoRA (Low-Rank Adaption)? | IBM)\u3011.</li> <li><code>target_modules</code>: a list or pattern specifying which submodules to apply LoRA to (for example, <code>[\"q_proj\", \"v_proj\"]</code> for attention query and value projection matrices, or <code>\"all.linear\"</code> to target all linear layers (What is LoRA (Low-Rank Adaption)? | IBM)\u3011. For many Transformer architectures, a common choice is all the W<sub>q</sub> and W<sub>v</sub> weight matrices in self-attention blocks, since those are big matrices and important for the model\u2019s behavior. However, you can target feed-forward layers or others as well. The Hugging Face PEFT library provides some defaults per <code>TaskType</code> (e.g., for causal language modeling it might default to the attention projections).</li> <li><code>lora_alpha</code>: the scaling factor \u03b1 (controls initial update magnitude (What is LoRA (Low-Rank Adaption)? | IBM)\u3011.</li> <li><code>lora_dropout</code>: dropout probability for LoRA layers (e.g. 0.05 or 0.1 to regularize (Efficient Large Language Model training with LoRA and Hugging Face)\u3011.</li> <li><code>bias</code>: whether to also allow biases to be trained or not (often kept <code>\"none\"</code> or <code>\"lora_only\"</code> meaning do not update any bias terms except maybe those within LoRA layers). By default, LoRA doesn\u2019t add trainable bias unless specified.</li> </ol> <p>For example, a config might be: rank=16, alpha=32, target_modules=[\"q\", \"v\"], dropout=0.1, for a seq2seq LM tas (Efficient Large Language Model training with LoRA and Hugging Face)\u3011. This would insert LoRA adapters in the Q and V linear layers of each Transformer block, each of rank 16, and scale their effect by 32 with a 10% dropout.</p> <ol> <li> <p>Inject LoRA Adapters into the Model: Using the configuration, apply LoRA to your model. With PEFT, this is one line: <code>model = get_peft_model(model, lora_config)</code>. This function will modify the model in-place, adding the new LoRA layers and freezing the original weights. After this, <code>model</code> is a <code>PeftModel</code> wrapper around the original model. You can verify the setup by printing trainable params as shown earlier \u2013 e.g., \u201ctrainable params: 0.17%\u201d of the mode (Efficient Large Language Model training with LoRA and Hugging Face)\u3011, confirming that only LoRA layers are unfrozen. Under the hood, each targeted <code>nn.Linear</code> module (or whatever target) is replaced or augmented by a LoRA-equipped counterpart. (If using another approach like <code>loralib</code>, at this step you would manually replace modules or call something like <code>lora.Linear(..., r=...)</code> to create layers with LoRA.)</p> </li> <li> <p>Prepare for Training: Since the vast majority of the model\u2019s weights are frozen (non-trainable), training with LoRA often allows a higher learning rate on the adapters. It\u2019s not uncommon to use learning rates in the range 1e-3 to 1e-4 for LoRA, whereas full-model fine-tuning might use 1e-5 or lower. This is because only the small adapter needs to be learned, and it can typically be learned faster without divergin (Efficient Large Language Model training with LoRA and Hugging Face)\u3011. You can use any PyTorch optimizer (AdamW is standard) \u2013 by default it will only update the LoRA parameters because others have <code>requires_grad=False</code>. If using the Hugging Face <code>Trainer</code> API, it will automatically ignore frozen params (no grads computed for them). You might also want to set <code>model.config.use_cache = False</code> during training if using certain models (e.g., T5, GPT-2) to avoid caching issues \u2013 this is a minor detail that the PEFT docs sometimes note for training with LoRA.</p> </li> </ol> <p>Also ensure your training data is ready (tokenized, wrapped in a Dataset or DataLoader). With LoRA, you can usually afford a larger batch size than full fine-tuning because memory usage per batch is lower (optimizer states for frozen weights aren\u2019t kept). Still, the total memory is dominated by the forward/backward activations, so you won\u2019t get massive batch size increases unless you also utilize gradient accumulation or a memory-saver like DeepSpeed. If needed, you can combine LoRA with techniques like gradient checkpointing to further reduce memory.</p> <ol> <li> <p>Train the Model: Run the training loop or HuggingFace Trainer. The training will focus on the LoRA adapter parameters. Thanks to the reduced size, training is typically much faster and uses less memory per step. For example, one report showed fine-tuning a 7B parameter model with LoRA on a single GPU in a few hours (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011. Another example: fine-tuning FLAN-T5-XXL (11B) with LoRA + 8-bit took ~10 hours on a single A10 GPU, whereas full fine-tuning the same model for 10 hours would have required multi-GPU and cost significantly mor (Efficient Large Language Model training with LoRA and Hugging Face)\u3011. Throughout training, monitor your validation metrics as usual to ensure the model is improving on your task.</p> </li> <li> <p>Save the Adapter (and perhaps the merged model): After fine-tuning, you have two choices:</p> </li> <li>Save only the LoRA adapter \u2013 i.e. the small matrices and config. If you used PEFT\u2019s <code>Trainer</code> or the <code>PeftModel</code>, calling <code>model.save_pretrained(\"my-lora-adapter\")</code> will save files like <code>adapter_model.bin</code> or <code>.safetensors</code> and an <code>adapter_config.json</code>, rather than the full model weights. This is very storage-efficient (often just a few MB). You would use this if you want to publish or reuse the adapter on top of the original base model. For instance, users can then load your adapter and combine it with the public base model to get the fine-tuned model. This is how many LoRA fine-tuned LLMs are shared on Hugging Face Hub \u2013 the adapter might be ~20MB versus a 10+GB base mode (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011.</li> <li>Merge and save the full model \u2013 you can merge the LoRA weights into the base model weights to create a new set of model weights that directly incorporate the fine-tuning. In code, PEFT provides <code>model.merge_and_unload()</code> which will add \\(A B\\) into \\(W\\) for all LoRA layers (and remove the LoRA hooks). After that, you can save the <code>model</code> as a normal <code>transformers</code> model (which will be the size of the full model). Merging is useful if you need a single file for deployment (for example, if the deployment environment doesn\u2019t support PEFT adapters) or want to do further tasks like quantizing the combined model. However, if your deployment can handle LoRA adapters, it\u2019s often preferable to keep them separate for flexibility.</li> </ol> <p>Example: Suppose we fine-tune a BERT-like model for a classification task with LoRA. We choose \\(r=8\\), target the attention projections, and train. At the end, we might have an adapter with ~1M parameters (just an example). We save this adapter. The base model (with, say, 110M parameters) remains untouched. If we want to fine-tune for another task, we can reuse the same 110M base and train another small adapter. Each task\u2019s adapter is small, and we avoid duplicating the whole BERT for each task. This modular approach is a key benefit of LoR (What is LoRA (Low-Rank Adaption)? | IBM)\u3011 \u2013 many adapters can plug into one model.</p>"},{"location":"reference/advanced_methods/lora/#inference-workflow-with-lora","title":"Inference Workflow with LoRA","text":"<p>After training, using a LoRA-adapted model for inference is very flexible. You can either load the base model and the LoRA adapter separately or use a combined approach. Here are common workflows for inference (applicable to both CPU and GPU execution):</p> <ul> <li> <p>Loading Base + LoRA Adapter: This is the most typical approach when you have saved a LoRA adapter. In code, you load the base pretrained model, then load the LoRA weights on top:   <pre><code>from transformers import AutoModelForSeq2SeqLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")        # load base\nmodel = PeftModel.from_pretrained(base_model, \"path/or/hub-id-of-my-lora-adapter\")\n</code></pre>   The <code>from_pretrained</code> of <code>PeftModel</code> will read the adapter files (which include a config that points to the base model name, in case you didn\u2019t pass an already loaded base), then inject the LoRA weights into the model. After this, <code>model</code> behaves just like the fine-tuned model. You can then do <code>model.eval()</code> and use it for generating predictions or embeddings or whatever the task is. This method works on CPU or GPU \u2013 if you want it on GPU, either move it with <code>model.to('cuda')</code> after loading, or use the <code>device_map</code> argument to load directly to CUDA. The memory overhead for the LoRA is negligible (a few MB of weights plus some extra small computations).</p> </li> <li> <p>Direct Loading from Hub (Base + LoRA together): If the LoRA adapter is hosted on the Hugging Face Hub and was saved with the proper <code>adapter_config.json</code> (containing <code>\"base_model_name_or_path\"</code>), you can directly load the merged model via the Transformers API. As noted earlier, Hugging Face Transformers &gt;= v4.30 integrates PEFT, so even using <code>AutoModelForCausalLM.from_pretrained()</code> can automatically combine LoRA if it finds adapter weights:   <pre><code>model = AutoModelForCausalLM.from_pretrained(\"username/my-lora-adapter-hubrepo\")\n</code></pre>   Under the hood, this will see that the repo contains an adapter config and weights, and then load the base model (it will download the base model if not cached) and apply the LoRA. Similarly, <code>pipeline()</code> will work if pointed to the adapter repo. This \u201cone-liner\u201d usage is enabled by the PEFT integration and makes it dead simple to use LoRA fine-tuned models \u2013 *the user doesn\u2019t even have to manually load base model (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums)\u3011. For example, if someone shares a LoRA fine-tuned OPT-350M model on the hub, you can load it in one command and it will give you the ready-to-use mode (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011. (Make sure to use a recent version of Transformers that has this capability.)</p> </li> <li> <p>Merged Model: In cases where you have merged the LoRA into the base weights and saved a full model, you just load it like any other model checkpoint (no special steps needed, since it\u2019s now a standard model). This might be done if, for instance, you export the model to a production environment that doesn\u2019t know about LoRA. However, note that by merging you lose the ability to easily swap the adapter off \u2013 it\u2019s now baked into the weights.</p> </li> <li> <p>Multi-Adapter Inference: An advanced scenario is having multiple LoRA adapters for different tasks and selecting between them at inference. The PEFT library allows loading multiple adapters and either merging them or switching which is active. For example, you could maintain one base model instance in memory and load two different LoRA adapters (say one for legal text generation and one for medical text). By toggling which adapter is applied, the model can serve different styles or tasks without loading a whole new model. This is especially useful in serving scenarios \u2013 as mentioned, Hugging Face\u2019s TGI server can even host dozens of LoRAs on one base model and route requests to the appropriate one with virtually no overhead of loading/unloadin (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums)\u3011. This kind of \u201cserve many models with one base\u201d is made possible only because the adapters are so lightweight.</p> </li> </ul> <p>Regardless of how you load the model, inference speed using LoRA-adapted models is essentially the same as the original model. When separate, the model does an extra add operation (<code>+ A(Bx)</code>), but this is a simple matrix multiplication addition which is negligible compared to the large matrix multiplications the Transformer is already doing. The original LoRA paper emphasized that LoRA adds no additional latency at inference by construction (What is LoRA (Low-Rank Adaption)? | IBM)\u3011. If you merged the weights, it literally* is the same as a fully fine-tuned model in terms of operations. Thus, you don\u2019t pay a runtime penalty for the parameter-efficient approach \u2013 a key advantage over some other adapter methods that, for example, insert additional layers and do increase compute.</p> <p>CPU vs GPU at inference: If the model is small (or quantized), you might run inference on CPU. LoRA doesn\u2019t change the feasibility of this \u2013 you still need enough RAM for the base model. For example, if a base model is 2GB on disk (FP16 ~4GB in RAM), and the LoRA adapter is 30MB, the total at runtime might be ~4.03GB which is essentially the same requirement. The extra matrix multiplies for LoRA will use some CPU cycles, but if the model was runnable on CPU before, it will still be after (with only a slight overhead that\u2019s usually dwarfed by the rest of the model\u2019s computation). In many cases, though, large models are run on GPU for speed, and LoRA is fully GPU-compatible. Just ensure you load or move both base and LoRA weights to the GPU.</p> <p>Finally, a note on quantization at inference: LoRA can be combined with model weight quantization (int8 or int4) to further shrink memory usage. A popular strategy is QLoRA (Quantized LoRA), where the base model is loaded in 4-bit precision and LoRA adapters are used to fine-tune. During inference, you can keep the base model in 4-bit and the LoRA in normal precision \u2013 this yields huge memory savings (for example, 33% less memory at cost of 30-40% more compute time in one stud (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011). The Hugging Face PEFT documentation even provides utilities to apply LoRA on top of 4-bit models (with the <code>BitsAndBytesConfig</code> from <code>transformers</code> and methods to handle LoRA initialization in quantized scenario (LoRA) (LoRA)\u3011). This is quite advanced, but it\u2019s worth mentioning that LoRA is compatible with quantization \u2013 you can load an 8-bit or 4-bit base model on GPU, attach LoRA, and still get the benefits of both techniques. In short, LoRA does not conflict with inference optimization methods; it plays nicely with half-precision, mixed-precision, and quantized models.</p>"},{"location":"reference/advanced_methods/lora/#best-practices-and-tips-for-using-lora","title":"Best Practices and Tips for Using LoRA","text":"<p>When integrating LoRA into modern training and inference pipelines, here are some best practices and considerations to keep in mind:</p> <ul> <li> <p>Use Established Libraries: Prefer using well-maintained libraries like Hugging Face\u2019s PEFT for LoRA, which abstract away the tricky parts of implementation. They handle naming, freezing, saving, and loading seamlessly. This reduces errors compared to writing your own LoRA logic. The Hugging Face implementation is widely tested and kept up-to-date with new features, which makes your life easier.</p> </li> <li> <p>Choose the Right Layers (target_modules): Decide which parts of the model to adapt based on a trade-off between flexibility and efficiency. Applying LoRA to all attention and feed-forward layers will give the adapter more reach to change the model\u2019s behavior (improving task performance), but means more parameters to train. Focusing on a subset (like just the attention projections) limits trainable params and memory. Empirically, many have found that applying LoRA across more layers (even all linear layers) tends to maximize performance on difficult task (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011. If you are fine-tuning for something that requires altering a lot of the model\u2019s knowledge, consider a higher coverage of layers. If you only need slight tweaks (or need extreme efficiency), target key layers (e.g. attention Q and V matrices). The PEFT library allows targeting by module name patterns, which you can customize for your architecture.</p> </li> <li> <p>Set an Appropriate Rank: The rank \\(r\\) is a crucial hyperparameter. Common default values are 8 or 16 for large language models; smaller models or simpler tasks might even use \\(r=4\\), whereas very complex tasks might benefit from \\(r=32\\) or more. A higher rank gives the LoRA adapter more capacity to learn nuanced changes (at the cost of more GPU memory and risk of overfitting). It\u2019s often recommended to start with a moderate rank (e.g. 8) and see if performance is satisfactory. If the model underfits (can\u2019t reach the desired accuracy), increasing the rank can help. Tune \\(r\\) in conjunction with the scaling factor \\(\\alpha\\) \u2013 a rule of thumb is $\\alpha = 2 \\times r (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011, which keeps the initial scale of updates roughly balanced. The IBM guidance notes that reducing the rank too much can cause some information los (What is LoRA (Low-Rank Adaption)? | IBM)\u3011, but because transformers are very overparameterized, they often tolerate surprisingly low ranks without much drop in performance.</p> </li> <li> <p>LoRA Alpha and Dropout: As mentioned, setting \\(\\alpha\\) (lora_alpha) to a couple times the rank is a reasonable heuristic. This doesn\u2019t usually need heavy tuning beyond that; it mainly affects training dynamics early on. For LoRA dropout, if your dataset is small or you notice overfitting, adding a bit of dropout (e.g. 0.05\u20130.1) on the LoRA weights can improve generalization. If your dataset is large, dropout may not be necessary.</p> </li> <li> <p>Monitor Trainable Parameter %: One nice feature of LoRA is the easy calculation of how many parameters you\u2019re actually training. Use <code>model.print_trainable_parameters()</code> (in PEFT) or manually sum <code>numel()</code> of parameters with <code>requires_grad=True</code>. This helps you verify that LoRA is applied correctly and gauge the size of the adapter. For instance, seeing \u201ctrainable params: 0.2%\u201d gives confidence you achieved a big efficiency gai (Efficient Large Language Model training with LoRA and Hugging Face)\u3011. You can also estimate memory needs: if you train 2 million params with AdamW, that\u2019s on the order of ~16 MB of optimizer states, which is trivial compared to many GBs for the full model\u2019s states.</p> </li> <li> <p>Training Regimen: Because LoRA reduces the risk of overfitting by reducing parameters, you might be tempted to train for many epochs. However, be cautious \u2013 overfitting can still occur. In fact, with very small training sets, a powerful model might overfit the few million adapter params quickly. It\u2019s often observed that a few epochs (or even just a single epoch with a large dataset) is sufficient. Keep an eye on validation loss and use early stopping if possible. Also, consider that not all tasks benefit from multi-epoch training with LoRA; sometimes a single pass is enough to imprint the new informatio (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011.</p> </li> <li> <p>Optimizer and Precision: LoRA doesn\u2019t mandate a specific optimizer. AdamW is commonly used and works well. One finding from LoRA practitioners is that fancy optimizers aren\u2019t strictly necessary \u2013 since you\u2019re training a small part of the model, even SGD can reach a good solution if used properly, though adaptive optimizers converge faste (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))\u3011. As for precision, you can train LoRA adapters in full fp32 or mixed precision. Mixed precision (fp16 or bf16) works fine for the gradients of LoRA weights and will speed up training on GPUs that support it. If using an 8-bit base model (via bitsandbytes), ensure you follow PEFT guidelines (e.g., use <code>prepare_model_for_int8_training</code> to keep certain layers in float32 for stabilit (Efficient Large Language Model training with LoRA and Hugging Face)\u3011).</p> </li> <li> <p>Combine with Other Techniques Cautiously: LoRA can be combined with other parameter-efficient methods (like prefix tuning, or even adding adapters on top of LoRA) but this gets complex. In the Hugging Face PEFT library you can actually stack adapters (they support e.g. LoRA + prompt tuning together). Generally, LoRA alone is powerful enough for most fine-tuning needs. If you do combine methods, ensure you understand how they interact (for example, prefix tuning adds tokens to inputs, while LoRA changes weights \u2013 they won\u2019t conflict, but the benefits might not be strictly additive). A more common combination is LoRA with model compression (quantization/pruning) as discussed \u2013 e.g. QLoRA for training and then maybe distilling the model afterward. These pipelines can get sophisticated, but the community has shown success with them (like fine-tuning 65B LLMs with 4-bit QLoRA on a single GP (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.)\u3011).</p> </li> <li> <p>Inference Deployment: When deploying a LoRA-adapted model, decide whether to use a framework that supports adapters or to merge the weights. If you\u2019re using the Hugging Face <code>transformers</code> library in your deployment, you can simply load the adapter as we described. If you\u2019re exporting to ONNX or using a C++ runtime, you might prefer to merge weights beforehand since those runtimes won\u2019t know about the separate LoRA weights. Merging is lossless, so it\u2019s a safe operation. As a best practice, keep a copy of the original adapter weights even if you merge \u2013 this way you can later update or swap adapters without needing to re-run fine-tuning. And if serving multiple tasks, consider a setup that can hot-swap adapters (Hugging Face\u2019s <code>TextGenerationInference</code> server or a custom PyTorch service) to maximize the benefits of LoRA\u2019s modularity.</p> </li> <li> <p>Community Resources and Models: Since LoRA is widely used, there are many existing LoRA adapters shared online for popular models (especially in the realm of large language models like LLaMA, GPT-J, etc.). You can often find a LoRA for a certain domain or task on Hugging Face Hub \u2013 using it might save you training time. These adapters can sometimes be combined or sequentially applied if they are compatible (research into merging LoRA weights from different fine-tunings is ongoin (Different results when merging LORA weights into the base model ...)\u3011). Keep an eye on the latest PEFT library features for merging multiple LoRAs (there are methods like <code>add_weighted_adapter</code> to combine adapters, useful in model merging scenario (Merge LoRAs - Hugging Face)\u3011). As of 2025, techniques like TIES and DARE are being explored to intelligently merge LoRA adapters from different models/task (Model merging) (Model merging)\u3011 \u2013 interesting for multi-skill models.</p> </li> </ul> <p>In conclusion, LoRA has become a key component of modern model fine-tuning pipelines due to its efficiency and simplicity. Libraries like Hugging Face PEFT make it easy to apply LoRA to transformers with just a few lines of code, and to integrate the resulting adapters into both training and inference workflows on CPU or GPU. By following best practices \u2013 choosing proper hyperparameters, monitoring training, and utilizing the robust tooling available \u2013 developers can adapt large pretrained models to new tasks quickly and cost-effectively. LoRA enables experimentation with large models that would otherwise be inaccessible to most, and it does so while maintaining performance and fast inference speed, making it a widely adopted technique in the NLP communit (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)) (PEFT)\u3011.</p> <p>Sources:</p> <ul> <li>Hu et al., LoRA: Low-Rank Adaptation of Large Language Models, arXiv 2021 ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models) ([2106.09685] LoRA: Low-Rank Adaptation of Large Language Models)\u3011.</li> <li>IBM Cloud Education, What is LoRA (Low-Rank Adaptation)? \u2013 Think Blog, 2023 (What is LoRA (Low-Rank Adaption)? | IBM) (What is LoRA (Low-Rank Adaption)? | IBM)\u3011.</li> <li>\ud83e\udd17 Hugging Face PEFT Documentation and GitHub READM (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.) (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.) (PEFT)0\u3011.</li> <li>P. Schmid, Efficient Large Language Model training with LoRA, 202 (Efficient Large Language Model training with LoRA and Hugging Face) (Efficient Large Language Model training with LoRA and Hugging Face)4\u3011.</li> <li>S. Raschka, Practical Tips for Finetuning LLMs Using LoRA, Ahead-of-AI Substack, 202 (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)) (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))4\u3011.</li> <li>Hugging Face Forums \u2013 discussions on LoRA usage and deploymen (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums) (Using Text Generation Inference with LoRA adapter - Beginners - Hugging Face Forums)7\u3011.</li> <li>Hugging Face blog and resources on PEFT/QLoRA (e.g., 4-bit LoRA on consumer GPUs (GitHub - huggingface/peft:  PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.) (Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation))2\u3011.</li> </ul>"},{"location":"reference/advanced_methods/af3/AF3_paper/","title":"Paper","text":"<p>Accurate structure prediction of biomolecular interactions with AlphaFold 3 In the format provided by the authors and uneditedNature | www.nature.com/nature Supplementary information https://doi.org/10.1038/s41586-024-07487-w Accurate structure prediction of biomolecular interactions with AlphaFold 3 Josh Abramson1, Jonas Adler1, Jack Dunger1, Richard Evans1, Tim Green1, Alexander Pritzel1, Olaf Ronneberger1, Lindsay Willmore1, Andrew J Ballard1, Joshua Bambrick2, Sebastian W Bodenstein1, David A Evans1, Chia-Chun Hung2, Michael O'Neill1, David Reiman1, Kathryn Tunyasuvunakool1, Zachary Wu1, Akvil\u0117 \u017demgulyt\u01171, Eirini Arvaniti3, Charles Beattie3, Ottavia Bertolli3, Alex Bridgland3, Alexey Cherepanov4, Miles Congreve4, Alexander I Cowen- Rivers3, Andrew Cowie3, Michael Figurnov3, Fabian B Fuchs3, Hannah Gladman3, Rishub Jain3, Yousuf A Khan3, Caroline M R Low4, Kuba Perlin3, Anna Potapenko3, Pascal Savy4, Sukhdeep Singh3, Adrian Stecula4, Ashok Thillaisundaram3, Catherine Tong4, Sergei Yakneen4, Ellen D Zhong3, Michal Zielinski3, Augustin \u017d\u00eddek3, Victor Bapst\u20201, Pushmeet Kohli\u20201, Max Jaderberg\u20202, Demis Hassabis\u20201,2, John M Jumper\u20201 Contributed equally 1Core Contributor, Google DeepMind, London, UK 2Core Contributor, Isomorphic Labs, London, UK 3Google DeepMind, London, UK 4Isomorphic Labs, London, UK \u2020 Jointly supervised Corresponding author emails: J. J. - jumper@google.com; D.H. - dhcontact@google.com; M.J. - jaderberg@isomorphiclabs.com Supplementary information for AlphaFold 3 Contents 1 Notation 1 2 Data pipeline 1 2.1 Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2.2 Genetic search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2.3 MSA processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.4 Template search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.5 Training data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.5.1 Weighted PDB dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.5.2 Distillation datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.5.3 Training set clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.5.4 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.6 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7 Cropping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7.1 Contiguous cropping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7.2 Spatial Cropping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7.3 Spatial Interface Cropping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.8 Featurisation and model inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3 Model architecture 8 3.1 Input embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1.1 Input embedder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1.2 Relative position encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.2 Sequence-local atom attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 MSA Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Triangle updates of the pair representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Template embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.6 Pairformer stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.7 Diffusion Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.7.1 Diffusion Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4 Auxiliary heads 25 4.1 Mini diffusion rollout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.2 Chain permutation and symmetry resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3 Model confidence prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3.1 Predicted local distance difference test (pLDDT) . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.3.2 Predicted aligned error (PAE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.3.3 Predicted distance error (PDE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3.4 Experimentally resolved prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4.3.5 Confidence head architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4.4 Distogram prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5 Training and inference 29 5.1 Structure filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Training stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.3 Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.4 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.5 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.6 Inference Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.7 Model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.8 Validation set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.9 Confidence measures and sample ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.9.1 Alignment-based confidence measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.9.2 Clashes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.9.3 Sample ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.10 Inference Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Supplementary information for AlphaFold 3 6 Evaluation 33 6.1 Recent PDB evaluation set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.2 Evaluation set clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.3 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.4 Aggregation of scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.5 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 7 Differences to AlphaFold 2 and AlphaFold-Multimer 35 8 Supplemental Results 36 8.1 Selected examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 9 Appendix: CCD code and PDB ID tables 37 List of Supplementary Figures 1 Sequence-local atom attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2 MSA Module. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 List of Supplementary Tables 1 Genetics databases for protein chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Genetics databases for RNA chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 Sampling of training examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 Weights for sampling cropping strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 5 Input features to the model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 6 Training stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 7 Weighting of accuracy metrics for model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 8 Inference time in seconds by complex size, using 16 A100 . . . . . . . . . . . . . . . . . . . . . . . . 33 9 Crystallization aids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 10 Ligand exclusion list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 11 CCD codes defining glycans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 12 Ions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 13 Standard residues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 14 Recent PDB test set with nucleic acid complexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 15 PoseBusters V2 Common Natural Ligands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 List of Algorithms 1 MainInferenceLoop Main Inference Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 InputFeatureEmbedder Construct an initial single (1D) embedding . . . . . . . . . . . . . . . . . . . 10 3 RelativePositionEncoding Relative position encoding . . . . . . . . . . . . . . . . . . . . . . . . . 10 4 one_hot One-hot encoding with nearest bin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 AtomAttentionEncoder Atom attention encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6 AtomAttentionDecoder Atom attention decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 7 AtomTransformer Atom Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 8 MsaModule MSA Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 9 OuterProductMean Outer product mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 10 MSAPairWeightedAveraging MSA row-wise gated self-attention using only pair bias . . . . . . . . . . 15 11 Transition Transition layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 12 TriangleMultiplicationOutgoing Triangular multiplicative update using \u201coutgoing\u201d edges . . . . . 16 13 TriangleMultiplicationIncoming Triangular multiplicative update using \u201cincoming\u201d edges . . . . . 16 14 TriangleAttentionStartingNode Triangular gated self-attention around starting node . . . . . . . . . 17 15 TriangleAttentionEndingNode Triangular gated self-attention around ending node . . . . . . . . . . 17 16 TemplateEmbedder Template embedder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 17 PairformerStack Pairformer stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Supplementary information for AlphaFold 3 18 SampleDiffusion Sample Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 19 CentreRandomAugmentation Centre and Random Augmentation . . . . . . . . . . . . . . . . . . . . . 20 20 DiffusionModule Diffusion Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 21 DiffusionConditioning Diffusion Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 22 FourierEmbedding Fourier Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 23 DiffusionTransformer Diffusion Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 24 AttentionPairBias Attention with pair bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 25 ConditionedTransitionBlock Conditioned Transition Block . . . . . . . . . . . . . . . . . . . . . . 23 26 AdaLN Adaptive LayerNorm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 27 SmoothLDDTLoss Smooth LDDT Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 28 weighted_rigid_align Weighted Rigid Align . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 29 expressCoordinatesInFrame Express coordinates in frame . . . . . . . . . . . . . . . . . . . . . . . 27 30 computeAlignmentError Compute alignment error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 31 ConfidenceHead Confidence head . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 Supplementary information for AlphaFold 3 1 1 Notation The notation used below largely follows the convention from the AlphaFold 2 paper [1], which we summarize here for convenience to the reader. We denote the number of tokens \u2013 the fundamental sequential unit entering the Pairformer \u2013 by Ntoken (cropped during training), the number of templates used in the model by Ntempl, the number of MSA rows used in the model by Nmsa. Concrete values for these parameters are given in the data pipeline (section 2) and training details (section 5). On the model side, we also denote the number of blocks in Pairformer-like stacks by Nblock (subsection 3.6), and the number of recycling iterations by Ncycle. We present architectural details in Algorithms, where we use the following conventions. We use capitalized operator names when they encapsulate learnt parameters, e.g. we use Linear for a linear transformation with a weights matrix W and a bias vector b, and LinearNoBias for the linear transformation without the bias vector. Note that when we have multiple outputs from the Linear operator at the same line of an algorithm, we imply different trainable weights for each output. A subscript index at the linear operator, e.g., LinearNoBiasa indicates also set of different trainable weights, where a selects the weight matrix to be used for that sample (used in Algorithm 31). We use LayerNorm for the layer normalization [2] operating on the channel dimensions with learnable per-channel gains and biases. We also use capitalized names for random operators, such as generators for random augmentations. For functions without parameters we use lower case operator names, e.g. sigmoid, softmax, stopgrad. We use \u2299 for the element-wise multiplication, \u2297 for the outer product, \u2295 for the outer sum, and a\u22a4b for the dot product of two vectors. Indices i, j, k always operate on the token dimension, indices l, m on the flat atom dimension, indices s, t on the sequence dimension (e.g. indexing MSA sequences or diffusion\u2019s time dimension), and index h on the attention heads dimension. The channel dimension is implicit and we type the channel-wise vectors in bold, e.g. zij . Algorithms operate on sets of such vectors, e.g. we use {zij } to denote all pair representations. Atomic positions (i.e. vectors in Euclidean space) are specified as\u20d7 x \u2208 R3. A full atomic structure is represented as a flat atom list {\u20d7 xl} where l indexes the atom. The mapping between tokens and their corresponding atoms is denoted i(l) \u2208 N which maps the flat-atom index l to the token index i and a = token_atom_idx(l) that maps the flat-atom index l to the within-token atom index a \u2208 {1, . . . , Nmax_atoms_per_token}. 2 Data pipeline The data pipeline is the first step when running AlphaFold. It takes an input an mmCIF file and produces input features for the model. 2.1 Parsing The data pipeline operates on mmCIF format files. In addition to parsing the _atom_site section, we extract ba- sic metadata (resolution, release date, method) and some non-coordinate information (bioassembly details, chemical component details, chain names and sequences, and covalent bond information). To simplify later code, the parser performs some basic structure cleanup. Alternative locations for atoms/residues are resolved by taking the one with the largest occupancy, MSE residues are converted to MET residues, waters are removed, and arginine naming ambiguities are fixed (ensuring NH1 is always closer to CD than NH2). The first bioassembly is then expanded, to encourage the model to predict biologically relevant complexes. During inference a dummy mmCIF is used as input with all atom coordinates zeroed out. This can either be con- structed from an mmCIF deposited in PDB[3], or from a collection of CCD code sequences and SMILES strings provided by the user. 2.2 Genetic search We have 5 databases available to search for protein chains (see Table 1) Jackhmmer searches use the following additional flags: -N 1 -E 0.0001 \u2013incE 0.0001 \u2013F1 0.0005 \u2013F2 0.00005 \u2013F3 0.0000005. HHBlits searches use: -n 3 -e 0.001 -realign_max 100000 -maxfilt 100000 -min_prefilter_hits 1000 -p 20 -Z 500. A further 3 databases are available for RNA chains (see Table 2). These databases are pre-processed by filtering to RNA entries only if necessary, then clustering with settings: --min-seq-id 0.9 -c 0.8. We search the cluster representative sequences. Nhmmer searches use flags: -E 0.001 --incE 0.001 --rna --watson --F3 0.00005 (--F3 0.02 for sequences shorter than 50 nucleotides). RNA hit sequences are realigned to the query with hmmalign. Supplementary information for AlphaFold 3 2 Table 1 Genetics databases for protein chains Database Search tool Database-specific flags Max sequences UniRef90 [4] jackhmmer [5]\u2217 --seq_limit 100000 10,000 UniProt [6] jackhmmer --seq_limit 500000 50,000 Uniclust30 [7] + BFD [8] HHBlits v3.0-beta.3 [9] None Reduced BFD [10] jackhmmer --seq_limit 50000 5,000 MGnify [11] jackhmmer --seq_limit 50000 5,000 \u2217All programs from the HMMER suite [12] are v3.3. Table 2 Genetics databases for RNA chains Database Clustering tool Search tool Max sequences Rfam [13] mmseqs easy-cluster [14] nhmmer [15] 10,000 RNACentral [16] mmseqs easy-linclust nhmmer 10,000 Nucleotide collection [17] mmseqs easy-cluster nhmmer 10,000 The databases are used as follows: \u2022 For training the models, we search UniRef90 (up to v2022_05), UniProt (v2020_05) with flag -Z 119222328, Reduced BFD or the combination Uniclust30 (v2018_08) + BFD, MGnify (up to v2022_05), Rfam (v14.9), RNACentral (v21.0), and Nucleotide collection (2023-02-23). \u2022 For inference on recent PDB, we search UniRef90 (v2022_05), UniProt (v2021_04) with flag -Z 138515945, Reduced BFD, MGnify (v2022_05), Rfam (v14.9), RNACentral (v21.0), and Nucleotide collection (2023-02- 23). \u2022 For inference on the PoseBusters set, we search UniRef90 (v2020_01), UniProt (v2020_05), Uniclust30 (v2018_08) + BFD, and MGnify (v2018_12). Individual MSA results are cropped to the maximum number of sequences stated above. If no hits are found, a length-1 MSA containing only the query sequence is returned. The UniProt search result is kept separate and used to provide cross-chain genetic information, as in [18]. All other results are stacked in the order listed and deduplicated to form the main MSA. During training, the main MSA for each sequence is subsampled from size n to size k = Uniform[1, n] by selecting sequences to keep at random. 2.3 MSA processing We construct an MSA with up to 16,384 rows (Nmsa \u2264 16,384). The first row is the query sequence, the next rows (up to 8,191) are constructed by pairing the UniProt MSA based on species, as described in the AlphaFold-Multimer paper [18]. Unlike in AlphaFold-Multimer, the rest of the MSA is constructed in a dense fashion, so if there are n paired rows then, for each chain, the last (16384 \u2212 n) rows in the output MSA are the first rows from the original MSA constructed in subsection 2.2. 2.4 Template search Template search is performed only for individual protein chains, we do not provide any multi-chain templates. The template search takes as input the deduplicated UniRef90 MSA described in subsection 2.2, which in training is cropped to the first 300 sequences. We then use hmmbuild to convert the MSA into an HMM, followed by hmmsearch with flags --noali --F1 0.1 --F2 0.1 --F3 0.1 --E 100 --incE 100 --domE 100 --incdomE 100. Hmmsearch retrieves template hits from a fasta file of all protein PDB sequences. Template hits are filtered based on their release date. For PDB-based training datasets, the template release date must be no less than 60 days before that of the example, while for distillation sets the max template date is 2018-04-30. At inference time the max template date is our training cutoff of 2021-09-30 unless otherwise stated. We also remove templates that contain the exact query sequence with greater than 95% coverage as well as short templates less than 10 residues or covering less than 10% of the query. Supplementary information for AlphaFold 3 3 Template structures are retrieved from the corresponding PDB mmCIF file, and we attempt to locate the template sequence reported by hmmsearch. This should be a subsequence of the chain returned by hmmsearch, but we fall back to checking other chains for a match, and if necessary realigning the query to the reported template chain with Kalign v.0.2.0 [19]. Once the relevant residues have been identified, that part of the template structure can be featurized similarly to the training example itself (see subsection 2.8). Templates are sorted by e-value. At most 20 templates can be returned by our search, and the model uses up to 4 (Ntempl \u2264 4). At inference time we take the first 4. At training time we choose k random templates out of the available n, where k \u223c min(Uniform[0, n], 4). This reduces the efficacy of simply copying the template. Note that templates are strictly single chain; when predicting a complex, we make no attempt to select templates from the same PDB file in order to gain information about inter-chain interactions. 2.5 Training data We train on a mixture of five structural datasets, as summarized in Table 3 below. Selecting an example for the model to train on proceeds as follows: 1. Sample a dataset according to a set of dataset weights (See the weights column of Table 3). 2. Draw an example from the selected dataset. The examples from PDB-based datasets are drawn according to our weighting sampling procedure (described in subsubsection 2.5.1 below); examples from other datasets are drawn uniformly. 3. Sample a structural crop from the selected example (described in subsection 2.7 below). Table 3 Sampling of training examples: Examples are drawn from a mixture of datasets with associated weight frac- tions (weight column). Within a dataset, examples are drawn either according to our weighted procedure or uniformly (Sampling strategy column). Name Description Sampl. strategy Weight Weighted PDB Ground truth PDB structures weighted 0.5 Disordered protein PDB distillation Proteins with unresolved residues weighted 0.02 Protein monomer distillation Protein monomer predictions from MGnify uniform 0.495 Short protein monomer distillation Protein short monomer predictions from MGnify uniform 0.005 RNA distillation RNA monomer predictions from Rfam uniform 0.05 Transcription factor negatives MGnify protein + random DNA uniform 0.011 Transcription factor positives DNA+protein predictions from JASPAR uniform 0.021 2.5.1 Weighted PDB dataset For training on PDB data, we sample single chains and chain-pair interfaces with a bespoke weighting meant to both redundancy-reduce (via clustering) and stratify molecule types that exist in the underlying data (via weight factors). First, a list of chains and interfaces are collected from our filtered PDB training set (filtering described in subsubsec- tion 2.5.4), with interfaces defined as pairs of chains with minimum heavy atom (i.e. non-hydrogen) separation less than 5 \u00c5. Each item from this dataset is then either a chain or interface and is sampled with a weight w dependent upon its item type r \u2208 {chain, interface}, chain count per type n{prot,nuc,ligand}2, and cluster size Nclust: w \u221d \u03b2r Nclust (\u03b1prot \u2217 nprot + \u03b1nuc \u2217 nnuc + \u03b1ligand \u2217 nligand). (1) We chose parameters \u03b2chain = 0.5, \u03b2interface = 1, \u03b1prot = 3, \u03b1nuc = 3, \u03b1ligand = 1. The cluster size Nclust is determined from our clustering procedure, described in subsubsection 2.5.3 below. The selected chain or interface is then used to bias the cropping procedure to select crops near the selected chain or interface (see subsection 2.7). 1Transcription factor training is only applied during fine tuning. 2For example, for a DNA or RNA chain item we would have nprot = 0, nnuc = 1, nligand = 0, and for a protein-protein interface item we would have nprot = 2, nnuc = 0, nligand = 0. Supplementary information for AlphaFold 3 4 2.5.2 Distillation datasets Other than PDB ground truth structures, our four other datasets were obtained via AlphaFold distillation, three of which coming from AlphaFold 2 predictions and one from AlphaFold 3 predictions. 1. Protein monomer distillation: AlphaFold 2 predictions of MGnify sequences with greater than 200 residues (same as used in AlphaFold-Multimer). 2. Short protein monomer distillation: AlphaFold 2 predictions of MGnify sequences between 4 and 200 residues (same inference run as for protein monomer distillation). N\u224841,000,000 structures between both the long and short protein monomer distillation sets. 3. Disordered protein PDB distillation: AlphaFold-Multimer v2.3 predictions of PDB proteins from the training set with ground truth nucleic acids and small molecules inserted after the prediction is aligned to the ground truth protein. Predictions are filtered to having at least 40 unresolved residues, at least 60 GDT with respect to the ground truth and no atom clashes after inserting nucleic acids and small molecules. The resulting distillation structures therefore include all resolved entities in the ground truth PDB structure, with predicted coordinates for all protein residues (including unresolved ones) and ground truth coordinates for all non-protein entities. This distillation set was used to ensure predictions of unstructured regions are consistent with disordered strands seen in AlphaFoldDB predictions. Only the first bioassembly was used for this distillation set. N\u224825,000 structures. 4. RNA distillation: We clustered Rfam (v14.9) using MMseqs2 with 90% sequence identity and 80% coverage, taking one sequence per cluster (the cluster representative). AlphaFold 3 predictions of sequences from this set were then used as an RNA distillation training set with a minimum of 10 residues and maximum average predicted distance error (PDE) of 2. N\u224865,000 structures after filtering. 5. Transcription factor positive distillation: We constructed a dataset of positive protein-DNA examples in the fol- lowing way. We first find transcription factors profiles from the JASPAR 9 database [20] which have gene ids matching those used in two high-throughput SELEX datasets [21, 22]. Next, for each profile in this filtered dataset we assign a protein sequence in two ways: 1) by taking the canonical sequence under the profile\u2019s Uniprot ID; 2) by searching across sequences used in at least one of two SELEX datasets mentioned above for the sequence with highest similarity to the Uniprot sequence and matching gene id. Sequence similarity is computed as the number of non-gap matches between aligned sequences (aligned via Kalign v2.0 [19]), divided by the minimum length of the two pre-aligned sequences. We next clustered the transcription factor sequences with 10% sequence identity and 60% coverage and randomly select 50% of clusters for training. Next we generate positive DNA sequences which should bind each transcription factor. Specifically, for each protein sequence, we sample 10 random single- stranded motifs from corresponding JASPAR profile\u2019s position frequency matrix (PFM) and remove any identical samples. For each random motif, the protein sequence, the DNA strand, and the DNA strand\u2019s reverse comple- ment are combined to form a single distillation example. This gives a total of 16,439 protein-DNA complexes containing 1,165 unique protein sequences. Predictions on this set were made with AlphaFold 3 (trained without positive transcription factor distillation). These predictions were further filtered to only those with a minimum protein-DNA predicted distance error (PDE) of 5 \u00c5, the majority of the predictions have much lower PDE (99% less than 3 \u00c5). During training the DNA helices in this set are randomly extended with idealized DNA helices, this is done with the following steps: a) Remove DNA leaving atoms (OP3) from the original prediction. b) Sample Nnew \u223c U {0, 100 \u2212 Norig}, where Norig is the number of base pairs in the original prediction. c) Sample Nstart \u223c U {0, Nnew} and set Nend \u2190 Nnew \u2212 Nstart. d) Construct two idealized B-DNA helices with random complementary base pairs of length Nstart and Nend. e) Align the last base pairs of one of the random helices to the first base pair in the original prediction, and align the first base pair in the other random helix to the last base pair in the original prediction. f) If the new extended structure has any new clashes or violations, then we return the original structure without modifications. A clash is defined as having two heavy atoms within 1.5 \u00c5, excluding neighbouring residues. A violation is defined if the bond angle differs by more than 20 degrees to the RDKit reference conformer, or if the bond lengths differ by more than 0.2 \u00c5 to the reference conformer. 6. Transcription factor negatives distillation: We constructed a dataset of negative protein-DNA examples. It was generated with the following steps: Supplementary information for AlphaFold 3 5 a) Sample a DNA length N \u223c U {60, 100} with 0.3 probability, and from the empirical distribution of DNA lengths in PDB with 0.7 probability. b) Randomly generate a DNA sequence with N bases and its reverse complement. c) Generate a prediction with AlphaFold 3 for the dsDNA. d) Sample a random protein prediction from the protein monomer distillation set. e) Centre and randomly rotate the protein and DNA, then translate them such that the minimum distance between the two is at least 25 \u00c5 + N (2, 3) \u00c5. 2.5.3 Training set clustering In order to reduce bias in the training and evaluation sets, clustering was performed on PDB chains and interfaces, as follows. \u2022 Chain-based clustering occur at 40% sequence homology for proteins, 100% homology for nucleic acids, 100% homology for peptides (&lt;10 residues) and according to CCD identity for small molecules (i.e. only identical molecules share a cluster). \u2022 Chain-based clustering of polymers with modified residues is first done by mapping the modified residues to a standard residue using SCOP [23, 24] convention (https://github.com/biopython/biopython/ blob/5ee5e69e649dbe17baefe3919e56e60b54f8e08f/Bio/Data/SCOPData.py). If the mod- ified residue could not be found as a mapping key or was mapped to a value longer than a single character, it was mapped to type unknown. \u2022 Interface-based clustering is a join on the cluster IDs of the constituent chains, such that interfaces I and J are in the same interface cluster Cinterface only if their constituent chain pairs {I1, I2}, {J1, J2} have the same chain cluster pairs {Cchain 1 , Cchain 2 }. 2.5.4 Filtering The PDB data is filtered with the following constraints. Filtering of targets: \u2022 The structure must have been released to the PDB before the cutoff date of 2021-09-30. \u2022 The structure must have a reported resolution of 9 \u00c5 or less. \u2022 The maximum number of polymer chains in a considered structure is 300 for training and 1000 for evaluation. \u2022 Any polymer chain containing fewer than 4 resolved residues is filtered out. Filtering of bioassemblies: \u2022 Hydrogens are removed. \u2022 Polymer chains with all unknown residues are removed. \u2022 Clashing chains are removed. Clashing chains are defined as those with &gt;30% of atoms within 1.7 \u00c5 of an atom in another chain. If two chains are clashing with each other, the chain with the greater percentage of clashing atoms will be removed. If the same fraction of atoms are clashing, the chain with fewer total atoms is removed. If the chains have the same number of atoms, then the chain with the larger chain id is removed. \u2022 For residues or small molecules with CCD codes, atoms outside of the CCD code\u2019s defined set of atom names are removed. \u2022 Leaving atoms (ligand atom or groups of atoms that detach when bonds form) for covalent ligands are filtered out. \u2022 Protein chains with consecutive C\u03b1 atoms &gt;10 \u00c5 apart are filtered out. \u2022 For bioassemblies with greater than 20 chains, we select a random interface token (with a centre atom &lt;15 \u00c5 to the centre atom of a token in another chain) and select the closest 20 chains to this token based on minimum distance between any tokens centre atom. \u2022 Crystallization aids are removed if the mmCIF method information indicates that crystallography was used (see Table 9). Supplementary information for AlphaFold 3 6 2.6 Tokenization In AlphaFold 2, the protein residue was the fundamental sequential unit entering the Evoformer single and pair stacks. In this work we have generalized the tokenization scheme so as to accommodate a wider variety of molecular types. We used the following procedure. \u2022 A standard amino acid residue (Table 13) is represented as a single token. \u2022 A standard nucleotide residue (Table 13) is represented as a single token. \u2022 A modified amino acid or nucleotide residue is tokenized per-atom (i.e. N tokens for an N-atom residue) \u2022 All ligands are tokenized per-atom For each token we also designate a token centre atom, used in various places below: \u2022 C\u03b1 for standard amino acids \u2022 C1\u2032 for standard nucleotides \u2022 For other cases take the first and only atom as they are tokenized per-atom. 2.7 Cropping Our cropping procedure is largely equivalent to that found in AlphaFold-Multimer [18], with the caveat that before the cropping was applied to protein residues and now it is applied to tokens. We have three main cropping strategies, described below, that are randomly selected from, with dataset-specific selection weights (see Table 4). Table 4 Weights for sampling cropping strategies Dataset Cropping weight Contiguous Spatial Spatial interface Weighted PDB 0.20 0.40 0.40 Disordered protein PDB complex 0.20 0.40 0.40 Monomer distillation 0.25 0.75 0.00 Short protein distillation 0.25 0.75 0.00 RNA distillation 0.25 0.75 0.00 2.7.1 Contiguous cropping Here contiguous sequences of polymer residues and/or ligand atoms are selected for each chain. For details see section 7.2.1 and Algorithm 1 from the AlphaFold-Multimer paper [18]. 2.7.2 Spatial Cropping In this procedure, polymer residues and ligand atoms are selected that are within close spatial distance of a reference atom. The reference atom is selected at random from the set of token centre atoms (defined in subsection 2.6). For examples coming out of the Weighted PDB or Disordered protein PDB complex datasets, where a preferred chain or interface is provided (subsection 2.5), the reference atom is selected at random from token centre atoms that exist within this chain or interface. Once the reference atom is selected, the final crop is determined via Algorithm 2 of AlphaFold-Multimer [18]. 2.7.3 Spatial Interface Cropping In this procedure, polymer residues and ligand atoms are selected that are within close spatial distance of an interface atom. The interface atom is selected at random from the set of token centre atoms (defined in subsection 2.6) with a distance under 15 \u00c5 to another chain\u2019s token centre atom. For examples coming out of the Weighted PDB or Disordered protein PDB complex datasets, where a preferred chain or interface is provided (subsection 2.5), the reference atom is selected at random from interfacial token centre atoms that exist within this chain or interface. Once the interface atom is selected, the final crop is determined via Algorithm 2 of AlphaFold-Multimer [18]. Supplementary information for AlphaFold 3 7 2.8 Featurisation and model inputs Table 5 lists the main model input features, which fall into the following categories: \u2022 Token features. Composed of per-token features, such as position indexes (token_index); chain identifiers (asym_id); masks (is_protein). \u2022 Reference features. Features derived from a residue, nucleotide or ligand\u2019s reference conformer. Given an input CCD code or SMILES string, the conformer is typically generated with RDKit v.2023_03_3 [25] using ETKDGv3 [26]. On error, we fall back to using the CCD ideal coordinates, or finally the representative coordinates if they are from before our training date cut-off (2021-09-30 unless otherwise stated). At the end, any atom coordinates still missing are set to zeros. \u2022 Msa features. Features derived from genetics search, e.g. the MSA itself, deletion matrix, and profile. \u2022 Template features. Features derived from template search, e.g. the template atom positions. \u2022 Bond features. Features providing bond information, e.g. the expected locations of polymer-ligand bonds, and of intra-/inter-ligand bonds. In the algorithms, features are referred as \u201cf \u201d with the feature name in superscript and the element indices in subscript, e.g. the one-hot encoded msa feature with shape [Nmsa, Ntoken, 32] is denoted as f msa si \u2208 N32 Table 5 Input features to the model. Feature dimensions: Ntoken is the number of tokens, Nmsa is the number of MSA sequences, Ntempl is the number of templates, Natom is the number of atoms, Nchains is the number of chains, Nbonds is the number of bonds, Nperm the number of atom permutations. Feature &amp; Shape Description residue_index [Ntoken] Residue number in the token\u2019s original input chain. token_index [Ntoken] Token number. Increases monotonically; does not restart at 1 for new chains. asym_id [Ntoken] Unique integer for each distinct chain. entity_id [Ntoken] Unique integer for each distinct sequence. sym_id [Ntoken] Unique integer within chains of this sequence. E.g. if chains A, B and C share a sequence but D does not, their sym_ids would be [0, 1, 2, 0]. restype [Ntoken, 32] One-hot encoding of the sequence. 32 possible values: 20 amino acids + unknown, 4 RNA nucleotides + unknown, 4 DNA nucleotides + un- known, and gap. Ligands represented as \u201cunknown amino acid\u201d. is_protein / rna / dna / ligand [Ntoken] 4 masks indicating the molecule type of a particular token. ref_pos [Natom, 3] Atom positions in the reference conformer, with a random rotation and translation applied. Atom positions are given in \u00c5. ref_mask [Natom] Mask indicating which atom slots are used in the reference conformer. ref_element [Natom, 128] One-hot encoding of the element atomic number for each atom in the reference conformer, up to atomic number 128. ref_charge [Natom] Charge for each atom in the reference conformer. Supplementary information for AlphaFold 3 8 Feature &amp; Shape Description ref_atom_name_chars [Natom, 4, 64] One-hot encoding of the unique atom names in the reference conformer. Each character is encoded as ord(c) \u2212 32, and names are padded to length 4. ref_space_uid [Natom] Numerical encoding of the chain id and residue index associated with this reference conformer. Each (chain id, residue index) tuple is as- signed an integer on first appearance. msa [Nmsa, Ntoken, 32] One-hot encoding of the processed MSA, using the same classes as restype. has_deletion [Nmsa, Ntoken] Binary feature indicating if there is a deletion to the left of each position in the MSA. deletion_value [Nmsa, Ntoken] Raw deletion counts (the number of deletions to the left of each MSA position) are transformed to [0, 1] using 2 \u03c0 arctan d 3 . profile [Ntoken, 32] Distribution across restypes in the main MSA. Computed before MSA processing (subsection 2.3). deletion_mean [Ntoken] Mean number of deletions at each position in the main MSA. Computed before MSA processing (subsection 2.3). template_restype [Ntempl, Ntoken] One-hot encoding of the template sequence, see restype. template_pseudo_beta_mask [Ntempl, Ntoken] Mask indicating if the C\u03b2 (C\u03b1 for glycine) has coordinates for the tem- plate at this residue. template_backbone_frame_mask [Ntempl, Ntoken] Mask indicating if coordinates exist for all atoms required to compute the backbone frame (used in the template_unit_vector feature). template_distogram [Ntempl, Ntoken, Ntoken, 39] A one-hot pairwise feature indicating the distance between C\u03b2 atoms (C\u03b1 for glycine). Pairwise distances are discretized into 38 bins of equal width between 3.25 \u00c5 and 50.75 \u00c5; one more bin contains any larger distances. template_unit_vector [Ntempl, Ntoken, Ntoken, 3] The unit vector of the displacement of the C\u03b1 atom of all residues within the local frame of each residue. Local frames are computed as in [1]. token_bonds [Ntoken, Ntoken] A 2D matrix indicating if there is a bond between any atom in token i and token j, restricted to just polymer-ligand and ligand-ligand bonds and bonds less than 2.4 \u00c5 during training. 3 Model architecture The model architecture is broadly based on AlphaFold 2; however, we made a number of changes to enable the model to predict a wider range of molecules than proteins and also to further increase the accuracy in protein structure predictions. The model is a conditional diffusion model, where unlike most other diffusion models most of the computation is happening in the conditioning. The conditioning part of the model is similar in overall architecture to the trunk (Template Module, MSA Module, and Pairformer) of AlphaFold 2, with a number of key differences. The algorithm corresponding to the architecture is shown in Algorithm 1 and in Main Article Fig. 1d. We introduce a more general tokenization scheme, where each amino acid residue corresponds to one token as in AlphaFold 2 and each nucleotide corresponds to one token, while for other molecules we encode each heavy atom as its own token. In order to get the initial trunk input feature we have a more sophisticated input feature embedder that performs attention over all atoms Supplementary information for AlphaFold 3 9 in order to encode the information about the chemical structure of all the molecules, leading to a single representation representing all the tokens. Given the input features, we build a pair representation in a manner similar to AlphaFold 2. This pair representation and the single representation are then fed into the main part of the conditioning network, which is recycled multiple times. The main part of the conditioning network consists of a TemplateEmbedder in a similar style to AlphaFold 2 which encodes information about provided templates into the pair representation. This is followed by an MSA Module that extracts information from the MSA and encodes it into the pair representation. Here we use MSAs for both protein sequences as well as RNA sequences. The resulting pair representation is then used as an input for the main Pairform- erStack, which also receives the single representation as an input and processes the representation further forming the main loop of the model. The resulting single and pair embeddings are then used to condition a diffusion process. Here the diffusion is parametrized by a Diffusion Module, which is a considerably cheaper sub-network that encodes a single denoising step. Notably it scales quadratically in the number of tokens rather than cubically. The resulting output structure from the Diffusion Module is then passed to a confidence head, which uses the pair and single representation together with the structure to provide confidence measures. The components of the model are described in the subsequent sections. Algorithm 1 Main Inference Loop def MainInferenceLoop({f }, Ncycle = 4, cs = 384, cz = 128) : 1: {sinputs i } = InputFeatureEmbedder({f }) 2: sinit i = LinearNoBias(sinputs i ) sinit i \u2208 Rcs 3: zinit ij = LinearNoBias(sinputs i ) + LinearNoBias(sinputs j ) zinit ij \u2208 Rcz 4: zinit ij += RelativePositionEncoding({f }) 5: zinit ij += LinearNoBias(f token_bonds ij ) 6: {\u02c6zij }, {\u02c6si} = 0, 0 7: for all c \u2208 [1, . . . , Ncycle] do 8: zij = zinit ij + LinearNoBias(LayerNorm(\u02c6zij )) zij \u2208 Rcz 9: {zij } += TemplateEmbedder({f }, {zij }) 10: {zij } += MsaModule({f msa Si }, {zij }, {sinputs i }) 11: si = sinit i + LinearNoBias(LayerNorm(\u02c6si)) si \u2208 Rcs 12: {si}, {zij } = PairformerStack({si}, {zij }) 13: {\u02c6si}, {\u02c6zij } \u2190 {si}, {zij } 14: end for 15: {\u20d7 xpred l } = SampleDiffusion({f }, {sinputs i }, {si}, {zij }) 16: {pplddt l }, {ppae ij }, {ppde ij }, {presolved l } = ConfidenceHead({sinputs i }, {si}, {zij }, {\u20d7 xpred l }) 17: pdistogram ij = DistogramHead(zij ) 18: return {\u20d7 xpred l }, {pplddt l }, {ppae ij }, {ppde ij }, {presolved l }, {pdistogram ij } 3.1 Input embeddings Any bonds provided by the user (via the token_bonds feature) are linearly embedded in Algorithm 1, the embedding of the other user inputs, along with the RDKit reference conformer are described below. 3.1.1 Input embedder The residue type, reference conformer and MSA summary features (profile and deletion_mean) are embedded in Al- gorithm 2. The reference conformer is embedded in a permutation invariant way using the AtomAttentionEncoder (Algorithm 5). Supplementary information for AlphaFold 3 10 Algorithm 2 Construct an initial 1D embedding def InputFeatureEmbedder({f *}) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#embed-per-atom-features","title":"Embed per-atom features.","text":"<p>1: {ai}, , , _ = AtomAttentionEncoder({f *}, \u2205, \u2205, \u2205, catom = 128, catompair = 16, ctoken = 384)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#concatenate-the-per-token-features","title":"Concatenate the per-token features.","text":"<p>2: si = concat(ai, f restype i , f profile i , f deletion_mean i ) 3: return {si} 3.1.2 Relative position encoding As in AlphaFold 2 and AlphaFold-Multimer, relative encodings are used to break symmetries across identical residues (arel_pos ij ) and chains (arel_chain ij ). In AlphaFold 3 we introduce a relative token encoding, applied to tokens within the same residue (see Algorithm 3). The relative position and token indices are clipped between [rmin, rmax], with rmax = 32. Relative chain indices are clipped between [smin, smax], with smax = 2. Algorithm 3 Relative position encoding def RelativePositionEncoding({f }, rmax, smax, cz = 128) : 1: bsame_chain ij = (f asym_id i == f asym_id j ) 2: bsame_residue ij = (f residue_index i == f residue_index j ) 3: bsame_entity ij = (f entity_id i == f entity_id j ) 4: dresidue ij = \uf8f1 \uf8f2 \uf8f3 clip(f residue_index i \u2212 f residue_index j + rmax, 0, 2 \u00b7 rmax) if bsame_chain ij 2 \u00b7 rmax + 1 else dresidue ij \u2208 N 5: arel_pos ij = one_hot(dresidue ij , [0, . . . , 2 \u00b7 rmax + 1]) 6: dtoken ij = \uf8f1 \uf8f2 \uf8f3 clip(f token_index i \u2212 f token_index j + rmax, 0, 2 \u00b7 rmax) if bsame_chain ij and bsame_residue ij 2 \u00b7 rmax + 1 else dtoken ij \u2208 N 7: arel_token ij = one_hot(dij , [0, . . . , 2 \u00b7 rmax + 1]) 8: dchain ij = \uf8f1 \uf8f2 \uf8f3 clip(f sym_id i \u2212 f sym_id j + smax, 0, 2 \u00b7 smax) if not bsame_chain ij 2 \u00b7 smax + 1 else dchain ij \u2208 N 9: arel_chain ij = one_hot(dchain ij , [0, . . . , 2 \u00b7 smax + 1]) 10: pij = LinearNoBias(concat([arel_pos ij , arel_token ij , bsame_entity ij , arel_chain ij ])) pij \u2208 R cz 11: return {pij } Supplementary information for AlphaFold 3 11 Algorithm 4 One-hot encoding with nearest bin def one_hot(x, vbins) : x \u2208 R, vbins \u2208 RNbins 1: p = 0 p \u2208 RNbins 2: b = arg min(|x \u2212 vbins|) 3: pb = 1 4: return p 3.2 Sequence-local atom attention The \u201csequence-local atom attention\u201d represents the whole structure as a flat list of atoms and allows all atoms to \u201ctalk\u201d directly to each other within a certain sequence neighbourhood. E.g., each subset of 32 atoms attends to the subset of the nearby 128 atoms (nearby in the sequence space). This gives the network the capacity to learn general rules about local atom constellations, independently from the coarse-grained tokenization where each standard residue is represented with a single token only. The restriction to a certain sequence neighbourhood is sub-optimal, but was necessary to keep the memory and compute costs within reasonable bounds. The resulting attention pattern is equivalent to computing the full affinity matrix and applying a neighbourhood mask that contains the rectangular blocks along the diagonal (see Suppl. Fig. 1).flat atoms flat atoms Supplementary Figure 1 Sequence-local atom attention. Each subset of atoms (rows) attends to a larger subset of atoms (columns). The blue area depicts the theoretical full Natoms \u00d7 Natoms attention matrix. The yellow rectangles represent the attentions that are realized. The conversion of the atom indexing by token index i \u2208 Ntokens and atom name a \u2208 Satom names, e. g. {\u20d7xa i } to a flat indexing with atom index l \u2208 Natoms is denoted by the mapping of flat atom index to the token index i = tok_idx(l) . Supplementary information for AlphaFold 3 12 Algorithm 5 Atom attention encoder def AtomAttentionEncoder({f }, {rl}, {strunk i }, {zij }, catom, catompair, ctoken) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#create-the-atom-single-conditioning-embed-per-atom-meta-data","title":"Create the atom single conditioning: Embed per-atom meta data","text":"<p>1: cl = LinearNoBias(concat(\u20d7f ref_pos l , f ref_charge l , f ref_mask l , f ref_element l , f ref_atom_name_chars l )) cl \u2208 Rcatom l \u2208 {1, . . . , Natoms}</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#embed-offsets-between-atom-reference-positions","title":"Embed offsets between atom reference positions","text":"<p>2:\u20d7 dlm =\u20d7 f ref_pos l \u2212\u20d7 f ref_pos m\u20d7 dlm \u2208 R3 3: vlm = (f ref_space_uid l == f ref_space_uid m ) vlm \u2208 R 4: plm = LinearNoBias(\u20d7dlm) \u00b7 vlm plm \u2208 Rcatompair</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#embed-pairwise-inverse-squared-distances-and-the-valid-mask","title":"Embed pairwise inverse squared distances, and the valid mask.","text":"<p>5: plm += LinearNoBias \u0012 1/ \u0010 1 + \u2225\u20d7 dlm\u22252 \u0011\u0013 \u00b7 vlm 6: plm += LinearNoBias(vlm) \u00b7 vlm</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#initialise-the-atom-single-representation-as-the-single-conditioning","title":"Initialise the atom single representation as the single conditioning.","text":"<p>7: ql = cl ql \u2208 Rcatom</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#if-provided-add-trunk-embeddings-and-noisy-positions","title":"If provided, add trunk embeddings and noisy positions.","text":"<p>8: if {rl}\u0338 = \u2205 then</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#broadcast-the-single-and-pair-embedding-from-the-trunk","title":"Broadcast the single and pair embedding from the trunk.","text":"<p>9: cl += LinearNoBias(LayerNorm(strunk tok_idx(l))) 10: plm += LinearNoBias(LayerNorm(ztok_idx(l) tok_idx(m)))</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#add-the-noisy-positions","title":"Add the noisy positions.","text":"<p>11: ql += LinearNoBias (rl) 12: end if</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#add-the-combined-single-conditioning-to-the-pair-representation","title":"Add the combined single conditioning to the pair representation.","text":"<p>13: plm += LinearNoBias(relu(cl)) + LinearNoBias(relu(cm))</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#run-a-small-mlp-on-the-pair-activations","title":"Run a small MLP on the pair activations.","text":"<p>14: plm += LinearNoBias(relu(LinearNoBias(relu(LinearNoBias(relu(plm))))))</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#cross-attention-transformer","title":"Cross attention transformer.","text":"<p>15: {ql} = AtomTransformer({ql}, {cl}, {plm}, Nblock = 3, Nhead = 4)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#aggregate-per-atom-representation-to-per-token-representation","title":"Aggregate per-atom representation to per-token representation","text":"<p>16: ai = mean l\u2208{1,...,Natoms} tok_idx(l)=i (relu(LinearNoBias(ql))) ai \u2208 Rctoken 17: qskip l , cskip l , pskip lm = ql, cl, plm 18: return {ai}, {qskip l }, {cskip l }, {pskip lm } Supplementary information for AlphaFold 3 13 Algorithm 6 Atom attention decoder def AtomAttentionDecoder({ai}, {qskip l }, {cskip l }, {pskip lm }) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#broadcast-per-token-activiations-to-per-atom-activations-and-add-the-skip-connection","title":"Broadcast per-token activiations to per-atom activations and add the skip connection","text":"<p>1: ql = LinearNoBias(atok_idx(l)) + qskip l</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#cross-attention-transformer_1","title":"Cross attention transformer.","text":"<p>2: {ql} = AtomTransformer({ql}, {cskip l }, {pskip lm }, Nblock = 3, Nhead = 4)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#map-to-positions-update","title":"Map to positions update.","text":"<p>3: rupdate l = LinearNoBias(LayerNorm(ql)) 4: return {rupdate l } Algorithm 7 Atom Transformer def AtomTransformer({ql}, {cl}, {plm}, Nblock = 3, Nhead, Nqueries = 32, Nkeys = 128, Ssubset centres = {15.5, 47.5, 79.5, . . . }) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#sequence-local-atom-attention-is-equivalent-to-self-attention-within-rectangular-blocks-along-the-diagonal","title":"sequence-local atom attention is equivalent to self attention within rectangular blocks along the diagonal.","text":"<p>1: \u03b2lm = \uf8f1 \uf8f2 \uf8f3 0 if |l \u2212 c| &lt; Nqueries/2 \u2227 |m \u2212 c| &lt; Nkeys/2 \u2200 c \u2208 Ssubset centres \u22121010 else 2: {ql}= DiffusionTransformer({ql}, {cl}, {plm}, {\u03b2lm}, Nblock, Nhead) 3: return {ql} Supplementary information for AlphaFold 3 14 3.3 MSA Module The MSA Module (Suppl. Fig. 2, Algorithm 8) in AlphaFold 3 fullfills a similar role to the Extra MSA Stack in AlphaFold 2 and hence has a fairly similar network architecture to AlphaFold-Multimer in the block. It samples a new iid random subset of the MSA for each recycling iteration, the MSA sequences and input features then get embedded into representation msi for each token in each sequence in the msa. The Network architecture of the MSA Module then consists of 4 homogeneous blocks which repeatedly process and combine the pair representation zij and the msa. The final pair representation then gets passed on to the Pairformer Stack. The overall structure of the block is very similar to the Pairformer Stack, where the MSA representation fullfills a role similar to the single representation. The individual blocks here are similar to the extra MSA stack in AlphaFold 2, the difference here is that the attention is independently performed for each row of the MSA and that attention weights are entirely projected from the pair representation, i.e. there is no key-query based attention. This also means that each row of the MSA combines information via attention in the same way, this reduces computation and memory usage in the attention. The MSA attention layer employs the same gating mechanism as the other attention layers. Otherwise this part of the model works the same as AlphaFold 2, meaning the pair representation gets passed through Triangular Multiplicative Update and Triangular self-attention layers and a transition block. In all the transition blocks we use SwiGLU instead of ReLU. Conceptually a difference to AlphaFold 2 is that here we do not combine information across different rows of the MSA directly, but rather all information has to flow via the pair representation. The motivation behind this is that the pair representation should contain as much information as possible about the proteins or nucleic acids as it forms the backbone for the rest of the network.pair representation (n,n,c) pair- weighted averaging + tran- sition Outer product mean + triangle update using \"outgoing\" edges triangle self- attention around starting node triangle self- attention around ending node triangle update using \"incoming\" edges tran- sition pair representation (n,n,c) + + + + + + 4 blocks subsample and embed + inputs rep. (n,c) MSA representation (s,n,c) HHSSGLVPRGSHMSGKIQHK HHSSGLVPRGSHMPSYTLHy HHSSGLVPRGSHMILKFDHi HHSSGLVPRGSHMRDPTQFE HHSSGLVPRGSHNMTQFEER HHSSGLVPRGSSGAFEDRDp HHSSGLVPRGSHMAAPIRFf HHSSGLVPRGSHMDLE---K Supplementary Figure 2 MSA Module. Supplementary information for AlphaFold 3 15 Algorithm 8 MSA Module def MsaModule({f *}, {zij }, {sinputs i }, Nblock = 4, cm = 64) : 1: mSi = concat(f msa Si , f has_deletion Si , f deletion_value Si ) 2: {s} = SampleRandomWithoutReplacement({S}) 3: msi \u2190 LinearNoBias(msi) msi \u2208 Rcm 4: msi+= LinearNoBias({sinputs i }) 5: for all l \u2208 [1, . . . , Nblock] do</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#communication","title":"Communication","text":"<p>6: {zij } += OuterProductMean({msi})</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#msa-stack","title":"MSA stack","text":"<p>7: {msi} += DropoutRowwise0.15(MSAPairWeightedAveraging({msi}, {zij }, c = 8)) 8: {msi} += Transition({msi})</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#pair-stack","title":"Pair stack","text":"<p>9: {zij } += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij })) 10: {zij } += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij })) 11: {zij } += DropoutRowwise0.25(TriangleAttentionStartingNode({zij })) 12: {zij } += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij })) 13: {zij } += Transition({zij }) 14: end for 15: return {zij } Algorithm 9 Outer product mean def OuterProductMean({msi}, c = 32, cz = 128) : 1: msi \u2190 LayerNorm(msi) 2: asi, bsi = LinearNoBias(msi) asi, bsi \u2208 Rc 3: oij = flatten means(asi \u2297 bsj )\u0001 oij \u2208 Rc\u00b7c 4: zij = Linear(oij ) zij \u2208 Rcz 5: return {zij } Algorithm 10 MSA pair weighted averaging with gating def MSAPairWeightedAveraging({msi}, {zij }, c = 32, Nhead = 8) : msi \u2208 Rcm</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#input-projections","title":"Input projections","text":"<p>1: msi \u2190 LayerNorm(msi) 2: vh si = LinearNoBias(msi) vh si \u2208 Rc, h \u2208 {1, . . . , Nhead} 3: bh ij = LinearNoBias(LayerNorm(zij )) 4: gh si = sigmoid LinearNoBias(msi)\u0001 gh si \u2208 Rc</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#weighted-average-with-gating","title":"Weighted average with gating","text":"<p>5: wh ij = softmaxj \u0010 bh ij \u0011 6: oh si = gh si \u2299 P j wh ij vh sj</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#output-projection","title":"Output projection","text":"<p>7: \u02dcmsi = LinearNoBias \u0010 concath(oh si) \u0011 \u02dcmsi \u2208 Rcm 8: return { \u02dcmsi} Supplementary information for AlphaFold 3 16 Algorithm 11 Transition layer def Transition(x, n = 4) : x \u2208 Rc 1: x \u2190 LayerNorm(x) 2: a = LinearNoBias(x) a \u2208 Rn\u00b7c 3: b = LinearNoBias(x) b \u2208 Rn\u00b7c 4: x \u2190 LinearNoBias(swish(a) \u2299 b) x \u2208 Rc 5: return x 3.4 Triangle updates of the pair representation AlphaFold 3 uses the same update / attention scheme for the pair representation as AlphaFold 2 which establishes a direct communication between the edges that connect 3 nodes (interpreting the pair representation as edge features of a fully connected graph where the tokens are the nodes). This allows the network to easily detect inconsistencies in its current belief about the spatial relationship and to update them accordingly. For more details, see [1]. Algorithm 12 Triangular multiplicative update using \u201coutgoing\u201d edges def TriangleMultiplicationOutgoing({zij }, c = 128) : zij \u2208 cz 1: zij \u2190 LayerNorm(zij ) 2: aij , bij = sigmoid LinearNoBias(zij )\u0001 \u2299 LinearNoBias(zij ) aij , bij \u2208 Rc 3: gij = sigmoid LinearNoBias(zij )\u0001 gij \u2208 Rcz 4: \u02dczij = gij \u2299 LinearNoBias(LayerNorm(P k aik \u2299 bjk)) \u02dczij \u2208 Rcz 5: return {\u02dczij } Algorithm 13 Triangular multiplicative update using \u201cincoming\u201d edges (differences to Algorithm 12 highlighted) def TriangleMultiplicationIncoming({zij }, c = 128) : zij \u2208 cz 1: zij \u2190 LayerNorm(zij ) 2: aij , bij = sigmoid LinearNoBias(zij )\u0001 \u2299 LinearNoBias(zij ) aij , bij \u2208 Rc 3: gij = sigmoid LinearNoBias(zij )\u0001 gij \u2208 Rcz 4: \u02dczij = gij \u2299 LinearNoBias(LayerNorm(P k aki \u2299 bkj )) \u02dczij \u2208 Rcz 5: return {\u02dczij } Supplementary information for AlphaFold 3 17 Algorithm 14 Triangular gated self-attention around starting node def TriangleAttentionStartingNode({zij }, c = 32, Nhead = 4) : zij \u2208 cz</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#input-projections_1","title":"Input projections","text":"<p>1: zij \u2190 LayerNorm(zij ) 2: qh ij , kh ij , vh ij = LinearNoBias(zij ) qh ij , kh ij , vh ij \u2208 Rc, h \u2208 {1, . . . , Nhead} 3: bh ij = LinearNoBias(zij ) 4: gh ij = sigmoid LinearNoBias(zij )\u0001 gh ij \u2208 Rc</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#attention","title":"Attention","text":"<p>5: ah ijk = softmaxk \u0010 1\u221ac qh ij \u22a4kh ik + bh jk \u0011 6: oh ij = gh ij \u2299 P k ah ijkvh ik</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#output-projection_1","title":"Output projection","text":"<p>7: \u02dczij = LinearNoBias \u0010 concath(oh ij ) \u0011 \u02dczij \u2208 Rcz 8: return {\u02dczij } Algorithm 15 Triangular gated self-attention around ending node (differences to Algorithm 14 highlighted) def TriangleAttentionEndingNode({zij }, c = 32, Nhead = 4) : zij \u2208 cz</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#input-projections_2","title":"Input projections","text":"<p>1: zij \u2190 LayerNorm(zij ) 2: qh ij , kh ij , vh ij = LinearNoBias(zij ) qh ij , kh ij , vh ij \u2208 Rc, h \u2208 {1, . . . , Nhead} 3: bh ij = LinearNoBias(zij ) 4: gh ij = sigmoid LinearNoBias(zij )\u0001 gh ij \u2208 Rc</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#attention_1","title":"Attention","text":"<p>5: ah ijk = softmaxk \u0012 1\u221ac qh ij \u22a4 kh kj + bh ki \u0013 6: oh ij = gh ij \u2299 P k ah ijk vh kj</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#output-projection_2","title":"Output projection","text":"<p>7: \u02dczij = LinearNoBias \u0012 concath \u0010 oh ij \u0011\u0013 \u02dczij \u2208 Rcz 8: return {\u02dczij } Supplementary information for AlphaFold 3 18 3.5 Template embedding The template embedding (Algorithm 16) combines all raw template features to a pair representation, and processes it together with the given pair representation zij (produced in the previous recyling iteration). This allows the network to attend to specific regions in the template based on its current belief about the structure. The template_backbone_frame_mask, template_distogram, template_restype, template_pseudo_beta_mask and tem- plate_unit_vector features from Table 5 are concatenated into a 2D feature. In the main recycling loop (Algorithm 1) the pairwise embeddings zij are passed into the template embedder (Algorithm 16). Each template is processed in- dependently with a PairformerStack, and the resulting activations are averaged to obtain a pairwise embedding of all templates. Algorithm 16 Template embedder def TemplateEmbedder({f *}, {zij }, Nblock = 2, c = 64) : 1: btemplate_backbone_frame_mask ij = f template_backbone_frame_mask ti \u00b7 f template_backbone_frame_mask tj 2: btemplate_pseudo_beta_mask ij = f template_pseudo_beta_mask ti \u00b7 f template_pseudo_beta_mask tj 3: atij = concat(f template_distogram tij , btemplate_backbone_frame_mask ij , f template_unit_vector tij , btemplate_pseudo_beta_mask ij ) 4: atij \u2190 atij \u2299 (f asym_id i == f asym_id j ) 5: atij \u2190 concat(atij , f template_restype ti , f template_restype tj ) 6: uij = 0 uij \u2208 Rc 7: for all t \u2208 [1, . . . , Ntemplates] do 8: vij = LinearNoBias(LayerNorm(zij )) + LinearNoBias(atij ) vij \u2208 Rc 9: {vij } += PairformerStack({vij }, Nblock) 10: uij += LayerNorm(vij ) 11: end for 12: uij \u2190 uij Ntemplates 13: uij \u2190 LinearNoBias(ReLU(uij )) uij \u2208 Rc 14: return {uij } 3.6 Pairformer stack The Pairformer Stack (Algorithm 17, Main Article Fig. 2a) fulfills a similar role to the Evoformer stack in AlphaFold 2, the Pairformer stack uses just a single representation si, rather than a representation for a subset of the MSA. Here the single representation plays a role similar to the privileged first row in the Evoformer in AlphaFold 2. As a consequence of this change there is no column-wise attention in the stack. The single attention with pair bias is the same as the row-wise attention used in AlphaFold 2, but only applied to a single sequence, which corresponds to the single represenation si. Furthermore unlike in AlphaFold 2 the single representation does not influence the pair representation, the pair rep- resentation is used to control information flow in the single representation by biasing the Attention logits. All Transition blocks use SwiGLU. Supplementary information for AlphaFold 3 19 Algorithm 17 Pairformer stack def PairformerStack({si}, {zij }, Nblock = 48) : 1: for all l \u2208 [1, . . . , Nblock] do</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#pair-stack_1","title":"Pair stack","text":"<p>2: {zij } += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij })) 3: {zij } += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij })) 4: {zij } += DropoutRowwise0.25(TriangleAttentionStartingNode({zij })) 5: {zij } += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij })) 6: {zij } += Transition({zij }) 7: {si} += AttentionPairBias({si}, \u2205, {zij }, \u03b2ij = 0, Nhead = 16) 8: {si} += Transition({si}) 9: end for 10: return {si}, {zij } 3.7 Diffusion Module In AlphaFold 2 the final structure was realised using a Structure Module using invariant point attention. For AlphaFold 3 we replaced it with a relatively standard non-equivariant point-cloud diffusion model over all atoms (Algorithm 18 and Main Article Fig. 2b). During training, we train a denoiser to remove Gaussian noise from the positions of all heavy atoms conditioned on the features from the main trunk. The denoiser is based on a modern transformer, but with several modifications to make it more amenable to the task. The main changes are: \u2022 We incorporate conditioning from the trunk in several ways: we initialise the activations from the single em- bedding, use a variant of Adaptive Layernorm [27] for the single conditioning and logit biasing for the pair conditioning. \u2022 We use standard modern transformer tricks (e.g. SwiGLU [28]) and methods used in AlphaFold 2 (gating). \u2022 We use a two-level architecture, working first on atoms, then tokens, then atoms again. Notably, the transformer only uses a single linear layer to embed all atom positions and and a single linear layer to project the updates at the end, there are no geometric biases involved (e.g. locality or SE(3) invariance). This is in contrast to contemporary trends of using stronger domain specific inductive biases. The details of the architecture are outlined in Algorithm 20. Supplementary information for AlphaFold 3 20 Algorithm 18 Sample Diffusion def SampleDiffusion( {f }, {sinputs i }, {strunk i }, {ztrunk ij }, Noise Schedule [c0, c1, . . . , cT ], \u03b30 = 0.8, \u03b3min = 1.0, noise scale \u03bb = 1.003, step scale \u03b7 = 1.5 ): 1:\u20d7 xl \u223c c0 \u00b7 N (\u20d70, I3)\u20d7 xl \u2208 R3 2: for all c\u03c4 \u2208 [c1, . . . , cT ] do 3: {\u20d7 xl} \u2190 CentreRandomAugmentation({\u20d7xl}) 4: \u03b3 = \u03b30 if c\u03c4 &gt; \u03b3min else 0 5: \u02c6t = c\u03c4 \u22121(\u03b3 + 1) 6:\u20d7 \u03bel = \u03bb q\u02c6t2 \u2212 c2 \u03c4 \u22121 \u00b7 N (\u20d7 0, I3)\u20d7 \u03bel \u2208 R3 7:\u20d7 xnoisy l =\u20d7 xl +\u20d7 \u03bel 8: {\u20d7 xdenoised l } = DiffusionModule({\u20d7xnoisy l }, \u02c6t, {f }, {sinputs i }, {strunk i }, {ztrunk ij }) 9:\u20d7 \u03b4l = (\u20d7xl \u2212\u20d7 xdenoised l )/\u02c6t 10: dt = c\u03c4 \u2212 \u02c6t 11:\u20d7 xl \u2190\u20d7 xnoisy l + \u03b7 \u00b7 dt \u00b7\u20d7 \u03b4l 12: end for 13: return {\u20d7xl} Algorithm 19 CentreRandomAugmentation def CentreRandomAugmentation({\u20d7 xl}, strans = 1 \u00c5) : 1:\u20d7 xl \u2190\u20d7 xl \u2212 mean l\u20d7 xl 2: R = UniformRandomRotation() 3:\u20d7 t \u223c strans \u00b7 N (\u20d70, I3) 4:\u20d7 xl \u2190 R \u00b7\u20d7 xl +\u20d7 t 5: return {\u20d7 xl} Supplementary information for AlphaFold 3 21 Algorithm 20 Diffusion Module def DiffusionModule({\u20d7 xnoisy l }, \u02c6t, {f *}, {sinputs i }, {strunk i }, {ztrunk ij }, \u03c3data = 16, catom = 128, catompair = 16, ctoken = 768) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#conditioning","title":"Conditioning","text":"<p>1: {si}, {zij }= DiffusionConditioning(\u02c6t, {f *}, {sinputs i }, {strunk i }, {ztrunk ij }, \u03c3data)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#scale-positions-to-dimensionless-vectors-with-approximately-unit-variance","title":"Scale positions to dimensionless vectors with approximately unit variance.","text":"<p>2: rnoisy l =\u20d7 xnoisy l / q \u02c6t2 + \u03c32 data rnoisy l \u2208 R3</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#sequence-local-atom-attention-and-aggregation-to-coarse-grained-tokens","title":"Sequence-local Atom Attention and aggregation to coarse-grained tokens","text":"<p>3: {ai}, {qskip l }, {cskip l }, {pskip lm } = AtomAttentionEncoder({f *}, {rnoisy l }, {strunk i }, {zij }, catom, catompair, ctoken) ai \u2208 Rctoken</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#full-self-attention-on-token-level","title":"Full self-attention on token level.","text":"<p>4: ai+= LinearNoBias(LayerNorm(si)) 5: {ai} \u2190 DiffusionTransformer({ai}, {si}, {zij }, \u03b2ij = 0, Nblock = 24, Nhead = 16) 6: ai \u2190 LayerNorm(ai)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#broadcast-token-activations-to-atoms-and-run-sequence-local-atom-attention","title":"Broadcast token activations to atoms and run Sequence-local Atom Attention","text":"<p>7: {rupdate l } = AtomAttentionDecoder({ai}, {qskip l }, {cskip l }, {pskip lm })</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#rescale-updates-to-positions-and-combine-with-input-positions","title":"Rescale updates to positions and combine with input positions","text":"<p>8:\u20d7 xout l = \u03c32 data/(\u03c32 data + \u02c6t2) \u00b7\u20d7 xnoisy l + \u03c3data \u00b7 \u02c6t/ q \u03c32 data + \u02c6t2 \u00b7 rupdate l 9: return {\u20d7xout l } Algorithm 21 Diffusion Conditioning def DiffusionConditioning(\u02c6t, {f *}, {sinputs i }, {strunk i }, {ztrunk ij }, \u03c3data, cz = 128, cs = 384) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#pair-conditioning","title":"Pair conditioning","text":"<p>1: zij = concat([ztrunk ij , RelativePositionEncoding({f *})]) 2: zij \u2190 LinearNoBias(LayerNorm(zij )) zij \u2208 Rcz 3: for all b \u2208 [1, 2] do 4: zij += Transition(zij , n = 2) 5: end for</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#single-conditioning","title":"Single conditioning","text":"<p>6: si = concat([strunk i , sinputs i ]) 7: si \u2190 LinearNoBias(LayerNorm(si)) si \u2208 Rcs 8: n = FourierEmbedding( 1 4 log(\u02c6t/\u03c3data), 256) 9: si+= LinearNoBias(LayerNorm(n)) 10: for all b \u2208 [1, 2] do 11: si+= Transition(si, n = 2) 12: end for 13: return {si}, {zij } Supplementary information for AlphaFold 3 22 Algorithm 22 Fourier Embedding def FourierEmbedding(\u02c6t, c)</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#randomly-generate-weightbias-once-before-training","title":"Randomly generate weight/bias once before training","text":"<p>1: w, b \u223c N (\u20d70, Ic) w, b \u2208 Rc</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#compute-embeddings","title":"Compute embeddings","text":"<p>2: return cos(2\u03c0(\u02c6tw + b)) Algorithm 23 Diffusion Transformer def DiffusionTransformer({ai}, {si}, {zij }, {\u03b2ij }, Nblock, Nhead): 1: for all n \u2208 [1, . . . , Nblock] do 2: {bi} = AttentionPairBias({ai}, {si}, {zij }, {\u03b2ij }, Nhead) 3: ai \u2190 bi + ConditionedTransitionBlock(ai, si) 4: end for 5: return {ai} Algorithm 24 DiffusionAttention with pair bias and mask def AttentionPairBias({ai}, {si}, {zij }, {\u03b2ij }, Nhead) : ai \u2208 Rca , c = ca/Nhead</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#input-projections_3","title":"Input projections","text":"<p>1: if {si}\u0338 = \u2205 then 2: ai \u2190 AdaLN(ai, si) 3: else 4: ai \u2190 LayerNorm(ai) 5: end if 6: qh i = Linear(ai) qh i \u2208 Rc, h \u2208 {1, . . . , Nhead} 7: kh i , vh i = LinearNoBias(ai) kh i , vh i \u2208 Rc, h \u2208 {1, . . . , Nhead} 8: bh ij \u2190 LinearNoBias(LayerNorm(zij )) + \u03b2ij 9: gh i \u2190 sigmoid LinearNoBias(ai)\u0001 gh i \u2208 Rc</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#attention_2","title":"Attention","text":"<p>10: Ah ij \u2190 softmaxj \u0010 1\u221ac qh i \u22a4kh j + bh ij \u0011 11: ai \u2190 LinearNoBias \u0012 concath \u0010 gh i \u2299 P j Ah ij vh j \u0011\u0013 ai \u2208 Rca</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#output-projection-from-adaln-zero-27","title":"Output projection (from adaLN-Zero [27])","text":"<p>12: if {si}\u0338 = \u2205 then 13: ai \u2190 sigmoid(Linear(si, biasinit=-2.0)) \u2299 ai 14: end if 15: return {ai} Supplementary information for AlphaFold 3 23 Algorithm 25 Conditioned Transition Block</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#swiglu-transition-block-with-adaptive-layernorm","title":"SwiGLU transition block with adaptive layernorm","text":"<p>def ConditionedTransitionBlock(a, s, n = 2) : a \u2208 Rc 1: a \u2190 AdaLN(a, s) 2: b \u2190 swish(LinearNoBias(a)) \u2299 LinearNoBias(a) b \u2208 Rn\u00b7c</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#output-projection-from-adaln-zero-27_1","title":"Output projection (from adaLN-Zero [27])","text":"<p>3: a \u2190 sigmoid(Linear(s, biasinit=-2.0)) \u2299 LinearNoBias(b) a \u2208 Rc 4: return a Algorithm 26 Adaptive LayerNorm def AdaLN(a, s) : 1: a \u2190 LayerNorm(a, scale=False, offset=False) 2: s \u2190 LayerNorm(s, offset=False) 3: a \u2190 sigmoid(Linear(s)) \u2299 a + LinearNoBias(s) 4: return a 3.7.1 Diffusion Training Our diffusion training methodology largely follows [29]. With some notable differences, mostly to the loss. To improve training efficiency we train the Diffusion Module with a larger batch size than the trunk (see Main Article Fig. 2c). To realise this, we run the trunk once and then create 48 versions of the input structure by randomly rotating and translating according to Algorithm 19 and adding independent noise to each structure. We then train the Diffusion Module over all of them in parallel. This is efficient since the Diffusion Module is much cheaper than the model trunk. We apply a weighted aligned MSE loss to the denoised structure output from the Diffusion Module. We first perform a rigid alignment of the ground truth\u20d7 xGT l on to the denoised structure\u20d7 xl as {\u20d7 xGT-aligned l } = weighted_rigid_align({\u20d7 xGT l }, {\u20d7 xl)}, {wl}) (2) with weights wl provided in Equation 4. We then compute a weighted MSE LMSE = 1 3 mean l \u0010 wl||\u20d7xl \u2212\u20d7 xGT-aligned l ||2\u0011 , (3) with upweighting of nucleotide and ligand atoms as wl = 1 + f is_dna l \u03b1dna + f is_rna l \u03b1rna + f is_ligand l \u03b1ligand (4) and hyperparameters \u03b1dna = \u03b1rna = 5, and \u03b1ligand = 10. To ensure that the bonds for bonded ligands (including bonded glycans) have the correct length, we introduce an auxiliary loss during fine tuning as Lbond = mean (l,m)\u2208B \u0012\u20d7 xl \u2212\u20d7 xm \u2212\u20d7 xGT l \u2212\u20d7 xGT m \u00132 , (5) where B is the set of tuples (start atom index, end atom index) defining the bond between the bonded ligand and its parent chain. We also apply an auxiliary structure-based loss based on smooth LDDT, as described in Algorithm 27. The final loss from the Diffusion Module is then: Ldiffusion = (\u02c6t2 + \u03c32 data)/(\u02c6t + \u03c3data)2 \u00b7 (LMSE + \u03b1bond \u00b7 Lbond) + Lsmooth_lddt (6) Where \u02c6t is the sampled noise level, \u03c3data is a constant determined by the variance of the data (set to 16) and \u03b1bond is 0 for regular training, and 1 for both fine tuning stages. Prior to computing these losses, we apply an optimal ground truth chain assignment as described in subsection 4.2. Supplementary information for AlphaFold 3 24 During training the noise level is sampled from \u03c3data \u00b7 exp(\u22121.2 + 1.5 \u00b7 N (0, 1)), during inference the noise schedule is defined as \u02c6t = \u03c3data \u00b7 (s1/p max + t \u00b7 (s1/p min \u2212 s1/p max))p (7) where smax = 160, smin = 4 \u00b7 10\u22124, p = 7 and t is distributed uniformly between [0, 1] with a step size of 1 200 . Algorithm 27 Smooth LDDT Loss def SmoothLDDTLoss({\u20d7 xl}, {\u20d7 xGT l }, {f is_dna l }, {f is_rna l }) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#compute-distances-between-all-pairs-of-atoms","title":"Compute distances between all pairs of atoms","text":"<p>1: \u03b4xlm \u2190 ||\u20d7xl \u2212\u20d7 xm|| 2: \u03b4xGT lm \u2190 ||\u20d7xGT l \u2212\u20d7 xGT m ||</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#compute-distance-difference-for-all-pairs-of-atoms","title":"Compute distance difference for all pairs of atoms","text":"<p>3: \u03b4lm \u2190 abs(\u03b4xGT lm \u2212 \u03b4xlm) 4: \u03f5lm \u2190 1 4 h sigmoid( 1 2 \u2212 \u03b4lm) + sigmoid(1 \u2212 \u03b4lm)) + sigmoid(2 \u2212 \u03b4lm) + sigmoid(4 \u2212 \u03b4lm) i</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#restrict-to-bespoke-inclusion-radius","title":"Restrict to bespoke inclusion radius","text":"<p>5: f is_nucleotide l \u2190 f is_dna l + f is_rna l 6: clm \u2190 (\u03b4xGT lm &lt; 30 \u00c5)f is_nucleotide l + (\u03b4xGT lm &lt; 15 \u00c5)(1 \u2212 f is_nucleotide l )</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#compute-mean-avoiding-self-term","title":"Compute mean, avoiding self term","text":"<p>7: lddt = mean l\u0338 =m (clm\u03f5lm)/ mean l\u0338 =m (clm) 8: return 1 \u2212 lddt Supplementary information for AlphaFold 3 25 Algorithm 28 Weighted Rigid Align def weighted_rigid_align({\u20d7 xl}, {\u20d7 xGT l }, {wl}) :</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#mean-centre-positions","title":"Mean-centre positions","text":"<p>1:\u20d7 \u03bc \u2190 mean l (wl\u20d7 xl)/ mean l (wl) 2:\u20d7 \u03bc GT \u2190 mean l (wl\u20d7 xGT l )/ mean l (wl) 3:\u20d7 xl \u2190\u20d7 xl \u2212\u20d7\u03bc 4:\u20d7 xGT l \u2190\u20d7 xGT l \u2212\u20d7\u03bc GT</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#find-optimal-rotation-from-singular-value-decomposition","title":"Find optimal rotation from singular value decomposition","text":"<p>5: U, V \u2190 svd(P l wl\u20d7 xGT l \u2297\u20d7 xl) 6: R \u2190 U V</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#remove-reflection","title":"Remove reflection","text":"<p>7: if det(R) &lt; 0 then 8: F \u2190 \uf8eb \uf8ec \uf8ed 1 0 0 0 1 0 0 0 \u22121 \uf8f6 \uf8f7 \uf8f8 9: R \u2190 U F V 10: end if</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#apply-alignment","title":"Apply alignment","text":"<p>11:\u20d7 xalign l = R\u20d7xl +\u20d7\u03bc 12: return stop_gradient(\u20d7 xalign l ) 4 Auxiliary heads 4.1 Mini diffusion rollout Several of the heads require predicted coordinates, therefore at training time we do a short rollout of the Diffusion Module from pure noise with 20 steps (Main Article Fig. 2c). No gradients are applied to this mini-rollout. 4.2 Chain permutation and symmetry resolution The assignment of names (chain-ids, like \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, ...) to chains with identical sequences in a physical structure is arbitrary, and every permutation of these names is equally correct. The same applies to multiple ligands of the same type. It is very likely that AlphaFold will use a different permutation than the authors who deposited the ground truth in the PDB. To compare a predicted structure to the ground truth in the confidence head (subsubsection 4.3.1) or to create the noised version of the ground truth for diffusion training (subsection 3.7), the ground truth chains must be renamed such that they match the predicted structure. We use the method described in [18] section 7.3 \u201cMulti-Chain Permutation Alignment\u201d to find a good assignment of the predicted chains to the ground truth chains. The predicted structure used for this alignment is the output of the mini-diffusion rollout (subsection 4.2). The same method is applied to ligands as well, with the extension that ligands covalently bonded to polymer chains will be permuted in sync with the corresponding chains. This is achieved by grouping all covalently bonded components together and assigning them the same entity id. Finally, we resolve atom naming ambiguities within each chemical component (ligand/residue): we generate permu- tations (up to 1000) using RDKit and permute the ground truth such that it has minimal RMSD to the prediction. 4.3 Model confidence prediction The model is trained to predict three confidence metrics, a per-atom confidence, predicted local distance difference (pLDDT), a pairwise atom-atom aligned error (PAE), and a pairwise atom-atom distance confidence, predicted distance error (PDE). It is also trained to predict whether an atom is experimentally resolved. The confidence losses are only Supplementary information for AlphaFold 3 26 applied for the PDB training set (i.e. it is not applied for any of the distillation sets), with a further filter that the resolution of the ground truth structure is between 0.1 and 4. The details of the confidence head and losses are outlined below. 4.3.1 Predicted local distance difference test (pLDDT) The per atom confidence is trained to predict a version of LDDT that takes into account distances from all atoms to polymer residues (e.g. for a ligand atom the confidence only takes into account interactions between the ligand atom and proteins/nucleic acids, no intra-ligand terms are included), it is defined for atom l as: lddtl = X m\u2208R 1 4 X c\u2208{0.5,1,2,4} dlm &lt; c (8) Where dlm is the distance between atom l and atom m in the mini-rollout prediction (subsection 4.1), l encompasses all atoms and the set of atoms m \u2208 R is defined as: \u2022 Atoms such that the distance in the ground truth between atom l and atom m is less than 15 \u00c5 if m is a protein atom or less than 30 \u00c5 if m is a nucleic acid atom. \u2022 Only atoms in polymer chains. \u2022 One atom per token - C\u03b1 for standard protein residues and C1\u2032 for standard nucleic acid residues. The single embedding (si) in the confidence head (Algorithm 31) is linearly projected into [Nmax_atoms_per_token, 50] values, where Nmax_atoms_per_token is the maximum number of atoms per token. This gives per atom confidences with 50 bins between 0 and 1 and probabilities pb l obtained via a softmax. The loss is then defined as: Lplddt = \u2212 1 Natom X l 50X b=1 lddtb l log pb l (9) A single value of pLDDT per-atom is obtained by taking the expectation across the 50 binned probabilities. Here lddtb l denotes a binned version of lddtl, i.e. lddtb l is 1 if lddtl falls within bin b and 0 otherwise. 4.3.2 Predicted aligned error (PAE) It is useful to have a pairwise confidence between atoms, to determine confidence of interfaces or specific interactions between atoms. To this end we follow AlphaFold 2 and predict a pairwise alignment error (PAE), an estimate of the error of one token when aligned according to the frame of another. We associate a reference frame to each token i using a set of three atoms which we denote as (ai, bi, ci). Let \u03a6i = (\u20d7ai,\u20d7 bi,\u20d7 ci) denote the coordinates of these frame atoms. An atom position\u20d7 x is expressed in a basis defined by \u03a6i according to Algorithm 29, which takes\u20d7 bi as its origin and applies to\u20d7 x a rotation defined by \u03a6i. To compute an aligned error eij between frame i and token j, \u03a6i and \u03a6true i are built from the predicted and ground truth frame atom coordinates for token i, and representative token atom coordinates from the prediction\u20d7 xj and ground truth\u20d7 xtrue j are expressed in the bases of \u03a6i and \u03a6true i , respectively. The error eij is then defined as the Euclidean distance between these two alignments. See Algorithm 30 for pseudocode. The atoms (ai, bi, ci) used to construct token i\u2019s frame depend on the chain type of i: Protein tokens use their residue\u2019s backbone (N, C\u03b1, C), while DNA and RNA tokens use (C1\u2032, C3\u2032, C4\u2032) atoms of their residue. All other tokens (small molecules, glycans, ions) contain only one atom per token. The token atom is assigned to bi, the closest atom to the token atom is ai, and the second closest atom to the token atom is ci. If this set of three atoms is close to colinear (less than 25 degree deviation), or if three atoms do not exist in the chain (e.g. a sodium ion), then the frame is marked as invalid. The PAE head does not train on invalid frames, nor do ranking and confidence metrics consider them. When computing the alignment error we use the predicted coordinates taken from the mini-rollout structure. Supplementary information for AlphaFold 3 27 Algorithm 29 Express coordinates in frame def expressCoordinatesInFrame(\u20d7 x, \u03a6) :\u20d7 x \u2208 R3</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#extract-frame-atoms","title":"Extract frame atoms","text":"<p>1: (\u20d7a,\u20d7 b,\u20d7 c) = \u03a6\u20d7 a,\u20d7 b,\u20d7 c \u2208 R3 2:\u20d7 w1 = (\u20d7a \u2212\u20d7 b)/\u20d7 a \u2212\u20d7 b 3:\u20d7 w2 = (\u20d7c \u2212\u20d7 b)/\u20d7 c \u2212\u20d7 b</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#build-orthonormal-basis","title":"Build orthonormal basis","text":"<p>4:\u20d7 e1 = (\u20d7w1 +\u20d7 w2)/\u20d7 w1 +\u20d7 w2 5:\u20d7 e2 = (\u20d7w2 \u2212\u20d7 w1)/\u20d7 w2 \u2212\u20d7 w1 6:\u20d7 e3 =\u20d7 e1 \u00d7\u20d7 e2</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#project-onto-frame-basis","title":"Project onto frame basis","text":"<p>7:\u20d7 d =\u20d7 x \u2212\u20d7 b 8:\u20d7 xtransformed = concat(\u20d7d \u00b7\u20d7 e1,\u20d7 d \u00b7\u20d7 e2,\u20d7 d \u00b7\u20d7 e3) 9: return\u20d7 xtransformed\u20d7 xtransformed \u2208 R3 Algorithm 30 Compute alignment error def computeAlignmentError({\u20d7 xi}, {\u20d7 xtrue i }, {\u03a6i}, {\u03a6true i }, \u03f5 = 1e\u22128 \u00c52) : 1: \u02c6\u20d7xij = expressCoordinatesInFrame(\u20d7 xj , \u03a6i) 2: \u02c6\u20d7xtrue ij = expressCoordinatesInFrame(\u20d7 xtrue j , \u03a6true i ) 3: eij = q \u2225\u02c6\u20d7xij \u2212 \u02c6\u20d7xtrue ij \u22252 + \u03f5 4: return {eij } To predict the alignment error, the pair embedding (zij ) in the confidence head (Algorithm 31) is linearly projected into 64 distance bins b, and probabilities pb ij are obtained via a softmax. There are 64 bins equally spaced from 0 \u00c5 to 32 \u00c5 in 0.5 \u00c5 increments. During training the final bin also captures larger errors. The prediction targets are the alignment errors eij . The loss is then defined as: Lpae = \u2212 1 N 2 token X i,j 64X b=1 eb ij log pb ij (10) Here eb ij denotes a binned version of eij , i.e. eb ij is 1 if eij falls within bin b and 0 otherwise. A single value of PAE per token-pair is obtained by taking the expectation across the 64 binned probabilities: PAEij = 64X b=1 \u2206b pb ij (11) where \u2206b are the distance bin centers. We also use the probabilities pb ij to compute pTM, an estimate of the TM-Score [30] as well as an ipTM, an interfacial variant that only considers (i, j) pairs from different chains. For full description of pTM and ipTM we refer the reader to Sec. 1.9.7 of [1] and Sec. 7.9 of [18], respectively. As previously, we find pTM and ipTM to be useful as confidence metrics (main text Figure 4) and for sample ranking (subsection 5.9). 4.3.3 Predicted distance error (PDE) In addition to an alignment error, the model is also trained to predict the error in absolute distances between atoms. The pair embedding (zij ) in the confidence head (Algorithm 31) is linearly projected into 64 distance bins b, and probabilities pb ij are obtained via a softmax. There are 64 bins equally spaced from 0 \u00c5 to 32 \u00c5 in 0.5 \u00c5 increments. Supplementary information for AlphaFold 3 28 During training the final bin also captures larger errors. The prediction targets eij are defined as eij = |dpred ij \u2212 dgt ij |, where dpred ij is the distance between representative token atoms i and j in the mini-rollout prediction (subsection 4.1), and dgt ij is the corresponding distance in the ground truth. The loss is then defined as: Lpde = \u2212 1 N 2 token X i,j 64X b=1 eb ij log pb ij (12) Here eb ij denotes a binned version of eij , i.e. eb ij is 1 if eij falls within bin b and 0 otherwise. A single value of PDE per token-pair is obtained by taking the expectation across the 64 binned probabilities. PDEij = 64X b=1 \u2206b pb ij (13) where \u2206b are the distance bin centers. 4.3.4 Experimentally resolved prediction The single embedding (si) in the confidence head is linearly projected into per-atom values (in the same way as subsub- section 4.3.1) with 2 bins. Then a softmax is used to predict whether the atom is resolved in the ground truth (yl), and loss defined as: Lresolved = \u2212 1 Natom X l 2X b=1 yb l log pb l (14) 4.3.5 Confidence head architecture The pLDDT, PAE, PDE and experimentally resolved outputs are projected from the same head, the architecture of this head is outlined in Algorithm 31. The pairwise distance between the representative atoms for each token are extracted, and combined with the single ({si}) and pair ({zij }) embeddings from the network trunk. lrep(i) provides the flat-atom index l of the representative atom for the given token i. During training, a stop gradient is applied to the single and pair embeddings and predicted structure. Algorithm 31 Confidence head def ConfidenceHead({sinputs i }, {si}, {zij }, {\u20d7xpred l }, Nblock = 4) : 1: zij += LinearNoBias(sinputs i ) + LinearNoBias(sinputs j )</p>"},{"location":"reference/advanced_methods/af3/AF3_paper/#embed-pair-distances-of-representative-atoms","title":"Embed pair distances of representative atoms:","text":"<p>2: dij =\u20d7 xpred lrep(i) \u2212\u20d7 xpred lrep(j) 3: zij += LinearNoBias(one_hot(dij , vbins = [33\u20448 \u00c5, 51\u20448 \u00c5, . . . , 213\u20448 \u00c5])) 4: {si}, {zij } += PairformerStack({si}, {zij }, Nblock) 5: ppae ij = softmax(LinearNoBias(zij )) ppae ij \u2208 Rbpae 6: ppde ij = softmax(LinearNoBias(zij + zji)) ppde ij \u2208 Rbpde 7: pplddt l = softmax(LinearNoBiastoken_atom_idx(l)(si(l))) pplddt l \u2208 Rbplddt 8: presolved l = softmax(LinearNoBiastoken_atom_idx(l)(si(l))) presolved l \u2208 R2 9: return {pplddt l }, {ppae ij }, {ppde ij }, {presolved l } 4.4 Distogram prediction The model is also trained to predict binned distances between all pairs of tokens (a distogram). This head and loss are identical to AlphaFold 2 [1], where the pairwise token distances use the representative atom for each token: C\u03b2 for protein residues (C\u03b1 for glycine), C4 for purines and C2 for pyrimidines. All ligands already have a single atom per token. Supplementary information for AlphaFold 3 29 5 Training and inference 5.1 Structure filters For both training and inference, standard crystallization agents (Table 9) are removed from the structure. Bonds for structures with homomeric subcomplexes lacking the corresponding homomeric symmetry are also removed \u2013 e.g. if a certain bonded ligand only exists for some of the symmetric copies, but not for all, we remove the corresponding bond information from the input. In consequence the model has to learn to infer these bonds by itself. 5.2 Training stages Training occurs in four stages with differences highlighted in Table 6. We train a randomly initialized model first with a sequence crop size of 384, and then fine tune from the stage one weights at a crop size of 640, followed by a second fine tuning stage with crop size 768. In the first stage of fine tuning, smooth lddt loss was turned off and the weight of the disordered protein PDB distillation set was reduced, but now with unmasked diffusion loss on non-protein chains in that distillation set. For the second stage of fine tuning, in addition, transcription factor distillation sets were turned on and the weight of the disordered protein PDB distillation set was increased back to its original level. Protein residues from the DNA motif distillation set with predicted probability of being experimentally resolved lower than 0.9 were treated as unresolved residues. In the third, final fine-tuning stage, we trained the PAE head while removing all structure-based losses (diffusion and distogram) from training and increased the maximum number of chains to 50. All other settings were kept as in the previous fine-tuning stage. The model trained for PoseBusters was trained similarly to the primary model, but without PDB structures released after 2021-09-30, without RNA MSA or RNA distillation sets, and without the predicted experimentally resolved filter on the DNA motif distillation set during fine tuning. Table 6 Training stages Initial training Fine tuning 1 Fine tuning 2 Fine tuning 3 Sequence crop size Ntoken 384 640 768 768 Parameters initialized from Random Initial training Fine tuning 1 Fine tuning 2 Sampling weight for disorder PDB distillation 0.02 0.01 0.02 0.02 Train on transcription factor distillation sets False False True True Masked diffusion loss for non-protein in disorder PDB distillation True False False False Train structure and distogram True True True False Train PAE head False False False True Diffusion batch size 48 32 32 32 Training samples (\u00b7106) \u224820 \u22481.5 \u22481.5 \u22481.8 Training times (days on 256 A100s) \u224810 \u22483 \u22485 \u22482 Polymer-ligand bond loss weight 0 1 1 1 Max number of chains 20 20 20 50 5.3 Loss The details of the losses from each module are given in the relevant sections, these are then combined into the final loss: Lloss = \u03b1confidence \u00b7 (Lplddt + Lpde + Lresolved + \u03b1pae \u00b7 Lpae) + \u03b1diffusion \u00b7 Ldiffusion + \u03b1distogram \u00b7 Ldistogram (15) Where \u03b1confidence = 10\u22124, \u03b1diffusion = 4, \u03b1distogram = 3 \u00b7 10\u22122 and \u03b1pae = 0 for all except for the final training stage, where it is set to 1. 5.4 Optimization For training we use the Adam [31] optimizer with parameters \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128. The base learning rate is 1.8 \u00b7 10\u22123, which is linearly increased from 0 over the first 1,000 steps. The learning rate is then decreased by a factor of 0.95 every 5 \u00b7 104 steps. The model is trained with a batch size of 256, and we apply gradient clipping if the global norm is greater than 10. Supplementary information for AlphaFold 3 30 5.5 Dropout Dropout is applied during training in the MsaModule (Algorithm 8), in the Pairformer stack (Algorithm 17) and in the template embedder (Algorithm 16). See the respective algorithms for the dropout rates. 5.6 Inference Setup For inference, we use an exponential moving average of the trained parameters [32] with the decay rate of 0.999. Structures are inferenced without any cropping. 5.7 Model selection For model selection during training stages, performance over a validation set was tracked during training, with all structures in the validation set released after 2021-09-30 and before 2023-01-13 (the maximum release date of training date structures was 2021-09-30). Details of the construction of the validation set are given in subsection 5.8. A single model selection metric was computed from single chain, interface and full complex scores as follows: 1. Scores were computed for all five samples across all low homology chains and interfaces in the validation set structures, as well as certain full complex metrics. 2. Scores were aggregated across samples via arithmetic mean of top1 (top-ranked) and top5 (best-of-5) predictions, according to global PDE ranking: gPDE = P ij pij PDEij P ij pij , (16) where the weight pij is the probability of contact of token pair i, j under the distogram (i.e. the sum of distogram probabilities corresponding to distogram bins under 8 \u00c5). 3. A final scalar was computed via a weighted mean across score types (see Table 7 for weights and description of metrics below), averaged over all targets. For each interface (chain pair) type and single chain type, LDDTs (local distance difference test) [33] were aggre- gated. For each pair of low homology chains, interface LDDT was calculated from distances between atoms across different chains in the interface. Intra-chain LDDTs were calculated from distances within a single low homology chain. Nucleic acid LDDTs (intra-chains and interface) were calculated with an inclusion radius of 30 \u00c5 compared to the usual 15 \u00c5 used for proteins, due to the larger size of nucleotides compared to amino acids. An additional metric, unresolved protein RASA (relative solvent accessible surface area) [34] was calculated for unresolved protein residues in our evaluation set. Table 7 Weighting of accuracy metrics for model selection Metric Initial training Fine tuning Protein-protein interface LDDT 20 20 DNA-protein interface LDDT 10 10 Protein-RNA interface LDDT 10 2 DNA-ligand interface LDDT 5 5 Ligand-protein interface LDDT 10 10 Ligand-RNA interface LDDT 5 2 Protein intra-chain LDDT 20 20 DNA intra-chain LDDT 4 4 RNA intra-chain LDDT 16 16 Ligand intra-chain LDDT 20 20 Modified residue intra-chain LDDT 10 0 Unresolved protein RASA 10 10 Supplementary information for AlphaFold 3 31 5.8 Validation set The validation set for model selection during training was composed of a all low homology chains and interfaces from a subset of all PDB targets released after 2021-09-30 and before 2023-01-13, with maximum length 2048 tokens. The process for selecting these targets was broken up into two separate stages. The first was for selecting multimers, the second for selecting monomers. Multimer selection proceeded as follows: 1. Take all targets released after 2021-09-30 and before 2023-01-13 and remove targets with total number of tokens greater than 2560, more than one thousand chains or resolution greater than 4.5, then generate a list of all interface chain pairs for all remaining targets. 2. Filter to only low homology interfaces, which are defined as those where no target in the training set contains two chains with high homology to the chains involved in the interface, where high homology here means &gt; 40% sequence identity for polymers or &gt; 0.85 tanimoto similarity for ligands. Additionally filter out interfaces involving a ligand with ranking model fit less than 0.5 or with multiple residues. 3. Assign interfaces to clusters as per subsubsection 2.5.3, other than for polymer-ligand interfaces which use cluster ID (polymer_cluster, CCD-code) and sample one interface per cluster. 4. Take the following interface types only, possibly reducing number of clusters by sampling a subset of clusters (number of samples given in brackets if reduced): protein-protein (600), protein-DNA (100), DNA-DNA (100), Protein-ligand (600), DNA-ligand (50), ligand-ligand (200), protein-RNA, RNA-RNA, DNA-RNA, RNA-ligand. 5. Take the set of all PDB targets containing the remaining interfaces with a final additional restriction of max total tokens 2048 and make the set of scored chains and interfaces equal to all low homology chains and interfaces in those targets. 6. Manually exclude a small set of targets (11 in our case) where alignment for scoring took too long to be practical for generating validation scores during experiments. Monomer selection proceeded similarly: 1. Take all polymer monomer targets released after 2021-09-30 and before 2023-01-13 (can include monomer poly- mers with ligand chains) and remove targets with total number of tokens greater than 2560 or resolution greater than 4.5 2. Filter to only low homology polymers. 3. Assign polymers to clusters as per subsubsection 2.5.3. 4. Sample 40 protein monomers and take all DNA and RNA monomers. 5. Add a final additional restriction of max total tokens 2048 and make the set of scored chains and interfaces equal to all low homology chains and interfaces in the remaining targets. 6. Manually exclude a set of RNA monomers (8 in our case) that all come from one over represented cluster. The end result was 1,220 PDB targets containing 2,333 low homology interfaces and 2,099 low homology chains. 5.9 Confidence measures and sample ranking 5.9.1 Alignment-based confidence measures To capture confidence from an alignment perspective we employ pTM and ipTM, PAE-based predictors that have been developed previously [1, 18]. The pTM is a predictor of TM-Score [30], an alignment-based measure of structural accuracy (see Sec. 1.9.7 of [1] for original description). For a given set of tokens D, the pTM is computed as pTM(D) = max i\u2208D has_frame(i) 1 |D| X j\u2208D NbinsX b=1 pb ij \uf8eb \uf8ec \uf8ec \uf8ed 1 1 + \u0010 \u2206b d0(|D|) \u00112 \uf8f6 \uf8f7 \uf8f7 \uf8f8 . (17) Here pb ij is the PAE probability of error bin b for token j aligned to frame i, \u2206b are the associated error bin centers, and Nbins is the number of bins (see subsubsection 4.3.2 for PAE description). The has_frame notation indicates that invalid Supplementary information for AlphaFold 3 32 frames are not considered when computing pTM (see subsubsection 4.3.2 for description of invalid frames). Finally, the factor d0(N ) = 1.24 3 pmaximum(N, 19) \u2212 15 \u2212 1.8 is the TM-score normalization constant. With the above equation we can compute multiple confidence measures: for a whole-structure confidence we compute a whole complex pTM, where D includes all tokens. For single chain confidence we compute a \"chain pTM\", where D is restricted to tokens in the chain of interest. The ipTM (originally described in [18]) is an interface variant of pTM that only considers interactions between different chains, ipTM(D) = max i\u2208D has_frame(i) 1 |D-chain(i)| X j\u2208D-chain(i) NbinsX b=1 pb ij \uf8eb \uf8ec \uf8ec \uf8ed 1 1 + \u0010 \u2206b d0(|D|) \u00112 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , (18) where D-chain(i) is the subset of D excluding all tokens in the chain of token i. We compute a full structure ipTM by evaluating Equation 18 on a set D containing all tokens. We also compute a \"chain pair ipTM\" confidence, where D is restricted to the set of tokens within a pair of chains defining the interface. 5.9.2 Clashes We sometimes employ a clash penalty in ranking (see subsubsection 5.9.3), whereby structures with heavily clashing polymer chains are down-weighted. The number of clashes between two chains is defined as clashes(A, B) = X i\u2208A X j\u2208B dij &lt; 1.1 \u00c5 where dij is the distance between atom i and atom j. A structure is marked as having a clash (has_clash) if for any two polymer chains A, B in the prediction clashes(A, B) &gt; 100 or clashes(A, B)/min(NA, NB ) &gt; 0.5 where NA is the number of atoms in chain A. See subsubsection 5.9.3 for how this is used when ranking predictions. 5.9.3 Sample ranking Sample ranking of the recent PDB evaluation set uses a combination of pTM, ipTM, and pLDDT-based summaries. The ranking depends on whether the metric is single-chain (e.g. protein intra) or an interface (e.g. DNA-protein), as well as on the chain types being scored: 1. Full complex ranking is similar to [18] according to a weighted average of the full-complex pTM and ipTM. We also include terms that penalize predictions with lots of clashes, and very slightly up weight predictions with more disorder (as defined by the relative solvent accessible surface area) to further reduce issues with hallucinations. (subsubsection 5.9.1) 0.8 \u00b7 ipTM + 0.2 \u00b7 pTM + 0.5 \u00b7 disorder \u2212 100 \u00b7 has_clash. (19) Where disorder is defined as 1 NP P i\u2208P (rasai &gt; 0.581), P being the set of all protein atoms. The rasa metric, and associated cutoff 0.581 were chosen based on previous work on predicting disorder with AlphaFold [34]. 2. Ranking of single-chain metrics (e.g. protein intra) is according to the chain pTM (see subsubsection 5.9.1), where the token subset is restricted to members of the chain. 3. Interface metrics (e.g. protein-protein) are ranked according to a bespoke ipTM aggregate representing the two chains in the interface. We first compute a [Nchains, Nchains] matrix M of chain pair ipTM values (see subsubsec- tion 5.9.1) for all chain pairs in the prediction. For each chain c \u2208 [A, B] in our interface we then compute its average interaction with all other chains: R(c) = mean ij (Mij ) restricted to i = c or j = c, i\u0338 = j, and chain i having at least one valid frame. Finally, we rank according to 1 2 R(A) + R(B) (20) Interfaces containing a small molecule, ion, or bonded ligand chain are ranked differently. Denoting this chain as C\u2217, the interface is ranked according to R(C\u2217) only. 4. Modified residue scores are ranked according to the average pLDDT of the modified residue. Sample ranking during the model selection phase was performed differently from above. See subsection 5.7 for details. Supplementary information for AlphaFold 3 33 5.10 Inference Time Example inference times using 10 trunk recycles on 16 NVIDIA A100 GPUs are shown in Table 8. This includes only GPU wallclock time, and MSA construction, data processing, compilation, and post-processing is not included. Table 8 Inference time in seconds by complex size, using 16 A100 Number of tokens Inference time (seconds) 1024 22 2048 71 3072 126 4096 228 5120 347 6 Evaluation 6.1 Recent PDB evaluation set The recent PDB evaluation set construction started by taking all 10,192 PDB entries released between 2022-05-01 and 2023-01-12, a date range falling after any data in our training set which had a maximum release date of 2021-09-30. Each entry in the date range was expanded from the asymmetric unit to Biological Assembly 1, then two filters were applied: \u2022 Filtering to non-NMR entries with resolution better than 4.5 \u00c5, leaving 9,636 complexes. \u2022 Filtering to complexes with less than 5,120 tokens under our tokenization scheme (see subsection 2.6), leaving 8,856 complexes. For each complex we generated a list of all individual entities and a list of all entity pairs where the minimal distance between heavy atoms in the two entities was less than 5 \u00c5 and at least one entity in the pair was a polymer. The procedures for determining homology and clusters for these entities and interfaces are described in later sections. Predictions on the recent PDB set were made on the full post-assembly complex, but crystallization aids (Table 9) were removed from the complex for prediction and scoring, along with all bonds for structures with homomeric sub- complexes lacking the corresponding homomeric symmetry. Not every entity and interface was included: \u2022 Peptide-peptide interfaces, peptide monomers and modified residues within peptides (where a peptide here is defined as a protein with less than 16 residues) were not included in scoring as their homology to the training set was not determined. \u2022 The system can predict other entities like DNA/RNA hybrids, Peptide Nucleic Acids (PNA) and (D) polypeptides, but these entities and interfaces involving them were not scored as they are too rare to get meaningful results on. \u2022 Eight structures were removed from the test set as matching predicted chains to ground truth chains took too long, due to large numbers of individual entities in the structure. \u2022 Four structures were removed from the set for technical reasons - three where all chains had fewer than 4 residues and one with bad metadata in the source file. \u2022 Eleven ligand-protein or ion-protein interfaces failed to score due to RMSD calculation errors. In addition to the full evaluation set described in the section above we create a \u201clow homology\u201d subset that is filtered on homology to this training set. Evaluation is done either on individual chains, or on specific interfaces extracted from the full complex prediction. For intra-chain metrics, we keep polymers that have less than 40% sequence identity to the training set. Here we define sequence identity as the percent of residues in the evaluation set chain that are identical to the training set chain. For interface metrics the following filters are applied: Supplementary information for AlphaFold 3 34 \u2022 Polymer-polymer interfaces: If both polymers have greater than 40% sequence identity to two chains in the same complex in the training set, then this interface is filtered out. \u2022 Peptide-polymer: For interfaces to a peptide (&lt;16 residues), the similarity of the non-peptide entity has to be novel (less than 40% sequence identity to anything in the training set). 6.2 Evaluation set clustering The evaluation data was clustered to allow for redundancy reduction. Individual polymer chains were clustered at a 40% sequence similarity clustering for proteins with more than 9 residues and 100% similarity for nucleic acids and protein with less than or equal to 9 residues. Ligands, ions, and metal entities were clustered according to CCD identity (only used for Main Article Fig. 4). When assigning a cluster ID to an interface: \u2022 Polymer-polymer interfaces are given a cluster ID of (polymer1_cluster, polymer2_cluster). \u2022 Polymer-ligand interfaces are given a cluster ID of the polymer_cluster only. \u2022 Polymer-modified_residue interfaces are given a cluster ID (polymer_cluster, CCD-code). 6.3 Evaluation metrics The evaluation procedure compares a predicted structure to the corresponding ground truth structure. If the complex contains multiple identical entities, the optimal assignment (maximising LDDT) of the predicted units to the ground truth units is found by either an exhaustive search over all permutations (for groups up to 8 members) or a simulated an- nealing optimization (for larger groups). After the chain assignment is found, the assignment in local symmetry groups of atoms in ligands is solved by exhaustive search over the first 1000 per-residue symmetries as given by RDKit [25]. We measure the quality of the predictions with DockQ, LDDT (local distance difference test) [33] or pocket-aligned RMSD (root mean square deviation). For nucleic-protein interfaces we measure interface accuracy via interface LDDT (iLDDT), which is calculated from distances between atoms across different chains in the interface. Nucleic acid LDDTs (intra-chains and interface) were calculated with an inclusion radius of 30 \u00c5 compared to the usual 15 \u00c5 used for proteins, owing to their larger scale. If not stated differently, the pocket-aligned RMSD is computed as follows: the pocket is defined as all heavy atoms within 10 \u00c5 of any heavy atom of the ligand in the ground truth structure. In ligand-protein interfaces the C\u03b1 atoms within the pocket are used to align the predicted structure to the ground truth structure by least squares rigid alignment, and then RMSD is computed on all heavy atoms of the ligand. In ligand-nucleic chain interfaces all heavy atoms are used for the alignment due to the larger spacing of the backbone atoms and the more rigid structure inside. We used three categories of evaluation metrics: 1. Complex level metrics (e.g. full complex LDDT score). For these we used the prediction with the highest whole complex confidence over the 5 model seeds. 2. Individual entity metrics (e.g. intra-chain LDDT). For these we used the prediction with the highest entity confi- dence for the entity being evaluated. 3. Interface metrics (e.g. pocket-aligned RMSD for ligand-protein interfaces). For these we used the prediction with the highest interface confidence for the interface being evaluated. 6.4 Aggregation of scores To avoid the overrepresentation of similar polymer chains or interfaces that were deposited under different PDB codes, we cluster those together (see subsection 6.2) and aggregate the individual scores such that each cluster gets the same weight. For mean values this is achieved by first computing the mean value per-cluster and then averaging over clusters. For median values and other percentiles we weight each sample by 1/(Nci ), where Nci is the size of the cluster the sample belongs to, and compute a weighted percentile. 6.5 Baselines As part of our ligand accuracy evaluation, we ran docking onto AlphaFold-Multimer v2.3 structures with AutoDock Vina 1.1 [35, 36], using Gypsum-DL [37] and Propka [38] for ligand and protein preparation. The pocket used for docking onto the AlphaFold structures was determined after a rigid alignment with the ground truth structures. We Supplementary information for AlphaFold 3 35 verified the performance of our docking protocol by recovering the published accuracy on ground truth structures from the PoseBusters set [39]. All other reported baseline numbers in the ligand section are from published literature. For benchmarking performance on nucleic acid structure prediction, we report baseline comparisons to an existing machine learning system for protein-nucleic acid and RNA tertiary structure prediction, RoseTTAFold2NA [40]. We run the open source RF2NA [41] with the same multiple sequence alignments (MSAs) as were used for AlphaFold 3 predictions. For comparison between AlphaFold 3 and RF2NA, a subset of our recent PDB evaluation set targets are chosen to meet the RF2NA criteria (&lt;1000 total residues and nucleotides). Also as RF2NA was not trained to predict systems with DNA and RNA, this analysis was limited to targets with only one nucleic acid type. Note, that AlphaFold 3 is capable of predicting systems with any combination of protein, DNA, RNA, and ligands. As an additional baseline for RNA tertiary structure prediction, we evaluate AlphaFold 3 performance on CASP15 RNA targets that are currently publicly available (R1116/8S95, R1117/8FZA, R11263, R1128/8BTZ, R1136/7ZJ4, R1138/[7PTK/7PTL], R1189/7YR7, and R1190/7YR6). We compare top-1 ranked predictions, and where multiple ground truth structures exist (R1136) the prediction is scored against the closest state. We display comparisons to RF2NA as a representative machine learning system and AIchemy_RNA2 as the top performing entrant. Both the RF2NA (CASP15 entry BAKER) and AIchemy_RNA2 predictions were downloaded from the CASP website. Independent work on RoseTTAFold All-Atom [42] was concurrently released that performs structure prediction across a wide range of biomolecular systems. This system is not available for baselining at the time of writing, but the RoseTTAFold All-Atom paper indicates their accuracy is below specialist predictors in almost all categories. The RoseTTAFold All-Atom PoseBusters benchmark score was included since it uses a comparable methodology. 7 Differences to AlphaFold 2 and AlphaFold-Multimer AlphaFold 3 differs in a number of ways from the previous versions of AlphaFold. A non-exhaustive list of key differ- ences is discussed below. Centrally, AlphaFold 3 is capable of modeling general molecules; in order to achieve this, we changed the inputs to allow specifying e.g. ligands and modified residues and the way coordinates are represented in the model. In AlphaFold 2, the protein structure was internally represented by associating a rigid body frame to each amino acid (relating the C\u03b1 atoms) and the side chains were parameterized by \u03c7-angles. These representations together were used to deduce the coordinates of all the atoms in the amino acid. That approach was handcrafted for proteins and does not generalize naturally to arbitrary molecules \u2013 therefore, in AlphaFold 3 we instead model the system as a collection of atoms, which each have independent global coordinates without any rigid constraints between them. The model learns to respect the chemical structure of the molecules as a result of training, not because the output space is parameterized to enforce it. In order to allow for efficient computation, we group atoms into tokens. For standard nucleic acids and standard amino acids, tokens correspond to entire residues or nucleotides; for others, each token corresponds to a single heavy atom. Further, in contrast to AlphaFold 2 and much other recent work, the structure prediction part of the model is not equivariant under spatial transformations. In fact, we employ very limited spatial inductive bias, simply embedding the positions with a single matrix multiplication. To predict the structure, we employ a generative diffusion-based head, that proceeds by simply adding Gaussian noise to the atom coordinates and training the head to remove the noise, driven by a simple MSE loss. Other than a bond loss to enforce the locations of bonded ligands and glycans (see Equation 5), no violation or clash losses are applied. The relax postprocessing step is rarely needed when using AlphaFold 3. While we no longer use rigid frames for the core prediction task, we do use frames for the PAE confidence head (subsubsection 4.3.2). This frame construction is largely the same as AlphaFold 2, with exceptions that now we have frames for non-protein entities as well and that the frames are constructed via a different combination of the atom position vectors (compare Algorithm 28 to Algorithm 21 of [1]). Furthermore, we simplified the core pieces of the architecture. For the main trunk of the model, we moved to a new simplified architecture called \u201cPairformer\u201d and we removed the MSA representation from the core part of the model and instead replace it with a single representation. As a direct consequence, there is also no column-wise attention. In Pairformer, we removed the outer product mean that the Evoformer had, meaning there is no information flow from the single to the pair representation. We also simplified the Evoformer stack. The Evoformer stack from AlphaFold 2 operated over both the MSA and residue pairs, and is now replaced with a Pairformer stack that operates over just the token pairs. 3Downloaded from https://predictioncenter.org/casp15/TARGETS_PDB/R1126.pdb Supplementary information for AlphaFold 3 36 In AlphaFold-Multimer, unpaired MSA sequences across chains were presented to the network in a block-diagonal fashion. In AlphaFold 3 these \u201cunpaired\u201d sequences are presented in a dense fashion - so that the network sees many more MSA sequences per-chain. Lastly, we replaced the ReLU activation in the model\u2019s transition blocks function by SwiGLU, which has been observed to yield better results [28]. ReLU is present in the atom attention (Algorithm 5). When it comes to data, we introduced new distillation sets, and the data gets clustered and re-balanced at the interface level, as opposed to just clustered at the individual chain level. For the protein monomer distillation set, we removed the restrictions on the pLDDT being above 80 and the length of the protein being above 200 residues. 8 Supplemental Results 8.1 Selected examples In Main Article Fig. 3 we highlight examples of predicted macromolecular complexes that are now possible with AF3, spanning a range of targets and molecule types, varying in size, complexity, and function. \u2022 Protein-RNA complex: Human 40S ribosomal subunit is a large complex with &gt;7,000 residues, critical to the initiation of human protein synthesis. eIF1A and eIF5B proteins remodel the complex to orient the Met-tRNAiMet into a conformation compatible with ribosomal subunit joining. AF3 is able to predict the full \u22487,600 residue complex (Main Article Fig. 3a). The eIF1A protein interaction with the ribosomal subunit, as well as the eIF5B intra-residue interactions, are modelled correctly. The complex is composed of a set of 43 chains, each of which has high sequence identity to the training set, but represents a novel combination of them as a whole (PDB ID = 7TQL). \u2022 PTM: Proteins harboring heparan sulfate (HS) chains play crucial roles in tissue homeostasis and signal trans- duction. Exostosin-like 3 (EXTL3) is a glycosyltransferase forming a homocomplex responsible for HS chain synthesis initiation. EXTL3 has 9.4% sequence identity to the training set. AF3 correctly predicts (Main Article Fig. 3b) the protein homocomplex and two N-glycans on each of the protomers (bonded glycan accuracy: 0.99 \u00c5 mean pocked-aligned RMSD, where the pocket is defined only using residues from the bonded protein chain, PDB ID = 7AU2). \u2022 Antibody-antigen: Mesothelin (MSLN) is a cell-surface protein that is a popular target for antibody-based ther- apies. A new antibody binding to a section of the juxtamembrane region inhibits MSLN shedding and results in more active CAR-T therapies. No similar sequences could be found for the antigen (residues 584-598 of MSLN), the antibodies have 88% (Fab heavy chain) and 97% (Fab light chain) sequence identity to the training set. AF3 predicts (Main Article Fig. 3c) the antibody-antigen interaction to high accuracy (0.82 DockQ, PDB ID = 7U8C). \u2022 Integral membrane protein: Wnt signalling is essential for adult tissue homeostasis, and requires palmitoleoyla- tion by PORCN, an endoplasmic reticulum-resident membrane-bound O-acyltransferase. AF3 correctly predicts (Main Article Fig. 3d) the PORCN complex with LGK974 and the WNT3A peptide, providing a structural rationale for the mechanism of action of the clinical stage molecule (PDB ID = 7URD). \u2022 Unique fold: Natural products containing an aziridine ring exhibit antitumor activities. The AziU3/U2 protein complex catalyses the formation of aziridine rings at a novel catalytic site formed by a complex with a novel fold. AF3 correctly predicts (Main Article Fig. 3e) this novel protein complex bound to the substrate (PDB ID = 7WUX). \u2022 Allosteric site: PI5P4K\u03b3 is a lipid kinase expressed to regulate cellular levels of the PI5P substrate and the PI(4,5)P2 product, implicated in cancer and immunological disorders. AF3 correctly predicts (Main Article Fig. 3f) the novel allosteric binding mode of a novel inhibitor (PDB ID = 7QIE). In Extended Data Fig. 6 we considered ligand docking cases studies chosen from the PoseBusters set. Here we com- pare AF3 ligand pose predictions to the best Vina or Gold docking poses reported in the PoseBusters paper [39], using the pocket-aligned ligand heavy atom RMSD (ligand RMSD). The three examples selected have AF3 ligand RMSD &lt; 2 \u00c5, and the best docking ligand RMSD &gt; 3 \u00c5. We measure ligand Tanimoto similarity using RDKit v.2023_03_3 Morgan fingerprints (radius 2,2048 bits) \u2022 Notum is a serine hydrolase enzyme that catalyses the delipidation of Wnt proteins, and a well-established drug target. While Notum has 99% sequence identity to the training set, the ligand (ARUK3004556) is novel, originally Supplementary information for AlphaFold 3 37 discovered by a virtual screen, not sharing the Murcko scaffold with any Notum ligand in the PDB, and maximum Tanimoto similarity to any ligand in the training set less than 0.4. AF3 predicts the docked ligand (Extended Data Fig. 6a) substantially better than both Gold and Vina docking tools as presented in the PoseBusters benchmark (AF3 ligand RMSD = 1.0 \u00c5, best docking ligand RMSD = 5.2 \u00c5, PDB ID = 8BTI). \u2022 An ethanolamine derivative, HEHEAA, is a potent effector of PipR-mediated gene regulation in plant-associated bacteria. The first step in the response to HEHEAA is the binding to the AapF protein. It is a relatively novel protein (28% sequence identity to the training set), while the maximum Tanimoto similarity of HEHEAA to any ligand in the training set is 0.43. AF3 predicts the docked ligand (Extended Data Fig. 6b) substantially better than the best docking algorithm (AF3 ligand RMSD = 1.4 \u00c5, best docking ligand RMSD = 5.4 \u00c5, PDB ID = 7KZ9). \u2022 Galectin-3 is a protein that binds \u03b2-galactosides through carbohydrate-recognition domains, and a well-known human drug target. This protein has 91% sequence identity to the training set, and while the ligand (compound 22) shares the core with a known scaffold, the phenyl-1,2,4 triazole side chain is novel. AF3 predicts a more accurate ligand pose (Extended Data Fig. 6c) than the best docking algorithm (AF3 ligand RMSD = 0.30 \u00c5, Best docking ligand RMSD = 7.2 \u00c5, PDB ID = 7XFA). 9 Appendix: CCD code and PDB ID tables Table 9 Crystallization aids SO4, GOL, EDO, PO4, ACT, PEG, DMS, TRS, PGE, PG4, FMT, EPE, MPD, MES, CD, IOD Table 10 Ligand exclusion list 144, 15P, 1PE, 2F2, 2JC, 3HR, 3SY, 7N5, 7PE, 9JE, AAE, ABA, ACE, ACN, ACT, ACY, AZI, BAM, BCN, BCT, BDN, BEN, BME, BO3, BTB, BTC, BU1, C8E, CAD, CAQ, CBM, CCN, CIT, CL, CLR, CM, CMO, CO3, CPT, CXS, D10, DEP, DIO, DMS, DN, DOD, DOX, EDO, EEE, EGL, EOH, EOX, EPE, ETF, FCY, FJO, FLC, FMT, FW5, GOL, GSH, GTT, GYF, HED, IHP, IHS, IMD, IOD, IPA, IPH, LDA, MB3, MEG, MES, MLA, MLI, MOH, MPD, MRD, MSE, MYR, N, NA, NH2, NH4, NHE, NO3, O4B, OHE, OLA, OLC, OMB, OME, OXA, P6G, PE3, PE4, PEG, PEO, PEP, PG0, PG4, PGE, PGR, PLM, PO4, POL, POP, PVO, SAR, SCN, SEO, SEP, SIN, SO4, SPD, SPM, SR, STE, STO, STU, TAR, TBU, TME, TPO, TRS, UNK, UNL, UNX, UPL, URE Table 11 CCD codes defining glycans 045, 05L, 07E, 07Y, 08U, 09X, 0BD, 0H0, 0HX, 0LP, 0MK, 0NZ, 0UB, 0V4, 0WK, 0XY, 0YT, 10M, 12E, 145, 147, 149, 14T, 15L, 16F, 16G, 16O, 17T, 18D, 18O, 1CF, 1FT, 1GL, 1GN, 1LL, 1S3, 1S4, 1SD, 1X4, 20S, 20X, 22O, 22S, 23V, 24S, 25E, 26O, 27C, 289, 291, 293, 2DG, 2DR, 2F8, 2FG, 2FL, 2GL, 2GS, 2H5, 2HA, 2M4, 2M5, 2M8, 2OS, 2WP, 2WS, 32O, 34V, 38J, 3BU, 3DO, 3DY, 3FM, 3GR, 3HD, 3J3, 3J4, 3LJ, 3LR, 3MG, 3MK, 3R3, 3S6, 3SA, 3YW, 40J, 42D, 445, 44S, 46D, 46Z, 475, 48Z, 491, 49A, 49S, 49T, 49V, 4AM, 4CQ, 4GC, 4GL, 4GP, 4JA, 4N2, 4NN, 4QY, 4R1, 4RS, 4SG, 4UZ, 4V5, 50A, 51N, 56N, 57S, 5GF, 5GO, 5II, 5KQ, 5KS, 5KT, 5KV, 5L3, 5LS, 5LT, 5MM, 5N6, 5QP, 5SP, 5TH, 5TJ, 5TK, 5TM, 61J, 62I, 64K, 66O, 6BG, 6C2, 6DM, 6GB, 6GP, 6GR, 6K3, 6KH, 6KL, 6KS, 6KU, 6KW, 6LA, 6LS, 6LW, 6MJ, 6MN, 6PZ, 6S2, 6UD, 6YR, 6ZC, 73E, 79J, 7CV, 7D1, 7GP, 7JZ, 7K2, 7K3, 7NU, 83Y, 89Y, 8B7, 8B9, 8EX, 8GA, 8GG, 8GP, 8I4, 8LR, 8OQ, 8PK, 8S0, 8YV, 95Z, 96O, 98U, 9AM, 9C1, 9CD, 9GP, 9KJ, 9MR, 9OK, 9PG, 9QG, 9S7, 9SG, 9SJ, 9SM, 9SP, 9T1, 9T7, 9VP, 9WJ, 9WN, 9WZ, 9YW, A0K, A1Q, A2G, A5C, A6P, AAL, ABD, ABE, ABF, ABL, AC1, ACR, ACX, ADA, AF1, AFD, AFO, AFP, AGL, AH2, AH8, AHG, AHM, AHR, AIG, ALL, ALX, AMG, AMN, AMU, AMV, ANA, AOG, AQA, ARA, ARB, ARI, ARW, ASC, ASG, ASO, AXP, AXR, AY9, AZC, B0D, B16, B1H, B1N, B2G, B4G, B6D, B7G, B8D, B9D, BBK, BBV, BCD, BDF, BDG, BDP, BDR, BEM, BFN, BG6, BG8, BGC, BGL, BGN, BGP, BGS, BHG, BM3, BM7, BMA, BMX, BND, BNG, BNX, BO1, BOG, BQY, BS7, BTG, BTU, BW3, BWG, BXF, BXP, BXX, BXY, BZD, C3B, C3G, C3X, C4B, C4W, C5X, CBF, CBI, CBK, CDR, CE5, CE6, CE8, CEG, CEZ, CGF, CJB, CKB, CKP, CNP, CR1, CR6, CRA, CT3, CTO, CTR, CTT, D1M, D5E, D6G, DAF, DAG, DAN, DDA, DDL, DEG, DEL, DFR, DFX, DG0, DGO, DGS, DGU, DJB, DJE, DK4, DKX, DKZ, DL6, DLD, DLF, DLG, DNO, DO8, DOM, DPC, DQR, DR2, DR3, DR5, DRI, DSR, DT6, DVC, DYM, E3M, E5G, EAG, EBG, EBQ, EEN, EEQ, EGA, EMP, EMZ, EPG, EQP, EQV, ERE, ERI, ETT, EUS, F1P, F1X, F55, F58, F6P, F8X, FBP, FCA, FCB, FCT, FDP, FDQ, FFC, FFX, FIF, FK9, FKD, FMF, FMO, FNG, FNY, FRU, FSA, FSI, FSM, FSW, FUB, FUC, FUD, FUF, FUL, FUY, FVQ, FX1, FYJ, G0S, G16, G1P, G20, G28, G2F, G3F, G3I, G4D, G4S, G6D, G6P, G6S, G7P, G8Z, GAA, GAC, GAD, GAF, GAL, GAT, GBH, GC1, GC4, GC9, GCB, GCD, GCN, GCO, GCS, GCT, GCU, GCV, GCW, GDA, GDL, GE1, GE3, GFP, GIV, GL0, GL1, GL2, GL4, GL5, GL6, GL7, GL9, GLA, GLC, GLD, GLF, GLG, GLO, GLP, GLS, GLT, GM0, GMB, GMH, GMT, GMZ, GN1, GN4, GNS, GNX, GP0, GP1, GP4, GPH, GPK, GPM, GPO, GPQ, GPU, GPV, GPW, GQ1, GRF, GRX, GS1, GS9, GTK, GTM, GTR, GU0, GU1, GU2, GU3, GU4, GU5, GU6, GU8, GU9, GUF, GUL, GUP, GUZ, GXL, GXV, GYE, GYG, GYP, GYU, GYV, GZL, H1M, H1S, H2P, H3S, H53, H6Q, H6Z, HBZ, HD4, HNV, HNW, HSG, HSH, HSJ, HSQ, HSX, HSY, HTG, HTM, HVC, IAB, IDC, IDF, IDG, IDR, IDS, IDU, IDX, IDY, IEM, IN1, IPT, ISD, ISL, ISX, IXD, J5B, JFZ, JHM, JLT, JRV, JSV, JV4, JVA, JVS, JZR, K5B, K99, KBA, KBG, KD5, KDA, KDB, KDD, KDE, KDF, KDM, KDN, KDO, KDR, KFN, KG1, KGM, KHP, KME, KO1, KO2, KOT, KTU, L0W, L1L, L6S, L6T, LAG, LAH, LAI, LAK, LAO, LAT, LB2, LBS, LBT, LCN, LDY, LEC, LER, LFC, LFR, LGC, LGU, LKA, LKS, LM2, LMO, LNV, LOG, LOX, LRH, LTG, LVO, LVZ, LXB, LXC, LXZ, LZ0, M1F, M1P, M2F, M3M, M3N, M55, M6D, M6P, M7B, M7P, M8C, MA1, MA2, MA3, MA8, MAB, MAF, MAG, MAL, MAN, MAT, MAV, MAW, MBE, MBF, MBG, MCU, MDA, MDP, MFB, MFU, MG5, MGC, MGL, MGS, MJJ, MLB, MLR, MMA, MN0, MNA, MQG, MQT, MRH, MRP, MSX, MTT, MUB, MUR, MVP, MXY, MXZ, MYG, N1L, N3U, N9S, NA1, NAA, NAG, NBG, NBX, NBY, NDG, NFG, NG1, NG6, NGA, NGC, NGE, NGK, NGR, NGS, NGY, NGZ, NHF, NLC, NM6, NM9, NNG, NPF, NSQ, NT1, NTF, NTO, NTP, NXD, NYT, OAK, OI7, OPM, OSU, OTG, OTN, OTU, OX2, P53, P6P, P8E, PA1, PAV, PDX, PH5, PKM, PNA, PNG, PNJ, PNW, PPC, PRP, PSG, PSV, PTQ, PUF, PZU, QDK, QIF, QKH, QPS, QV4, R1P, R1X, R2B, R2G, RAE, RAF, RAM, RAO, RB5, RBL, RCD, RER, RF5, RG1, RGG, RHA, RHC, RI2, RIB, RIP, RM4, RP3, RP5, RP6, RR7, RRJ, RRY, RST, RTG, RTV, RUG, RUU, RV7, RVG, RVM, RWI, RY7, RZM, S7P, S81, SA0, SCG, SCR, SDY, SEJ, SF6, SF9, SFU, SG4, SG5, SG6, SG7, SGA, SGC, SGD, SGN, SHB, SHD, SHG, SIA, SID, SIO, SIZ, SLB, SLM, SLT, SMD, SN5, SNG, SOE, SOG, SOL, SOR, SR1, SSG, SSH, STW, STZ, SUC, SUP, SUS, SWE, SZZ, T68, T6D, T6P, T6T, TA6, TAG, TCB, TDG, TEU, TF0, TFU, TGA, TGK, TGR, TGY, TH1, TM5, TM6, TMR, TMX, TNX, TOA, TOC, TQY, TRE, TRV, TS8, TT7, TTV, TU4, TUG, TUJ, TUP, TUR, TVD, TVG, TVM, TVS, TVV, TVY, TW7, TWA, TWD, TWG, TWJ, TWY, TXB, TYV, U1Y, U2A, U2D, U63, U8V, U97, U9A, U9D, U9G, U9J, U9M, UAP, UBH, UBO, UDC, UEA, V3M, V3P, V71, VG1, VJ1, VJ4, VKN, VTB, W9T, WIA, WOO, WUN, WZ1, WZ2, X0X, X1P, X1X, X2F, X2Y, X34, X6X, X6Y, XDX, XGP, XIL, XKJ, XLF, XLS, XMM, XS2, XXM, XXR, XXX, XYF, XYL, XYP, XYS, XYT, XYZ, YDR, YIO, YJM, YKR, YO5, YX0, YX1, YYB, YYH, YYJ, YYK, YYM, YYQ, YZ0, Z0F, Z15, Z16, Z2D, Z2T, Z3K, Z3L, Z3Q, Z3U, Z4K, Z4R, Z4S, Z4U, Z4V, Z4W, Z4Y, Z57, Z5J, Z5L, Z61, Z6H, Z6J, Z6W, Z8H, Z8T, Z9D, Z9E, Z9H, Z9K, Z9L, Z9M, Z9N, Z9W, ZB0, ZB1, ZB2, ZB3, ZCD, ZCZ, ZD0, ZDC, ZDO, ZEE, ZEL, ZGE, ZMR Table 12 Ions 118, 119, 1AL, 1CU, 2FK, 2HP, 2OF, 3CO, 3MT, 3NI, 3OF, 4MO, 4PU, 4TI, 543, 6MO, AG, AL, ALF, AM, ATH, AU, AU3, AUC, BA, BEF, BF4, BO4, BR, BS3, BSY, CA, CAC, CD, CD1, CD3, CD5, CE, CF, CHT, CO, CO5, CON, CR, CS, CSB, CU, CU1, CU2, CU3, CUA, CUZ, CYN, DME, DMI, DSC, DTI, DY, E4N, EDR, EMC, ER3, EU, EU3, F, FE, FE2, FPO, GA, GD3, GEP, HAI, HG, HGC, HO3, IN, IR, IR3, IRI, IUM, K, KO4, LA, LCO, LCP, LI, LU, MAC, MG, MH2, MH3, MMC, MN, MN3, MN5, MN6, MO, MO1, MO2, MO3, MO4, MO5, MO6, MOO, MOS, MOW, MW1, MW2, MW3, NA2, NA5, NA6, NAO, NAW, NET, NI, NI1, NI2, NI3, NO2, NRU, O4M, OAA, OC1, OC2, OC3, OC4, OC5, OC6, OC7, OC8, OCL, OCM, OCN, OCO, OF1, OF2, OF3, OH, OS, OS4, OXL, PB, PBM, PD, PER, PI, PO3, PR, PT, PT4, PTN, RB, RH3, RHD, RU, SB, SE4, SEK, SM, SMO, SO3, T1A, TB, TBA, TCN, TEA, TH, THE, TL, TMA, TRA, V, VN3, VO4, W, WO5, Y1, YB, YB2, YH, YT3, ZCM, ZN, ZN2, ZN3, ZNO, ZO3, ZR Supplementary information for AlphaFold 3 38 Table 13 Standard residues ALA, ARG, ASN, ASP, CYS, GLN, GLU, GLY, HIS, ILE, LEU, LYS, MET, PHE, PRO, SER, THR, TRP, TYR, VAL, UNK, A, G, C, U, DA, DG, DC, DT, N, DN Table 14 Recent PDB test set with nucleic acid complexes 7B0C, 7BCA, 7BJQ, 7EDS, 7EOF, 7F3J, 7F8Z, 7F9H, 7M4L, 7MKT, 7MWH, 7MZ0, 7MZ1, 7MZ2, 7N5U, 7N5V, 7N5W, 7NQF, 7NRP, 7OGS, 7OOO, 7OOS, 7OOT, 7OUE, 7OWF, 7OY7, 7OZZ, 7P0W, 7P3F, 7P8L, 7P9J, 7P9Z, 7PSX, 7PTQ, 7PZA, 7PZB, 7Q3O, 7Q4N, 7Q94, 7QAZ, 7QP2, 7R6R, 7R6T, 7R8G, 7R8H, 7R8I, 7RCC, 7RCD, 7RCE, 7RCF, 7RCG, 7RCU, 7RGU, 7RSR, 7RSS, 7S03, 7S68, 7S9J, 7S9K, 7S9L, 7S9M, 7S9N, 7S9O, 7S9P, 7S9Q, 7SOP, 7SOS, 7SOT, 7SOU, 7SOV, 7SOW, 7SUM, 7SUV, 7SVB, 7SX5, 7SXE, 7T18, 7T19, 7T1A, 7T1B, 7T8K, 7TDW, 7TDX, 7TEA, 7TEC, 7TO1, 7TO2, 7TQW, 7TUV, 7TXC, 7TZ1, 7TZR, 7TZS, 7TZT, 7TZU, 7TZV, 7U76, 7U79, 7U7A, 7U7B, 7U7C, 7U7F, 7U7G, 7U7I, 7U7J, 7U7K, 7U7L, 7UBL, 7UBU, 7UCR, 7UPZ, 7UQ6, 7UR5, 7URI, 7URM, 7UU4, 7UXD, 7UZ0, 7V2Z, 7VE5, 7VFT, 7VG8, 7VKI, 7VKL, 7VN2, 7VNV, 7VNW, 7VO9, 7VOU, 7VOV, 7VOX, 7VP1, 7VP2, 7VP3, 7VP4, 7VP5, 7VP7, 7VSJ, 7VTI, 7WM3, 7WQ5, 7X5E, 7X5F, 7X5G, 7X5L, 7X5M, 7XHV, 7XI3, 7XQ5, 7XRC, 7XS4, 7YHO, 7YZE, 7YZF, 7YZG, 7Z0U, 7Z5A, 7ZHH, 7ZVN, 7ZVX, 8A1C, 8A4I, 8AMG, 8AMI, 8AMJ, 8AMK, 8AML, 8AMM, 8AMN, 8B0R, 8CSH, 8CTZ, 8CU0, 8CZQ, 8D28, 8D2A, 8D2B, 8D5L, 8D5O, 8DVP, 8DVR, 8DVS, 8DVU, 8DVY, 8DW0, 8DW1, 8DW4, 8DW8, 8DWM, 8DZK, 8E2P, 8E2Q, 8EDJ, 8EF9, 8EFC, 8EFK, 8GMS, 8GMT, 8GMU Table 15 PoseBusters V2 Common Natural Ligands 2BA, 5AD, A3P, ACP, ADP, AKG, ANP, APC, APR, ATP, BCN, BDP, BGC, C5P, CDP, CTP, DGL, DSG, F15, FAD, FDA, FMN, GSH, GSP, GTP, H4B, IPE, MFU, MTA, MTE, NAD, NAI, NCA, NGA, OGA, PGA, PHO, PJ8, PLG, PLP, PRP, SAH, SFG, SIN, SLB, TPP, UD1, UDP, UPG, URI References [1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tun- yasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, 2021. [2] Jimmy Ba, Jamie R Kiros, and Geoffrey E Hinton. Layer Normalization. arXiv preprint arXiv:1607.06450, 2016. [3] H M Berman. The protein data bank. Nucleic Acids Res., 28(1):235\u2013242, January 2000. [4] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926\u2013932, 2015. [5] L Steven Johnson, Sean R Eddy, and Elon Portugaly. Hidden markov model speed heuristic and iterative hmm search procedure. BMC Bioinformatics, 11(1):1\u20138, 2010. [6] The UniProt Consortium. Uniprot: the universal protein knowledgebase in 2023. Nucleic Acids Research, 51(D1):D523\u2013D531, 2023. [7] Milot Mirdita, Lars Von Den Driesch, Clovis Galiez, Maria J Martin, Johannes S\u00f6ding, and Martin Steinegger. Uniclust databases of clustered and deeply annotated protein sequences and alignments. Nucleic acids research, 45(D1):D170\u2013D176, 2017. [8] Martin Steinegger, Milot Mirdita, and Johannes S\u00f6ding. Protein-level assembly increases protein sequence recov- ery from metagenomic samples manyfold. Nature methods, 16(7):603\u2013606, 2019. [9] Michael Remmert, Andreas Biegert, Andreas Hauser, and Johannes S\u00f6ding. Hhblits: lightning-fast iterative pro- tein sequence searching by hmm-hmm alignment. Nature Methods, 9(2):173\u2013175, 2012. [10] Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin \u017d\u00eddek, Alex Bridg- land, Andrew Cowie, Clemens Meyer, Agata Laydon, et al. Highly accurate protein structure prediction for the human proteome. Nature, 596(7873):590\u2013596, 2021. [11] Alex L Mitchell, Alexandre Almeida, Martin Beracochea, Miguel Boland, Josephine Burgin, Guy Cochrane, Michael R Crusoe, Varsha Kale, Simon C Potter, Lorna J Richardson, Ekaterina Sakharova, Maxim Scheremet- jew, Anton Korobeynikov, Alex Shlemov, Olga Kunyavskaya, Alla Lapidus, and Robert D Finn. MGnify: the microbiome analysis resource in 2020. Nucleic Acids Research, 48(D1):D570\u2013D578, 11 2019. [12] Sean R Eddy. Accelerated profile hmm searches. PLoS computational biology, 7(10):e1002195, 2011. [13] Ioanna Kalvari, Eric P Nawrocki, Nancy Ontiveros-Palacios, Joanna Argasinska, Kevin Lamkiewicz, Manja Marz, Sam Griffiths-Jones, Claire Toffano-Nioche, Daniel Gautheret, Zasha Weinberg, et al. Rfam 14: expanded cover- age of metagenomic, viral and microrna families. Nucleic Acids Research, 49(D1):D192\u2013D200, 2021. [14] Martin Steinegger and Johannes S\u00f6ding. Clustering huge protein sequence sets in linear time. Nature Communi- cations, 9(1):1\u20138, 2018. Supplementary information for AlphaFold 3 39 [15] Travis J Wheeler and Sean R Eddy. nhmmer: Dna homology search with profile hmms. Bioinformatics, 29(19):2487\u20132489, 2013. [16] RNAcentral Consortium. Rnacentral 2021: secondary structure integration, improved sequence search and new member databases. Nucleic acids research, 49(D1):D212\u2013D220, 2021. [17] Eric W Sayers, Evan E Bolton, J Rodney Brister, Kathi Canese, Jessica Chan, Donald C Comeau, Catherine M Farrell, Michael Feldgarden, Anna M Fine, Kathryn Funk, et al. Database resources of the national center for biotechnology information in 2023. Nucleic acids research, 51(D1):D29\u2013D38, 2023. [18] Richard Evans, Michael O\u2019Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin \u017d\u00eddek, Russ Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with AlphaFold-Multimer. bioRxiv preprint bioRxiv:10.1101/2021.10.04.463034, 2021. [19] Timo Lassmann, Oliver Frings, and Erik L L Sonnhammer. Kalign2: high-performance multiple alignment of protein and nucleotide sequences allowing external features. Nucleic Acids Research, 37:858\u2013865, 2009. [20] Jaime A Castro-Mondragon, Rafael Riudavets-Puig, Ieva Rauluseviciute, Roza Berhanu Lemma, Laura Turchi, Romain Blanc-Mathieu, Jeremy Lucas, Paul Boddie, Aziz Khan, Nicol\u00e1s Manosalva P\u00e9rez, et al. Jaspar 2022: the 9th release of the open-access database of transcription factor binding profiles. Nucleic acids research, 50(D1):D165\u2013D173, 2022. [21] Yimeng Yin, Ekaterina Morgunova, Arttu Jolma, Eevi Kaasinen, Biswajyoti Sahu, Syed Khund-Sayeed, Pratyush K Das, Teemu Kivioja, Kashyap Dave, Fan Zhong, et al. Impact of cytosine methylation on dna binding specificities of human transcription factors. Science, 356(6337):eaaj2239, 2017. [22] Arttu Jolma, Yimeng Yin, Kazuhiro R Nitta, Kashyap Dave, Alexander Popov, Minna Taipale, Martin Enge, Teemu Kivioja, Ekaterina Morgunova, and Jussi Taipale. Dna-dependent formation of transcription factor pairs alters their binding specificity. Nature, 527(7578):384\u2013388, 2015. [23] Alexey G Murzin, Steven E Brenner, Tim Hubbard, and Cyrus Chothia. SCOP: a structural classification of proteins database for the investigation of sequences and structures. Journal of molecular biology, 247(4):536\u2013 540, 1995. [24] Peter JA Cock, Tiago Antao, Jeffrey T Chang, Brad A Chapman, Cymon J Cox, Andrew Dalke, Iddo Friedberg, Thomas Hamelryck, Frank Kauff, Bartek Wilczynski, et al. Biopython: freely available Python tools for compu- tational molecular biology and bioinformatics. Bioinformatics, 25(11):1422, 2009. [25] Rdkit: Open-source cheminformatics software. https://www.rdkit.org/. [26] Shuzhe Wang, Jagna Witek, Gregory A Landrum, and Sereina Riniker. Improving conformer generation for small rings and macrocycles based on distance geometry and experimental torsional-angle preferences. Journal of chemical information and modeling, 60(4):2044\u20132058, 2020. [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2023. [28] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. [29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based Generative Models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 26565\u201326577. Curran Associates, Inc., 2022. [30] Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702\u2013710, 2004. [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the Interna- tional Conference on Learning Representations, 2015. [32] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, July 1992. Supplementary information for AlphaFold 3 40 [33] Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lddt: A local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics, 29(21):2722\u20132728, 2013. [34] Damiano Piovesan, Alexander Miguel Monzon, and Silvio C. E. Tosatto. Intrinsic protein disorder and conditional folding in AlphaFoldDB. Protein Science, 31(11), October 2022. [35] Jerome Eberhardt, Diogo Santos-Martins, Andreas F. Tillack, and Stefano Forli. Autodock vina 1.2.0: New docking methods, expanded force field, and python bindings. Journal of Chemical Information and Modeling, 61(8):3891\u20133898, Aug 2021. [36] Oleg Trott and Arthur J. Olson. AutoDock vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31(2):455\u2013461, June 2009. [37] Patrick J. Ropp, Jacob O. Spiegel, Jennifer L. Walker, Harrison Green, Guillermo A. Morales, Katherine A. Milliken, John J. Ringe, and Jacob D. Durrant. Gypsum-dl: an open-source program for preparing small-molecule libraries for structure-based virtual screening. Journal of Cheminformatics, 11(1):34, May 2019. [38] Mats H. M. Olsson, Chresten R. S\u00f8ndergaard, Michal Rostkowski, and Jan H. Jensen. Propka3: Consistent treat- ment of internal and surface residues in empirical pka predictions. Journal of Chemical Theory and Computation, 7(2):525\u2013537, Feb 2011. [39] Martin Buttenschoen, Garrett M. Morris, and Charlotte M. Deane. Posebusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. arXiv preprint arXiv:2308.05777, 2023. [40] Minkyung Baek, Ryan McHugh, Ivan Anishchenko, David Baker, and Frank DiMaio. Accurate pre- diction of nucleic acid and protein-nucleic acid complexes using RoseTTAFoldNA. bioRxiv preprint bioRxiv:10.1101/2022.09.09.507333, 2022. [41] Frank DiMaio, Blake Riley, and Alex Morehead. RoseTTAFold2NA, April 2023. [42] Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S. Morey-Burrows, Ivan Anishchenko, Ian R. Humphreys, Ryan McHugh, Dionne Vafeados, Xinting Li, George A. Sutherland, Andrew Hitchcock, C. Neil Hunter, Alex Kang, Evans Brackenbrough, Asim K. Bera, Minkyung Baek, Frank DiMaio, and David Baker. Generalized biomolecular modeling and design with rosettafold all-atom. Science, March 2024.</p> <p>Skip to main content Nature</p> <pre><code>View all journals\n\nSearch\nLog in\n\nExplore content\n</code></pre> <p>About the journal Publish with us</p> <pre><code>Sign up for alerts\n\nRSS feed\n\nnature\n</code></pre> <p>articles</p> <pre><code>article\n</code></pre> <p>Accurate structure prediction of biomolecular interactions with AlphaFold 3 Download PDF</p> <pre><code>Article\nOpen access\nPublished: 08 May 2024\n</code></pre> <p>Accurate structure prediction of biomolecular interactions with AlphaFold 3</p> <pre><code>Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael O\u2019Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvil\u0117 \u017demgulyt\u0117, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, \u2026John M. Jumper\n</code></pre> <p>Nature volume 630, pages 493\u2013500 (2024)Cite this article</p> <pre><code>775k Accesses\n\n2007 Altmetric\n\nMetrics details\n</code></pre> <p>An Addendum to this article was published on 27 November 2024 Abstract</p> <p>The introduction of AlphaFold\u200921 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2,3,4,5,6. Here we describe our AlphaFold\u20093 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein\u2013ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein\u2013nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody\u2013antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework. Similar content being viewed by others Integrated structure prediction of protein\u2013protein docking with experimental restraints using ColabDock Article 05 August 2024 Highly accurate protein structure prediction with AlphaFold Article Open access 15 July 2021 SurfDock is a surface-informed diffusion generative model for reliable and accurate protein\u2013ligand complex prediction Article 27 November 2024 Main</p> <p>Accurate models of biological complexes are critical to our understanding of cellular functions and for the rational design of therapeutics2,3,4,9. Enormous progress has been achieved in protein structure prediction with the development of AlphaFold1, and the field has grown tremendously with a number of later methods that build on the ideas and techniques of AlphaFold\u20092 (AF2)10,11,12. Almost immediately after AlphaFold became available, it was shown that simple input modifications would enable surprisingly accurate protein interaction predictions13,14,15 and that training AF2 specifically for protein interaction prediction yielded a highly accurate system7.</p> <p>These successes lead to the question of whether it is possible to accurately predict the structure of complexes containing a much wider range of biomolecules, including ligands, ions, nucleic acids and modified residues, within a deep-learning framework. A wide range of predictors for various specific interaction types has been developed16,17,18,19,20,21,22,23,24,25,26,27,28, as well as one generalist method developed concurrently with the present work29, but the accuracy of such deep-learning attempts has been mixed and often below that of physics-inspired methods30,31. Almost all of these methods are also highly specialized to particular interaction types and cannot predict the structure of general biomolecular complexes containing many types of entities.</p> <p>Here we present AlphaFold\u20093 (AF3)\u2014a model that is capable of high-accuracy prediction of complexes containing nearly all molecular types present in the Protein Data Bank32 (PDB) (Fig. 1a,b). In all but one category, it achieves a substantially higher performance than strong methods that specialize in just the given task (Fig. 1c and Extended Data Table 1), including higher accuracy at protein structure and the structure of protein\u2013protein interactions. Fig. 1: AF3 accurately predicts structures across biomolecular complexes. figure 1</p> <p>a,b, Example structures predicted using AF3. a, Bacterial CRP/FNR family transcriptional regulator protein bound to DNA and cGMP (PDB 7PZB; full-complex LDDT47, 82.8; global distance test (GDT)48, 90.1). b, Human coronavirus OC43 spike protein, 4,665 residues, heavily glycosylated and bound by neutralizing antibodies (PDB 7PNM; full-complex LDDT, 83.0; GDT, 83.1). c, AF3 performance on PoseBusters (v.1, August 2023 release), our recent PDB evaluation set and CASP15 RNA. Metrics are as follows: percentage of pocket-aligned ligand r.m.s.d.\u2009&lt;\u20092\u2009\u00c5 for ligands and covalent modifications; interface LDDT for protein\u2013nucleic acid complexes; LDDT for nucleic acid and protein monomers; and percentage DockQ\u2009&gt;\u20090.23 for protein\u2013protein and protein\u2013antibody interfaces. All scores are reported from the top confidence-ranked sample out of five model seeds (each with five diffusion samples), except for protein\u2013antibody scores, which were ranked across 1,000 model seeds for both models (each AF3 seed with five diffusion samples). Sampling and ranking details are provided in the Methods. For ligands, n indicates the number of targets; for nucleic acids, n indicates the number of structures; for modifications, n indicates the number of clusters; and for proteins, n indicates the number of clusters. The bar height indicates the mean; error bars indicate exact binomial distribution 95% confidence intervals for PoseBusters and by 10,000 bootstrap resamples for all others. Significance levels were calculated using two-sided Fisher\u2019s exact tests for PoseBusters and using two-sided Wilcoxon signed-rank tests for all others; *P\u2009&lt;\u20090.001, P\u2009&lt;\u20090.01. Exact P values (from left to right) are as follows: 2.27\u2009\u00d7\u200910\u221213, 2.57\u2009\u00d7\u200910\u22123, 2.78\u2009\u00d7\u200910\u22123, 7.28\u2009\u00d7\u200910\u221212, 1.81\u2009\u00d7\u200910\u221218, 6.54\u2009\u00d7\u200910\u22125 and 1.74\u2009\u00d7\u200910\u221234. AF-M 2.3, AlphaFold-Multimer v.2.3; dsDNA, double-stranded DNA. d, AF3 architecture for inference. The rectangles represent processing modules and the arrows show the data flow. Yellow, input data; blue, abstract network activations; green, output data. The coloured balls represent physical atom coordinates. Full size image</p> <p>This is achieved by a substantial evolution of the AF2 architecture and training procedure (Fig. 1d) both to accommodate more general chemical structures and to improve the data efficiency of learning. The system reduces the amount of multiple-sequence alignment (MSA) processing by replacing the AF2 evoformer with the simpler pairformer module (Fig. 2a). Furthermore it directly predicts the raw atom coordinates with a diffusion module, replacing the AF2 structure module that operated on amino-acid-specific frames and side-chain torsion angles (Fig. 2b). The multiscale nature of the diffusion process (low noise levels induce the network to improve local structure) also enable us to eliminate stereochemical losses and most special handling of bonding patterns in the network, easily accommodating arbitrary chemical components. Fig. 2: Architectural and training details. figure 2</p> <p>a, The pairformer module. Input and output: pair representation with dimension (n,\u2009n,\u2009c) and single representation with dimension (n,\u2009c). n is the number of tokens (polymer residues and atoms); c is the number of channels (128 for the pair representation, 384 for the single representation). Each of the 48 blocks has an independent set of trainable parameters. b, The diffusion module. Input: coarse arrays depict per-token representations (green, inputs; blue, pair; red, single). Fine arrays depict per-atom representations. The coloured balls represent physical atom coordinates. Cond., conditioning; rand. rot. trans., random rotation and translation; seq., sequence. c, The training set-up (distogram head omitted) starting from the end of the network trunk. The coloured arrays show activations from the network trunk (green, inputs; blue, pair; red, single). The blue arrows show abstract activation arrays. The yellow arrows show ground-truth data. The green arrows show predicted data. The stop sign represents stopping of the gradient. Both depicted diffusion modules share weights. d, Training curves for initial training and fine-tuning stages, showing the LDDT on our evaluation set as a function of optimizer steps. The scatter plot shows the raw datapoints and the lines show the smoothed performance using a median filter with a kernel width of nine datapoints. The crosses mark the point at which the smoothed performance reaches 97% of its initial training maximum. Full size image Network architecture and training</p> <p>The overall structure of AF3 (Fig. 1d and Supplementary Methods 3) echoes that of AF2, with a large trunk evolving a pairwise representation of the chemical complex followed by a structure module that uses the pairwise representation to generate explicit atomic positions, but there are large differences in each major component. These modifications were driven both by the need to accommodate a wide range of chemical entities without excessive special casing and by observations of AF2 performance with different modifications. Within the trunk, MSA processing is substantially de-emphasized, with a much smaller and simpler MSA embedding block (Supplementary Methods 3.3). Compared with the original evoformer from AF2, the number of blocks is reduced to four, the processing of the MSA representation uses an inexpensive pair-weighted averaging and only the pair representation is used for later processing steps. The \u2018pairformer\u2019 (Fig. 2a and Supplementary Methods 3.6) replaces the evoformer of AF2 as the dominant processing block. It operates only on the pair representation and the single representation; the MSA representation is not retained and all information passes through the pair representation. The pair processing and the number of blocks (48) is largely unchanged from AF2. The resulting pair and single representation together with the input representation are passed to the new diffusion module (Fig. 2b) that replaces the structure module of AF2.</p> <p>The diffusion module (Fig. 2b and Supplementary Methods 3.7) operates directly on raw atom coordinates, and on a coarse abstract token representation, without rotational frames or any equivariant processing. We had observed in AF2 that removing most of the complexity of the structure module had only a modest effect on the prediction accuracy, and maintaining the backbone frame and side-chain torsion representation add quite a bit of complexity for general molecular graphs. Similarly AF2 required carefully tuned stereochemical violation penalties during training to enforce chemical plausibility of the resulting structures. We use a relatively standard diffusion approach33 in which the diffusion model is trained to receive \u2018noised\u2019 atomic coordinates and then predict the true coordinates. This task requires the network to learn protein structure at a variety of length scales, whereby the denoising task at small noise emphasizes understanding very local stereochemistry and the denoising task at high noise emphasizes the large-scale structure of the system. At the inference time, random noise is sampled and then recurrently denoised to produce a final structure. Importantly, this is a generative training procedure that produces a distribution of answers. This means that, for each answer, the local structure will be sharply defined (for example, side-chain bond geometry) even when the network is uncertain about the positions. For this reason, we are able to avoid both torsion-based parametrizations of the residues and violation losses on the structure, while handling the full complexity of general ligands. Similarly to some recent work34, we find that no invariance or equivariance with respect to global rotations and translation of the molecule are required in the architecture and we therefore omit them to simplify the machine learning architecture.</p> <p>The use of a generative diffusion approach comes with some technical challenges that we needed to address. The biggest issue is that generative models are prone to hallucination35, whereby the model may invent plausible-looking structure even in unstructured regions. To counteract this effect, we use a cross-distillation method in which we enrich the training data with structures predicted by AlphaFold-Multimer (v.2.3)7,8. In these structures, unstructured regions are typically represented by long extended loops instead of compact structures, and training on them \u2018teaches\u2019 AF3 to mimic this behaviour. This cross-distillation greatly reduced the hallucination behaviour of AF3 (Extended Data Fig. 1 for disorder prediction results on the CAID 236 benchmark set).</p> <p>We also developed confidence measures that predict the atom-level and pairwise errors in our final structures. In AF2, this was done directly by regressing the error in the output of the structure module during training. However, this procedure is not applicable to diffusion training, as only a single step of the diffusion is trained instead of a full-structure generation (Fig. 2c). To remedy this, we developed a diffusion \u2018rollout\u2019 procedure for the full-structure prediction generation during training (using a larger step size than normal; Fig. 2c (mini-rollout)). This predicted structure is then used to permute the symmetric ground-truth chains and ligands, and to compute the performance metrics to train the confidence head. The confidence head uses the pairwise representation to predict a modified local distance difference test (pLDDT) and a predicted aligned error (PAE) matrix as in AF2, as well as a distance error matrix (PDE), which is the error in the distance matrix of the predicted structure as compared to the true structure (details are provided in Supplementary Methods 4.3).</p> <p>Figure 2d shows that, during initial training, the model learns quickly to predict the local structures (all intrachain metrics go up quickly and reach 97% of the maximum performance within the first 20,000 training steps), while the model needs considerably longer to learn the global constellation (the interface metrics go up slowly and protein\u2013protein interface LDDT passes the 97% bar only after 60,000 steps). During AF3 development, we observed that some model abilities topped out relatively early and started to decline (most likely due to overfitting to the limited number of training samples for this capability), while other abilities were still undertrained. We addressed this by increasing or decreasing the sampling probability for the corresponding training sets (Supplementary Methods 2.5.1) and by performing early stopping using a weighted average of all of the above metrics and some additional metrics to select the best model checkpoint (Supplementary Table 7). The fine-tuning stages with the larger crop sizes improve the model on all metrics with an especially high uplift on protein\u2013protein interfaces (Extended Data Fig. 2). Accuracy across complex types</p> <p>AF3 can predict structures from input polymer sequences, residue modifications and ligand SMILES (simplified molecular-input line-entry system). In Fig. 3 we show a selection of examples highlighting the ability of the model to generalize to a number of biologically important and therapeutically relevant modalities. In selecting these examples, we considered novelty in terms of the similarity of individual chains and interfaces to the training set (additional information is provided in Supplementary Methods 8.1). Fig. 3: Examples of predicted complexes. figure 3</p> <p>Selected structure predictions from AF3. Predicted protein chains are shown in blue (predicted antibody in green), predicted ligands and glycans in orange, predicted RNA in purple and the ground truth is shown in grey. a, Human 40S small ribosomal subunit (7,663 residues) including 18S ribosomal RNA and Met-tRNAiMet (opaque purple) in a complex with translation initiation factors eIF1A and eIF5B (opaque blue; PDB 7TQL; full-complex LDDT, 87.7; GDT, 86.9). b, The glycosylated globular portion of an EXTL3 homodimer (PDB 7AU2; mean pocket-aligned r.m.s.d., 1.10\u2009\u00c5). c, Mesothelin C-terminal peptide bound to the monoclonal antibody 15B6 (PDB 7U8C; DockQ, 0.85). d, LGK974, a clinical-stage inhibitor, bound to PORCN in a complex with the WNT3A peptide (PDB 7URD; ligand r.m.s.d., 1.00\u2009\u00c5). e, (5S,6S)-O7-sulfo DADH bound to the AziU3/U2 complex with a novel fold (PDB 7WUX; ligand r.m.s.d., 1.92\u2009\u00c5). f, Analogue of NIH-12848 bound to an allosteric site of PI5P4K\u03b3 (PDB 7QIE; ligand r.m.s.d., 0.37\u2009\u00c5). Full size image</p> <p>We evaluated the performance of the system on recent interface-specific benchmarks for each complex type (Fig. 1c and Extended Data Table 1). Performance on protein\u2013ligand interfaces was evaluated on the PoseBusters benchmark set, which is composed of 428 protein\u2013ligand structures released to the PDB in 2021 or later. As our standard training cut-off date is in 2021, we trained a separate AF3 model with an earlier training-set cutoff (Methods). Accuracy on the PoseBusters set is reported as the percentage of protein\u2013ligand pairs with pocket-aligned ligand root mean squared deviation (r.m.s.d.) of less than 2\u2009\u00c5. The baseline models come in two categories: those that use only protein sequence and ligand SMILES as an input and those that additionally leak information from the solved protein\u2013ligand test structure. Traditional docking methods use the latter privileged information, even though that information would not be available in real-world use cases. Even so, AF3 greatly outperforms classical docking tools such as Vina37,38 even while not using any structural inputs (Fisher\u2019s exact test, P\u2009=\u20092.27\u2009\u00d7\u200910\u221213) and greatly outperforms all other true blind docking like RoseTTAFold All-Atom (P\u2009=\u20094.45\u2009\u00d7\u200910\u221225). Extended Data Fig. 3 shows three examples in which AF3 achieves accurate predictions but docking tools Vina and Gold do not37. PoseBusters analysis was performed using a training cut-off of 30 September 2019 for AF3 to ensure that the model was not trained on any PoseBusters structures. To compare with the RoseTTAFold All-Atom results, we used PoseBusters version 1. Version 2 (crystal contacts removed from the benchmark set) results including quality metrics are shown in Extended Data Fig. 4b\u2013f and Extended Data Table 1. We use multiple seeds to ensure correct chirality and avoid slight protein\u2013ligand clashing (as opposed to a method like diffusion guidance to enforce) but we are typically able to produce high-quality stereochemistry. Separately, we also train a version of AF3 that receives the \u2018pocket information\u2019 as used in some recent deep-learning work24,26 (the results are shown in Extended Data Fig. 4a).</p> <p>AF3 predicts protein\u2013nucleic complexes and RNA structures with higher accuracy than RoseTTAFold2NA15 (Fig. 1c (second plot)). As RoseTTAFold2NA is validated only on structures below 1,000 residues, we use only structures below 1,000 residues from our recent PDB evaluation set for this comparison (Methods). AF3 is able to predict protein\u2013nucleic structures with thousands of residues, an example of which is shown in Fig. 3a. Note that we do not compare directly to RoseTTAFold All-Atom, but benchmarks indicate that RoseTTAFold All-Atom is slightly less accurate than RoseTTAFold2NA for nucleic acid predictions29.</p> <p>We also evaluated AF3 performance on the ten publicly available Critical Assessment of Structure Prediction 15 (CASP15) RNA targets: we achieve a higher average performance than RoseTTAFold2NA and AIchemy_RNA27 (the best AI-based submission in CASP1518,31) on the respective common subsets of our and their predictions (detailed results are shown in Extended Data Fig. 5a). We did not reach the performance of the best human-expert-aided CASP15 submission AIchemy_RNA239 (Fig. 1c (centre left)). Owing to limited dataset sizes, we do not report significance test statistics here. Further analysis of the accuracy of predicting nucleic acids alone (without proteins) is shown in Extended Data Fig. 5b.</p> <p>Covalent modifications (bonded ligands, glycosylation, and modified protein residues and nucleic acid bases) are also accurately predicted by AF3 (Fig. 1c (centre right)). Modifications include those to any polymer residue (protein, RNA or DNA). We report accuracy as the percentage of successful predictions (pocket r.m.s.d. &lt;\u20092\u2009\u00c5). We apply quality filters to the bonded ligands and glycosylation dataset (as does PoseBusters): we include only ligands with high-quality experimental data (ranking_model_fit\u2009&gt;\u20090.5, according to the RCSB structure validation report, that is, X-ray structures with a model quality above the median). As with the PoseBusters set, the bonded ligands and glycosylation datasets are not filtered by homology to the training dataset. Filtering on the basis of the bound polymer chain homology (using polymer template similarity\u2009&lt;\u200940) yielded only five clusters for bonded ligands and seven clusters for glycosylation. We exclude multi-residue glycans here because the RCSB validation report does not provide a ranking_model_fit value for them. The percentage of successful predictions (pocket r.m.s.d.\u2009&lt;\u20092\u2009\u00c5) for multi-residue glycans on all-quality experimental data is 42.1% (n\u2009=\u2009131 clusters), which is slightly lower than the success rate for single-residue glycans on all-quality experimental data of 46.1% (n\u2009=\u2009167). The modified residues dataset is filtered similarly to our other polymer test sets: it contains only modified residues in polymer chains with low homology to the training set (Methods). See Extended Data Table 1 for detailed results, and Extended Data Fig. 6 for examples of predicted protein, DNA and RNA structures with covalent modifications, including analysis of the impact of phosphorylation on predictions.</p> <p>While expanding in modelling abilities, AF3 has also improved in protein complex accuracy relative to AlphaFold-Multimer (v.2.3)7,8. Generally, protein\u2013protein prediction success (DockQ\u2009&gt;\u20090.23)40 has increased (paired Wilcoxon signed-rank test, P\u2009=\u20091.8\u2009\u00d7\u200910\u221218), with antibody\u2013protein interaction prediction in particular showing a marked improvement (Fig. 1c (right); paired Wilcoxon signed-rank test, P\u2009=\u20096.5\u2009\u00d7\u200910\u22125, predictions top-ranked from 1,000 rather than the typical 5 seeds; further details are provided in Fig. 5a). Protein monomer LDDT improvement is also significant (paired Wilcoxon signed-rank test, P\u2009=\u20091.7\u2009\u00d7\u200910\u221234). AF3 has a very similar dependence on MSA depth to AlphaFold-Multimer v.2.3; proteins with shallow MSAs are predicted with lower accuracy (a comparison of the dependence of single-chain LDDT on MSA depth is shown in Extended Data Fig. 7a). Predicted confidences track accuracy</p> <p>As with AF2, AF3 confidence measures are well calibrated with accuracy. Our confidence analysis is performed on the recent PDB evaluation set, with no homology filtering and including peptides. The ligands category is filtered to high-quality experimental structures as described above, and considers standard non-bonded ligands only. See Extended Data Fig. 8 for a similar assessment on bonded ligand and other interfaces. All statistics are cluster-weighted (Methods) and consider the top-ranked prediction only (ranking details are provided in Supplementary Methods 5.9.3).</p> <p>In Fig. 4a (top row), we plot the chain pair interface-predicted TM (ipTM) score41 (Supplementary Methods 5.9.1) against interface accuracy measures: protein\u2013protein DockQ, protein\u2013nucleic interface LDDT (iLDDT) and protein\u2013ligand success, with success defined as the percentage of examples under thresholded pocket-aligned r.m.s.d. values. In Fig. 4a (bottom row), we plot the average pLDDT per protein, nucleotide or ligand entity against our bespoke LDDT_to_polymer metric (metrics details are provided in the Methods), which is closely related to the training target of the pLDDT predictor. Fig. 4: AF3 confidences track accuracy. figure 4</p> <p>a, The accuracy of protein-containing interfaces as a function of chain pair ipTM (top). Bottom, the LDDT-to-polymer accuracy was evaluated for various chain types as a function of chain-averaged pLDDT. The box plots show the 25\u201375% confidence intervals (box limits), the median (centre line) and the 5\u201395% confidence intervals (whiskers). n values report the number of clusters in each band. b, The predicted structure of PDB 7T82 coloured by pLDDT (orange, 0\u201350; yellow, 50\u201370; cyan, 70\u201390; and blue, 90\u2013100). c, The same prediction coloured by chain. d, DockQ scores for protein\u2013protein interfaces. e, PAE matrix of same prediction (darker is more confident), with chain colouring of c on the side bars. The dashed black lines indicate the chain boundaries. Full size image</p> <p>In Fig. 4b\u2013e, we highlight a single example prediction of 7T82, in which per-atom pLDDT colouring identifies unconfident chain tails, somewhat confident interfaces and otherwise confident secondary structure. In Fig. 4c, the same prediction is coloured by chain, along with DockQ interface scores in Fig. 4d and per-chain colouring displayed on the axes for reference. We see from Fig. 4e that PAE confidence is high for pink\u2013grey and blue\u2013orange residue pairs for which DockQ\u2009&gt;\u20090.7, and least confident about pink\u2013orange and pink\u2013blue residue pairs that have DockQ\u2009\u2248\u20090. A similar PAE analysis of an example with protein and nucleic acid chains is shown in Extended Data Fig. 5c,d. Model limitations</p> <p>We note model limitations of AF3 with respect to stereochemistry, hallucinations, dynamics and accuracy for certain targets.</p> <p>On stereochemistry, we note two main classes of violations. The first is that the model outputs do not always respect chirality (Fig. 5b), despite the model receiving reference structures with correct chirality as input features. To address this in the PoseBusters benchmark, we included a penalty for chirality violation in our ranking formula for model predictions. Despite this, we still observe a chirality violation rate of 4.4% in the benchmark. The second class of stereochemical violations is a tendency of the model to occasionally produce overlapping (clashing) atoms in the predictions. This sometimes manifests as extreme violations in homomers in which entire chains have been observed to overlap (Fig. 5e). Penalizing clashes during ranking (Supplementary Methods 5.9.3) reduces the occurrence of this failure mode but does not eliminate them. Almost all remaining clashes occur for protein\u2013nucleic complexes with both greater than 100 nucleotides and greater than 2,000 residues in total. Fig. 5: Model limitations. figure 5</p> <p>a, Antibody prediction quality increases with the number of model seeds. The quality of top-ranked, low-homology antibody\u2013antigen interface predictions as a function of the number of seeds. Each datapoint shows the mean over 1,000 random samples (with replacement) of seeds to rank over, out of 1,200 seeds. Confidence intervals are 95% bootstraps over 10,000 resamples of cluster scores at each datapoint. Samples per interface are ranked by protein\u2013protein ipTM. Significance tests were performed using by two-sided Wilcoxon signed-rank tests. n\u2009=\u200965 clusters. Exact P values were as follows: 2.0\u2009\u00d7\u200910\u22125 (percentage correct) and P\u2009=\u20090.009 (percentage very high accuracy). b, Prediction (coloured) and ground-truth (grey) structures of Thermotoga maritima \u03b1-glucuronidase and beta-d-glucuronic acid\u2014a target from the PoseBusters set (PDB: 7CTM). AF3 predicts alpha-d-glucuronic acid; the differing chiral centre is indicated by an asterisk. The prediction shown is top-ranked by ligand\u2013protein ipTM and with a chirality and clash penalty. c, Conformation coverage is limited. Ground-truth structures (grey) of cereblon in open (apo, PDB: 8CVP; left) and closed (holo mezigdomide-bound, PDB: 8D7U; right) conformations. Predictions (blue) of both apo (with 10 overlaid samples) and holo structures are in the closed conformation. The dashed lines indicate the distance between the N-terminal Lon protease-like and C-terminal thalidomide-binding domain. d, A nuclear pore complex with 1,854 unresolved residues (PDB: 7F60). The ground truth (left) and predictions from AlphaFold-Multimer v.2.3 (middle) and AF3 (right) are shown. e, Prediction of a trinucleosome with overlapping DNA (pink) and protein (blue) chains (PDB: 7PEU); highlighted are overlapping protein chains B and J and self-overlapping DNA chain AA. Unless otherwise stated, predictions are top-ranked by our global complex ranking metric with chiral mismatch and steric clash penalties (Supplementary Methods 5.9.1). Full size image</p> <p>We note that the switch from the non-generative AF2 model to the diffusion-based AF3 model introduces the challenge of spurious structural order (hallucinations) in disordered regions (Fig. 5d and Extended Data Fig. 1). Although hallucinated regions are typically marked as very low confidence, they can lack the distinctive ribbon-like appearance that AF2 produces in disordered regions. To encourage ribbon-like predictions in AF3, we use distillation training from AF2 predictions, and we add a ranking term to encourage results with more solvent accessible surface area36.</p> <p>A key limitation of protein structure prediction models is that they typically predict static structures as seen in the PDB, not the dynamical behaviour of biomolecular systems in solution. This limitation persists for AF3, in which multiple random seeds for either the diffusion head or the overall network do not produce an approximation of the solution ensemble.</p> <p>In some cases, the modelled conformational state may not be correct or comprehensive given the specified ligands and other inputs. For example, E3 ubiquitin ligases natively adopt an open conformation in an apo state and have been observed only in a closed state when bound to ligands, but AF3 exclusively predicts the closed state for both holo and apo systems42 (Fig. 5c). Many methods have been developed, particularly around MSA resampling, that assist in generating diversity from previous AlphaFold models43,44,45 and may also assist in multistate prediction with AF3.</p> <p>Despite the large advance in modelling accuracy in AF3, there are still many targets for which accurate modelling can be challenging. To obtain the highest accuracy, it may be necessary to generate a large number of predictions and rank them, which incurs an extra computational cost. A class of targets in which we observe this effect strongly is antibody\u2013antigen complexes, similar to other recent work46. Figure 5a shows that, for AF3, top-ranked predictions keep improving with more model seeds, even at as many as 1,000 (Wilcoxon signed-rank test between 5 and 1,000 seeds, P\u2009=\u20092.0\u2009\u00d7\u200910\u22125 for percentage correct and P\u2009=\u20090.009 for percentage very high accuracy; ranking by protein\u2013protein interface ipTM). This large improvement with many seeds is not observed in general for other classes of molecules (Extended Data Fig. 7b). Using only one diffusion sample per model seed for the AF3 predictions rather than five (not illustrated) does not change the results significantly, indicating that running more model seeds is necessary for antibody score improvements, rather than just more diffusion samples. Discussion</p> <p>The core challenge of molecular biology is to understand and ultimately regulate the complex atomic interactions of biological systems. The AF3 model takes a large step in this direction, demonstrating that it is possible to accurately predict the structure of a wide range of biomolecular systems in a unified framework. Although there are still substantial challenges to achieve highly accurate predictions across all interaction types, we demonstrate that it is possible to build a deep-learning system that shows strong coverage and generalization for all of these interactions. We also demonstrate that the lack of cross-entity evolutionary information is not a substantial blocker to progress in predicting these interactions and, moreover, substantial improvement in antibody results suggests AlphaFold-derived methods are able to model the chemistry and physics of classes of molecular interactions without dependence on MSAs. Finally, the large improvement in protein\u2013ligand structure prediction shows that it is possible to handle the wide diversity of chemical space within a general deep-learning framework and without resorting to an artificial separation between protein structure prediction and ligand docking.</p> <p>The development of bottom-up modelling of cellular components is a key step in unravelling the complexity of molecular regulation within the cell, and the performance of AF3 shows that developing the right deep-learning frameworks can massively reduce the amount of data required to obtain biologically relevant performance on these tasks and amplify the impact of the data already collected. We expect that structural modelling will continue to improve not only due to advances in deep learning but also because continuing methodological advances in experimental structure determination, such as the substantial improvements in cryo-electron microscopy and tomography, will provide a wealth of new training data to further the improve the generalization ability of such models. The parallel developments of experimental and computational methods promise to propel us further into an era of structurally informed biological understanding and therapeutic development. Methods Full algorithm details</p> <p>Extensive explanations of the components are available in Supplementary Methods 2\u20135. Moreover, pseudocode is available in Supplementary Algorithms 1\u201331, network diagrams in Figs. 1d and  2a\u2013c and Supplementary Fig. 2, input features in Supplementary Table 5 and additional hyperparameters for training in Supplementary Tables 3, 4 and 7. Training regime</p> <p>No structural data used during training were released after 30 September 2021 and, for the model used in PoseBusters evaluations, we filtered out PDB32 structures released after 30 September 2021. One optimizer step uses a mini batch of 256 input data samples and during initial training 256\u2009\u00d7\u200948\u2009=\u200912,288 diffusion samples. For fine-tuning, the number of diffusion samples is reduced to 256\u2009\u00d7\u200932 = 8,192. The model is trained in three stages\u2014the initial training with a crop size of 384 tokens and two sequential fine tuning stages with crop sizes of 640 and 768 tokens. Further details are provided in Supplementary Methods 5.2. Inference regime</p> <p>No inference time templates or reference ligand position features were released after 30 September 2021, and in the case of PoseBusters evaluation, an earlier cut-off date of 30 September 2019 was used. The model can be run with different random seeds to generate alternative results, with a batch of diffusion samples per seed. Unless otherwise stated, all results are generated by selecting the top confidence sample from running 5 seeds of the same trained model, with 5 diffusion samples per model seed, for a total of 25 samples to choose from. Standard crystallization aids are excluded from predictions (Supplementary Table 8).</p> <p>Results are shown for the top-ranked sample and sample ranking depends on whether trying to select the overall best output globally, or the best output for some chain, interface or modified residue. Global ranking uses a mix of pTM and ipTM along with terms to reduce cases with large numbers of clashes and increase rates of disorder; individual chain ranking uses a chain specific pTM measure; interface ranking uses a bespoke ipTM measure for the relevant chain pair; and modified residue ranking uses average pLDDT over the residue of interest (Supplementary Methods 5.9.3). Metrics</p> <p>Evaluation compares a predicted structure to the corresponding ground-truth structure. If the complex contains multiple identical entities, assignment of the predicted units to the ground-truth units is found by maximizing LDDT. Assignment in local symmetry groups of atoms in ligands is solved by exhaustive search over the first 1,000 per-residue symmetries as given by RDKit.</p> <p>We measure the quality of the predictions with DockQ, LDDT or pocket-aligned r.m.s.d. For nucleic\u2013protein interfaces, we measure interface accuracy through iLDDT, which is calculated from distances between atoms across different chains in the interface. DockQ and iLDDT are highly correlated (Extended Data Fig. 9), so the standard cut-offs for DockQ can be translated to equivalent iLDDT cut-offs. Nucleic acid LDDTs (intrachains and interface) were calculated with an inclusion radius of 30\u2009\u00c5 compared with the usual 15\u2009\u00c5 used for proteins, owing to their larger scale. For confidence calibration assessment, we use a bespoke LDDT (LDDT_to_polymer) metric that considers differences from each atom of a given entity to any C\u03b1 or C1\u2032 polymer atom within its inclusion radius. This is closely related to how the confidence prediction is trained (Supplementary Methods 4.3.1).</p> <p>Pocket-aligned r.m.s.d. is computed as follows: the pocket is defined as all heavy atoms within 10\u2009\u00c5 of any heavy atom of the ligand, restricted to the primary polymer chain for the ligand or modified residue being scored, and further restricted to only backbone atoms for proteins. The primary polymer chain is defined variously: for PoseBusters, it is the protein chain with the most atoms within 10\u2009\u00c5 of the ligand; for bonded ligand scores, it is the bonded polymer chain; and for modified residues, it is the chain in which the residue is contained (minus that residue). The pocket is used to align the predicted structure to the ground-truth structure with least-squares rigid alignment and then the r.m.s.d. is computed on all heavy atoms of the ligand. Recent PDB evaluation set</p> <p>General model evaluation was performed on our recent PDB set consisting of 8,856 PDB complexes released between 1 May 2022 and 12 January 2023. The set contains almost all PDB complexes released during that period that are less than 5,120 model tokens in size (Supplementary Methods 6.1). Single chains and interfaces within each structure were scored separately rather than only looking at full complex scores, and clustering was then applied to chains and interfaces so that scores could be aggregated first within clusters and then across clusters for mean scores, or using a weighting of inverse cluster size for distributional statistics (Supplementary Methods 6.2 and 6.4).</p> <p>Evaluation on ligands excludes standard crystallization aids (Supplementary Table 8), our ligand exclusion list (Supplementary Table 9) and glycans (Supplementary Table 10). Bonded and non-bonded ligands are evaluated separately. Ions are only included when specifically mentioned (Supplementary Table 11).</p> <p>The recent PDB set is filtered to a low homology subset (Supplementary Methods 6.1) for some results where stated. Homology is defined as sequence identity to sequences in the training set and is measured by template search (Supplementary Methods 2.4). Individual polymer chains in evaluation complexes are filtered out if the maximum sequence identity to chains in the training set is greater than 40%, where sequence identity is the percentage of residues in the evaluation set chain that are identical to the training set chain. Individual peptide chains (protein chains with less than 16 residues) are always filtered out. For polymer\u2013polymer interfaces, if both polymers have greater than 40% sequence identity to two chains in the same complex in the training set, then the interface is filtered out. For interfaces to a peptide, the interface is filtered out if the non-peptide entity has greater than 40% sequence identity to any chain in the training set.</p> <p>To compare the quality of prediction of protein\u2013protein interfaces and protein monomers against that of AlphaFold-Multimer (v.2.3)8, and to compare the dependence of single-protein-chain prediction quality on MSA depth, we restrict the low-homology recent PDB set to complexes with fewer than 20 protein chains and fewer than 2,560 tokens. We compare against unrelaxed AlphaFold-Multimer v.2.3 predictions.</p> <p>To study antibody-antigen interface prediction, we filter the low homology recent PDB set to complexes that contain at least one protein\u2013protein interface where one of the protein chains is in one of the two largest PDB chain clusters (these clusters are representative of antibodies). We further filter to complexes with at most 2,560 tokens and with no unknown amino acids in the PDB to allow extensive comparison against relaxed predictions of AlphaFold-Multimer v2.3. That leaves 71 antibody\u2013antigen complexes, containing 166 antibody\u2013antigen interfaces spanning 65 interface clusters.</p> <p>MSA depth analysis (Extended Data Fig. 7a) was based on computing the normalized number of effective sequences (Neff) for each position of a query sequence. Per-residue Neff values were obtained by counting the number of non-gap residues in the MSA for this position and weighting the sequences using the Neff scheme49 with a threshold of 80% sequence identity measured on the region that is non-gap in either sequence. Nucleic acid prediction baseline</p> <p>For benchmarking performance on nucleic acid structure prediction, we report baseline comparisons to an existing machine learning system for protein\u2013nucleic acid and RNA tertiary structure prediction, RoseTTAFold2NA18. We run the open source RF2NA50 with the same MSAs as those that were used for AF3 predictions. For comparison between AF3 and RF2NA, a subset of our recent PDB set was chosen to meet the RF2NA criteria (&lt;1,000 total residues and nucleotides). As RF2NA was not trained to predict systems with DNA and RNA, analysis is limited to targets with only one nucleic acid type. No system was publicly available at time of writing for baseline comparisons on data with arbitrary combinations of biomolecular types in PDB.</p> <p>As an additional baseline for RNA tertiary structure prediction, we evaluate AF3 performance on CASP15 RNA targets that were publicly available as of 1 December 2023 (R1116/8S95, R1117/8FZA, R1126 (downloaded from the CASP15 website https://predictioncenter.org/casp15/TARGETS_PDB/R1126.pdb), R1128/8BTZ, R1136/7ZJ4, R1138/[7PTK/7PTL], R1189/7YR7 and R1190/7YR6). We compare the top-1 ranked predictions and, where multiple ground-truth structures exist (R1136), the prediction is scored against the closest state. We display comparisons to RF2NA as a representative machine learning system; AIchemy_RNA2 as the top performing entrant with human intervention; and AIchemy_RNA as the top performing machine learning system. All entrants\u2019 predictions were downloaded from the CASP website and scored internally. PoseBusters</p> <p>While other analyses used an AlphaFold model trained on PDB data released before a cut-off of 30 September 2021, our PoseBusters analysis was conducted on a model (with identical architecture and similar training schedule) differing only in the use of an earlier 30 September 2019 cut-off. This analysis therefore did not include training data, inference time templates or \u2018ref_pos\u2019 features released after this date.</p> <p>Inference was performed on the asymmetric unit from specified PDBs, with the following minor modifications. In several PDB files, chains clashing with the ligand of interest were removed (7O1T, 7PUV, 7SCW, 7WJB, 7ZXV, 8AIE). Another PDB entry (8F4J) was too large to inference the entire system (over 5,120 tokens), so we included only protein chains within 20\u2009\u00c5 of the ligand of interest. Five model seeds, each with five diffusion samples, were produced per target, resulting in 25 predictions, which were ranked by quality and predicted accuracy: the ranking score was calculated from an ipTM aggregate (Supplementary Methods 5.9.3 (point 3)), then further divided by 100 if the ligand had chirality errors or had clashes with the protein.</p> <p>For pocket-aligned r.m.s.d., first alignment between the predicted and ground-truth structures was conducted by aligning to the ground-truth pocket backbone atoms (CA, C or N atoms within 10\u2009\u00c5 of the ligand of interest) from the primary protein chain (the chain with the greatest number of contacts within 10\u2009\u00c5 of the ligand). The PoseBusters Python package v.0.2.751 was used to score r.m.s.d. and violations from the pocket-aligned predictions.</p> <p>While AlphaFold models are \u2018blind\u2019 to the protein pocket, docking is often performed with knowledge of the protein pocket residues. For example, Uni-Mol specifies the pocket as any residue within 6\u2009\u00c5 of the heavy atoms in the ligand of interest26. To evaluate the ability of AF3 to dock ligands accurately when given pocket information, we fine-tuned a 30 September 2019 cut-off AF3 model with an additional token feature specifying pocket\u2013ligand pairs (Supplementary Methods 2.8). Specifically, an additional token feature was introduced, set to true for a ligand entity of interest and any pocket residues with heavy atoms within 6\u2009\u00c5 of the ligand entity. At training time, a single random ligand entity is chosen to use in this feature. Note that multiple ligand chains with the same entity (CCD code) may be selected. At inference time, the ligand entity was chosen based on the ligand of interest\u2019s CCD code, so again multiple ligand chains were occasionally chosen. The results of this analysis are shown in Extended Data Fig. 4. Model performance analysis and visualization</p> <p>Data analysis used Python v.3.11.7 (https://www.python.org/), NumPy v.1.26.3 (https://github.com/numpy/numpy), SciPy v.1.9.3 (https://www.scipy.org/), seaborn v.0.12.2 (https://github.com/mwaskom/seaborn), Matplotlib v.3.6.1 (https://github.com/matplotlib/matplotlib), pandas v.2.0.3 (https://github.com/pandas-dev/pandas), statsmodels v.0.12.2 (https://github.com/statsmodels/statsmodels), RDKit v.4.3.0 (https://github.com/rdkit/rdkit) and Colab (https://research.google.com/colaboratory). TM-align v.20190822 (https://zhanglab.dcmb.med.umich.edu/TM-align/) was used for computing TM-scores. Structure visualizations were created in Pymol v.2.55.5 (https://github.com/schrodinger/pymol-open-source). Reporting summary</p> <p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability</p> <p>All scientific datasets used to create training and evaluation inputs are freely available from public sources. Structures from the PDB were used for training and as templates (https://files.wwpdb.org/pub/pdb/data/assemblies/mmCIF/; sequence clusters are available at https://cdn.rcsb.org/resources/sequence/clusters/clusters-by-entity-40.txt; sequence data are available at https://files.wwpdb.org/pub/pdb/derived_data/). Training used a version of the PDB downloaded 12 January 2023, while template search used a version downloaded 28 September 2022. We also used the Chemical Components Dictionary downloaded on 19 October 2023 (https://www.wwpdb.org/data/ccd). We show experimental structures from the PDB under accession numbers 7PZB (ref. 52), 7PNM (ref. 53), 7TQL (ref. 54), 7AU2 (ref. 55), 7U8C (ref. 56), 7URD (ref. 57), 7WUX (ref. 58), 7QIE (ref. 59), 7T82 (ref. 60), 7CTM (ref. 61), 8CVP (ref. 42), 8D7U (ref. 42), 7F60 (ref. 62), 8BTI (ref. 63), 7KZ9 (ref. 64), 7XFA (ref. 65), 7PEU (ref. 66), 7SDW (ref. 67), 7TNZ (ref. 68), 7R6R (ref. 69), 7USR (ref. 70) and 7Z1K (ref. 71). We also used the following publicly available databases for training or evaluation. Detailed usage is described in Supplementary Methods 2.2 and 2.5.2. UniRef90 v.2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/release-2020_01/uniref/), UniRef90 v.2020_03 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/release-2020_03/uniref/), UniRef90 v.2022_05 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/release-2022_05/uniref/), Uniclust30 v.2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), Uniclust30 v.2021_03 (https://wwwuser.gwdg.de/~compbiol/uniclust/2021_03/), MGnify clusters v.2018_12 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/), MGnify clusters v.2022_05 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2022_05/), BFD (https://bfd.mmseqs.com), RFam v.14.9 (https://ftp.ebi.ac.uk/pub/databases/Rfam/14.9/), RNAcentral v.21.0 (https://ftp.ebi.ac.uk/pub/databases/RNAcentral/releases/21.0/), Nucleotide Database (as of 23 February 2023) (https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nt.gz), JASPAR 2022 (https://jaspar.elixir.no/downloads/; see https://jaspar.elixir.no/profile-versions for version information), SELEX protein sequences from the supplementary tables of ref. 72 and SELEX protein sequences from the supplementary tables of ref. 73. Code availability</p> <p>AlphaFold\u20093 will be available as a non-commercial usage only server at https://www.alphafoldserver.com, with restrictions on allowed ligands and covalent modifications. Pseudocode describing the algorithms is available in the Supplementary Information. Code is not provided. References</p> <pre><code>Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583\u2013589 (2021).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nKreitz, J. et al. Programmable protein delivery with a bacterial contractile injection system. Nature 616, 357\u2013364 (2023).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLim, Y. et al. In silico protein interaction screening uncovers DONSON\u2019s role in replication initiation. Science 381, eadi3448 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nMosalaganti, S. et al. AI-based structure prediction empowers integrative structural analysis of human nuclear pores. Science 376, eabm9506 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nAnand, N. &amp; Achim, T. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. Preprint at arXiv https://doi.org/10.48550/arXiv.2205.15019 (2022).\n\nYang, Z., Zeng, X., Zhao, Y. &amp; Chen, R. AlphaFold2 and its applications in the fields of biology and medicine. Signal Transduct. Target. Ther. 8, 115 (2023).\n\nArticle\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nEvans, R. et al. Protein complex prediction with AlphaFold-Multimer. Preprint at bioRxiv https://doi.org/10.1101/2021.10.04.463034 (2022).\n\n\u017didek, A. AlphaFold v.2.3.0 Technical Note. GitHub https://github.com/google-deepmind/alphafold/blob/main/docs/technical_note_v2.3.0.md (2022).\n\nIsert, C., Atz, K. &amp; Schneider, G. Structure-based drug design with geometric deep learning. Curr. Opin. Struct. Biol. 79, 102548 (2023).\n\nLin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science 379, 1123\u20131130 (2023).\n\nArticle\n\nADS\n\nMathSciNet\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nBaek, M. et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science https://doi.org/10.1126/science.abj8754 (2021).\n\nWu, R. et al. High-resolution de novo structure prediction from primary sequence. Preprint at bioRxiv https://doi.org/10.1101/2022.07.21.500999 (2022).\n\nBryant, P., Pozzati, G. &amp; Elofsson, A. Improved prediction of protein-protein interactions using AlphaFold2. Nat. Commun. 13, 1265 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nMoriwaki, Y. Post on X. X https://x.com/Ag_smith/status/1417063635000598528?lang=en-GB (2021).\n\nBaek, M. Post on X. X https://x.com/minkbaek/status/1417538291709071362?lang=en (2021).\n\nQiao, Z. et al. State-specific protein\u2013ligand complex structure prediction with a multiscale deep generative model. Nat. Mach. Intell. 6, 195\u2013208 (2024).\n\nNakata, S., Mori, Y. &amp; Tanaka, S. End-to-end protein\u2013ligand complex structure generation with diffusion-based generative models. BMC Bioinform. 24, 233 (2023).\n\nArticle\n\nCAS\n\nGoogle Scholar\n\n\nBaek, M. et al. Accurate prediction of protein\u2013nucleic acid complexes using RoseTTAFoldNA. Nat. Methods 21, 117\u2013121 (2024).\n\nTownshend, R. J. L. et al. Geometric deep learning of RNA structure. Science 373, 1047\u20131051 (2021).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nJiang, D. et al. InteractionGraphNet: a novel and efficient deep graph representation learning framework for accurate protein-ligand interaction predictions. J. Med. Chem. 64, 18209\u201318232 (2021).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nJiang, H. et al. Predicting protein\u2013ligand docking structure with graph neural network. J. Chem. Inf. Model. https://doi.org/10.1021/acs.jcim.2c00127 (2022).\n\nCorso, G., St\u00e4rk, H., Jing, B., Barzilay, R. &amp; Jaakkola, T. DiffDock: diffusion steps, twists, and turns for molecular docking. Preprint at arXiv https://doi.org/10.48550/arXiv.2210.01776 (2022).\n\nSt\u00e4rk, H., Ganea, O., Pattanaik, L., Barzilay, D. &amp; Jaakkola, T. EquiBind: Geometric deep learning for drug binding structure prediction. In Proc. 39th International Conference on Machine Learning (eds Chaudhuri, K. et al.) 20503\u201320521 (PMLR, 2022).\n\nLiao, Z. et al. DeepDock: enhancing ligand-protein interaction prediction by a combination of ligand and structure information. In Proc. 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 311\u2013317 (IEEE, 2019).\n\nLu, W. et al. TANKBind: trigonometry-aware neural networks for drug-protein binding structure prediction. Adv. Neural Inf. Process. Syst. 35, 7236\u20137249 (2022).\n\nADS\n\nGoogle Scholar\n\n\nZhou, G. et al. Uni-Mol: a universal 3D molecular representation learning framework. Preprint at ChemRxiv https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581 (2023).\n\nShen, T. et al. E2Efold-3D: end-to-end deep learning method for accurate de novo RNA 3D structure prediction. Preprint at arXiv https://arxiv.org/abs/2207.01586 (2022).\n\nvan Dijk, M. &amp; Bonvin, A. M. J. J. Pushing the limits of what is achievable in protein\u2013DNA docking: benchmarking HADDOCK\u2019s performance. Nucleic Acids Res. 38, 5634\u20135647 (2010).\n\nArticle\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nKrishna, R. et al. Generalized biomolecular modeling and design with RoseTTAFold All-Atom. Science 384, eadl2528 (2024).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nButtenschoen, M., Morris, G. M. &amp; Deane, C. M. PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. Chem. Sci. 15, 3130\u20133139 (2024).\n\nDas, R. et al. Assessment of three-dimensional RNA structure prediction in CASP15. Proteins 91, 1747\u20131770 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nBerman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, 235\u2013242 (2000).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nKarras, T., Aittala, M., Aila, T. &amp; Laine, S. Elucidating the design space of diffusion-based generative models. Adv. Neural Inf. Process. Syst. 35, 26565\u201326577 (2022).\n\nWang, Y., Elhag, A. A., Jaitly, N., Susskind, J. M. &amp; Bautista, M. A. Generating molecular conformer fields. Preprint at arXiv https://doi.org/10.48550/arXiv.2311.17932 (2023).\n\nJi, Z., et al. Survey of hallucination in natural language generation. ACM Comput. Surv. 55, 248 (2023).\n\nDel Conte, A. et al. Critical assessment of protein intrinsic disorder prediction (CAID)\u2014results of round 2. Proteins 91, 1925\u20131934 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nTrott, O. &amp; Olson, A. J. AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J. Comput. Chem. 31, 455\u2013461 (2010).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nMiller, E. B. et al. Reliable and accurate solution to the induced fit docking problem for protein\u2013ligand binding. J. Chem. Theory Comput. https://doi.org/10.1021/acs.jctc.1c00136 (2021).\n\nChen, K., Zhou, Y., Wang, S. &amp; Xiong, P. RNA tertiary structure modeling with BRiQ potential in CASP15. Proteins 91, 1771\u20131778 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nBasu, S. &amp; Wallner, B. DockQ: a quality measure for protein-protein docking models. PLoS ONE 11, e0161879 (2016).\n\nArticle\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nZhang, Y. &amp; Skolnick, J. Scoring function for automated assessment of protein structure template quality. Proteins 57, 702\u2013710 (2004).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nWatson, E. R. et al. Molecular glue CELMoD compounds are regulators of cereblon conformation. Science 378, 549\u2013553 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nWayment-Steele, H. K. et al. Predicting multiple conformations via sequence clustering and AlphaFold2. Nature 625, 832\u2013839 (2024).\n\ndel Alamo, D., Sala, D., Mchaourab, H. S. &amp; Meiler, J. Sampling alternative conformational states of transporters and receptors with AlphaFold2. eLife https://doi.org/10.7554/eLife.75751 (2022).\n\nHeo, L. &amp; Feig, M. Multi-state modeling of G-protein coupled receptors at experimental accuracy. Proteins 90, 1873\u20131885 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nWallner, B. AFsample: improving multimer prediction with AlphaFold using massive sampling. Bioinformatics 39, btad573 (2023).\n\nMariani, V., Biasini, M., Barbato, A. &amp; Schwede, T. lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics 29, 2722\u20132728 (2013).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nZemla, A. LGA: A method for finding 3D similarities in protein structures. Nucleic Acids Res. 31, 3370\u20133374 (2003).\n\nWu, T., Hou, J., Adhikari, B. &amp; Cheng, J. Analysis of several key factors influencing deep learning-based inter-residue contact prediction. Bioinformatics 36, 1091\u20131098 (2020).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nDiMaio, F. RF2NA v.0.2. GitHub https://github.com/uw-ipd/RoseTTAFold2NA/releases/tag/v0.2 (2023).\n\nButtenschoen, M. PoseBusters v.0.2.7. GitHub https://github.com/maabuu/posebusters/releases/tag/v0.2.7 (2023).\n\nWerel, L. et al. Structural basis of dual specificity of Sinorhizobium meliloti Clr, a cAMP and cGMP receptor protein. MBio 14, e0302822 (2023).\n\nArticle\n\nPubMed\n\nGoogle Scholar\n\n\nWang, C. et al. Antigenic structure of the human coronavirus OC43 spike reveals exposed and occluded neutralizing epitopes. Nat. Commun. 13, 2921 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLapointe, C. P. et al. eIF5B and eIF1A reorient initiator tRNA to allow ribosomal subunit joining. Nature 607, 185\u2013190 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nWilson, L. F. L. et al. The structure of EXTL3 helps to explain the different roles of bi-domain exostosins in heparan sulfate synthesis. Nat. Commun. 13, 3314 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLiu, X. et al. Highly active CAR T cells that bind to a juxtamembrane region of mesothelin and are not blocked by shed mesothelin. Proc. Natl Acad. Sci. USA 119, e2202439119 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLiu, Y. et al. Mechanisms and inhibition of Porcupine-mediated Wnt acylation. Nature 607, 816\u2013822 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nKurosawa, S. et al. Molecular basis for enzymatic aziridine formation via sulfate elimination. J. Am. Chem. Soc. 144, 16164\u201316170 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nBoffey, H. K. et al. Development of selective phosphatidylinositol 5-phosphate 4-kinase \u03b3 inhibitors with a non-ATP-competitive, allosteric binding mode. J. Med. Chem. 65, 3359\u20133370 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nBuckley, P. T. et al. Multivalent human antibody-centyrin fusion protein to prevent and treat Staphylococcus aureus infections. Cell Host Microbe 31, 751\u2013765 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nMohapatra, S. B. &amp; Manoj, N. Structural basis of catalysis and substrate recognition by the NAD(H)-dependent \u03b1-d-glucuronidase from the glycoside hydrolase family 4. Biochem. J. 478, 943\u2013959 (2021).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nGao, X. et al. Structural basis for Sarbecovirus ORF6 mediated blockage of nucleocytoplasmic transport. Nat. Commun. 13, 4782 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nAtkinson, B. N. et al. Designed switch from covalent to non-covalent inhibitors of carboxylesterase Notum activity. Eur. J. Med. Chem. 251, 115132 (2023).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLuo, S. et al. Structural basis for a bacterial Pip system plant effector recognition protein. Proc. Natl Acad. Sci. USA 118, e2019462118 (2021).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nLiu, C. et al. Identification of monosaccharide derivatives as potent, selective, and orally bioavailable inhibitors of human and mouse galectin-3. J. Med. Chem. 65, 11084\u201311099 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nDombrowski, M., Engeholm, M., Dienemann, C., Dodonova, S. &amp; Cramer, P. Histone H1 binding to nucleosome arrays depends on linker DNA length and trajectory. Nat. Struct. Mol. Biol. 29, 493\u2013501 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nVecchioni, S. et al. Metal-mediated DNA nanotechnology in 3D: structural library by templated diffraction. Adv. Mater. 35, e2210938 (2023).\n\nArticle\n\nPubMed\n\nGoogle Scholar\n\n\nWang, W. &amp; Pyle, A. M. The RIG-I receptor adopts two different conformations for distinguishing host from viral RNA ligands. Mol. Cell 82, 4131\u20134144 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nMcGinnis, R. J. et al. A monomeric mycobacteriophage immunity repressor utilizes two domains to recognize an asymmetric DNA sequence. Nat. Commun. 13, 4105 (2022).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nDietrich, M. H. et al. Nanobodies against Pfs230 block Plasmodium falciparum transmission. Biochem. J. 479, 2529\u20132546 (2022).\n\nArticle\n\nCAS\n\nPubMed\n\nGoogle Scholar\n\n\nAppel, L.-M. et al. The SPOC domain is a phosphoserine binding module that bridges transcription machinery with co- and post-transcriptional regulators. Nat. Commun. 14, 166 (2023).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nYin, Y. et al. Impact of cytosine methylation on DNA binding specificities of human transcription factors. Science 356, eaaj2239 (2017).\n\nArticle\n\nPubMed\n\nPubMed Central\n\nGoogle Scholar\n\n\nJolma, A. et al. DNA-dependent formation of transcription factor pairs alters their binding specificity. Nature 527, 384\u2013388 (2015).\n\nArticle\n\nADS\n\nCAS\n\nPubMed\n\nGoogle Scholar\n</code></pre> <p>Download references Acknowledgements</p> <p>We thank G. Arena, \u017d. Avsec, A. Baryshnikov, R. Bates, M. Beck, A. Bond, N. Bradley-Schmieg, J. Cavojska, B. Coppin, E. Dupont, S. Eddy, M. Fiscato, R. Green, D. Hariharan, K. Holsheimer, N. Hurley, C. Jones, K. Kavukcuoglu, J. Kelly, E. Kim, A. Koivuniemi, O. Kovalevskiy, D. Lasecki, M. Last, A. Laydon, W. McCorkindale, S. Miller, A. Morris, L. Nicolaisen, E. Palmer, A. Paterson, S. Petersen, O. Purkiss, C. Shi, G. Thomas, G. Thornton and H. Tomlinson for their contributions. Author information Author notes</p> <pre><code>These authors contributed equally: Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore\n\nThese authors jointly supervised this work: Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, John M. Jumper\n</code></pre> <p>Authors and Affiliations</p> <pre><code>Core Contributor, Google DeepMind, London, UK\n\nJosh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Sebastian W. Bodenstein, David A. Evans, Michael O\u2019Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvil\u0117 \u017demgulyt\u0117, Victor Bapst, Pushmeet Kohli, Demis Hassabis &amp; John M. Jumper\n\nCore Contributor, Isomorphic Labs, London, UK\n\nJoshua Bambrick, Chia-Chun Hung, Max Jaderberg &amp; Demis Hassabis\n\nGoogle DeepMind, London, UK\n\nEirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Kuba Perlin, Anna Potapenko, Sukhdeep Singh, Ashok Thillaisundaram, Ellen D. Zhong, Michal Zielinski &amp; Augustin \u017d\u00eddek\n\nIsomorphic Labs, London, UK\n\nAlexey Cherepanov, Miles Congreve, Caroline M. R. Low, Pascal Savy, Adrian Stecula, Catherine Tong &amp; Sergei Yakneen\n\nDepartment of Molecular and Cellular Physiology, Stanford University, Stanford, CA, USA\n\nYousuf A. Khan\n\nDepartment of Computer Science, Princeton University, Princeton, NJ, USA\n\nEllen D. Zhong\n</code></pre> <p>Contributions</p> <p>The equally contributing authors are alphabetically ordered, as are the remaining core contributor authors (excluding jointly supervising authors) and similar for all remaining non-supervising authors. D.H., M.J. and J.M.J. led the research. M.J., J.M.J. and P.K. developed research strategy. J. Abramson, V.B., T.G. and C.-C.H. led key research pillars. T.G. and A. \u017didek led the technical framework for research. O.B., H.G. and S.S. coordinated and managed the research project. J. Abramson, J. Adler, E.A., A.J.B., J.B., V.B., A.I.C.-R., J.D., R.E., D.A.E., M.F., F.B.F., T.G., C.-C.H., M.J., J.M.J., Y.A.K., A. Potapenko, A. Pritzel, D.R., O.R., A.T., C.T., K.T., L.W., Z.W. and E.D.Z. developed the neural network architecture and training procedure. J. Abramson, A.J.B., J.B., V.B., C.B., S.W.B., A.B., A. Cherepanov, A.I.C.-R., A. Cowie, J.D., T.G., R.J., M.O., K.P., D.R., O.R., M.Z., A. \u017demgulyt\u0117 and A. \u017d\u00eddek developed the training, inference, data and evaluation infrastructure. J. Abramson, J. Adler, A.J.B., V.B., A.I.C.-R., R.E., D.A.E., T.G., D.H., M.J., J.M.J., P.K., K.P., A. Pritzel, O.R., P.S., S.S., A.S., K.T. and L.W. contributed to the writing of the paper. M.C., C.M.R.L. and S.Y. advised on the project. Corresponding authors</p> <p>Correspondence to Max Jaderberg, Demis Hassabis or John M. Jumper. Ethics declarations Competing interests</p> <p>Author-affiliated entities have filed US provisional patent applications including 63/611,674, 63/611,638 and 63/546,444 relating to predicting 3D structures of molecule complexes using embedding neural networks and generative models. All of the authors other than A.B., Y.A.K. and E.D.Z. have commercial interests in the work described. Peer review Peer review information</p> <p>Nature thanks Justas Dapkunas, Roland Dunbrack and Hashim Al-Hashimi for their contribution to the peer review of this work. Additional information</p> <p>Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Extended data figures and tables Extended Data Fig. 1 Disordered region prediction.</p> <p>a, Example prediction for a disordered protein from AlphaFoldMultimer v2.3, AlphaFold 3, and AlphaFold 3 trained without the disordered protein PDB cross distillation set. Protein is DP02376 from the CAID 2 (Critical Assessment of protein Intrinsic Disorder prediction) set. Predictions coloured by pLDDT (orange: pLDDT &lt;= 50, yellow: 50 &lt; pLDDT &lt;= 70, light blue: 70 &lt; pLDDT &lt;= 90, and dark blue: 90 &lt;= pLDDT &lt; 100). b, Predictions of disorder across residues in proteins in the CAID 2 set, which are also low homology to the AF3 training set. Prediction methods include RASA (relative accessible surface area) and pLDDT (N = 151 proteins; 46,093 residues). Extended Data Fig. 2 Accuracy across training.</p> <p>Training curves for initial training and fine tuning showing LDDT (local distance difference test) on our evaluation set as a function of optimizer steps. One optimizer step uses a mini batch of 256 trunk samples and during initial training 256 * 48 = 12,288 diffusion samples. For fine tuning the number of diffusion samples is reduced to 256 * 32 = 8,192. The scatter plot shows the raw data points and the lines show the smoothed performance using a median filter with a kernel width of 9 data points. The dashed lines mark the points where the smoothed performance passes 90% and 97% of the initial training maximum for the first time. Extended Data Fig. 3 AlphaFold 3 predictions of PoseBusters examples for which Vina and Gold were inaccurate.</p> <p>Predicted protein chains are shown in blue, predicted ligands in orange, and ground truth in grey. a, Human Notum bound to inhibitor ARUK3004556 (PDB ID 8BTI, ligand RMSD: 0.65 \u00c5). b, Pseudomonas sp. PDC86 Aapf bound to HEHEAA (PDB ID 7KZ9, ligand RMSD: 1.3 \u00c5). c, Human Galectin-3 carbohydrate-recognition domain in complex with compound 22 (PDB ID 7XFA, ligand RMSD: 0.44 \u00c5). Extended Data Fig. 4 PoseBusters analysis.</p> <p>a, Comparison of AlphaFold 3 and baseline method protein-ligand binding success on the PoseBusters Version 1 benchmark set (V1, August 2023 release). Methods classified by the extent of ground truth information used to make predictions. Note all methods that use pocket residue information except for UMol and AF3 also use ground truth holo protein structures. b, PoseBusters Version 2 (V2, November 2023 release) comparison between the leading docking method Vina and AF3 2019 (two-sided Fisher exact test, N = 308 targets, p = 2.3 * 10\u22128). c, PoseBusters V2 results of AF3 2019 on targets with low, moderate, and high protein sequence homology (integer ranges indicate maximum sequence identity with proteins in the training set). d, PoseBusters V2 results of AF3 2019 with ligands split by those characterized as \u201ccommon natural\u201d ligands and others. \u201cCommon natural\u201d ligands are defined as those which occur greater than 100 times in the PDB and which are not non-natural (by visual inspection). A full list may be found in Supplementary Table 15. Dark bar indicates RMSD &lt; 2 \u00c5 and passing PoseBusters validity checks (PB-valid). e, PoseBusters V2 structural accuracy and validity. Dark bar indicates RMSD &lt; 2 \u00c5 and passing PoseBusters validity checks (PB-valid). Light hashed bar indicates RMSD &lt; 2 \u00c5 but not PB valid. f, PoseBusters V2 detailed validity check comparison. Error bars indicate exact binomial distribution 95% confidence intervals. N = 427 targets for RoseTTAFold All-Atom and 428 targets for all others in Version 1; 308 targets in Version 2. Extended Data Fig. 5 Nucleic acid prediction accuracy and confidences.</p> <p>a, CASP15 RNA prediction accuracy from AIChemy_RNA (the top AI-based submission), RoseTTAFold2NA (the AI-based method capable of predicting proteinRNA complexes), and AlphaFold 3. Ten of the 13 targets are available in the PDB or via the CASP15 website for evaluation. Predictions are downloaded from the CASP website for external models. b, Accuracy on structures containing low homology RNA-only or DNA-only complexes from the recent PDB evaluation set. Comparison between AlphaFold 3 and RoseTTAFold2NA (RF2NA) (RNA: N = 29 structures, paired Wilcoxon signed-rank test, p\u2009=\u20091.6 * 10\u22127; DNA: N\u2009=\u200963 structures, paired two-sided Wilcoxon signed-rank test, p\u2009=\u20095.2 * 10\u221212). Note RF2NA was only trained and evaluated on duplexes (chains forming at least 10 hydrogen bonds), but some DNA structures in this set may not be duplexes. Box, centerline, and whiskers boundaries are at (25%, 75%) intervals, median, and (5%, 95%) intervals. c Predicted structure of a mycobacteriophage immunity repressor protein bound to double stranded DNA (PDB ID 7R6R), coloured by pLDDT (left; orange: 0\u201350, yellow: 50\u201370, cyan 70\u201390, and blue 90\u2013100) and chain id (right). Note the disordered N-terminus not entirely shown. d, Predicted aligned error (PAE) per token-pair for the prediction in c with rows and columns labelled by chain id and green gradient indicating PAE. Extended Data Fig. 6 Analysis and examples for modified proteins and nucleic acids.</p> <p>a, Accuracy on structures. containing common phosphorylation residues (SEP, TPO, PTR, NEP, HIP) from the recent PDB evaluation set. Comparison between AlphaFold 3 with phosphorylation modelled, and AlphaFold 3 without modelling phosphorylation (N = 76 clusters, paired two-sided Wilcoxon signed-rank test, p\u2009=\u20091.6 * 10\u22124). Note, to predict a structure without modelling phosphorylation, we predict the parent (standard) residue in place of the modification. AlphaFold 3 generally achieves better backbone accuracy when modelling phosphorylation. Error bars indicate exact binomial distribution 95% confidence intervals. b, SPOC domain of human SHARP in complex with phosphorylated RNA polymerase II C-terminal domain (PDB ID 7Z1K), predictions coloured by pLDDT (orange: 0\u201350, yellow: 50\u201370, cyan 70\u201390, and blue 90\u2013100). Left: Phosphorylation modelled (mean pocket-aligned RMSDC\u03b1 2.104 \u00c5). Right: Without modelling phosphorylation (mean pocketaligned RMSDC\u03b1 10.261 \u00c5). When excluding phosphorylation, AlphaFold 3 provides lower pLDDT confidence on the phosphopeptide. c, Structure of parkin bound to two phospho-ubiquitin molecules (PDB ID 7US1), predictions similarly coloured by pLDDT. Left: Phosphorylation modelled (mean pocket-aligned RMSDC\u03b1 0.424 \u00c5). Right: Without modelling phosphorylation (mean pocket-aligned RMSDC\u03b1 9.706 \u00c5). When excluding phosphorylation, AlphaFold 3 provides lower pLDDT confidence on the interface residues of the incorrectly predicted ubiquitin. d, Example structures with modified nucleic acids. Left: Guanosine monophosphate in RNA (PDB ID 7TNZ, mean pocket-aligned modified residue RMSD 0.840 \u00c5). Right: Methylated DNA cytosines (PDB ID 7SDW, mean pocket-aligned modified residue RMSD 0.502 \u00c5). Welabel residues of the predicted structure for reference. Ground truth structure in grey; predicted protein in blue, predicted RNA in purple, predicted DNA in magenta, predicted ions in orange, with predicted modifications highlighted via spheres. Extended Data Fig. 7 Model accuracy with MSA size and number of seeds.</p> <p>a, Effect of MSA depth on protein prediction accuracy. Accuracy is given as single chain LDDT score and MSA depth is computed by counting the number of non-gap residues for each position in the MSA using the Neff weighting scheme and taking the median across residues (see Methods for details on Neff). MSA used for AF-M 2.3 differs slightly from AF3; the data uses the AF3 MSA depth for both to make the comparison clearer. The analysis uses every protein chain in the low homology Recent PDB set, restricted to chains in complexes with fewer than 20 protein chains and fewer than 2,560 tokens (see Methods for details on Recent PDB set and comparisons to AF-M 2.3). The curves are obtained through Gaussian kernel average smoothing (window size is 0.2 units in log10(Neff)); the shaded area is the 95% confidence interval estimated using bootstrap of 10,000 samples. b, Increase in ranked accuracy with number of seeds for different molecule types. Predictions are ranked by confidence, and only the most confident per interface is scored. Evaluated on the low homology recent PDB set, filtered to less than 1,536 tokens. Number of clusters evaluated: dna-intra = 386, protein-intra = 875, rnaintra = 78, protein-dna = 307, protein-rna = 102, protein-protein (antibody = False) = 697, protein-protein (antibody = True) = 58. Confidence intervals are 95% bootstraps over 1,000 samples. Extended Data Fig. 8 Relationship between confidence and accuracy for protein interactions with ions, bonded ligands and bonded glycans.</p> <p>Accuracy is given as the percentage of interface clusters under various pocket-aligned RMSD thresholds, as a function of the chain pair ipTM of the interface. The ions group includes both metals and nonmetals. N values report the number of clusters in each band. For a similar analysis on general ligand-protein interfaces, see Fig. 4 of main text. Extended Data Fig. 9 Correlation of DockQ and iLDDT for protein-protein interfaces.</p> <p>One data point per cluster, 4,182 clusters shown. Line of best fit with a Huber regressor with epsilon 1. DockQ categories correct (&gt;0.23), and very high accuracy (&gt;0.8) correspond to iLDDTs of 23.6 and 77.6 respectively. Extended Data Table 1 Prediction accuracy across biomolecular complexes Full size table Supplementary information Supplementary Information</p> <p>Supplementary Information 1 (notation), 2 (data pipeline), 3 (model architecture), 4 (auxiliary heads), 5 (training and inference), 6 (evaluation), 7 (differences to AlphaFold2 and AlphaFold-Multimer), 8 (Supplementary Results) and 9 (Appendix, including CCD Code and PDB ID tables). Reporting Summary Supplementary Data</p> <p>AF3 predictions for the Posebusters benchmark and outputs of the Posebusters checks for those predictions. Rights and permissions</p> <p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.</p> <p>Reprints and permissions About this article Check for updates. Verify currency and authenticity via CrossMark Cite this article</p> <p>Abramson, J., Adler, J., Dunger, J. et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature 630, 493\u2013500 (2024). https://doi.org/10.1038/s41586-024-07487-w</p> <p>Download citation</p> <pre><code>Received19 December 2023\n\nAccepted29 April 2024\n\nPublished08 May 2024\n\nIssue Date13 June 2024\n\nDOIhttps://doi.org/10.1038/s41586-024-07487-w\n</code></pre> <p>Share this article</p> <p>Anyone you share the following link with will be able to read this content:</p> <p>Provided by the Springer Nature SharedIt content-sharing initiative Subjects</p> <pre><code>Drug discovery\nMachine learning\nProtein structure predictions\nStructural biology\n</code></pre> <p>Download PDF Associated content</p> <p>Collection Nobel Prize in Chemistry 2024</p> <p>Nature Outlook Robotics and artificial intelligence Alphafold 3.0: the AI protein predictor gets an upgrade</p> <pre><code>Benjamin ThompsonNick Petri\u0107 Howe\n</code></pre> <p>Nature Nature Podcast 08 May 2024 AlphaFold3 takes a step toward decoding molecular behavior and biological computation</p> <pre><code>Rohit RoyHashim M. Al-Hashimi\n</code></pre> <p>Nature Structural &amp; Molecular Biology News &amp; Views 08 Jul 2024 The protein universe in 3D</p> <pre><code>Isaac EllmenMatthew I. J. RaybouldCharlotte M. Deane\n</code></pre> <p>Nature Chemical Biology News &amp; Views 24 Dec 2024</p> <pre><code>Abstract\nMain\nNetwork architecture and training\nAccuracy across complex types\nPredicted confidences track accuracy\nModel limitations\nDiscussion\nMethods\nData availability\nCode availability\nReferences\nAcknowledgements\nAuthor information\nEthics declarations\nPeer review\nAdditional information\nExtended data figures and tables\nSupplementary information\nRights and permissions\nAbout this article\n</code></pre> <p>Nature (Nature) ISSN 1476-4687 (online) ISSN 0028-0836 (print) nature.com sitemap About Nature Portfolio</p> <pre><code>About us\nPress releases\nPress office\nContact us\n</code></pre> <p>Discover content</p> <pre><code>Journals A-Z\nArticles by subject\nprotocols.io\nNature Index\n</code></pre> <p>Publishing policies</p> <pre><code>Nature portfolio policies\nOpen access\n</code></pre> <p>Author &amp; Researcher services</p> <pre><code>Reprints &amp; permissions\nResearch data\nLanguage editing\nScientific editing\nNature Masterclasses\nResearch Solutions\n</code></pre> <p>Libraries &amp; institutions</p> <pre><code>Librarian service &amp; tools\nLibrarian portal\nOpen research\nRecommend to library\n</code></pre> <p>Advertising &amp; partnerships</p> <pre><code>Advertising\nPartnerships &amp; Services\nMedia kits\nBranded content\n</code></pre> <p>Professional development</p> <pre><code>Nature Careers\nNature Conferences\n</code></pre> <p>Regional websites</p> <pre><code>Nature Africa\nNature China\nNature India\nNature Italy\nNature Japan\nNature Middle East\n\nPrivacy Policy Use of cookies\n\nLegal notice Accessibility statement Terms &amp; Conditions Your US state privacy rights\n</code></pre> <p>Springer Nature</p> <p>\u00a9 2025 Springer Nature Limited Nature Briefing: Translational Research</p> <p>Sign up for the Nature Briefing: Translational Research newsletter \u2014 top stories in biotechnology, drug discovery and pharma. Email address I agree my information will be processed in accordance with the Nature and Springer Nature Limited Privacy Policy.</p>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/","title":"\ud83d\ude80 AlphaFold 3 RNA Structure Prediction: Detailed Progress &amp; Comprehensive Action Plan","text":""},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#introduction","title":"\ud83d\udccc Introduction","text":"<p>Your project demonstrates significant progress toward replicating AlphaFold\u202f3 (AF3) for RNA and biomolecular complex structure prediction. Your modular, organized codebase distinctly highlights completed elements and clearly identifies areas requiring further development.</p>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#achievements-current-implementation","title":"\u2705 Achievements &amp; Current Implementation","text":""},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#data-feature-preparation","title":"\ud83d\udcc2 Data &amp; Feature Preparation","text":"<ul> <li> <p>Streaming Dataset Approach:</p> <ul> <li>Implemented via <code>dataset_loader.py</code>, leveraging Hugging Face\u2019s <code>bprna-spot</code> dataset for RNA-specific training and benchmarks.</li> </ul> </li> <li> <p>Synthetic Feature Dictionaries:</p> <ul> <li>Clearly structured synthetic features (<code>ref_pos</code>, <code>ref_charge</code>, <code>ref_element</code>) in <code>main.py</code> and <code>benchmark.py</code>, facilitating debugging and performance validation.</li> </ul> </li> <li> <p>Atom &amp; Token Representation:</p> <ul> <li>Atom-to-token strategy (<code>atom_to_token</code>) accurately mirrors AF3's methodology\u2014standard nucleotides (A, C, G, U) use single tokens, whereas non-standard residues and ligands use per-atom tokens.</li> </ul> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#atomattentionencoder-inputfeatureembedder","title":"\u2699\ufe0f AtomAttentionEncoder &amp; InputFeatureEmbedder","text":"<ul> <li> <p>Sequence-local Atom Attention:</p> <ul> <li>Implemented in <code>atom_transformer.py</code> using a local block-sparse attention mechanism (<code>block_sparse.py</code>), aligning closely with AF3\u2019s approach.</li> </ul> </li> <li> <p>Per-atom \u2192 Token Aggregation:</p> <ul> <li>Atom embeddings aggregated via <code>scatter_mean</code> into tokens, matching AF3's approach precisely.</li> </ul> </li> <li> <p>Trunk Recycling Stubs:</p> <ul> <li>Placeholders (<code>trunk_sing</code>, <code>trunk_pair</code>) in place to support AF2/AF3 recycling concepts.</li> </ul> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#code-organization-benchmarks","title":"\ud83d\udee0\ufe0f Code Organization &amp; Benchmarks","text":"<ul> <li> <p>Directory Structure:</p> <ul> <li>Organized directories: <code>benchmarks/</code>, <code>models/</code>, <code>scripts/</code>, <code>utils/</code>, and main demonstration (<code>main.py</code>).</li> </ul> </li> <li> <p>Benchmark Scripts:</p> <ul> <li>Comprehensive benchmarks (<code>benchmark_input_embedding()</code>, <code>benchmark_decoding_latency_and_memory()</code>) measuring forward/backward pass efficiency and GPU memory usage.</li> </ul> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#comparison-with-af3-pipeline-detailed-gaps-required-implementations","title":"\ud83d\udd0d Comparison with AF3 Pipeline: Detailed Gaps &amp; Required Implementations","text":""},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#data-pipeline-multi-dataset-training","title":"\ud83d\udcda Data Pipeline &amp; Multi-dataset Training","text":"<p>Current Status:</p> <ul> <li> <p>Single dataset (<code>bprna-spot</code>) loader without multi-dataset weighting or advanced cropping.</p> </li> <li> <p>Lacks genetic database searches (jackhmmer/nhmmer) for MSA/template.</p> </li> </ul> <p>Required Implementations:</p> <ul> <li> <p>Integrate diverse datasets explicitly: Weighted PDB chains/interfaces, MGnify monomers, Rfam RNA, disordered predictions, and transcription factors.</p> </li> <li> <p>Implement advanced cropping methods: contiguous, spatial, and interface-based.</p> </li> <li> <p>Optionally add genetic database template searches.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#msa-module","title":"\ud83e\uddec MSA Module","text":"<p>Current Status: - Basic MSA feature embedding without a dedicated module.</p> <p>Required Implementations:</p> <ul> <li> <p>Develop an explicit <code>MsaModule</code> performing row-wise attention and merging into pair representations.</p> </li> <li> <p>Optionally include <code>TemplateEmbedder</code> for single-chain templates.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#pairformer-stack","title":"\ud83e\udde9 Pairformer Stack","text":"<p>Current Status: - Basic pair embedding implementation without complete triangular updates.</p> <p>Required Implementations:</p> <ul> <li> <p>Full Pairformer stack (~48 blocks), explicitly including:</p> </li> <li> <p><code>TriangleMultiplicationOutgoing/Incoming</code></p> </li> <li> <p>Triangular self-attention mechanisms</p> </li> <li> <p>Single representation updated via pair-bias attention (<code>AttentionPairBias</code>).</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#diffusion-head-generative-module","title":"\ud83c\udf2b\ufe0f Diffusion Head (Generative Module)","text":"<p>Current Status: - No generative diffusion-based module present.</p> <p>Required Implementations:</p> <ul> <li> <p>Explicit generative <code>DiffusionModule</code> for coordinate prediction via multi-step denoising.</p> </li> <li> <p>Training strategy: replicate trunk embeddings (~48 noisy seeds per mini-batch), alignment-based MSE, bond length penalties, and smooth LDDT loss.</p> </li> <li> <p>Mini-rollouts for supporting confidence predictions.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#confidence-heads-plddt-pae-pde-distogram","title":"\ud83c\udfaf Confidence Heads (pLDDT, PAE, PDE, Distogram)","text":"<p>Current Status: - Currently missing confidence evaluation modules.</p> <p>Required Implementations:</p> <ul> <li> <p>Explicitly develop ConfidenceHeads:</p> </li> <li> <p>pLDDT (per-atom local confidence)</p> </li> <li> <p>PAE (pairwise alignment error)</p> </li> <li> <p>PDE (pairwise distance error)</p> </li> <li> <p>Distogram (token-to-token distances)</p> </li> <li> <p>Experimentally resolved prediction flags</p> </li> <li> <p>Initial implementation: prioritize pLDDT and PDE for immediate confidence estimation.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#multi-stage-training-routines","title":"\ud83d\uddd3\ufe0f Multi-Stage Training Routines","text":"<p>Current Status: - Demonstration-level routines only.</p> <p>Required Implementations:</p> <ul> <li> <p>Explicit multi-stage training pipeline clearly following AF3's progression: tokens progressing from 384 \u2192 640 \u2192 768 \u2192 final PAE evaluation stage.</p> </li> <li> <p>Explicit weighted mixture (~50% Weighted PDB, ~50% distillation datasets).</p> </li> <li> <p>Large-batch diffusion training strategy (trunk executed once per batch with ~48 diffusion iterations).</p> </li> <li> <p>Integrate memory optimizations (multi-GPU training or gradient checkpointing).</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#comprehensive-action-items","title":"\ud83d\uddd2\ufe0f Comprehensive Action Items","text":""},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#1-data-pipeline","title":"\ud83d\udccc 1. Data Pipeline","text":"<ul> <li> <p>Integrate Weighted PDB structures, MGnify, Rfam, transcription factors, and disordered predictions explicitly.</p> </li> <li> <p>Advanced cropping methods clearly implemented.</p> </li> <li> <p>Optional genetic database searches (jackhmmer/nhmmer).</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#2-msa-pairformer","title":"\ud83e\uddec\ud83e\udde9 2. MSA &amp; Pairformer","text":"<ul> <li> <p>Develop explicit <code>MsaModule</code> for MSA integration.</p> </li> <li> <p>Construct a complete Pairformer stack (~48 blocks), including TriangleMultiplication, TriangleAttention, Transition blocks, and pair-bias attention.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#3-diffusion-module","title":"\ud83c\udf2b\ufe0f 3. Diffusion Module","text":"<ul> <li> <p>Build explicit diffusion generative head for multi-step coordinate predictions.</p> </li> <li> <p>Detailed training strategy with alignment-based losses, bond constraints, and confidence-supporting mini-rollouts.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#4-confidence-heads","title":"\ud83c\udfaf 4. Confidence Heads","text":"<ul> <li>Implement comprehensive ConfidenceHeads explicitly: pLDDT, PDE, PAE, Distogram, and experimentally resolved flags.</li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#5-multi-stage-training","title":"\ud83d\uddd3\ufe0f 5. Multi-stage Training","text":"<ul> <li> <p>Clearly structured multi-stage training script following explicit AF3 progression.</p> </li> <li> <p>Data mixture clearly weighted.</p> </li> <li> <p>Explicit large-batch diffusion implementation.</p> </li> <li> <p>Incorporate memory optimization techniques explicitly.</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#concluding-remarks","title":"\ud83c\udf89 Concluding Remarks","text":""},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#your-project-foundation-is-robust-notably","title":"Your project foundation is robust, notably:","text":"<ul> <li> <p>AtomAttentionEncoder with local atom attention</p> </li> <li> <p>Block-sparse memory-efficient implementation</p> </li> <li> <p>Modular and structured codebase</p> </li> </ul>"},{"location":"reference/advanced_methods/af3/AlphaFold3_progress/#to-fully-replicate-af3-explicitly-implement","title":"To fully replicate AF3, explicitly implement:","text":"<ul> <li> <p>Complete Pairformer stack</p> </li> <li> <p>Dedicated MSA integration module</p> </li> <li> <p>Generative Diffusion module</p> </li> <li> <p>Detailed ConfidenceHeads</p> </li> <li> <p>Structured multi-stage training routines</p> </li> </ul> <p>Your progress is impressive\u2014continue the excellent work! \ud83c\udf1f</p>"},{"location":"reference/advanced_methods/af3/Pairwise_Distance_Based_Prediction/","title":"Distance Prediction","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d: \u201cHere\u2019s an overview of how AlphaFold 3\u2019s 48-block Pairformer stack works\u2014covering the triangular updates and the single-representation-with-pair-bias piece\u2014plus how it all supports \u2018pairwise distance\u2019 prediction.\u201d</p> <p>[emoji]: Detailed Explanation</p> <p>Below is a step-by-step look at the \u201cPairformer\u201d trunk that AF3 uses after the MSA Module. Each of the 48 Pairformer blocks transforms an (N \\times N) pair-representation array ({z_{ij}}) plus a single-representation vector for each token ({s_i}). In total:     1.  Pair Representation:     \u2022   An (N \\times N)-shaped representation where each entry \\mathbf{z}_{ij} (the \u201cpair embedding\u201d) encodes how token i (residue or ligand atom) relates to token j.     \u2022   These pair embeddings are continuously refined with \u201ctriangle updates\u201d (both multiplicative and attention-based).     2.  Single Representation:     \u2022   A length-N array {\\mathbf{s}_i} whose elements store information about each token in isolation (e.g., summarizing the local environment, MSA context, etc.).     \u2022   AF3 processes that single representation side-by-side with the pair representation, letting them exchange information.</p> <p>\u2e3b</p> <p>(A) Triangular Multiplication (Outgoing &amp; Incoming)</p> <p>Each block begins with two multiplicative updates on the pair array:     1.  TriangleMultiplicationOutgoing (\\text{\u201coutgoing\u201d edges})     2.  TriangleMultiplicationIncoming (\\text{\u201cincoming\u201d edges})</p> <p>Conceptually, the pair embedding \\mathbf{z}{ij} is viewed as an \u201cedge\u201d connecting node i and node j. The \u201ctriangle\u201d part means each edge can be updated by looking at a shared vertex k.     \u2022   Outgoing means: \u201cUpdate \\mathbf{z} by considering edges from i to k and j to k.\u201d     \u2022   Incoming means: \u201cUpdate \\mathbf{z}_{ij} by considering edges from k to i and k to j.\u201d</p> <p>Each such update is a learned transformation that ensures geometric consistency among triplets (i,j,k). In simpler terms, it fuses information from the \u2018third node\u2019 k, telling \\mathbf{z}_{ij} how i\\rightarrow k and j\\rightarrow k compare or multiply.</p> <p>\u2e3b</p> <p>(B) Triangular Self-Attention (Starting &amp; Ending Node)</p> <p>Next come two specialized attention layers:     1.  TriangleAttentionStartingNode     2.  TriangleAttentionEndingNode</p> <p>They again treat \\mathbf{z}{ij} like edges in a complete graph. One triangular attention uses i as the \u201cfocal\u201d node, scanning across possible k to update \\mathbf{z}{ij}. The other uses j as the focus.     \u2022   \u201cStarting node\u201d means each edge (i \\to j) pays attention to edges (i \\to k).     \u2022   \u201cEnding node\u201d means the edge (i \\to j) pays attention to edges (k \\to j).</p> <p>Hence it is a form of pairwise attention that ensures each pair embedding \\mathbf{z}_{ij} is consistent with adjacency edges sharing one vertex. This further enforces consistent \u201ctriangle geometry.\u201d</p> <p>\u2e3b</p> <p>(C) Single Representation Update via Pair-Bias Attention</p> <p>After the triangular updates, the model updates the single representation \\mathbf{s}i. In AF3, each token\u2019s single vector \\mathbf{s}i does a self-attention step in which the attention logits are \u201cbiased\u201d by the pair representation \\mathbf{z}{ij}.     \u2022   Concretely, we run an attention over {\\mathbf{s}i} with something like \\text{logit}(i,j) \\,+=\\, \\text{Linear}(\\mathbf{z}{ij}).     \u2022   This means: if pair (i,j) is known (via \\mathbf{z}_j is boosted.     \u2022   The end result is that single vectors incorporate pairwise knowledge.     \u2022   After attention, we pass {\\mathbf{s}_i} through a small transition (feed-forward) block.}) to be highly interacting or geometrically close, the attention between single vectors \\mathbf{s}_i and \\mathbf{s</p> <p>Hence each Pairformer block effectively does:     1.  Triangular multiplication (outgoing + incoming) on \\mathbf{z}{ij}     2.  Triangular attention (starting + ending node) on \\mathbf{z}     3.  A \u201ctransition\u201d on \\mathbf{z}_{ij}     4.  Single attention with pair bias on \\mathbf{s}_i     5.  A \u201ctransition\u201d on \\mathbf{s}_i</p> <p>\u2e3b</p> <p>(D) Ties to \u2018Pairwise Distance\u2019\u2013Based Prediction</p> <p>Because each \\mathbf{z}{ij} is a learned representation of how token i and j relate, it is naturally well-suited for predicting distances between them. Indeed, AF3\u2019s final distogram head uses the last {\\mathbf{z}{ij}} to produce a binned distribution of distances.     \u2022   The triangular updates effectively ensure that any triplet (i,j,k) sees consistent constraints. This is crucial for capturing geometry.     \u2022   Unlike a simple binary contact map, the pair representation is a continuous, multi-channel embedding that eventually yields a distribution over distances (the \u201cdistogram\u201d)\u2014thus under-the-hood, it is still a \u201cpairwise distance\u201d predictor.</p> <p>Hence, the 48-block Pairformer architecture is the engine for learning geometry from pairwise tokens, letting the model resolve local/long-range distances. Finally, the Distogram head\u2014reading from {\\mathbf{z}_{ij}}\u2014produces the discrete distance bins that underlie AF3\u2019s structural accuracy.</p> <p>Would you like more on how the diffusion module consumes these pair embeddings to generate coordinates?</p>"},{"location":"reference/advanced_methods/diffusion/s4_diffusion/","title":"S4 Diffusion","text":"<p>Below is a helpful overview of the main experimental findings and performance of Liquid-S4 compared to a wide range of baselines on various long-sequence benchmarks. We also include methodological details such as pseudocode for the Liquid-S4 kernel, references to the underlying math, and a hyperparameter table for reproducing the results.</p> <p>\u2e3b</p> <p>Main Contributions Recap</p> <p>Liquid-S4 is a new state-space model that leverages a linear variant of the Liquid Time-Constant network (LTC). It keeps the core ideas of Structured State-Space (S4) layers (i.e., diagonal-plus-low-rank matrix parameterization and HiPPO-based initialization), and introduces a liquid convolution kernel to incorporate higher-order auto-correlations of input signals.     1.  Liquid-S4 Kernel: We derive an extra convolution kernel that captures covariance terms among input samples, without losing the efficient Cauchy kernel approach introduced by S4.     2.  Empirical Results: Liquid-S4 consistently achieves better generalization on the Long Range Arena (LRA) benchmark, 1D pixel-level image classification (sCIFAR), raw Speech Commands, and BIDMC medical time series, surpassing S4, S4D, and other strong sequence models.     3.  Implementation &amp; Efficiency: Liquid-S4 is straightforward to implement, extending S4\u2019s convolution kernel with only minor overhead for the input-correlation kernel.</p> <p>Below, we present:     1.  Comparison to Baselines: Tables for LRA and Speech Commands, sCIFAR, and BIDMC datasets.     2.  Liquid-S4 Kernel Pseudocode     3.  Hyperparameters</p> <p>\u2e3b</p> <ol> <li>Summary of Experimental Results</li> </ol> <p>A. Long Range Arena (LRA)</p> <p>Model   ListOps IMDB    AAN CIFAR   PathF.  Path-X  Avg. Transformer 36.37   64.27   57.46   42.44   71.40   \u2013   54.39 Reformer    37.27   56.10   53.40   38.07   68.50   \u2013   50.56 S4-LegS 59.60   86.82   90.90   88.65   94.20   96.35   86.09 S4D-LegS    60.47   86.18   89.46   88.19   93.06   91.95   84.89 S5 (Simplified-S4)  61.00   86.51   88.26   86.14   87.57   85.25   82.46 Liquid-S4 (ours)    62.75   89.02   91.20   89.50   94.80   96.66   87.32</p> <p>Key Takeaways:     \u2022   Liquid-S4 achieves new best results on four of the six tasks (ListOps, IMDB, AAN, CIFAR) and ties closely on Pathfinder and Path-X.     \u2022   Overall average of 87.32% surpasses prior strong methods like S4-LegS (86.09%) and S4D-LegS (84.89%).</p> <p>\u2e3b</p> <p>B. BIDMC Vital Signs (Medical Time Series)</p> <p>Model   HR  RR  SpO2 CKConv (Conv)   2.05    1.214   1.051 S4-LegS 0.332   0.247   0.090 S4-(LegS/FouT)  0.344   0.163   0.080 Liquid-S4 (ours)    0.303   0.158   0.066</p> <p>(Lower is better: Root-Mean-Squared Error.)</p> <p>Key Takeaways:     \u2022   Liquid-S4 obtains lower RMSE than all S4, S4D, and convolution/RNN-based methods on HR, RR, and SpO2 tasks.     \u2022   Particularly large gains on SpO2 (0.066 vs. 0.080\u20130.102 for prior S4 variants).</p> <p>\u2e3b</p> <p>C. 1-D Pixel-Level CIFAR (sCIFAR)</p> <p>Model   Accuracy (%) LSTM    63.01 IndRNN  96.0 S4-LegS 91.80 S4D-Inv 90.69 Liquid-S4 (ours)    92.02</p> <p>Key Takeaways:     \u2022   Liquid-S4 sets a new state-of-the-art of 92.0% on sCIFAR (1-D flattening).</p> <p>\u2e3b</p> <p>D. Raw Speech Commands (Full 35-Way Task)</p> <p>Model   # Params    16kHz   8kHz (0-shot) ResNet-18   216k    77.86   8.74 S4-LegS 307k    96.08   91.32 S4D-Lin 306k    96.25   91.58 Liquid-S4 (ours)    224k    96.78   90.00</p> <p>Key Takeaways:     \u2022   On the standard 16kHz test, Liquid-S4 obtains the best accuracy (96.78%), with ~30% fewer parameters vs. S4.     \u2022   On the zero-shot 8kHz test, S4D-lin does slightly better (91.58% vs. 90.00%), but Liquid-S4 remains competitive.</p> <p>\u2e3b</p> <ol> <li>Liquid-S4 Kernel Computation</li> </ol> <p>Recall that a standard S4 kernel (for discrete input sequence {u_k} of length L) is computed via K(\\ell) \\;=\\; C\\,A^\\ell\\,B, \\quad \\ell = 0, 1, \u2026, L-1. We can turn the linear state-space model into a liquid variant by adding input-dependent transitions. In a linearized LTC system, the continuous-time ODE is: \\frac{d}{dt} x(t) \\;=\\; (A + B\\,u(t))\\,x(t) + B\\,u(t), \\quad y(t) = C\\,x(t). Upon discretization, the resulting unrolled convolution kernel has two components:     1.  The standard S4 kernel for {u_k}.     2.  An extra \u201cliquid\u201d kernel that accounts for correlations like u_i \\cdot u_j, u_i \\cdot u_j \\cdot u_k, etc., up to order p.</p> <p>PB (Power-of-B) Mode</p> <p>A more efficient variant is to replace A by the identity matrix for the correlation portion. This yields:</p> <p>\\textstyle K_{\\mathrm{liquid}}^{(p)} \\;=\\; C \\; \\bigl(B^{p}\\bigr) \\quad\\text{(with flipping/anti-diagonal alignment for time indices).} In practice, the code merges these kernels and uses the same diagonal-plus-low-rank structure for A. The correlation order p is a small hyperparameter (often 2\u20134). See the pseudocode.</p> <p>Liquid-S4 Kernel Pseudocode (in JAX-like style)</p>"},{"location":"reference/advanced_methods/diffusion/s4_diffusion/#pseudocode-for-the-liquid-s4-kernel-in-pb-mode","title":"Pseudocode for the Liquid-S4 kernel in PB mode","text":""},{"location":"reference/advanced_methods/diffusion/s4_diffusion/#easier-to-compute-typical-default-choice","title":"(easier to compute, typical default choice).","text":"<p>def liquid_s4_kernel_PB(A_params, B, C, P, L):     \"\"\"     Args:        A_params: Parameters for the S4 kernel (e.g. diag + low-rank).        B:        [N,] input vector        C:        [N,] output vector        P:        integer, max correlation order        L:        integer, sequence length     Returns:        kernel_s4: the standard S4 kernel, shape [L,]        kernel_liquid: the correlation kernel, shape [p &lt;= P, L,]     \"\"\"     # 1) Compute the base S4 kernel via standard approach     kernel_s4 = s4_convolution_kernel(A_params, B, C, L)  # shape [L,]</p> <pre><code># 2) Build correlation kernels for each order p = 2..P\n#    We skip the repeated matrix-power of A, using identity for correlation part.\nkernel_liquid = []\nfor p_order in range(2, P+1):\n    # \"Power of B\" approach: simply do C * (B^(p_order)), plus a temporal flip.\n    # Implementation detail: you'd broadcast along time dimension L.\n    # We denote flipping by an anti-diagonal pass. \n    # Below is conceptual.\n    corr_kernel_p = C * (B ** p_order)   # shape [N]\n    # replicate or flip for each time-lag combination ...\n    # final shape -&gt; [L,], matched with special indexing\n    kernel_liquid.append(corr_kernel_p)\n\nreturn kernel_s4, kernel_liquid\n</code></pre> <p>\u2e3b</p> <ol> <li>Hyperparameter Settings</li> </ol> <p>Below are typical settings that gave best results. We highlight that Liquid-S4 often requires fewer states or hidden features than S4, thanks to the correlation kernel.</p> <p>Table: Per-task hyperparameters for Liquid-S4 (with PB kernel).</p> <p>Task    Depth   H (features)    StateSize   Norm    Dropout LR  Batch   Epochs  WD  p (order) LRA-ListOps 9   128 7   BN  0.01    0.002   12  30  0.03    3 LRA-IMDB    4   128 7   BN  0.1 0.003   8   50  0.01    2 LRA-Retrieval   6   256 64  BN  0.2 0.005   16  20  0.05    2 LRA-Image   6   512 512 LN  0.1 0.010   16  200 0.03    2 LRA-Pathfinder  6   256 64  BN  0.0 0.0004  4   200 0.03    2 LRA-PathX   6   320 64  BN  0.0 0.001   8   60  0.05    2 sCIFAR  6   512 512 LN  0.1 0.010   50  200 0.03    3 SpeechCmd (35)  6   128 7   BN  0.0 0.008   10  50  0.05    2 BIDMC (RR/HR)   6   128 256 LN  0.0 0.005\u20130.01  32  500 0.01    2\u20134</p> <p>Notes:     \u2022   \u201cDepth\u201d = # of Liquid-S4 blocks stacked.     \u2022   \u201cH\u201d = # of features in the hidden dimension for the feedforward or mixing layers.     \u2022   \u201cStateSize\u201d = dimension N in the S4 parameterization for the convolution kernel. We often choose smaller N for Liquid-S4.     \u2022   \u201cp (order)\u201d = max correlation terms. Typically 2 or 3 suffices.</p> <p>\u2e3b</p> <p>Conclusions</p> <p>Liquid-S4 combines the continuous-time insight of Liquid Networks with the diagonal-plus-low-rank S4 approach to produce an additional liquid correlation kernel. With minimal overhead, it achieves top performance across LRA, speech, medical, and image tasks. It surpasses the strong S4 and S4D baselines and remains highly efficient due to the same Cauchy kernel computations that S4 employs.</p> <p>Code Availability: A reference PyTorch/JAX code will be made available at GitHub: https://github.com/raminmh/liquid-s4.</p> <p>We hope this encourages broader exploration of combined continuous-time state-space models and polynomial expansions for capturing long-range dependencies.</p> <p>Below is an addendum highlighting additional points that help complete the bigger picture behind all the core state-space modeling references (on S4, LTC, S5, and now Liquid-S4). These points are often either only mentioned in passing or omitted from the main summary but can be important for completeness.</p> <p>\u2e3b</p> <p>Addendum to Liquid-S4 &amp; Related Papers</p> <ol> <li>Full Theoretical Details on the S4 Cauchy Kernel &amp; Orthogonal Bases</li> </ol> <p>A crucial aspect of the S4 framework is the detailed derivation of its efficient convolution kernel:     \u2022   Matrix Powers {A^k} are typically expensive for large k. S4 addresses this using a specialized Cauchy Kernel evaluation in the frequency domain.     \u2022   It involves evaluating \\sum_{k=0}^{L-1} C\\,A^k\\,B quickly via a polynomial transform. In particular, the sum of terms A^k is tackled by viewing \\Lambda = \\mathrm{diag}(\\lambda_i) plus low-rank updates, converting the problem into a Cauchy matrix inversion step.     \u2022   The \\mathcal{O}(N + L\\log L) or \\mathcal{O}(N + L)-type complexities come from carefully applying FFT plus the \u201cblack-box\u201d factorization.</p> <p>For thoroughness, the user might want references to the specific theorem in Gu et al. (2022) (or Gu et al. (2021)) that details how the Cauchy kernel is set up, and how \u201cWoodbury identity + a set of points on the unit circle\u201d (the roots of unity) factor in.</p> <p>Additional references:     \u2022   S4: Efficiently Modeling Long Sequences with Structured State Spaces (Gu et al., ICLR 2022).     \u2022   Combining Recurrent, Convolutional, and Continuous-Time Models with Linear State Space Layers (Gu et al., NeurIPS 2021).</p> <ol> <li>Construction of the LegS Matrix &amp; \u201cNormal + Low-rank\u201d vs. \u201cDiagonal + Low-rank\u201d</li> </ol> <p>The summary mentions the HiPPO-LegS matrix but does not detail how that LegS matrix arises:     \u2022   LegS stands for \u201cLegendre State,\u201d which is derived from a particular continuous weighting measure on [0,1].     \u2022   The \u201cscaled Legendre measure\u201d leads to a companion matrix that encodes how polynomials up to degree N track (and compress) the input function\u2019s history in an exponentially decaying manner.     \u2022   In practice, we rarely keep the entire raw LegS matrix. Instead, we represent it via Normal + Low-Rank or a simpler Diagonal + Low-Rank parameterization.     \u2022   S4 typically uses: A_{\\mathrm{LegS}} \\;=\\; V\\,\\Lambda\\,V^* \\;-\\; P\\,Q^\\top \\quad\\text{(the so-called NPLR form)} with further constraints to keep eigenvalues stable.</p> <p>Hence, if the user wants a deeper conceptual view, they might read about how the Legendre polynomials and the continuous-time approach tie to \u201cHiPPO (Highly Productive Polynomial Projections)\u201d to preserve memory of past inputs.</p> <ol> <li>Handling Time-Varying or Irregular Sampling</li> </ol> <p>While LTCs and S4 can each handle \u201ctime-varying\u201d aspects, the summary only briefly points out LTC\u2019s advantage in continuous domains:     \u2022   Irregular data: LTC or CfC can handle adaptive time steps easily because one can solve the ODE with variable \\Delta t. S5 (or S4 in convolution mode) typically wants uniform steps.     \u2022   Liquid-S4 might, in principle, handle partial irregularities with a dynamic kernel, but the exact method for that is not spelled out in the main text.     \u2022   For tasks like \u201chealth monitoring with irregular intervals,\u201d the continuous-time representation (like LTC) can be more direct; the user might rely on direct ODE integration or an equivalent parallel-scan approach if the discrete steps vary.</p> <ol> <li>Additional Baselines or Sequence Labeling Tasks</li> </ol> <p>Some references compare these methods on tasks that require, e.g., alignment or partial derivatives:     \u2022   E.g., S4 or LTC in seq2seq tasks with alignment might require more advanced decoding.     \u2022   Cauchy- or polynomial-based transforms can be adapted to multi-layer RNN stacks with gating.</p> <p>These are secondary but can matter if the user wants to see how these SSM-based layers handle advanced problems like language modeling with \u201cpartial derivatives\u201d or structured prediction tasks (beyond classification).</p> <ol> <li>BPTT, Memory Usage, and Parallelization Details     \u2022   Backprop Through Time (BPTT): The summary mentions Liquid-S4\u2019s efficiency but omits explicit memory cost.     \u2022   For S4 in convolution mode, offline inference can do all steps in \\mathcal{O}(N + L \\log L). However, the training memory can differ if done naively.     \u2022   Some frameworks store partial states or do partial recomputations to keep memory feasible.     \u2022   Parallelization:     \u2022   S4 transforms the sequence to the frequency domain and does big parallel ops.     \u2022   LTC-based or S5-based models often do \u201cscan\u201d type parallel.     \u2022   It can be important to note that S5 had a parallel scan approach giving \\mathcal{O}(L \\log L) time with \\mathcal{O}(L) processors, whereas S4 uses convolution. Liquid-S4 inherits S4\u2019s approach with an added correlation kernel overhead \\sim p \\times L.</li> </ol> <p>\u2e3b</p> <p>Quick \u201cKey Theorem or Structural Insights\u201d Recap     1.  HiPPO     \u2022   Theorem: The LegS matrix arises from an orthonormal polynomial basis that can approximate the entire past of a signal on [0,1] under exponential decay.     \u2022   This ensures near-perfect memory for potentially unbounded input lengths.     2.  Liquid Time-Constant     \u2022   Insight: The input modifies the time constant of each neuron, allowing a \u201cbilinear term\u201d (A + B\\,u)x. This yields a dynamic system that can more flexibly adapt to input data during inference.     3.  S4     \u2022   Core: The diagonal-plus-low-rank (DPLR) or normal-plus-low-rank approach to factor the LegS matrix. Then a black-box Cauchy kernel method for {CA^kB}.     \u2022   Results in \\mathcal{O}(N + L) or \\mathcal{O}(N + L \\log L) for large sequences.     4.  S5     \u2022   Core: Switch from many single-input SSMs (S4\u2019s block-diagonal) to a single multi-input SSM, plus diagonalization for parallel-scan.     \u2022   Gains simpler time-domain approach at similar complexity.     5.  Liquid-S4     \u2022   Core: Combine the polynomial expansions of S4 with an additional correlation-based kernel from LTC\u2019s (A + B\\,u). Gains better representation with minimal overhead.</p> <p>Hence, the user can see how each of these builds or modifies something from the previous: LTC adds input gating, S4 speeds up standard linear SSM training, S5 reorganizes the S4 block structure, and Liquid-S4 merges LTC\u2019s gating with the S4 kernel approach.</p> <p>\u2e3b</p> <p>Final Note</p> <p>In practice, each method or extension is addressing a slightly different \u201cgap\u201d:     \u2022   S4 and S4D: Fast kernel-based linear state-space layers for extremely long sequences.     \u2022   LTC/CfC: Causal continuous-time RNNs with input-dependent gating, strong out-of-distribution reliability.     \u2022   S5: A simpler time-domain approach to the S4 concept.     \u2022   Liquid-S4: Merges LTC\u2019s correlation + gating with S4\u2019s diagonal-plus-low-rank memorization.</p> <p>If the user wants more advanced tasks (e.g., multi-step forecasting, speech generation, partial observation), or more advanced theorems, the original references are recommended.</p> <pre><code>In summary, these notes fill in some of the minor gaps about the LegS matrix derivation, advanced time-varying extensions, memory usage, and how each approach handles big-latency tasks. They also clarify that for truly irregular sampling, LTC-based methods might have an advantage with direct ODE solvers, while S4-based methods (including Liquid-S4) typically assume uniform steps or process data with convolution/FFT.\n</code></pre> <p>Below is a paper-focused analysis of AlphaFold\u202f3\u2019s architecture (main text + Supplementary) with an eye toward where one might latch onto \u201cS4-like\u201d (liquid S4 or otherwise) ideas despite the dynamic, 3D adjacency. I\u2019ll highlight specific aspects of the AF3 design that might be overlooked but could help in \u201cbypassing\u201d or re-purposing adjacency to enable large-T diffusion unrolling:</p> <p>\u2e3b</p> <ol> <li>AF3\u2019s Key Architecture Pieces &amp; Where the Bottleneck Arises<ol> <li>Token + Pair Representation (Sections 2.6, 3.6) \u2022   AF3 merges the idea of \u201cresidues \u2192 tokens\u201d but also lumps each nonstandard residue/ligand atom into per-atom tokens. \u2022   The main representation is a 2D pair array \\mathbf{z}_{ij} (plus a 1D single representation \\mathbf{s}_i). \u2022   Even though it\u2019s nominally \u201cN tokens,\u201d each token might be an entire residue or a single ligand atom. Because adjacency is effectively global (the pair array is size N \\times N), it\u2019s not trivially 1D.</li> <li>Atom Attention (Sections 3.2, 3.7) \u2022   AF3 does a \u201csequence-local atom attention\u201d (Supplementary Fig.\u202f1 and Alg.\u202f5\u20137) to incorporate fine-grained local geometry before building the token representation. \u2022   They chunk the full set of atoms into blocks along the \u201csequence axis\u201d (the tokens) so each atom can attend to a bigger window of ~128 neighboring atoms. This is already a quasi-1D block approach to adjacency, but it\u2019s still complicated by the fact that the \u201cneighborhood\u201d is purely index-based with a fallback to the true distances for actual geometry (line 3 in Algorithm 5, etc.).</li> <li>Diffusion Steps Are Typically Low \u2022   AF3\u2019s final diffusion module is run for \\sim50 steps at inference by default, or ~200 in certain cases. The paper notes they do only 20 steps in the \u201cmini-rollout\u201d for training. \u2022   So, the typical method for geometry-based diffusion is to keep T small because each step involves a big forward pass over the trunk (or at least a partial pass).</li> </ol> </li> </ol> <p>In your scenario, you want to:</p> <pre><code>    \u2022   \u201cSwap the big trunk for an S4-based model that can handle up to 10k steps more cheaply at test time,\u201d\n\u2022   Potentially \u201clower parameter count\u201d but still do large unrolls in time.\n</code></pre> <p>Hence the question: does the AF3 pipeline contain a trick or submodule that we can adapt into an S4-friendly (1D) approach\u2014without incurring the full 2D adjacency penalty at each step?</p> <p>\u2e3b</p> <ol> <li>Where AF3 Might Offer a Bridge for Large-T</li> </ol> <p>2.1 \u201cSequence-Local Atom Attention\u201d as a 1D Surrogate</p> <p>In Supplementary Fig.\u202f1 &amp; Algorithm\u202f5/7, they mention:</p> <pre><code>    \u2022   They only do a banded self-attention of size (N_{\\mathrm{atoms}} \\times N_{\\mathrm{atoms}}) but in rectangular blocks along the diagonal.\n\u2022   Conceptually, each subset of 32 atoms attends to 128 neighbors (Algorithm\u202f7: Nqueries=32, Nkeys=128).\n</code></pre> <p>Why This Matters for S4     \u2022   S4 can handle a single dimension well; if we interpret the \u201c\\sim N_{\\text{atoms}}\u201d dimension as a 1D axis, we might try to treat it as a 1D sequence.     \u2022   AF3 does so with partial block-based attention, effectively a local 1D adjacency in the \u201catom index\u201d space.</p> <p>However, the catch is that the adjacency in 3D is only approximately captured by that block-limited approach. A standard S4 or Liquid-S4 would rely on a fixed or small-rank operator A over an entire length N. In AF3, that block is a hand-coded mask that slides over the (atom index). So if you want 10k diffusion steps, you could:     1.  Keep the same block \u201cmask\u201d (i.e. let each atom attend to a 128-neighbor region in the 1D ordering)     2.  Replace that 2D attention module with a 1D S4-based module (plus some local gating for adjacency).</p> <p>But you\u2019d still need to figure out how to incorporate actual geometric distances if the atoms move significantly. AF3\u2019s local block is \u201csequence-based,\u201d not distance-based. So it only implicitly respects geometry once tokens are formed.</p> <p>2.2 The \u201cMini-Rollout\u201d Mechanism</p> <p>Section 4.1 of the Supplement: AF3 does a \u201cshort diffusion rollout\u201d from random noise for ~20 steps during training to produce an approximate final structure used for alignment or confidence-head training. This doesn\u2019t train the entire trunk with 20 steps unrolled. They do it partially \u201coff to the side.\u201d     \u2022   If you want 10k steps for your final inference, maybe you can adopt a similar approach:     \u2022   Train a smaller S4-based \u201cdecoder\u201d that is unrolled for fewer steps during training (like 20\u201350).     \u2022   At inference, you ramp up to 10k steps if you want finer increments.     \u2022   The AF3 approach of \u201cmini-rollout\u201d is basically truncated unrolling plus a final alignment. That\u2019s reminiscent of truncated backprop in RNNs.</p> <p>Implication: The paper\u2019s training pipeline already acknowledges you don\u2019t want to fully unroll (the trunk is too big). They do partial. That\u2019s conceptually close to a \u201ctime-aware S4 approach,\u201d except they aren\u2019t using the trunk repeatedly at each step. Instead, they do it once or a few times and then rely on the cheaper diffusion module.</p> <p>2.3 Using the \u201cPairformerStack\u201d Fewer Times</p> <p>AF3 has \u201cNcycle = 4\u201d recycles in the trunk, and that\u2019s it. Then the diffusion module is a cheap sub-network that sees the trunk embeddings. So the paper does:     1.  Run Pairformer + MSA stack ~4 times (the big trunk).     2.  Pass the final embeddings into a smaller DiffusionModule for ~50 steps at inference.</p> <p>Hence: If you want a large number of diffusion steps T=10k, you only pay for the diffusion module repeated 10k times, not the entire trunk. That\u2019s already in AF3. Possibly you can make that DiffusionModule an S4-based module (like you propose). The trunk is not repeated 10k times.     \u2022   This is spelled out in Algorithm\u202f1 (MainInferenceLoop) and Algorithm\u202f18 (SampleDiffusion). Notice that the trunk (Pairformer, etc.) is used only a handful of times (Ncycle=4). The big repeated loop (line 2\u201312 in Algorithm\u202f18) calls DiffusionModule  (not\\ the trunk).     \u2022   So the foundation for \u201cTest-time scaling on T is already in the code: they do the trunk once, the diffusion 50 times. Just push 50 \u2192 10k.</p> <p>The real adjacency question: Inside the diffusion module, do you see reference to the pair representation \\(\\mathbf{z}{ij}\\)? Yes, they do feed \\mathbf{z}{ij} in each time, but that\u2019s presumably frozen after the trunk has finished. The geometry changes at each step, but the trunk features \\mathbf{z}_{ij} do not. That\u2019s how they sidestep re-building adjacency for each step. They do a random center and random rotation in each step (Algorithm\u202f19), but that\u2019s a cheap transform, not a re-run of the trunk.</p> <p>Hence the big question: does that mismatch hamper accuracy if you do 10k steps? Possibly it\u2019s fine, or possibly you want to re-check adjacency after big geometry changes. But the \u201clack of adjacency re-check\u201d is exactly how AF3 keeps it cheap.</p> <p>2.4 PDE Head &amp; The \u201cNo Re-run\u201d Trick</p> <p>Another overlooked detail:     \u2022   They do not re-run the pair embedder or MSA after each partial geometry update. They keep a \u201cstatic\u201d pair representation from the trunk.     \u2022   Then the diffusion module is a purely non-geometric transformer with local cross attention.     \u2022   Even the \u201catom cross attention\u201d in the diffusion module is partial, not a full adjacency re-check. They are effectively ignoring the fact that adjacency might shift as the structure changes.</p> <p>Thus, the main \u201cbottleneck\u201d they mention\u2014\u201c3D geometry or a dynamic adjacency matrix is not obviously \u2018fixed\u2019 or \u20181D\u2019,\u201d\u2014they solve by ignoring dynamic adjacency at diffusion time. They rely on the trunk to have gleaned enough local geometry constraints, so the diffusion \u201cjust polishes\u201d the structure.</p> <p>\u2e3b</p> <ol> <li>\u201cSecret Overlooked Levers\u201d in the Paper</li> </ol> <p>Based on the text, three relevant levers might be under-discussed but helpful:     1.  Masking or Approx. Adjacency in the Diffusion Decoder     \u2022   The diffusion module (Alg.\u202f20) uses local \u201cSequence-local Atom Attention\u201d as a step. But that local shape is chosen somewhat arbitrarily.     \u2022   You could re-implement that local shape as a 1D S4 if you interpret the atoms in some linear order. That\u2019s effectively a \u201csliding window.\u201d     \u2022   The upshot: it\u2019s exactly how they do it in the trunk\u2019s \u201cAtomAttentionEncoder,\u201d so presumably you could unify or share that approach inside the diffusion module.     2.  Static \\mathbf{z}{ij} Instead of Recomputing     \u2022   They keep the pair representation \\mathbf{z} from the trunk. That means they do not attempt to update adjacency for large geometry changes.     \u2022   If you want 10k steps, you might try the same trick\u2014just keep a static adjacency or a static representation, to feed your S4 \u201ctime-slicing.\u201d     \u2022   This is simpler (and matches the official code). The model may degrade on large domain motions, but that\u2019s the approach AF3 uses.     3.  \u201cDiscrete PDE\u201d Insight     \u2022   If you re-check the \u201csmooth LDDT loss\u201d or the notion that each step can see local constraints, you might glean that AF3 lumps all the local geometry constraints into a final MSE and LDDT. They do not worry about intermediate adjacency steps.     \u2022   So you can treat the final or near-final adjacency as \u201cclose enough.\u201d</p> <p>\u2e3b</p> <ol> <li>Possible Plan to Incorporate S4 at 10k Steps</li> </ol> <p>Putting it all together:     1.  Do the trunk once with Ncycle=4, produce the big pair representation \\mathbf{z}{ij}.     2.  Create an S4-based diffuser that (like AF3\u2019s diffusion module) is repeated T=10k times.     \u2022   Input: The final single representation \\mathbf{s}i and pair representation \\mathbf{z}{ij} from the trunk (both remain fixed).     \u2022   At each step, we have current coordinates {\\mathbf{x}_l}. Possibly convert them to a 1D format, letting each atom be an index in the \u201ctime series.\u201d     \u2022   Then apply a 1D S4 to that per-atom or per-token sequence. That yields an updated coordinate shift.     3.  Retain the existing \u201cstatic adjacency\u201d from \\mathbf{z}. If you want some local doping (like a gating vector that depends on distance to help the S4 know if two tokens are near each other), you can do what the paper does: partial cross attention with a logistic or gating. But do not re-run the entire Pairformer.     4.  Because S4 is \\mathcal{O}(N \\log N) or \\mathcal{O}(N) in sequence length, you can manage 10k steps as long as your number of tokens is not monstrous. AF3 tries to keep it below a few thousand tokens.</p> <p>Yes, adjacency might become stale after many steps of large rearrangements. But that\u2019s how AF3 is done today. If you can live with that approximation, it\u2019s the official route.</p> <p>\u2e3b</p> <ol> <li>Concluding Thoughts     \u2022   AF3\u2019s Paper Already \u201cIgnores\u201d Dynamic Adjacency at diffusion time to keep it cheap. They effectively treat the trunk\u2019s pair representation as static. This is precisely how they bypass the \\mathcal{O}(N^2) adjacency cost.     \u2022   Large T is feasible if the diffusion sub-network is small enough and does not re-run big adjacency expansions. That\u2019s the principle.     \u2022   If you specifically want an S4-lens, you can drop in S4 for the \u201cAtomAttentionEncoder/Decoder\u201d in the diffusion module. Overlooked detail: the \u201csequence-local\u201d chunking can be replaced by a simpler 1D convolution-based SSM (S4).     \u2022   So the real \u201cbottleneck\u201d is not truly \u201clack of 1D.\u201d The model overcame that by letting adjacency remain static and by chunking. As a result, 10k steps is plausible if you have enough memory and time to do 10k passes with a small S4-based diffuser.</li> </ol> <p>In short, the two big \u201coverlooked\u201d enablers in the paper are (1) that the trunk is not re-run at each step, (2) the adjacency is effectively not dynamic during the diffusion. They pinned it in place from the final trunk\u2019s pair representation. That\u2019s how you might integrate a 1D S4 approach for large T without re-labelling adjacency at each iteration.</p> <p>[emoji]: That\u2019s the gist. By focusing on the DiffusionModule in the paper\u2014and how it already does not rely on dynamic adjacency\u2014you can see a path to an \u201cS4-based big unroll\u201d that leverages 10k steps while side-stepping the \u201c3D adjacency explosion.\u201d</p> <p>Below is a paper-focused analysis of AlphaFold\u202f3\u2019s architecture (main text + Supplementary) with an eye toward where one might latch onto \u201cS4-like\u201d (liquid S4 or otherwise) ideas despite the dynamic, 3D adjacency. I\u2019ll highlight specific aspects of the AF3 design that might be overlooked but could help in \u201cbypassing\u201d or re-purposing adjacency to enable large-T (e.g. 10,000-step) diffusion unrolling at test time:</p> <p>\u2e3b</p> <ol> <li>AF3\u2019s Key Architecture Pieces &amp; Where the Bottleneck Arises<ol> <li>Tokens &amp; Pair Representations \u2022   AF3 merges \u201cresidues \\to tokens,\u201d plus lumps nonstandard residue/ligand atoms into per-atom tokens. \u2022   It then builds a 2D pair array of size (N\\times N) (as well as a 1D \u2018single representation\u2019), leading to pairwise adjacency. \u2022   Because adjacency is effectively global at this stage, it\u2019s not trivially \u201c1D,\u201d so standard 1D S4 can\u2019t just be dropped in for the trunk.</li> <li>Atom Attention \u2022   Before building the token representation, AF3 does a \u201csequence-local atom attention\u201d (Supplementary Fig.\u202f1, Algs.\u202f5\u20137) on blocks of atoms along a \u201csequence axis\u201d in chunks of size 32 or 128. \u2022   Conceptually this is already a pseudo-1D approach to adjacency\u2014the adjacency is purely index-based or block-based, not distance-based.</li> <li>Diffusion Steps Are Typically Small \u2022   AF3\u2019s final diffusion module is unrolled for ~50\u2013200 steps at inference, rarely more. \u2022   So they do not re-run the expensive trunk for each small diffusion step. Instead, they freeze the trunk outputs and feed them once into a cheaper diffusion sub-network that runs multiple times.</li> </ol> </li> </ol> <p>Hence, if you want to push T to 10k steps, you need to see how AF3 is already skipping the big adjacency expansions for each step.</p> <p>\u2e3b</p> <ol> <li>Where AF3 Might Provide a Bridge to Large \\mathbf{T}</li> </ol> <p>2.1 Static Pair Representation + Cheap Diffusion</p> <p>Crucially, in AF3:     \u2022   They run the big trunk (PairFormer, MSA module) a handful of times (Ncycle=4).     \u2022   They then store the final embeddings \\mathbf{z}_{ij} and \\mathbf{s}_i.     \u2022   The diffusion module sees only these final embeddings, does not recast adjacency, and iterates \\sim50 times.</p> <p>That approach already \u201cignores\u201d dynamic adjacency in the diffusion loop. They rely on a single snapshot of pair representation (and some local \u2018atom attention\u2019 step) to apply each incremental geometry update \\Delta \\mathbf{x}.</p> <p>If you want to do 10k steps, you can do the same:     1.  Run the trunk once.     2.  Keep the final pair representation \\mathbf{z}_{ij}.     3.  Substitute AF3\u2019s small diffusion transformer with a 1D S4-based \u201cdiffuser\u201d that you unroll 10k times.</p> <p>2.2 \u201cAtomAttentionEncoder/Decoder\u201d Substitution</p> <p>Inside the diffusion module (Algorithm\u202f20 and the \u201cAtomAttentionEncoder/Decoder\u201d calls), there is a local, block-based 1D self-attention over subsets of atoms. You could:     \u2022   Replace that local self-attention with a 1D S4 approach (sliding window or full).     \u2022   Maintain the trunk\u2019s final embeddings \\mathbf{z}_{ij} as a static gating or skip connection.</p> <p>Hence you get a 1D S4-lens on your geometry (just as AF3 does with a local attention lens), repeated for each diffusion step, but not re-running the entire trunk adjacency.</p> <p>2.3 \u201cNo Re-run\u201d Trick &amp; Potential Accuracy Gaps</p> <p>Because AF3 does not re-run adjacency for each step, that\u2019s the \u201ctrick\u201d that keeps it feasible. If you want truly correct adjacency after big structural changes, you\u2019d have to re-run the trunk, which is \\mathcal{O}(N^2). But AF3\u2019s official pipeline doesn\u2019t do that either\u2014they rely on the trunk\u2019s final embeddings and accept some inaccuracy from ignoring newly formed or broken contacts at large step counts.</p> <p>\u2e3b</p> <ol> <li>Suggested Path to \u201cS4-like\u201d Diffusion in AF3</li> </ol> <p>Putting it all together:     1.  Trunk Once: Just like AF3, run the trunk 4 times, produce final embeddings \\mathbf{z}_{ij}, \\mathbf{s}_i.     2.  S4-based Diffuser: At test time, do 10k steps with a custom S4 block that sees the current coordinates \\mathbf{x} and the trunk embeddings. Possibly also keep the partial \u201catom-attention\u201d if you want a small local fix for adjacency, or skip it.     3.  Large T: Because the trunk is never re-run, you can scale T up to 10k. The main cost is the repeated S4 pass over the 1D sequence of N atoms. Provided N is a few thousand, that can be feasible.</p> <p>Yes, you lose truly dynamic adjacency updates. But that\u2019s exactly how AF3 does it (the trunk is not re-called at each step). So your approach is consistent with the official code\u2019s \u201cfrozen adjacency\u201d logic, meaning you can unroll many times.</p> <p>\u2e3b</p> <ol> <li>Key Observations to Leverage<ol> <li>Static adjacency: The pair \\mathbf{z}_{ij} is not re-computed, so you don\u2019t pay \\mathcal{O}(N^2) each step.</li> <li>1D \u201catom index\u201d: The diffusion sub-network does local \u201csequence-based\u201d attention. That can be replaced by S4\u2019s 1D kernel, ignoring that the underlying data is 3D geometry.</li> <li>Partial adjacency: If needed, you can keep some small gating that references \\mathbf{z}_{ij} or distances for local neighbors.</li> </ol> </li> </ol> <p>So from a \u201cS4-like\u201d perspective, the overlooked detail is that AF3\u2019s diffusion is basically done in a 1D style anyway, ignoring dynamic adjacency. This is where you can insert Liquid-S4 or standard S4 for a 10k-step unroll, while still depending on the trunk\u2019s final adjacency for global constraints.</p> <p>\u2e3b</p> <ol> <li>Concluding Outline     \u2022   AF3\u2019s trunk is expensive and used only a handful of times.     \u2022   The diffusion is a purely local 1D stepping process with a static adjacency embedding.     \u2022   Hence you can swap out AF3\u2019s small diffusion transformer for a 1D S4 (or Liquid-S4) block\u2014the adjacency is basically not re-queried anyway.     \u2022   That\u2019s how to get large-T test-time scaling via an S4-lens, and no new big adjacency expansions are needed.</li> </ol> <p>If you accept that geometry-based adjacency is \u201cfrozen\u201d after the trunk, you get the best of both worlds: a big diffusion unroll at test time, with a powerful 1D S4-based \u201cdecoder,\u201d and no repeated \\mathcal{O}(N^2) cost each step.</p> <p>Below is a conceptual pseudocode draft for how you might create a unified Python-style library that combines the various SSM methods (S4, S4D, LTC gating, correlation kernels, etc.) under one flexible interface. The idea is that you can switch on or off certain features like \u201cdiagonal vs. NPLR,\u201d \u201cLTC gating,\u201d \u201ccorrelation order p,\u201d \u201coffline convolution vs. parallel scan,\u201d etc., all in one core class.</p> <p>This is only illustrative code\u2014i.e., it won\u2019t run out of the box\u2014and you\u2019ll likely replace placeholders (TODO) with actual implementations for the kernel computations, matrix builds, ODE solvers, etc.</p> <p>class StateSpaceBlock:     \"\"\"     A unified class to handle different SSM-based sequence layers:       - S4  or  S4D    (by controlling 'ssm_type': 'NPLR' vs 'diagonal')       - LTC gating     (by controlling 'gating_mode': 'LTC' or None)       - correlation order p    (by controlling 'corr_order')       - offline vs. time-domain approach (by controlling 'inference_mode': 'convolution', 'parallel_scan', or 'ode')     \"\"\"</p> <pre><code>def __init__(self,\n             ssm_size,        # e.g. N, dimension of the internal state\n             input_dim,       # dimension of input features\n             output_dim,      # dimension of output features\n             ssm_type='NPLR', # 'NPLR' for S4-like, 'diagonal' for S4D-like\n             gating_mode=None,# e.g. None or 'LTC'\n             corr_order=1,    # e.g. 1 =&gt; no correlation kernel, &gt;=2 =&gt; Liquid-S4\n             inference_mode='convolution', # or 'parallel_scan', or 'ode'\n             **kwargs):\n    \"\"\"\n    Args:\n        ssm_size (int): dimension N of the SSM.\n        input_dim (int): input channel dimension for this layer.\n        output_dim (int): output channel dimension for this layer.\n        ssm_type (str): 'NPLR' or 'diagonal' or other future expansions.\n        gating_mode (str): e.g. 'LTC' for input-based gating, or None.\n        corr_order (int): p, how many auto-correlation orders to consider, if using Liquid approach.\n        inference_mode (str): how to apply in forward pass: 'convolution', 'parallel_scan', or 'ode'.\n        kwargs: placeholders for e.g. hyperparams like timescale range, rank for low-rank factor, etc.\n    \"\"\"\n    self.ssm_size = ssm_size\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.ssm_type = ssm_type\n    self.gating_mode = gating_mode\n    self.corr_order = corr_order\n    self.inference_mode = inference_mode\n\n    # Additional parameters for SSM initialization (e.g. timescale sampling)\n    # from kwargs we might parse 'delta_min', 'delta_max', etc.\n    self.delta_min = kwargs.get('delta_min', 0.001)\n    self.delta_max = kwargs.get('delta_max', 0.1)\n\n    # 1) Build or sample state matrix A, input matrix B, output matrix C, ...\n    self._init_ssm_parameters()\n\n    # 2) If gating_mode == 'LTC', define any gating or time-constant parameters\n    if self.gating_mode == 'LTC':\n        self._init_ltc_gating()  # e.g. small MLP or direct param\n    else:\n        self.gating_params = None\n\ndef _init_ssm_parameters(self):\n    \"\"\" Initialize (A, B, C, D, etc.) with normal+low-rank or diagonal approach. \"\"\"\n    if self.ssm_type == 'NPLR':\n        # =&gt; S4-like approach: we do Normal + Low-rank\n        # possibly load the LegS matrix or param from a \u201cHippo\u201d function.\n        self.A_params = self._build_nplr_legS(self.ssm_size)\n    elif self.ssm_type == 'diagonal':\n        # =&gt; S4D-like approach: diagonal\n        self.A_params = self._build_diagonal_legS(self.ssm_size)\n    else:\n        raise NotImplementedError(\"Unknown ssm_type\")\n\n    # B, C, D, etc. can be built similarly (perhaps random or certain init).\n    self.B = nn.Parameter(torch.randn(self.ssm_size, self.input_dim))\n    self.C = nn.Parameter(torch.randn(self.output_dim, self.ssm_size))\n    # Possibly feed-through D or timescales:\n    self.log_timescale = nn.Parameter(torch.zeros(self.ssm_size))\n\ndef _init_ltc_gating(self):\n    \"\"\"Define gating parameters for the LTC approach, e.g. a small function f(u). \"\"\"\n    # We might keep it simple by a learnable linear or MLP from input -&gt; gating scales\n    # e.g. gating_params = MLP with hidden dim?\n    self.gating_mlp = MyTinyMLP(self.input_dim, out_dim=self.ssm_size)\n    # or just define param vectors that will be used to do: A + gating\n    return\n\ndef _build_nplr_legS(self, N):\n    \"\"\"Placeholder for building a normal+low-rank representation from LegS matrix. \"\"\"\n    # e.g. do the standard LegS procedure: get V, Lambda, P, Q, etc.\n    # return them in a dictionary\n    return {\"V\": None, \"Lambda\": None, \"P\": None, \"Q\": None}  # TODO\n\ndef _build_diagonal_legS(self, N):\n    \"\"\"Placeholder for building a diagonal S4D approach. \"\"\"\n    # e.g. we might store just a vector of diag(A)\n    return {\"diagA\": nn.Parameter(torch.zeros(N))}  # TODO\n\ndef forward(self, input_sequence):\n    \"\"\"\n    input_sequence: shape [batch_size, seq_len, input_dim]\n\n    We'll route to a different function depending on 'inference_mode'.\n    \"\"\"\n    if self.inference_mode == 'convolution':\n        return self._forward_convolution(input_sequence)\n    elif self.inference_mode == 'parallel_scan':\n        return self._forward_parallel(input_sequence)\n    elif self.inference_mode == 'ode':\n        # For LTC, might step an ODE solver, or do a naive BPTT approach\n        return self._forward_ode(input_sequence)\n    else:\n        raise NotImplementedError(\"Unknown mode\")\n\ndef _forward_convolution(self, x):\n    \"\"\"\n    Offline application: we can build the S4 kernel =&gt; K_s4,\n    then possibly build the 'liquid' kernel =&gt; K_liquid,\n    convolve them with x, sum up. \n    \"\"\"\n    # 1) Compute standard S4 convolution kernel:\n    K_s4 = compute_s4_kernel(self.A_params, self.B, self.C, x.shape[1])  # length = seq_len\n    # 2) If corr_order &gt; 1, compute the \"liquid\" kernel part:\n    if self.corr_order &gt; 1:\n        K_liquid = compute_liquid_kernel(self.A_params, self.B, self.C, self.corr_order, x.shape[1])\n        # Then combine them, e.g. sum or do the correlated convolve\n        y = convolve_with_liquid(x, K_s4, K_liquid, corr_order=self.corr_order)\n    else:\n        y = convolve_basic(x, K_s4)\n    return y  # shape [batch_size, seq_len, output_dim]\n\ndef _forward_parallel(self, x):\n    \"\"\"Parallel-scan approach, akin to S5. If gating_mode is LTC, incorporate that. \"\"\"\n    # For example, we do a diagonal approach for 'A', then a prefix-scan style recurrence\n    if self.ssm_type == 'diagonal':\n        # s5-like approach\n        y = s5_parallel_scan_diagonal(self.A_params, x)\n    else:\n        # or we might do a partial approach with nplr\n        y = s5_like_scan_nplr(self.A_params, x)\n    # incorporate correlation if corr_order&gt;1 ...\n    return y\n\ndef _forward_ode(self, x):\n    \"\"\"\n    For LTC gating in a truly continuous sense, we might unroll the ODE solver. \n    This can be slow for large L, but helpful if we want actual time steps that vary.\n    \"\"\"\n    # Suppose we do a simple Euler or any standard solver across the seq steps.\n    dt = (self.delta_min + self.delta_max)/2  # or adapt\n    hidden = torch.zeros(x.size(0), self.ssm_size)  # batch_size x N\n    outputs = []\n    for t in range(x.shape[1]):\n        u_t = x[:, t, :]  # shape [batch_size, input_dim]\n        if self.gating_mode == 'LTC':\n            # compute gating =&gt; modifies A\n            gating_vec = self.gating_mlp(u_t)  # e.g. shape [batch_size, N]\n            # we do A + B*gating etc.\n            hidden = euler_update(hidden, gating_vec, dt)\n        else:\n            # standard step\n            hidden = euler_update(hidden, None, dt)\n        out_t = self.C @ hidden.transpose(0,1)  # shape [output_dim, batch_size], then .T\n        outputs.append(out_t.transpose(0,1)) \n    y = torch.stack(outputs, dim=1)  # shape [batch_size, seq_len, output_dim]\n    return y\n\n# Additional utility methods omitted for brevity \n# e.g. log_timescale usage, partial-lrk expansions, memory management, etc.\n</code></pre> <p>Notes on This Draft     1.  Multiple Approach Support You can see how we define \u201cinference_mode\u201d to let a single code base decide at forward pass time whether to do:     \u2022   Convolution-based offline (like S4),     \u2022   Parallel scan (like S5), or     \u2022   An ODE solver loop (like LTC).     2.  corr_order for the Liquid Kernel The function _forward_convolution calls compute_liquid_kernel(...) if corr_order&gt;1. That function would implement the power-of-B approach (PB) or the \u201ckernel \u00d7 B\u201d approach from the Liquid-S4 paper.     3.  Parameter Tying If you want to tie gating matrices across multiple layers, you\u2019d simply reuse or pass references to certain nn.Parameters. Or do that in _init_ltc_gating() for a global gating MLP.     4.  Simplifications     \u2022   Real code might use s4_convolution_kernel from the official S4 library.     \u2022   The LTC \u201cODE solver\u201d here is just a naive Euler method; you might want higher-order integrators or variable-step integrators to handle time-varying \\Delta t.     5.  Initialization This draft places _init_ssm_parameters() in the constructor with placeholders. You\u2019d fill in your chosen HiPPO-LegS building block or diagonal approximation, etc.</p> <p>\u2e3b</p> <p>Final Remarks</p> <p>This snippet demonstrates a unified approach for a flexible \u201cStateSpaceBlock\u201d that can do:     \u2022   S4 or S4D: by toggling ssm_type and the underlying _build_nplr_legS vs _build_diagonal_legS.     \u2022   (Liquid) LTC gating: by toggling gating_mode='LTC'.     \u2022   Correlation (corr_order&gt;1 =&gt; Liquid-S4).     \u2022   Offline or Online: by toggling inference_mode='convolution', 'parallel_scan', or 'ode'.</p> <p>You can similarly expand or reorganize the code. The main takeaway is that the code structure enumerates all the design-time flags from S4, LTC, S5, Liquid-S4, etc., into a single layered library.</p>"},{"location":"reference/advanced_methods/diffusion/test_time_scaling/","title":"Time Scaling","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d: You\u2019re essentially asking if one can \u201ctreat the diffusion steps as a test-time knob\u201d\u2014shortening or lengthening the reverse denoising procedure to trade off sampling speed versus quality.</p> <p>[emoji]: Short answer: Yes. In fact, many works on diffusion models (e.g., DDPM, Score-SDE, etc.) explicitly show that one can reduce the number of sampling steps at test time (leading to faster synthesis) or increase them (to improve quality). Below is a more structured breakdown:</p> <p>\u2e3b</p> <p>Why This Works     1.  Diffusion Basics     \u2022   Traditional diffusion models use a forward noising process over T steps. At inference, one reverses that process for T steps to generate samples.     \u2022   If you skip some steps, or equivalently \u201cmerge\u201d multiple steps at once, you effectively reduce test-time compute but often at the cost of slightly lower sample quality (since each step is a discrete approximation of the continuous reverse-time SDE/ODE).     2.  Learning-Free vs. Learning-Based Approaches     \u2022   Learning-Free: For example, you can simply choose fewer solver steps (e.g., with DDIM, DPM-solver, Heun\u2019s method, etc.) at test time. Each step is bigger but runs fewer times overall.     \u2022   Learning-Based: Methods like \u201cknowledge distillation\u201d or \u201ctruncated diffusion\u201d train an additional model (or parameter head) to mimic the original diffusion process but in fewer steps. This approach can produce higher fidelity than na\u00efvely skipping steps.</p> <p>\u2e3b</p> <p>Using \u201cDiffusion Steps\u201d as Test-Time Scaling     1.  Shorter Inference for Speed     \u2022   If you only do, say, 20 reverse steps rather than the baseline 50\u2013200, you\u2019ll sample much faster. However, there can be visible artifacts or slightly worse FID.     \u2022   This short path is akin to \u201cspending less test-time compute.\u201d     2.  Longer Inference for Higher Quality     \u2022   Conversely, if you let your sampler run with more, finer-grained steps (like 100 or 200 instead of 50), you can often improve sample fidelity or reduce artifacts, albeit at a higher test-time cost.     3.  Dynamic Step Adaptation     \u2022   Some recent samplers use adaptive step sizing or error estimates to decide if they need another denoising iteration. This further generalizes the idea that you can treat the number of steps as a test-time knob.</p> <p>\u2e3b</p> <p>Practical Tips     \u2022   Continuous vs. Discrete: If your diffusion model is formulated in continuous time (Score SDE/ODE), you can pick any solver with a chosen step size. Finer steps \u2192 better quality; coarser steps \u2192 speed.     \u2022   Discrete DDPM: If you have a discrete-time DDPM, you can skip certain time steps or rely on, for example, DDIM or DPM-solver to jump in larger increments.     \u2022   Tradeoff: Typically the relationship is monotonic: fewer steps \u2192 lower compute but worse image/audio/etc. quality, more steps \u2192 better quality but slower sampling.</p> <p>\u2e3b</p> <p>\ud83e\uddd9\ud83c\udffe\u200d: Does that clarify how you can treat diffusion steps as test-time scaling? Any other points you\u2019d like to explore further?</p>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/","title":"RNA Isostericity Design \ud83e\uddec","text":""},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#1-introduction","title":"1. Introduction \ud83c\udf1f","text":"<p>When an RNA 3D structure is available\u2014experimentally determined (X-ray, cryo-EM) or reliably modeled\u2014local geometry can be directly utilized to propose mutations preserving the overall fold. Traditionally, methods rely on multiple-sequence alignments (MSAs) for identifying co-variation. Here, RNA geometry directly informs isosteric or near-isosteric substitutions, making MSAs secondary or optional.</p>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#key-concepts","title":"Key Concepts \ud83d\udd11","text":"<ul> <li> <p>Leontis\u2013Westhof Classification:</p> <ul> <li>Classifies RNA base pairs into 12 geometric families:<ul> <li><code>cWW, tWW, cWH, tWH, cWS, tWS, cHH, tHH, cHS, tHS, cSS, tSS</code></li> </ul> </li> <li>Based on hydrogen-bond edges and glycosidic bond orientations (cis/trans).</li> </ul> </li> <li> <p>Isostericity &amp; IsoDiscrepancy Index (IDI):</p> <ul> <li>Isosteric Pairs (IDI \u2264 2.0): Overlay well, preserving backbone geometry.</li> <li>Near-isosteric Pairs (2.0 &lt; IDI \u2264 3.3): Mildly perturb geometry, potentially affecting stability.</li> </ul> </li> <li> <p>Environmental Constraints:</p> <ul> <li>Base triples/quadruples, base\u2013phosphate contacts, stacking interactions, bridging waters, syn/anti configurations, and base\u2013protein interactions.</li> </ul> </li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#goal","title":"Goal \ud83c\udfaf","text":"<p>Develop a robust pipeline to:</p> <ul> <li>Accept RNA sequence, secondary structure, and 3D coordinates.</li> <li>Identify and classify base pairs and tertiary contacts directly from structure.</li> <li>Generate geometry-preserving substitution sets.</li> <li>Filter substitutions based on detailed environmental constraints.</li> <li>Generate, score, and rank candidate RNA sequences.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#2-data-structures-inputs","title":"2. Data Structures &amp; Inputs \ud83d\udcc2","text":"<ul> <li>RNA Sequence: String (e.g., <code>\"ACGUGC\"</code>).</li> <li>Secondary Structure: Base pairs (dot-bracket notation or explicit <code>(i,j)</code> pairs).</li> <li>3D Coordinates: Atomic coordinates (PDB or mmCIF files).</li> <li>Isosteric/IDI Data: Tables/matrices for each geometric family indicating isosteric and near-isosteric substitutions.</li> <li>Environment Constraints: Optional but highly recommended (bridging waters, base\u2013phosphate contacts, base\u2013protein interactions, syn/anti conformations, triples, quadruples).</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#3-high-level-workflow","title":"3. High-Level Workflow \ud83d\udee0\ufe0f","text":"<ol> <li> <p>Load &amp; Parse 3D Structure:</p> <ul> <li>Identify canonical/noncanonical base pairs and tertiary interactions.</li> </ul> </li> <li> <p>Classify Base Pairs:</p> <ul> <li>Apply Leontis\u2013Westhof classification (e.g., using FR3D).</li> </ul> </li> <li> <p>Extract Isosteric Constraints:</p> <ul> <li>Determine geometry-compatible substitutions using IDI data.</li> </ul> </li> <li> <p>Apply Environment Filters:</p> <ul> <li>Exclude substitutions conflicting with bridging waters, syn/anti configurations, base\u2013protein, base\u2013phosphate contacts.</li> </ul> </li> <li> <p>Constraint Integration:</p> <ul> <li>Merge constraints consistently across nucleotides involved in multiple interactions.</li> </ul> </li> <li> <p>Sequence Generation &amp; Constraint Satisfaction:</p> <ul> <li>Generate candidate sequences systematically using backtracking or constraint-solving algorithms.</li> </ul> </li> <li> <p>Scoring &amp; Ranking:</p> <ul> <li>Evaluate and rank based on geometric accuracy, IDI penalties, substitution frequencies, and thermodynamic considerations.</li> <li>Optional brief 3D refinement for top sequences.</li> </ul> </li> <li> <p>Output:</p> <ul> <li>Clearly ranked feasible RNA sequences.</li> </ul> </li> </ol>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#4-detailed-implementation-pseudo-code","title":"4. Detailed Implementation &amp; Pseudo-Code \ud83e\uddd1\u200d\ud83d\udcbb","text":""},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#41-detect-classify-base-pairs","title":"4.1. Detect &amp; Classify Base Pairs \ud83d\udd0d","text":"<pre><code>def detect_and_classify_base_pairs(coords, sequence):\n    base_pairs = []\n    for (i, j) in candidate_pairs(coords):\n        if geometric_criteria_satisfied(i, j, coords):\n            family = classify_geometric_family(i, j, coords)\n            env_info = gather_environment_info(i, j, coords)\n            base_pairs.append((i, j, family, sequence[i], sequence[j], env_info))\n    return base_pairs\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#42-build-isosteric-substitution-sets","title":"4.2. Build Isosteric Substitution Sets \ud83d\udcd0","text":"<pre><code>def get_isosteric_substitutions(family, orig_pair, env_info, isosteric_db):\n    return isosteric_db[family].get(orig_pair, set())\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#43-environment-filtering","title":"4.3. Environment Filtering \ud83c\udf0a","text":"<pre><code>def filter_by_env(possible_pairs, env_info):\n    filtered = set()\n    for (X, Y) in possible_pairs:\n        if environment_satisfied(X, Y, env_info):\n            filtered.add((X, Y))\n    return filtered\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#44-integrate-constraints","title":"4.4. Integrate Constraints \ud83d\udccc","text":"<pre><code>def build_pair_options_3D(base_pairs, isosteric_db, sequence):\n    n = len(sequence)\n    per_position_allowed = {i: set('ACGU') for i in range(n)}\n    base_pair_options = {}\n\n    for (i, j, fam, b1, b2, env) in base_pairs:\n        candidates = get_isosteric_substitutions(fam, (b1, b2), env, isosteric_db)\n        filtered_pairs = filter_by_env(candidates, env)\n        base_pair_options[(i, j)] = filtered_pairs\n        per_position_allowed[i] &amp;= {x[0] for x in filtered_pairs}\n        per_position_allowed[j] &amp;= {x[1] for x in filtered_pairs}\n\n    return per_position_allowed, base_pair_options\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#45-sequence-generation-backtracking","title":"4.5. Sequence Generation (Backtracking) \ud83d\udd04","text":"<pre><code>def generate_sequences(sequence, per_pos_allowed, pair_options):\n    solutions, partial = [], [None]*len(sequence)\n\n    def backtrack(pos):\n        if pos == len(sequence):\n            solutions.append(''.join(partial))\n            return\n        for nt in per_pos_allowed[pos]:\n            partial[pos] = nt\n            if local_constraints_ok(pos, partial, pair_options):\n                backtrack(pos+1)\n\n    backtrack(0)\n    return solutions\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#46-scoring-ranking","title":"4.6. Scoring &amp; Ranking \ud83d\udcca","text":"<pre><code>def rank_solutions(solutions, base_pairs, scoring_params=None):\n    scored_list = []\n    for seq_candidate in solutions:\n        cost = sum(compute_pair_cost(...) for pair in base_pairs)\n        scored_list.append((seq_candidate, cost))\n    scored_list.sort(key=lambda x: x[1])\n    return scored_list\n</code></pre>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#5-additional-implementation-notes","title":"5. Additional Implementation Notes \ud83d\udcdd","text":""},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#51-key-implementation-considerations","title":"5.1. Key Implementation Considerations \ud83d\udccc","text":"<ul> <li>Explicitly handle triple/quadruple interactions and bridging waters \ud83d\udca7.</li> <li>Clearly differentiate partial vs. full redesign scope to manage computational complexity \u2699\ufe0f.</li> <li>Explicitly address syn/anti constraints \ud83d\udd04.</li> <li>Apply heuristic strategies to enhance computational efficiency \ud83d\ude80.</li> <li>MSAs are optional and intended for functional validation\u2014not primary design \ud83d\udccb.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#52-isodiscrepancy-index-idi-thresholds-and-isosteric-subsets","title":"5.2. IsoDiscrepancy Index (IDI) Thresholds and Isosteric Subsets \ud83d\udcd0","text":"<ul> <li>Isosteric pairs (IDI \u2264 2.0): Overlay accurately, preserving backbone geometry \ud83d\udfe2.</li> <li>Near-isosteric pairs (2.0 &lt; IDI \u2264 3.3): Mildly perturb geometry; allowable selectively with caution \ud83d\udfe1.</li> <li>Non-isosteric pairs (IDI &gt; 3.3): Introduce significant geometric distortions and should generally be avoided \ud83d\udd34.</li> </ul> <p>This nuanced approach enables precise acceptance criteria during substitution selection in redesign pipelines.</p>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#53-frequency-data-and-statistical-weighting","title":"5.3. Frequency Data and Statistical Weighting \ud83d\udcca","text":"<ul> <li>Incorporate frequency-based weighting informed by large-scale analyses (Stombaugh et al., 2009).</li> <li>Favor commonly observed isosteric substitutions over rare or near-isosteric alternatives, leveraging statistical prevalence to guide effective sequence design \ud83d\udcc8.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#54-bridging-water-and-basephosphate-contacts","title":"5.4. Bridging Water and Base\u2013Phosphate Contacts \ud83d\udca6","text":"<ul> <li>Ensure substitutions maintain critical hydrogen-bond donor/acceptor groups for motifs dependent on bridging waters and base\u2013phosphate hydrogen bonds.</li> <li>Environment filters must penalize or discard substitutions disrupting these essential structural interactions, particularly prevalent in large rRNAs (Leontis &amp; Westhof, 2001) \ud83c\udf0a.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#55-handling-partial-vs-composite-motifs-fr3d-perspective","title":"5.5. Handling Partial vs. Composite Motifs (FR3D Perspective) \ud83c\udf33","text":"<ul> <li>Leverage FR3D\u2019s subgraph approach (Sarver et al., 2008) to flexibly accept nucleotides from disparate regions.</li> <li>Enable consideration of partial, composite, or tertiary motifs, beyond continuous hairpin loops, to broaden the redesign capabilities \ud83c\udf10.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#56-msa-derived-conservation-vs-direct-3d-geometry","title":"5.6. MSA-Derived Conservation vs. Direct 3D Geometry \ud83e\udde9","text":"<ul> <li>While traditional MSA-driven methods effectively capture co-variation, direct 3D geometry significantly expands motif coverage to less frequently sampled structures.</li> <li>Hybrid approaches optionally combine direct geometry-based isostericity data with MSA-derived conservation to validate sequences based on natural occurrence and functional residue conservation \ud83d\udd2c.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#57-emphasizing-near-isosteric-as-an-engineering-tolerance","title":"5.7. Emphasizing \u201cNear-isosteric\u201d as an Engineering Tolerance \u2696\ufe0f","text":"<ul> <li>Allow near-isosteric substitutions selectively, recognizing minor backbone distortions (IDI 2.0\u20133.3) as an acceptable trade-off for increased design diversity.</li> <li>Near-isosteric substitutions should be considered if critical functional sites remain uncompromised, enabling greater sequence variability while maintaining structural integrity \ud83c\udf9b\ufe0f.</li> </ul>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#6-conclusion","title":"6. Conclusion \u2705","text":"<p>This comprehensive RNA redesign pipeline integrates structural geometry, isostericity, environmental constraints, and computational approaches, enabling reliable RNA redesign without primary reliance on MSAs.</p>"},{"location":"reference/advanced_methods/isosteric_substitutions/RNA_isostericity/#7-references","title":"7. References \ud83d\udcda","text":"<ul> <li>Leontis\u2013Westhof Classification: Leontis &amp; Westhof (RNA, 2001); Leontis, Stombaugh &amp; Westhof (NAR, 2002).</li> <li>IsoDiscrepancy Index (IDI): Stombaugh et al. (NAR, 2009).</li> <li>FR3D: Sarver et al. (J. Math. Biol., 2008). ```</li> </ul>"},{"location":"reference/external_lit/2d_structure_prediction_papers/","title":"2D Structure","text":"<p>===== Published online 18 November 2021 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 https://doi.org/10.1093/nar/gkab1074 UFold: fast and accurate RNA secondary structure prediction with deep learning Laiyi Fu 1,2,\u2020, Yingxin Cao2,5,6,\u2020, Jie Wu 3 , Qinke Peng 1 , Qing Nie 4,5,6 and Xiaohui Xie2, 1 Systems Engineering Institute, School of Electronic and Information Engineering, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi 710049, China, 2 Department of Computer Science, University of California, Irvine, CA 92697, USA, 3 Department of Biological Chemistry, University of California, Irvine, CA 92697, USA, 4 Department of Mathematics, University of California, Irvine, CA 92697, USA, 5 Center for Complex Biological Systems, University of California, Irvine, CA 92697, USA and 6 NSF-Simons Center for Multiscale Cell Fate Research, University of California, Irvine, CA 92697, USA Received April 30, 2021; Revised September 15, 2021; Editorial Decision October 18, 2021; Accepted October 19, 2021 ABSTRACT For many RNA molecules, the secondary structure is essential for the correct function of the RNA. Pre- dicting RNA secondary structure from nucleotide se- quences is a long-standing problem in genomics, but the prediction performance has reached a plateau over time. Traditional RNA secondary structure pre- diction algorithms are primarily based on thermo- dynamic models through free energy minimization, which imposes strong prior assumptions and is slow to run. Here, we propose a deep learning- based method, called UFold, for RNA secondary structure prediction, trained directly on annotated data and base-pairing rules. UFold proposes a novel image-like representation of RNA sequences, which can be efficiently processed by Fully Convolutional Networks (FCNs). We benchmark the performance of UFold on both within- and cross-family RNA datasets. It significantly outperforms previous meth- ods on within-family datasets, while achieving a sim- ilar performance as the traditional methods when trained and tested on distinct RNA families. UFold is also able to predict pseudoknots accurately. Its prediction is fast with an inference time of about 160 ms per sequence up to 1500 bp in length. An online web server running UFold is available at https://ufold.ics.uci.edu. Code is available at https: //github.com/uci-cbcl/UFold. INTRODUCTION The biology of RNA is diverse and complex. Aside from its conventional role as an intermediate between DNA and protein, cellular RNA consists of many other functional classes, including ribosomal RNA (rRNA), transfer RNA (tRNA), small nuclear RNA (snRNA), microRNA and other noncoding RNAs (1\u20134). Some RNAs possess cat- alytic functionality, playing a role similar to protein en- zymes. The spliceosome, which performs intron splicing, is assembled from several snRNAs. The microRNAs are abundant in many mammalian cell types, targeting \u223c60% of genes (5), and are often regarded as biomarkers for di- verse diseases (6). Cellular RNA is typically single-stranded. RNA fold- ing is in large part determined by nucleotide base pair- ing, including canonical base pairing\u2013\u2013A\u2013U, C\u2013G and non- Watson\u2013Crick pairing G-U, and non-canonical base pair- ing (7,8). The base-paired structure is often referred to as the secondary structure of RNA (9). For many RNA molecules, the secondary structure is essential for the cor- rect function of the RNA, in many cases, more than the pri- mary sequence itself. As evidence of this, many homologous RNA species demonstrate conserved secondary structures, although the sequences themselves may diverge (10). RNA secondary structure can be determined from atomic coordinates obtained from X-ray crystallography, nuclear magnetic resonance (NMR), or cryogenic elec- tron microscopy (11\u201313). However, these methods have low throughput. Only a tiny fraction of RNAs have experimen- tally determined structures. To address this limitation, ex- perimental methods have been proposed to infer base par- ing by using probes based on enzymes, chemicals, and cross- linking techniques coupled with high throughput sequenc- ing (14\u201317). Although promising, these methods are still at the early stage of development, unable to provide precise base-pairing at a single nucleotide solution. Computationally predicting the secondary structure of RNA is a long-standing problem in genomics and bioinfor- matics. Many methods have been proposed over the past two decades. They can be broadly classified into two cate- gories: (i) single sequence prediction methods and (ii) com- * To whom correspondence should be addressed. Tel: +1 949 824 9289; Fax: +1 949 824 4056; Email: xhx@ics.uci.edu \u2020The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors. C\u00a9 The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research. This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 2 OF 12 parative methods. In the first category, the most common method is to search for thermodynamically stable states through free energy minimization. If the secondary struc- ture contains only nested base pairing, the energy minimiza- tion can be efficiently solved through dynamic program- ming, such as those implemented in Vienna RNAfold (18), MFold (19), RNAstructure (20) and CONTRAfold (21). Faster implementations that try to improve the speed of dy- namic programming include Rfold (22), Vienna RNAplfold (23), LocalFold (24) and LinearFold (25). Efficient dy- namic programming algorithms that sample suboptimal secondary structures from the Boltzmann ensembles of structures have also been proposed, for example, Centroid- Fold (26). However, some dynamic programming-based methods break down when base pairs contain non-nested patterns, called pseudoknots, which include two stem\u2013loop structures with half of one stem intercalating between the two halves of another stem. Predicting secondary struc- tures with pseudoknots is hard and has shown to be NP- complete under the energy minimization framework (27). Methods in the secondary category utilize covariance meth- ods by aligning related RNA sequences and identifying cor- related compensatory mutations. The second category of methods such as (28\u201330) analyze multiple sequences to de- termine points of base covariance within the sequences to help infer base pair positions, and try to predict conserved structures. Although the list of proposed methods in each of the two categories is long and diverse (31), the performance of these methods has not been significantly improved over time, reaching a performance ceiling of about 80% (32). It is possible because they fail to account for base pairing re- sulting from tertiary interactions (33), unstacked base pairs, pseudoknot, noncanonical base pairing, or other unknown factors (8). Recently deep learning techniques have started to emerge as an alternative approach to functional structure predic- tion problems including RNA secondary structure predic- tion problems (34\u201338). Compared to the thermodynamic model-based approaches, the learning-based methods ben- efit from making few assumptions, allowing pseudoknots, and accounting for tertiary interactions, noncanonical base pairing, or other previously unrecognized base-pairing con- straints. Existing deep learning methods differ in model ar- chitectural design and their choices of model input and out- put. These methods either treat the input as a sequence, utilizing LSTM (39) or transformer encoder (40) to cap- ture long-range interactions between nucleotides (37,41,42). Other methods aim to integrate deep learning techniques with dynamic programming or thermodynamic methods to alleviate prediction biases (34,35,41). However, existing deep learning approaches still face several challenges: First, both LSTM and transformer encoder modules involve a huge number of model parameters, which lead to high com- putational cost and low efficiency. Second, integrating with thermodynamic optimization methods will push the models to assume the assumptions underlying traditional methods, which can hinder the model performance. Third, because the performance of deep learning models depends heavily on the distribution of training data, we need to think about how to improve the performance of these models on previ- ously unseen classes of RNA structures (41). Because many new RNA families have yet to be discovered, it would be im- portant for the learning-based models to have a good gen- eralization ability. Instead of using the nucleotide sequence itself, the in- put of our model consists of all possible base-pairing maps within the input sequence. Each map, first represented by a square matrix of the same dimension as the input sequence length, denotes the occurrences of one of the 16 possible base pairs between the input nucleotides. Under this new representation, the input is treated as a 2D \u2018image\u2019 with 16 channels, allowing the model to explicitly consider all long- range interactions and all possible base pairing, including non-canonical ones. We include one additional channel to store the pairing probability between input base pairs cal- culated based on three paring rules (34) and concatenate it with the previous 16 channel representation. So, an over- all 17 channel 2D map is used as our model input. We use an encoder-decoder framework to extract multi-scale long- and short-range interaction features of the input sequence, implemented in a U-Net model (43). For this reason, we will refer to our method as UFold (stands for U-Net based on RNA folding). The output of UFold is the predicted contact score map between the bases of the input sequence. UFold is fully convolutional, and as such, it can readily handle input sequences with variable lengths. We conduct experiments on both known family RNA sequences and cross family RNA sequences to compare the performance of UFold against both the traditional energy minimization-based methods and recent learning- based methods. We show that UFold yields substantial performance gain over previous methods on within-family datasets, highlighting its promising potential in solving the RNA secondary structure prediction problem. We also show how to use synthetic data to improve the generaliza- tion of UFold on the more challenging cases of cross-family RNA structure prediction. UFold is fast with an inference time of an average of 160 ms per sequence for RNA sequences with lengths of up to 1500 bp. We have developed an online web server running UFold RNA secondary structure prediction. The server is freely available, allowing users to enter sequences and visu- alize predicted secondary structures. MATERIALS AND METHODS Datasets Several benchmark datasets are used in this study: (a) RNAStralign (44), which contains 30 451 unique sequences from 8 RNA families; (b) ArchiveII (45), which contains 3975 sequences from 10 RNA families and is the most widely used dataset for benchmarking RNA structure pre- diction performance; (c) bpRNA-1m (46), which contains 102 318 sequences from 2588 families and is one of the most comprehensive RNA structure datasets available and (d) bpRNA-new, derived from Rfam 14.2 (41,47), contain- ing sequences from 1500 new RNA families. RNA families occurring in bpRNA-1m or any other dataset are excluded from bpRNA-new. e) PDB dataset from bpRNA and PDB database (46,48), which contains high-resolution (&lt;3.5 \u02daA) RNA X-ray structures, we also manually downloaded se- quences that were submitted to PDB from July 2017 to Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 PAGE 3 OF 12 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 October 2020. In this work, the bpRNA-new dataset is treated as a cross-family dataset to assess cross-family model generalization. The RNAStralign dataset is randomly split into training and test sets, with 24 895 and 2854 samples, respectively. Re- dundant sequences between test and training are removed in the same way as processed in e2efold (36) and MXFold2 (41). For the bpRNA-1m dataset, we followed the same pro- cessing procedure used in MXfold2 (41) by using the CD- HIT program (49) to remove redundant sequences and ran- domly split the dataset into two sub-datasets for training and testing, named TR0 and TS0, respectively. Redundancy removed ArchiveII and bpRNA-new are used only for test- ing. As for the PDB dataset, we used PDB sequences re- trieved from bpRNA database and PDB database as train- ing data, and then referred to the name of datasets TS1, TS2, TS3 from (50) as test set and manually collect their high-quality RNA secondary structure from the PDB file using RNApdbee 2.0 (51). Sequences with similarity scores of greater than 80% to the training data were discarded us- ing CD-HIT-EST. Details of statistics of the datasets are listed in Supplementary Tables S1 and S2. In addition, we also include data augmentation strategy to enlarge the training set, which is detailed in Results section. All in all, the training datasets we used in the paper are RNAStralign training dataset, TR0, augmented training data, and PDB training data. The test datasets are ArchiveII, TS0, bpRNA- new and PDB test data (TS1, TS2 and TS3). Input and output representation The general problem of the RNA secondary struc- ture prediction is to predict base pairing patterns given an input sequence. Let x = (x1, x2, \u00b7 \u00b7 \u00b7 , xL) with xi \u2208 { \u2032A\u2032, \u2032U\u2032, \u2032C\u2032, \u2032G\u2032 } be an input sequence of length L. The goal is to predict the secondary structure of x, represented by a contact matrix A \u2208 {0, 1}L\u00d7L with Ai j = 1 denoting a base pairing between bases xi and x j , and 0 otherwise. UFold utilizes a deep neural network to predict the con- tact matrix given the input. Next, we describe several design choices behind UFold (Figure 1). Most existing learning-based methods treat the input as a sequence and use recurrent neural nets (RNNs) to model the interaction between different bases. Gated RNNs, such as LSTMs and GRUs, are often the method of choice for dealing with sequential data because of their ability to model long-range dependencies. However, RNN mod- els need to be run sequentially, causing issues in both train- ing and inference. Newer RNA structure prediction mod- els based on transformers, which do not require the se- quential data to be processed in order, have also been proposed (36). Unlike the previous models, UFold converts the input sequence directly into an \u2018image\u2019. This is done by first en- coding x with one-hot representation, representing the se- quence with an L \u00d7 4 binary matrix X \u2208 {0, 1}L\u00d74 . x is then transformed into a 16 \u00d7 L \u00d7 Ltensor through a Kronecker product between x and itself, followed by reshaping dimen- sions (Figure 1a), K = X \u2297 X (1) In this representation, input K \u2208 {0, 1}16\u00d7L\u00d7L can be un- derstood as an image of size L \u00d7 L with 16 color channels. Each channel specifies one of the 16 possible base-pairing rules; K(i, j, k) denotes whether bases x j and xk are paired according to the i-th base-pairing rule (e.g. i = 2 for A\u2013C pairing). To overcome the sparsity bringing by converting sequenc- ing into 16 channels, we also adopt an extra channel used in CDPFold (34), which reflects the implicit matching be- tween bases (more details in Supplementary notes section 1 and Figure S1). We calculate the paring possibilities be- tween each nucleotide and others from one sequence ac- cording to three paring rules (34), using these rules we could calculate the specific values of each nucleotide position with other nucleotides. These non-binary values may help allevi- ate the sparsity of the model and provide more information on paring bases. The calculated matrix W \u2208 R1\u00d7L\u00d7L is then concatenated with K along the first dimension to get the final UFold input I of dimension 17 \u00d7 L \u00d7 L. UFold takes I as input and computes Y = f (I; \u03b8) with a deep convolutional neural net (Figure 1b). The output Y \u2208 [0, 1]L\u00d7L is a L \u00d7 Lmatrix, with Yi j denoting the probability score of nucleotides bases xi and x j being paired. The new input representation taken by UFold has several advantages: first, using an image representation allows it to model all possible long-range interactions explicitly. Base pairing between distant sequence segments shows up locally in the image representation. Second, it considers all pos- sible base pairing patterns, making no distinction between canonical and non-canonical base pairs. Third, it allows us to implement a fully convolutional neural model that can handle variable sequence length, eliminating the need of padding the input sequence to a fixed length. Input and scoring network architecture UFold uses an encoder-decoder architecture for computing the predicted contact score matrix Y (Figure 1). The model consists of a sequence of down-sampling layers (encoder) to derive increasingly complex semantic representations of the input, followed by a sequence of up-sampling layers (de- coder), with lateral connections from the encoder to fill in contextual information. The overall design follows the U- Net model, widely used in the field of image segmentation. More detail on the framework is illustrated in Supplemen- tary file (Section 2). All operations in UFold are fully convolutional. Thus, the input sequence can be of variable length, with the out- put matrix changing correspondingly. This feature is espe- cially beneficial for RNA secondary structure as the range of the input sequence length is very large, from tens of nu- cleotides for small RNAs to thousands of nucleotides for large RNAs. Padding input sequences to the same length as done in other methods would have significantly impacted the efficiency of the algorithm. UFold is trained by minimizing the cross-entropy be- tween the predicted probability contact matrix Y and the true contact matrix A, using stochastic gradient descent. The predicted matrix of pairs represents the base-pairing probabilities, which are strictly positive in our model. Our final layer of activation function takes the form of a sigmoid Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 4 OF 12 Figure 1. The overall architecture of UFold. (A) The input sequence is first converted into one-hot representation. A novel representation of the sequence is then introduced by taking outer product of all combinations of base pair channels, resulting in an image-like representation with 16 channels and with the same size as the contact map. We calculate a paring possibilities matrix according to three paring rules and concatenate this extra matrix with previous feature to obtain the final 17 channel input. (B) Detailed architecture of our framework. The input is a 17 \u00d7 L \u00d7 Ltensor representation of the original sequence. The U-Net takes the 17 \u00d7 L \u00d7 L tensor as input and outputs an L \u00d7 L symmetric score matrix Y. After postprocessing, matrix \u02c6Y\u2217 is the final prediction of the contact map. activation \u03c3 (x) = 1 1+e\u2212x , where x is an unbounded output from the previous layer. A positive weight \u03c9 of 300 is added to leverage the imbalanced 0/1 distribution to derive the loss function as below. Loss (Y, A; \u03b8) = \u2212 \u2211 i j [Ai j log (Yi j ) + (1 \u2212 Ai j ) log (1 \u2212 Yi j )] . (2) where \u03b8 is used to represent all parameters in the neural net- work. Postprocessing After the symmetric contact scoring matrix Y is computed by UFold, we use a postprocessing procedure to derive the final secondary structure. The postprocessing proce- dure takes into account four hard constraints in the sec- ondary structure: (i) the contact matrix should be symmet- ric; (ii) only canonical plus U\u2013G paring rules are allowed (this can be relaxed by including other non-canonical base pairs); (iii) no sharp loops are allowed, for which we set Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 PAGE 5 OF 12 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 Ai j = 0, \u2200i, j with |i \u2212 j |lt; 4 and (iv) no overlapping pairs are allowed, that is, A1 \u2264 1. We follow the steps used in e2efold by encoding constraints (ii) and (iii) into a matrix M, defined as M(x):=1 if nucleotides xi and x j can be paired under constraints (ii) and (iii) and equals to 0 otherwise. To address the first two constraints, we transform Y ac- cording to T (Y) := 1 2 (Y + YT ) \u25e6 M(x) (3) where \u25e6 denotes element-wise multiplication. It ensures that the transformed Y is symmetric and satisfies constraints (i), (ii) and (iii). To address the last constraint, we relax it into a linear programming problem, \u02c6Y\u2217 = argmax \u02c6Y\u2208RL\u00d7L \u3008 \u02c6Y, T (Y)\u3009 \u2212 \u03c1\u2016 \u02c6Y\u2016, subject to \u02c6Y1 \u2264 1 (4) which tries to find an optimal scoring matrix \u02c6Y that is most similar to T (Y) while at the same time satisfying the nonoverlapping pair constraint. The similarity is measured in terms of the inner product between \u02c6Yand T (Y). \u03c1 is a hyperparameter controlling the sparsity of the final output. The final predicted binary contact map is taken to be \u02c6Y\u2217 after thresholding it with an offset, which is chosen through a grid search. Training and evaluation During training, stratified sampling (36) is applied to the training set to balance the number of training samples from each RNA family. The hyperparameters of UFold are tuned based on the validation set. The number of parameters is listed in Supplementary Table S3. To improve model transferability on previously unseen RNA families, we augment the training set with synthetic data to train UFold. The synthetic data are generated by randomly mutating sequences in the bpRNA-new dataset (previously unseen RNA families). We then use Contrafold to generate predicted structures on the synthetic data and treat them as ground truth. Precision is defined as Pr ec = T P T P+F P , evaluated on all predicted base pairs. Recall is defined as Recall = T P T P+F N . And F1 score is the harmonic mean of precision and recall, defined as F1 = 2 \u00b7 Pr ec\u00b7Recall Pr ec+Recall . We use CPU version of In- tel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz, and for the GPU version we are choosing is Nvidia Titan Xp. RESULTS To benchmark the performance of different models, we first conduct three experimental studies: (a) train models on the RNAStralign training set and evaluate on the RN- Stralign test set and ArchiveII; and (b) train the exact same model on the bpRNA-1m training set (TR0) and evaluate on the bpRNA-1m test set (TS0) as well as on bpRNA- new(bpnew). (c) fine-tune previous model on PDB train- ing dataset and evaluate on a standalone test set. Pub- lished deep learning models usually report results from ei- ther Study A or Study B. To have a fair and direct com- parison with previous models, we report results from both, Figure 2. Violin plot on the ArchiveII dataset. Visualization of F1 value of UFold versus other 11 RNA secondary structure predictions methods. following the same data splitting, preprocessing, and evalu- ation protocols. In comparing the results from different models, we treat within- versus cross-family results separately. In both studies, the test sets, except bpRNA-new(bpnew), contain mostly within family RNA species, that is, RNA species from a similar family occurring in the training set. By con- trast, the bpRNA-new dataset contains only cross-family RNA species, that is, none of them shares the same RNA family as those in the training set. Although RNAs that are from a known family are easier digging into, their folding patterns can provide more useful information of formation secondary structure, which it is helpful for the model\u2019s per- formance on previously unseen families to assess its model transferability. Experimental results on within family datasets In this section, we report the results of our model on within-family test sets. Figure 2 and Supplementary Ta- ble S4 summarizes the evaluation results of UFold on the ArchieveII test set (from Study A), together with the results of a collection of traditional energy-based, including Con- textfold (52), Contrafold (21), Linearfold (25), Eternafold (53), RNAfold (18), RNAStructure (Fold) (54), RNAsoft (55) and Mfold (19), and recent learning-based methods MXfold2 (41), SPOT-RNA (37) and e2efold (36). The tra- ditional methods achieve an F1 score in the range of 0.55\u2013 0.84. A recent state-of-the-art learning-based method im- proves the F1 score to 0.77 (MXfold2). UFold can further improve the performance, achieving an F1 score of 0.91. Compared with MXfold2, UFold achieves an 18% increase in F1 score, a 22% increase in recall, and a 13% increase in precision. Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 6 OF 12 Figure 3. Violin plot on the TS0 dataset. Visualization of F1 value of UFold versus other 11 RNA secondary structure predictions methods. Figure 3 and Supplementary Table S5 summarizes the evaluation results on the TS0 test set (from Study B). Since this dataset was also used in two other deep learning-based methods\u2013\u2013SPOT-RNA and MXfold2, we compare UFold with these two methods along with other energy-based methods. Again, UFold outperforms both the deep learning-based and the energy-based methods. UFold achieves a mean F1 score of 0.654 on this dataset, cor- responding to a 5.7% improvement over SPOT-RNA, the state-of-the-art method on this dataset, and 15% improve- ment over traditional methods. Improvements in recall and precision also surpass all other methods. We conduct an experiment to demonstrate whether the \u2018image-like\u2019 encoding of sequences helps improve the pre- diction of long-range interactions. For this experiment, we use the TS0 dataset as a test dataset since it contains more versatile sequences of different length and various RNA families. For each sequence of length L, we define the paired and unpaired bases with intervals longer than L/2 as long- range base pairing. We then calculate the precision, recall as well as F1 score of UFold on these long-range pairing pre- dictions and compare them to other methods. The results are reported in Supplementary Figure S2 and Supplemen- tary Table S6. We find that UFold achieves significantly bet- ter results than other methods on these long-range pairing predictions. Moreover, the results also show that the per- formance of UFold on long-range base pairing prediction is similar to its performance on short-range base pairings (Figure 2). By contrast, the performances of all other meth- ods significantly deteriorate when evaluated on long-range interactions. These results demonstrate the \u2018image-like\u2019 en- coding facilitates the prediction of long-range interactions. Table 1. Evaluation results of RNA structures with pseudoknots on the RNAStralign test dataset Method Recall Precision Specificity Accuracy UFold 99% 96.2% 96.8% 87.5% SPOT-RNA 97.8% 67.7% 61.8% 31.4% E2Efold 99% 84.4% 84.0% 78.8% RNAstructure (ProbKnot) 76.1% 77.8% 81.5% 38.5% NuPack 93.3% 72.4% 72.2% 51.4% HotKnotsa 56.5% 50% 83.1% 42.7% a The sequence number here is 2021, the rest sequence number is 2826. Predicting secondary structures with pseudoknots is es- pecially challenging for thermodynamic models. We also validate the performance of UFold on predicting base pair- ing in the presence of pseudoknots. For this purpose, we use all RNA structures in the RNAStralign test set, on which we then benchmark UFold against other methods that can pre- dict pseudoknots, including SPOT-RNA, e2efold, RNAs- tructure(ProbKnot) (56), NuPack (57) and HotKnots (58). We examined whether ground truth and predictions have pseudoknot respectively and summarized results in Table 1. As shown in Table 1, all other methods tend to predict pseudoknot structures for normal sequences. The number of the pseudoknot pairs of different types is listed in Sup- plementary Table S7 and accuracy of the pseudoknotted pairs is also measured. The result is shown in Table 1 as well. By contrast, UFold still achieves higher recall, pre- cision and specificity values, while maintaining the high- est pseudoknotted pairs prediction accuracy compared with others, highlighting the robustness of UFold predictions in the presence of pseudoknots. Experimental results on cross family datasets In this section, we evaluate the performance of UFold on previously unseen RNA families. We expect learning-based methods do poorly on these RNAs since they are not repre- sented in the training set as shown in Supplementary Table S8. To address this problem, methods integrating free en- ergy minimization with deep learning methods have been proposed, like MXfold2 (41). However, these methods in- advertently introduce biases into the prediction model and likely lead to reduced performance on within family RNAs. Although UFold does not involve any energy minimiza- tion term in its original design, it uses data augmentation to improve the performance on cross-family RNAs with the help of another model Contrafold (21), a probabilis- tic model which generalizes upon stochastic context-free grammars (SCFGs) by using discriminative training and feature-rich scoring found in typical thermodynamic mod- els. Specifically, for each sequence we randomly choose 20\u2013 30% present of single nucleotides to perform random mu- tation. For each real sequence, we first generate 3 synthetic sequences to create a pool of synthetic sequences. We then use CD-HIT 80 to remove any sequences that have similar- ity over 80% to real sequences. The resulting synthetic se- quence pool is then used for generating synthetic data with size 2000. The synthetic ground truth labels are generated with Contrafold, which then use to train UFold. Those data Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 PAGE 7 OF 12 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 Figure 4. Violin plot on the bpRNA-new dataset. Visualization of F1 value of UFold versus other 11 RNA secondary structure predictions methods. are then merged with the TR0 training set for model train- ing. Figure 4 and Supplementary Table S8 show the eval- uation results of UFold using the previously pre-trained model on the bpRNA-new dataset, containing about 1500 previously unseen RNA families. Note that here UFold is trained only once based on all the training data for the three testing experiments including ArchiveII, TS0 and bpRNA- new datasets. UFold can achieve a similar performance on bpRNA-new dataset as other methods like MXfold2, all of which involve thermodynamic terms or constraints in their objectives. By contrast, UFold is a pure learning-based method. Through data augmentation, it can learn to pre- dict the structures of RNAs not represented in the training set and further improved the performance on previously un- seen family sequences (i.e. bpnew dataset). Furthermore, UFold is also benchmarked on high- resolution based RNA secondary structures derived from the PDB dataset, whose secondary structures have been ex- perimentally validated. We used pretrained model and fine- tuned it on PDB sequences retrieved from bpRNA database and PDB database. Following the partition used in SPOT- RNA2 (50), we divided the PDB sequences into three sub- sets: TS1, TS2 and TS3. The overall result is reported in Figure 5, more detailed results are presented in Supple- mentary Table S9-S11. Based on the results, UFold is deal- ing well in recognizing these dense pairing RNA secondary structures compared with others on this high-quality exper- imentally validated dataset. We also notice another recent model SPOT-RNA2 (50) which incorporates evolutionary- based features besides sequence features, but all the com- pared models in our results are all only sequence based so we do not include it in our summarized results. The re- Figure 5. Violin plot on the PDB dataset. Visualization of F1 value of UFold versus other 11 RNA secondary structure predictions methods. sults of splitting these datasets (TS1, TS2 and TS3) are shown in Supplementary Supplementary Figure S3 and Supplementary Table S9-S11. In addition, we benchmarked 6 RNAs from PDB dataset, which is measured in SPOT- RNA paper. We confirmed that none of these sequences appeared in our training dataset. As shown in Supplemen- tary Figure S4, UFold produced consistently better results than SPOT-RNA and other predictors on these 6 RNAs. Since PDB dataset contains multiple non-canonical pairs, so we systematically measured the performance of UFold against SPOT-RNA which is also capable of predicting non- canonical pairs. The higher mean F1 value in three datasets indicates the superior ability of predicting non-canonical pairs of UFold as shown in Supplementary Table S12. These findings support the effectiveness of UFold in handling non- canonical pairs. We also explored how the UFold performs on different Rfam families. We mapped all the sequences from PDB dataset to Rfam families using Rfam webserver (https://rfam.xfam.org), during which we found 34 RNA families matched to Rfam families, covering 47 of the se- quences in the test set. Among those, we found 26 RNA families (including 39 sequences) that are overlapped with training families. We then evaluated the performance of F1 value on two groups: no Rfam family which contains se- quences that do not match any Rfam or other families in the training set, and within-family which contains sequences matching a family in the training set. As reported the re- sults in Supplementary Figure S5, the sequences that do not match to any Rfam families even achieve higher mean F1 value as it is shown in Supplementary Figure S6. This fur- ther demonstrates UFold\u2019s robust performance. In order to further validate the effectiveness of UFold prediction, we include the assessment of the statistical sig- nificance on the performance comparisons between UFold and other methods. Two types of statistical significance Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 8 OF 12 Figure 6. Visualization of two example UFold RNA secondary structure predictions. From top to bottom: ground truth, UFold prediction, and E2Efold prediction. Two RNA sequences are (A) Aspergillus fumigatus species, the RNA ID is GSP-41122, as recorded in SRPDB database. and (B) Alphaproteobac- teria subfamily 16S rRNA sequence whose database ID is U13162, as recorded in RNAStralign database(http://rna.urmc.rochester.edu). Non-canonical base pairs are colored in light green. In both cases, UFold produces predictions more aligned with the ground-truth. measures are calculated: one based on paired t-tests and the other based on bootstrapping. The paired t-test P-value re- sults are shown in Supplementary Table S13, which shows that UFold performs better than the other methods in a sta- tistically significant way, with most P-values less than 0.05. For the PDB dataset, because its three subsets (TS1, TS2 and TS3) have limited number of sequences, we used bootstrapping strategy on these datasets to estimate the sta- tistical significance. The results are summarized in Supple- mentary Figure S7, which shows that the performance of UFold is significantly better than nearly all other meth- ods. For bootstrapping, margins of improvements reside outside the 95% confidence intervals with steady interval width (Supplementary Figure S8 and Supplementary Table S14). Altogether, our results support previous conclusions and the performance improvements of UFold over previous methods are statistically significant. Another point worth noting is that, since UFold chooses Kronecker product to construct the input, in order to vali- date whether this is a good choice compared to other con- catenation such as outer concatenation adopted in SPOT- RNA (37). We added one additional ablation study, in which we replace the Kronecker product with outer con- catenation by first extending the one-hots column wise and row wise and then concatenating them together to create a new input matrix. We retrain the whole UFold model with this input while keeping the rest the same. We use ArchiveII and bpnew dataset to test the performance in our ablation study. As it is shown in Supplementary Figure S9, on both datasets we tested, the Kronecker product design yields bet- ter results. We think the reason is that the Kronecker prod- uct design provides a more direct representation of base- Table 2. Inference time on the RNAStralign test set Method Time per seq UFold (Pytorch) 0.16 s (GPU) MXfold2(Pytorch) 0.31 s (GPU) E2Efold (Pytorch) 0.40 s (GPU) SPOT-RNA(Pytorch) 77.80 s (GPU) CDPfold (tensorflow) 300.107 s LinearFold (C++) 0.43 s Eternafold (C++) 6.42 s RNAsoft (C++) 4.58 s Mfold (C) 7.65 s RNAstructure (C) 142.02 s RNAfold (C) 0.55 s CONTRAfold (C++) 30.58 s pairing information. On the other hand, outer concatena- tion design in theory contains the same information en- coded in the Kronecker product, but requires more com- plicated modellings to process this information. Visualization After quantitively evaluating the prediction performance, we visualize the RNA secondary structures predicted by UFold to check the pairing details of each nucleotide. For this purpose, the predicted contact maps were first con- verted to a bpseq format according to base pair positions. Raw sequences with the corresponding predicted struc- tures were fed into the VARNA tool (59) to obtain the visualization result. As a comparison, we also show the predicted structures from the other three best-performed methods, MXfold2, SPOT-RNA and e2efold as well as Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 PAGE 9 OF 12 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 Table 3. Functionality comparison of different RNA structure prediction web servers Servers Supported functions UFold SPOT-RNA RNAfold MXfold2 Linearfold Contextfold RNAsoft Contrafold Sequence type-in Yes Yes Yes Yes Yes Yes Yes Yes Fasta file Yes No Yes No Yes Yes Yes Yes Length &gt;600 bp Yes Yes No No Yes Yes No Yes Online visualization Yes Yes Yes Yes Yes N/A No N/A Support multi-samples Yes No No No No No No No the ground-truth structures. Two examples are from the Aspergillus fumigatus species and Alphaproteobacteria subfamily 16S rRNA, their RNA IDs are GSP-41122, as recorded in SRPDB database (60) and U13162 as recorded in RNAStralign database (http://rna.urmc.rochester.edu), respectively. They are drawn and shown in Figure 6. In both cases, UFold generates RNA secondary structures more similar to the ground-truth when compared with other state-of-the-art methods like MXfold2, SPOT-RNA and E2Efold, showing the closest secondary structure to the ground truth structure. In addition, we also visualized more examples from PDB database, whose sequences are retrieved from 2019 to 2021. As the results shown in Sup- plementary Figures S10 and S11, UFold is capable of pre- dicting those structures including pseudoknots and non- canonical pairs more resemble to ground truth structures. Inference time The speed of the prediction algorithm is an important factor in RNA secondary structure prediction, especially for mul- tiple sequences predicting simultaneously. Traditional en- ergy minimization-based methods tend to be slow because of the time complexity of the minimization algorithm. Deep learning-based methods like MXfold2 and SPOT-RNA uti- lize LSTM structure, which require significantly more pa- rameters than UFold, resulting in low efficiency. UFold in- ference, on the other hand, runs on feedforward neural nets only. Specifically, it is comprised of a fully connected con- volutional neural network, which greatly reduces the run- ning time since all operations are readily parallelizable. It can also handle multiple sequences at once, leading to sig- nificantly higher throughput. The average inference time per sequence of UFold on the RNAStralign test set (containing sequences longer than 1000 bp) is reported in Table 2, together with the av- erage running times of other methods. UFold is much faster than both learning-based and energy-based meth- ods. UFold is nearly two times faster than MXfold2, and orders-of-magnitude faster than RNAstruture (Fold), an- other popular energy-based method. The running times of UFold and three other recent deep learning-based meth- ods are also shown in Table 2. All these methods are im- plemented in PyTorch (61) and thus it allows us to com- pare their model efficiency directly. Our model is still the fastest one among all the other deep learning methods, fur- ther demonstrating the efficiency of UFold. To study the ef- fect of sequence length on runtime, we demonstrated two scatter plots of runtime versus length of the sequences. Most computations of UFold are performed on GPU. We first plotted the running time cost on GPU calculation which is shown in Supplementary Figure S12, the runtime is not significantly affected by sequences length since GPUs have efficient parallelization supported by modern deep learning libraries. We then calculated the total runtime (with con- tact map inference and postprocessing) and compared with two other fastest methods, RNAfold and Linearfold, which can deal with variable sequence length of up to 1500 bp. As shown in Supplementary Figure S13, UFold is almost 5 times faster than the other two methods on the most com- mon length sequence (\u223c600 bp) and is at least two times faster in longer sequences (up to 1500 bp). Web server To facilitate the accessibility of UFold, we developed a web server running UFold on the backend and made it freely available. Users can type in or upload RNA sequences in FASTA format. Our server predicts RNA secondary struc- tures using the pre-trained UFold model (trained on all the datasets) and stores predicted structures in a dot-bracket file or bpseq file for end-users to download. Users may also choose to predict non-canonical pairs or not directly in the option panel. The server further provides an inter- face connection to the VARNA tool (59) for visualizing predicted structures. Most existing RNA prediction servers only permit predicting one RNA sequence at a time, such as RNAfold, MXfold2 and SPOT-RNA, and restrict the length of the input sequence. Our server does not have such limitations. Its main functionality differences compared to other servers are highlighted in Table 3. The interface of our web server is shown in Figure 7. DISCUSSION In this study, we present UFold, a new deep learning- based model for RNA secondary structure prediction. We benchmark UFold on both within- and cross-family RNA datasets and demonstrate that UFold significantly outperforms previous methods on within-family datasets, achieving 10\u201330% performance improvement over tradi- tional thermodynamic methods, and 5\u201327% improvement in F1 score over the state-of-the-art learning-based method, bringing in substantial gains in RNA secondary prediction accuracy. In the meantime, it achieves a similar performance as the traditional methods when trained and tested on dis- tinct RNA families. In addition, UFold is fast, being able to generate predictions at roughly 160ms per sequence. A key difference between UFold and previous learning- based methods is its architectural design. Instead of us- ing raw sequences as input, UFold converts sequences into Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 10 OF 12 Figure 7 UFold web server interface (available at https://ufold.ics.uci.edu). UFold web server allows users to type in or upload their own fasta file with multiple sequences (no number limits) and the backend pretrained model will predict the corresponding RNA secondary structures and provide users either ct or bpseq file to download or directly visualize them online. Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 PAGE 11 OF 12 Nucleic Acids Research, 2022, Vol. 50, No. 3 e14 \u2018images\u2019, explicitly modeling all possible base pairing be- tween the nucleotides of the input sequence. This choice of input representation has several important implications: First, base pairing patterns between distant sequence seg- ments show up locally in the image representation, mak- ing the detection and learning of these distant base pair- ing patterns easier. Second, all base pairing patterns are explicitly represented in the input, allowing the model to pick up all potential base pairing rules that might con- tribute to the formation of the secondary structure. Lastly, but perhaps most importantly, the image representation al- lows us to implement a fully convolutional model to pick up base-pairing features across multiple scales through an encoder-decoder architecture. This implementation is not only efficient, with operations highly parallelable and allow- ing for variable input sequence length, but also highly effec- tive in combining both local and global features for the final prediction. Although UFold demonstrates great potential in solv- ing the RNA secondary structure prediction problem, as a learning-based method, its performance is inevitably closely attached to the quality of training data. Unfortu- nately, the number of experimentally resolved RNA sec- ondary structures through X-ray crystallography or NMR remains small. Many secondary structures in the RNAS- tralign dataset are computationally generated by align- ing homologous sequences. Fortunately, high-throughput methods for determining or constraining the secondary structures of RNAs are starting to emerge (62,63). We should also mention that UFold currently predicts RNA structures only based on sequences. It is well-known that RNA structures also depend on other factors, such as tem- perature and salt concentration. How to take these factors into account in deep learning models remains an open ques- tion. Because UFold uses a flexible network architecture, we expect it to be able to incorporate the high-throughput data and specific factors to improve model training and inference. We should note that the method presented here can po- tentially be applied for protein structure prediction as well. The number of amino acids is much higher than the num- ber of bases. It is worth exploring whether all amino acid pairs, which have 400 pairs, or a subset of them should be considered in the input representation. In summary, we show the promising potential of deep learning in solving the long-standing RNA secondary struc- ture problem. The new framework presented here brings in a significant performance gain. We expect the prediction ac- curacy to be further improved as more and higher quality training data are becoming available. DATA AVAILABILITY An online web server running UFold is available at https: //ufold.ics.uci.edu. Code is available at https://github.com/ uci-cbcl/UFold. SUPPLEMENTARY DATA Supplementary Data are available at NAR Online. ACKNOWLEDGEMENTS We acknowledge helpful discussions with MH Celik and members of the Xie lab. FUNDING NSF [IIS-1715017]; NSF [DMS-1763272]; NIH [U54- CA217378]; Simons Foundation [594598]. Funding for open access charge: NSF [IIS-1715017]; NSF [DMS- 1763272]; NIH [U54-CA217378]. Conflict of interest statement. None declared. REFERENCES 1. Noller,H.F. (1984) Structure of ribosomal RNA. Annu. Rev. Biochem., 53, 119\u2013162. 2. Rich,A. and RajBhandary,U. (1976) Transfer RNA: molecular structure, sequence, and properties. Annu. Rev. Biochem., 45, 805\u2013860. 3. Allmang,C., Kufel,J., Chanfreau,G., Mitchell,P., Petfalski,E. and Tollervey,D. (1999) Functions of the exosome in rRNA, snoRNA and snRNA synthesis. EMBO J., 18, 5399\u20135410. 4. Geisler,S. and Coller,J. (2013) RNA in unexpected places: long non-coding RNA functions in diverse cellular contexts. Nat. Rev. Mol. Cell Biol., 14, 699\u2013712. 5. Gebert,L.F. and MacRae,I.J. (2019) Regulation of microRNA function in animals. Nat. Rev. Mol. Cell Biol., 20, 21\u201337. 6. Fu,L. and Peng,Q. (2017) A deep ensemble model to predict miRNA-disease association. Sci. Rep., 7, 14482. 7. Fallmann,J., Will,S., Engelhardt,J., Gr \u00a8uning,B., Backofen,R. and Stadler,P.F. (2017) Recent advances in RNA folding. J. Biotechnol., 261, 97\u2013104. 8. Westhof,E. and Fritsch,V. (2000) RNA folding: beyond Watson\u2013Crick pairs. Structure, 8, R55\u2013R65. 9. Fox,G.E. and Woese,C.R. (1975) 5S RNA secondary structure. Nature, 256, 505\u2013507. 10. Mathews,D.H., Moss,W.N. and Turner,D.H. (2010) Folding and finding RNA secondary structure. Cold Spring Harb. Perspect. Biol., 2, a003665. 11. F \u00a8urtig,B., Richter,C., W \u00a8ohnert,J. and Schwalbe,H. (2003) NMR spectroscopy of RNA. ChemBioChem, 4, 936\u2013962. 12. Cheong,H.-K., Hwang,E., Lee,C., Choi,B.-S. and Cheong,C. (2004) Rapid preparation of RNA samples for NMR spectroscopy and X-ray crystallography. Nucleic Acids Res., 32, e84. 13. Fica,S.M. and Nagai,K. (2017) Cryo-electron microscopy snapshots of the spliceosome: structural insights into a dynamic ribonucleoprotein machine. Nat. Struct. Mol. Biol., 24, 791. 14. Ehresmann,C., Baudin,F., Mougel,M., Romby,P., Ebel,J.-P. and Ehresmann,B. (1987) Probing the structure of RNAs in solution. Nucleic Acids Res., 15, 9109\u20139128. 15. Knapp,G. (1989) [16]Enzymatic approaches to probing of RNA secondary and tertiary structure. Methods Enzymol., 180, 192\u2013212. 16. Bevilacqua,P.C., Ritchey,L.E., Su,Z. and Assmann,S.M. (2016) Genome-wide analysis of RNA secondary structure. Annu. Rev. Genet., 50, 235\u2013266. 17. Underwood,J.G., Uzilov,A.V., Katzman,S., Onodera,C.S., Mainzer,J.E., Mathews,D.H., Lowe,T.M., Salama,S.R. and Haussler,D. (2010) FragSeq: transcriptome-wide RNA structure probing using high-throughput sequencing. Nat. Methods, 7, 995\u20131001. 18. Lorenz,R., Bernhart,S.H., Zu Siederdissen,C.H., Tafer,H., Flamm,C., Stadler,P.F. and Hofacker,I.L. (2011) ViennaRNA Package 2.0. Algorith. Mol. Biol., 6, 26. 19. Zuker,M. (2003) Mfold web server for nucleic acid folding and hybridization prediction. Nucleic Acids Res., 31, 3406\u20133415. 20. Mathews,D.H. and Turner,D.H. (2006) Prediction of RNA secondary structure by free energy minimization. Curr. Opin. Struct. Biol., 16, 270\u2013278. 21. Do,C.B., Woods,D.A. and Batzoglou,S. (2006) CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinformatics, 22, e90\u2013e98. Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 e14 Nucleic Acids Research, 2022, Vol. 50, No. 3 PAGE 12 OF 12 22. Kiryu,H., Kin,T. and Asai,K. (2008) Rfold: an exact algorithm for computing local base pairing probabilities. Bioinformatics, 24, 367\u2013373. 23. Bernhart,S.H., Hofacker,I.L. and Stadler,P.F. (2006) Local RNA base pairing probabilities in large sequences. Bioinformatics, 22, 614\u2013615. 24. Lange,S.J., Maticzka,D., M \u00a8ohl,M., Gagnon,J.N., Brown,C.M. and Backofen,R. (2012) Global or local? Predicting secondary structure and accessibility in mRNAs. Nucleic Acids Res., 40, 5215\u20135226. 25. Huang,L., Zhang,H., Deng,D., Zhao,K., Liu,K., Hendrix,D.A. and Mathews,D.H. (2019) LinearFold: linear-time approximate RNA folding by 5\u2032-to-3\u2032dynamic programming and beam search. Bioinformatics, 35, i295\u2013i304. 26. Sato,K., Hamada,M., Asai,K. and Mituyama,T. (2009) CENTROIDFOLD: a web server for RNA secondary structure prediction. Nucleic Acids Res., 37, W277\u2013W280. 27. Wang,X. and Tian,J. (2011) Dynamic programming for NP-hard problems. Procedia Eng., 15, 3396\u20133400. 28. Gardner,P.P. and Giegerich,R. (2004) A comprehensive comparison of comparative RNA structure prediction approaches. BMC Bioinformatics, 5, 140. 29. Havgaard,J.H. and Gorodkin,J. (2014) RNA structural alignments. Part I. Sankoff-based approaches for structural alignments. In: RNA Sequence, Structure, and Function: Computational and Bioinformatic Methods. Springer, pp. 275\u2013290. 30. Washietl,S., Bernhart,S.H. and Kellis,M. (2014) Energy-based RNA consensus secondary structure prediction in multiple sequence alignments. In: RNA Sequence, Structure, and Function: Computational and Bioinformatic Methods. 31. Kings Oluoch,I., Akalin,A., Vural,Y. and Canbay,Y. (2018) A review on RNA secondary structure prediction algorithms. In: 2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT). IEEE, ANKARA, Turkey, pp. 18\u201323. 32. Seetin,M.G. and Mathews,D.H. (2012) RNA structure prediction: an overview of methods. In: Bacterial Regulatory RNA. Springer, pp. 99\u2013122. 33. Nowakowski,J. and Tinoco,I. Jr (1997) RNA structure and stability. In: Seminars in Virology. Elsevier, Vol. 8, pp. 153\u2013165. 34. Zhang,H., Zhang,C., Li,Z., Li,C., Wei,X., Zhang,B. and Liu,Y. (2019) A new method of RNA secondary structure prediction based on convolutional neural network and dynamic programming. Front. Genet., 10, 467. 35. Wang,L., Liu,Y., Zhong,X., Liu,H., Lu,C., Li,C. and Zhang,H. (2019) DMFold: A novel method to predict RNA secondary structure with pseudoknots based on deep learning and improved base pair maximization principle. Front. Genet., 10, 143. 36. Chen,X., Li,Y., Umarov,R., Gao,X. and Song,L. (2019) RNA secondary structure prediction by learning unrolled algorithms. In: International Conference on Learning Representations. 37. Singh,J., Hanson,J., Paliwal,K. and Zhou,Y. (2019) RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nat. Commun., 10, 5407. 38. Wang,S., Peng,J., Ma,J. and Xu,J. (2016) Protein secondary structure prediction using deep convolutional neural fields. Sci. Rep., 6, 18962. 39. Hochreiter,S. and Schmidhuber,J. (1997) Long short-term memory. Neural Comput., 9, 1735\u20131780. 40. Cer,D., Yang,Y., Kong,S., Hua,N., Limtiaco,N., John,R.S., Constant,N., Guajardo-C\u00b4espedes,M., Yuan,S., Tar,C. et al. (2018) Universal sentence encoder. arXiv doi: https://arxiv.org/abs/1803.11175v1, 13 April 2018, preprint: not peer reviewed. 41. Sato,K., Akiyama,M. and Sakakibara,Y. (2021) RNA secondary structure prediction using deep learning with thermodynamic integration. Nat. Commun., 12, 941. 42. Chen,X., Li,Y., Umarov,R., Gao,X. and Song,L. (2019) RNA secondary structure prediction by learning unrolled algorithms. In: International Conference on Learning Representations. 43. Ronneberger,O., Fischer,P. and Brox,T. (2015) U-net: convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 234\u2013241. 44. Tan,Z., Fu,Y., Sharma,G. and Mathews,D.H. (2017) TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs. Nucleic Acids Res., 45, 11570\u201311581. 45. Sloma,M.F. and Mathews,D.H. (2016) Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures. RNA, 22, 1808\u20131818. 46. Danaee,P., Rouches,M., Wiley,M., Deng,D., Huang,L. and Hendrix,D. (2018) bpRNA: large-scale automated annotation and analysis of RNA secondary structure. Nucleic Acids Res., 46, 5381\u20135394. 47. Kalvari,I., Nawrocki,E.P., Ontiveros-Palacios,N., Argasinska,J., Lamkiewicz,K., Marz,M., Griffiths-Jones,S., Toffano-Nioche,C., Gautheret,D., Weinberg,Z. et al. (2021) Rfam 14: expanded coverage of metagenomic, viral and microRNA families. Nucleic Acids Res., 49, D192\u2013D200. 48. Rose,P.W., Prli\u00b4c,A., Altunkaya,A., Bi,C., Bradley,A.R., Christie,C.H., Costanzo,L.D., Duarte,J.M., Dutta,S., Feng,Z. et al. (2016) The RCSB protein data bank: integrative view of protein, gene and 3D structural information. Nucleic Acids Res., 45, D271\u2013D281. 49. Li,W. and Godzik,A. (2006) Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinformatics, 22, 1658\u20131659. 50. Singh,J., Paliwal,K., Zhang,T., Singh,J., Litfin,T. and Zhou,Y. (2021) Improved RNA secondary structure and tertiary base-pairing prediction using evolutionary profile, mutational coupling and two-dimensional transfer learning. Bioinformatics, 37, 2589\u20132600. 51. Zok,T., Antczak,M., Zurkowski,M., Popenda,M., Blazewicz,J., Adamiak,R.W. and Szachniuk,M. (2018) RNApdbee 2.0: multifunctional tool for RNA structure annotation. Nucleic Acids Res., 46, W30\u2013W35. 52. Zakov,S., Goldberg,Y., Elhadad,M. and Ziv-Ukelson,M. (2011) Rich parameterization improves RNA structure prediction. J. Comput. Biol., 18, 1525\u20131542. 53. Wayment-Steele,H.K., Kladwang,W., Participants,E. and Das,R. (2020) RNA secondary structure packages ranked and improved by high-throughput experiments. bioRxiv doi: https://doi.org/10.1101/2020.05.29.124511, 31 May 2020, preprint: not peer reviewed. 54. Reuter,J.S. and Mathews,D.H. (2010) RNAstructure: software for RNA secondary structure prediction and analysis. BMC Bioinformatics, 11, 129. 55. Andronescu,M., Aguirre-Hernandez,R., Condon,A. and Hoos,H.H. (2003) RNAsoft: a suite of RNA secondary structure prediction and design software tools. Nucleic Acids Res., 31, 3416\u20133422. 56. Bellaousov,S. and Mathews,D.H. (2010) ProbKnot: fast prediction of RNA secondary structure including pseudoknots. RNA, 16, 1870\u20131880. 57. Zadeh,J.N., Steenberg,C.D., Bois,J.S., Wolfe,B.R., Pierce,M.B., Khan,A.R., Dirks,R.M. and Pierce,N.A. (2011) NUPACK: analysis and design of nucleic acid systems. J. Comput. Chem., 32, 170\u2013173. 58. Ren,J., Rastegari,B., Condon,A. and Hoos,H.H. (2005) HotKnots: heuristic prediction of RNA secondary structures including pseudoknots. RNA, 11, 1494\u20131504. 59. Darty,K., Denise,A. and Ponty,Y. (2009) VARNA: Interactive drawing and editing of the RNA secondary structure. Bioinformatics, 25, 1974. 60. Andersen,E.S., Rosenblad,M.A., Larsen,N., Westergaard,J.C., Burks,J., Wower,I.K., Wower,J., Gorodkin,J., Samuelsson,T. and Zwieb,C. (2006) The tmRDB and SRPDB resources. Nucleic Acids Res., 34, D163\u2013D168. 61. Paszke,A., Gross,S., Massa,F., Lerer,A., Bradbury,J., Chanan,G., Killeen,T., Lin,Z., Gimelshein,N., Antiga,L. et al. (2019) PyTorch: An imperative style, high-performance deep learning library. In: Wallach,H., Larochelle,H., Beygelzimer,A., dAlch\u00b4e-Buc,F., Fox,E. and Garnett,R. (eds). Advances in Neural Information Processing Systems 32. Curran Associates, Inc., pp. 8024\u20138035. 62. Strobel,E.J., Yu,A.M. and Lucks,J.B. (2018) High-throughput determination of RNA structures. Nat. Rev. Genet., 19, 615\u2013634. 63. Lusvarghi,S., Sztuba-Solinska,J., Purzycka,K.J., Rausch,J.W. and Le Grice,S.F. (2013) RNA secondary structure prediction using high-throughput SHAPE. JoVE (J. Visual. Exp.), e50243. Downloaded from https://academic.oup.com/nar/article/50/3/e14/6430845 by Ripon College Library user on 16 March 2025 === ARTICLE RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning Jaswinder Singh 1 , Jack Hanson 1, Kuldip Paliwal 1 * &amp; Yaoqi Zhou 2 The majority of our human genome transcribes into noncoding RNAs with unknown struc- tures and functions. Obtaining functional clues for noncoding RNAs requires accurate base- pairing or secondary-structure prediction. However, the performance of such predictions by current folding-based algorithms has been stagnated for more than a decade. Here, we propose the use of deep contextual learning for base-pair prediction including those non- canonical and non-nested (pseudoknot) base pairs stabilized by tertiary interactions. Since only &lt;250 nonredundant, high-resolution RNA structures are available for model training, we utilize transfer learning from a model initially trained with a recent high-quality bpRNA dataset of &gt;10,000 nonredundant RNAs made available through comparative analysis. The resulting method achieves large, statistically significant improvement in predicting all base pairs, noncanonical and non-nested base pairs in particular. The proposed method (SPOT-RNA), with a freely available server and standalone software, should be useful for improving RNA structure modeling, sequence alignment, and functional annotations. https://doi.org/10.1038/s41467-019-13395-9 OPEN 1 Signal Processing Laboratory, School of Engineering and Built Environment, Griffith University, Brisbane, QLD 4111, Australia. 2 Institute for Glycomics and School of Information and Communication Technology, Griffith University, Parklands Dr., Southport, QLD 4222, Australia. email: k.paliwal@griffith.edu.au; yaoqi.zhou@griffith.edu.au NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 1 1234567890():,; RNA secondary structure is represented by a list of the nucleotide bases paired by hydrogen bonding within its nucleotide sequence. Stacking these base pairs forms the scaffold driving the folding of RNA three-dimensional struc- tures1 . As a result, the knowledge of the RNA secondary structure is essential for modeling RNA structures and understanding their functional mechanisms. As such, many experimental methods have been developed to infer paired bases by using one- dimensional or multiple-dimensional probes, such as enzymes, chemicals, mutations, and cross-linking techniques coupled with next-generation sequencing 2,3 . However, precise base-pairing information at the resolution of single base pairs still requires high-resolution, three-dimensional RNA structures determined by X-ray crystallography, nuclear magnetic resonance (NMR), or cryogenic electron microscopy. With &lt;0:01% of 14 million noncoding RNAs collected in RNAcentral 4 having experimentally determined structures 5 , it is highly desirable to develop accurate and cost-effective computational methods for direct prediction of RNA secondary structure from sequence. Current RNA secondary-structure prediction methods can be classified into comparative sequence analysis and folding algo- rithms with thermodynamic, statistical, or probabilistic scoring schemes6 . Comparative sequence analysis determines base pairs conserved among homologous sequences. These methods are highly accurate7 if a large number of homologous sequences are available and those sequences are manually aligned with expert knowledge. However, only a few thousand RNA families are known in Rfam8 . As a result, the most commonly used approach for RNA secondary-structure prediction is to fold a single RNA sequence according to an appropriate scoring function. In this approach, RNA structure is divided into substructures such as loops and stems according to the nearest-neighbor model 9 . Dynamic programming algorithms are then employed for locat- ing the global minimum or probabilistic structures from these substructures. The scoring parameters of each substructure can be obtained experimentally 10 (e.g., RNAfold11 , RNAstructure 12 , and RNAshapes 13 ) or by machine learning (e.g., CONTRAfold 14 , CentroidFold 15 , and ContextFold 16 ). However, the overall precision (the fraction of correctly predicted base pairs in all predicted base pairs) appears to have reached a \u201cperformance ceiling\u201d6 at about 80%17,18 . This is in part because all existing methods ignore some or all base pairs that result from tertiary interactions19 . These base pairs include lone (unstacked), pseu- doknotted (non-nested), and noncanonical (not A\u2013U, G\u2013C, and G\u2013U) base pairs as well as triplet interactions19,20 . While some methods can predict RNA secondary structures with pseudoknots (e.g., pknotsRG 21 , Probknot22 , IPknot 23 , and Knotty24 ) and others can predict noncanonical base pairs (e.g., MC-Fold 25 , MC- Fold-DP26 , and CycleFold 27 ), none of them can provide a com- putational prediction for both, not to mention lone base pairs and base triplets. The work presented in this paper is inspired by a recent advancement in the direct prediction of protein contact maps from protein sequences by Raptor-X28 and SPOT-Contact29 with deep- learning neural network algorithms such as Residual Networks (ResNets)30 and two-dimensional Bidirectional Long Short-Term Memory cells (2D-BLSTMs)31,32. SPOT-Contact treats the entire protein \u201cimage\u201d as context and used an ensemble of ultra-deep hybrid networks of ResNets coupled with 2D-BLSTMs for pre- diction. ResNets can capture contextual information from the whole sequence \u201cimage\u201d at each layer and map the complex rela- tionship between input and output. Also, 2D-BLSTMs proved very effective in propagating long-range sequence dependencies in protein structure prediction29 because of the ability of LSTM cells to remember the structural relationship between the residues that are far from each other in their sequence positions during training. Similar to protein contact map, a RNA secondary structure is a two-dimensional contact matrix, although its contacts are defined differently (hydrogen bonds for RNA base pairs and distance cutoff for protein contacts, respectively). However, unlike proteins, the small number of nonredundant RNA structures available in the Protein Data Bank (PDB)5 makes deep-learning methods unsui- table for direct single-sequence-based prediction of RNA secondary structure. As a result, machine-learning techniques are rarely utilized. To our knowledge, the only example is mxfold33 that employs a small-scale machine-learning algorithm (structured support vector machines) for RNA secondary-structure prediction. Its performance after combining with a thermodynamic model makes some improvement over folding-based techniques. How- ever, mxfold is limited to canonical base pairs without accounting for pseudoknots. Recently, a large database of more than 100,000 RNA sequences (bpRNA 34 ) with automated annotation of secondary structure was released. While this database is large enough for us to employ deep-learning techniques, the annotated secondary structures from the comparative analysis may not be reliable at the single base-pair level. To overcome this limitation, we first employed bpRNA to train an ensemble of ResNets and LSTM networks, similar to the ensemble used by us for protein contact map prediction by SPOT-Contact29 . We then further trained the large model with a small database of precise base pairs derived from high-resolution RNA structures. This transfer-learning technique 35 is used successfully by us for identifying molecular recognition features in intrinsically disordered regions of pro- teins 36 . The resulting method, called SPOT-RNA, is a deep- learning technique for predicting all bases paired, regardless if they are associated with tertiary interactions. The new method provides more than 53%, 47%, and 10% improvement in F1 score for non-nested, noncanonical, and all base pairs, respectively, over the next-best method, compared with an independent test set of 62 high-resolution RNA structures by X-ray crystal- lography. The performance of SPOT-RNA is further confirmed by a separate test set of 39 RNA structures determined by NMR and 6 recently released nonredundant RNAs in PDB. Results Initial training by bpRNA. We trained our models of ResNets and LSTM networks by building a nonredundant set of RNA sequences with annotated secondary structure from bpRNA34 at 80% sequence-identity cutoff, which is the lowest sequence-identity cutoff allowed by the program CD-HIT-EST37 and has been employed previously by many studies for the same purpose38,39. This dataset of 13,419 RNAs after excluding those &gt;80% sequence identities was further randomly divided into 10,814 RNAs for training (TR0), 1300 for validation (VL0), and 1,305 for an inde- pendent test (TS0). By using TR0 for training, VL0 for validation, and the single sequence (a one-hot vector of Lx4) as the only input, we trained many two-dimensional deep-learning models with various combinations in the numbers and sizes of ResNets, BLSTM, and FC layers with a layout shown in Fig. 1. The per- formance of an ensemble of the best 5 models (validated by VL0 only) on VL0 and TS0 is shown in Table 1. Essentially the same performance with Matthews correlation coefficient (MCC) at 0.632 for VL0 and 0.629 for TS0 suggests the robustness of the ensemble trained. The F1 scores, the harmonic mean of precision, and sen- sitivity are also essentially the same between validation and test (0.629 vs. 0.626). Supplementary Table 1 further compared the performance of individual models to the ensemble. The MCC improves by 2% from 0.617 (the best single model) to 0.629 in TS0, confirming the usefulness of an ensemble to eliminate random prediction errors in individual models. ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 2 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications Transfer learning with RNA structures. The models obtained from the bpRNA dataset were transferred to further train on base pairs derived from high-resolution nonredundant RNA structures with TR1 (training set), VL1 (validation set), and TS1 (test set) having 120, 30, and 67 RNAs, respectively. The TS1 set is inde- pendent of the training data (TR0 and TR1) as it was obtained by first filtering through CD-HIT-EST at the lowest allowed sequence-identity cutoff (80%). To further remove potential homologies, we utilized BLAST-N 40 against the training data (TR0 and TR1) with an e-value cutoff of 10. To examine the consistency of the models built, we performed 5-fold cross- validation by combining TR1 and VL1 datasets. The results of cross-validation on training data (TR1+VL1) and unseen TS1 for the ensemble of the same top 5 models are shown in Table 1. The minor fluctuations on 5-fold with MCC of 0.701 \u00b1 0.02 and F1 of 0.690 \u00b1 0.02 and small difference between 5-fold cross-validation and test set TS1 (0.701 vs. 0.690 for MCC) indicate the robustness of the models trained for the unseen data. Table 1 also shows that the direct application of the model trained by bpRNA leads to a reasonable but inferior performance on TS1 compared with the model after transfer learning. The improvement in MCC is 6% before (0.650) and after (0.690) transfer learning on TS1. Sup- plementary Tables 2 and 3 compare the result of the ensemble of models and five individual models for five-fold cross-validation (TR1+VL1) and independent test set (TS1), respectively. Sig- nificant improvement of the ensemble over the best single model is observed with 3% improvement in MCC for cross-validation and independent tests. Comparison between transfer learning and direct learning. To demonstrate the usefulness of transfer learning, we also perform the direct training of the 5 models with the same ensemble net- work architecture and hyperparameters (the number of layers, the depth of layers, the kernel size, the dilation factor, and the learning rate) on the structured RNA train set (TR1) and vali- dated by VL1 and tested by TS1. The performance of the ensemble of five models by direct learning on VL1 and TS1 is shown in Table 1. Similar performance between validation and test with MCC = 0.583, 0.571, respectively, confirms the robust- ness of direct learning. However, this performance is substantially lower than that of transfer learning (21% reduction of the MCC value and 30% reduction in F1 score). This confirms the difficulty of direct learning with a small training dataset of TR1 and the need for using a large dataset (bpRNA) that can effectively utilize capabilities of deep-learning networks. Supplementary Table 4 further compared the performance of individual models with the ensemble by direct learning on TR1. Figure 2a compares the precision-recall (PR) curves given by initial training (SPOT- RNA-IT), direct training (SPOT-RNA-DT), and transfer learning (SPOT-RNA) on the independent test set TS1. The results are from a reduced TS1 (62 RNAs rather than 67) because some other methods shown in the same figure do not predict secondary structure for sequences with missing or invalid bases. Interest- ingly, direct training starts with 100% precision at very low sensitivity (recall), whereas both initial training and transfer learning have high but &lt;100% precision at the lowest achievable sensitivities for the highest possible threshold that separates positive from negative prediction. This suggests that the existence of false positives in bpRNA \u201ccontaminated\u201d the initial training. Nevertheless, the transfer learning achieves a respectable 93.2% precision at 50% recall. This indicates that the fraction of potential false positives in bpRNA is small. Comparison with other secondary-structure predictors. Figure 2a further compares precision/recall curves given by our transfer-learning ensemble model with 12 other available RNA Table 1 Performance of SPOT-RNA on validation and test set after initial training, transfer learning, and direct training. Method Training set Analysis set MCCa F1b Precision Sensitivity Initial training TR0 VL0 0.632 0.629 0.712 0.563 TR0 TS0 0.629 0.626 0.709 0.560 TR0 TS1 0.650 0.630 0.897 0.485 Transfer learning TR1+VL1 TR1+VL1 0.701 (0.02c) 0.690 (0.02c) 0.853 (0.02c) 0.580 (0.03c) TR1+VL1 TS1 0.690 (0.02c) 0.687 (0.01c) 0.888 (0.02c) 0.562 (0.02c) Direct training TR1 VL1 0.583 0.546 0.854 0.401 TR1 TS1 0.571 0.527 0.870 0.378 aMatthews correlation coefficient b Harmonic mean of precision and sensitivity c Standard deviation based on five-fold cross-validation Initial 3 \u00d7 3 convolution Act./norm./dropout 3 \u00d7 3 Convolution Act./norm./dropout 5\u00d7 5 Convolution Block A x (NA-1) Act./norm. 2D-BLSTM Fully-connected layer Act./norm./dropout Block B x (NB-1) Output layer with sigmoid activation Block B Block A Initial training on large data set Transfer learning on experimental PDB data RNA one-hot encoding Lx4 Outer concatenation LxLX8 bpRNA data set Model 0 PreT Model 1 PreT Model 2 PreT Model 3 PreT Model 4 PreT RNA one-hot encoding Lx4 LxLX8 Outer concatenation PDB data set Model 0 Model 1 Model 2 Model 3 Model 4 Ensemble averaging Hairpin loop Internal loop Multiloop Noncanonical BulgeStem Pseudoknot 40 30 50 20 10 70 60 77 1 Fig. 1 Generalized model architecture of SPOT-RNA. The network layout of the SPOT-RNA, where L is the sequence length of a target RNA, Act. indicates the activation function, Norm. indicates the normalization function, and PreT indicates the pretrained (initial trained) models trained on the bpRNA dataset. NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 3 secondary-structure predictors on independent test set TS1. Two predictors (CONTRAfold and CentroidFold) with probabilistic outputs are also represented by the PR curves with the remaining shown as a singular point. The performance of most existing methods is clustered around the sensitivity of 50% and precision of 67\u201383% (Table 2). By comparison, our method SPOT-RNA improves by 9% in MCC and more than 10% in F1 score over the next-best mxfold. The results presented in Fig. 2a are the overall performance at the base-pair level. Figure 2b shows the distribution of the F1 score among individual RNAs in terms of median, 25th, and 75th percentiles. SPOT-RNA has the highest median F1 score along with the highest F1 score (0.348) for the worst-performing RNA, compared with nearly 0 for all other methods. This highlights the highly stable performance of SPOT-RNA, relative to all other folding-based techniques, including mxfold, which mixes thermodynamic and machine-learning models. The difference between SPOT-RNA and the next-best mxfold on TS1 is statistically significant with P value &lt; 0.006 obtained through a paired t test. Also, we calculated the ensemble defect (see the \u201cMethods\u201d section) from the predicted base-pair probabilities for SPOT-RNA, CONTRAfold, and CentroidFold on TS1. The ensemble defect metric describes the deviation of probabilistic structural ensembles from their corresponding native RNA secondary structure, where 0 represents a perfect prediction. The ensemble defect for SPOT-RNA was 0.19 as compared with 0.24 and 0.25 for CONTRAfold and Centroid- Fold, respectively, showing that the structural ensemble predicted by SPOT-RNA is more similar to target structures in comparison with the other two predictors. Our method was trained for RNAs with a maximum length of 500 nucleotides, due to hardware limitations. It is of interest to determine how our method performs in terms of size depen- dence. As the maximum sequence length in TS1 was 189, therefore, we added 32 RNAs of sequence length from 298 to 1500 to TS1 by relaxing the resolution requirement to 4 \u00c5 and including RNA chains complexed with other RNAs (but ignored inter-RNA base pairs). The reason for relaxing the resolution to 4 \u00c5 and including RNA chains complexed with other RNAs because there were not many high-resolution and single-chain long RNAs in PDB. Supplementary Fig. 1 compares the F1 score of each RNA given by SPOT-RNA with that from the next-best mxfold as a function of the length of RNAs. There is a trend of lower performance for a longer RNA chain for both methods as expected. SPOT-RNA consistently outperforms mxfold within 500 nucleotides that our method was trained on. Supplementary Fig. 1 also shows that mxfold performs better with an average of F1 score at 0.50, compared with 0.35 by SPOT-RNA on 21 long RNAs (L &gt; 1000). We found that the poor performance of SPOT- RNA is mainly because of the failure of SPOT-RNA to capture ultra long-distance pairs with sequence separation &gt;300. This failure is caused by the limited long RNA data in training. By comparison, the thermodynamic algorithm in mxfold can locate the global minimum regardless of the distance between sequence positions of the base pairs. The above comparison may be biased toward our method because almost all other methods compared can only predict canonical base pairs, which include Watson\u2013Crick (A\u2013U and G\u2013C) pairs and Wobble pairs (G\u2013U). To address this potential bias, Table 2 further compares the performance of SPOT-RNA with others on canonical pairs, Watson\u2013Crick pairs (A\u2013U and G\u2013C pairs), and Wobble pairs (G\u2013U), separately on TS1. Indeed, all methods have a performance boost when noncanonical pairs are excluded from performance measurement. SPOT-RNA continues to have the best performance with 6% improvement in F1 score for canonical pairs and Watson\u2013Crick pairs over the next-best mxfold and 7% improvement for Wobble pairs over the next-best ContextFold. mxfold does not perform as well in predicting Wobble pairs and is only the fourth best. Base pairs associated with pseudoknots are challenging for both folding-based and machine-learning-based approaches because they are often associated with tertiary interactions that are difficult to predict. To make a direct comparison in the capability of predicting base pairs in pseudoknots, we define pseudoknot pairs as the minimum number of base pairs that can be removed to result in a pseudoknot-free secondary structure. The program bpRNA 34 (available at https://github.com/hendrixlab/bpRNA) was used to obtain base pairs in pseudoknots from both native and predicted secondary structures. Table 3 compares the performance of SPOT-RNA with all 12 other methods regardless if they can handle pseudoknots or not for those 40 RNAs with at least one pseudoknot in the independent test TS1. As none of the other methods predict multiplets, we ignore the base pairs associated with the multiplets in the analysis. mxfold remains the 0 0.2 0.4 0.6 0.8 1 Sensitivity/recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision a SPOT_RNA SPOT-RNA-IT SPOT-RNA-DT mxfold ContextFold CONTRAfold Knotty IPknot RNAfold Probknot CentroidFold RNAstructure RNAshapes (MFE) pkiss CycleFold SPOT-RNA mxfold ContextFold CONTRAfold Knotty IPknot RNAfold Probknot CentroidFold RNAstructure RNAshapes (MFE) pkiss CycleFold 0 0.2 0.4 0.6 0.8 1 F1-score b Fig. 2 Performance comparison of SPOT-RNA with 12 other predictors by using PR curve and boxplot on the test set TS1. a Precision-recall curves on the independent test set TS1 by initial training (SPOT-RNA-IT, the green dashed line), direct training (SPOT-RNA-DT, the blue dot-dashed line), and transfer learning (SPOT-RNA, the solid magenta line). Precision and sensitivity results from ten currently used predictors are also shown as labeled with open symbols for the methods accounting for pseudoknots and filled symbols for the methods not accounting for pseudoknots. CONTRAfold and CentroidFold were also shown as curves (Gold and Black) because their methods provide predicted probabilities. b Distribution of F1 score for individual RNAs on the independent test set TS1 given by various methods as labeled. On each box, the central mark indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The outliers are plotted individually by using the \u201c+\u201d symbol. ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 4 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications second best behind SPOT-RNA although it is unable to predict pseudoknots, due to the number of base pairs in pseudoknots accounting for only 10% of all base pairs (see Supplementary Table 7). Table 3 shows that all methods perform poorly with F1 score &lt; 0.3 for base pairs associated with pseudoknots. Despite the challenging nature of this problem, SPOT-RNA makes a substantial improvement over the next-best (pkiss) by 52% in F1 score. Noncanonical pairs, triplets, and lone base pairs are also associated with tertiary interactions other than pseudoknots. Here, lone base pairs refer to a single base pair without neighboring base pairs (i.e., [i, j] in the absence of [i \u2212 1, j + 1] and [i + 1, j \u2212 1]). Triplets refer to the rare occasion of one base forming base pairs with two other bases. As shown in Supplementary Table 5, SPOT-RNA makes a 47% improvement in F1 score for predicting noncanonical base pairs over CycleFold. Although the sensitivity of prediction given by SPOT-RNA is low (15.4%), the precision is high at 73.2%. Very low performance for triplets and lone pairs (F1 score &lt; 0.2) is observed. Secondary structure of RNAs is characterized by structural motifs in their layout. For each native or predicted secondary structure, the secondary-structure motif was classified by program bpRNA 34 . The performance in predicting bases in different secondary structural motifs by different methods is shown in Table 4. According to the F1 score, SPOT-RNA makes the best prediction in stem base pairs (6% improvement over the next best), hairpin loop nucleotide (8% improvement), and bulge nucleotide (11% improvement), although it performs slightly worse than CONTRAfold in multiloop (by 2%). mxfold is best for internal loop prediction over the second-best predictor Knotty by 18%. To demonstrate the SPOT-RNA\u2019s ability to predict tertiary interactions along with canonical base pairs, Supplementary Figs. 2 and 3 show two examples (riboswitch 41 and t-RNA42 ) from TS1 with one high performance and one average performance, respectively. For both the examples, SPOT-RNA is able to predict noncanonical base pairs (in green), pseudoknot base pairs, and lone pair (in blue), while mxfold and IPknot remain unsuccessful to predict noncanonical and pseudoknot base pairs. To further confirm the performance of SPOT-RNA, we compiled another test set (TS2) with 39 RNA structures solved by NMR. As with TS1, TS2 was made nonredundant to our training data by using CD-HIT-EST and BLAST-N. Figure 3a compares precision-recall curves given by SPOT-RNA with 12 other RNA secondary-structure predictors on the test set TS2. SPOT-RNA outperformed all other predictors on this test set (Supplementary Table 6). Furthermore, Fig. 3b shows the distribution of the F1 score among individual RNAs in terms of median, 25th, and 75th percentiles. SPOT-RNA achieved the highest median F1 score with the least fluctuation although the difference between SPOT-RNA and the next-best (Knotty this time) on individual RNAs (shown in Supplementary Fig. 4) is not significant with P value &lt; 0.16 obtained through a paired t test. Ensemble defect on TS2 is the smallest by SPOT-RNA (0.14 for SPOT-RNA as compared with 0.18 and 0.19 by CentroidFold and CONTRAfold, respectively). Here, we did not compare the performance in pseudoknots because the number of base pairs in pseudoknots (a total of 21) in this dataset is too small to make statistically meaningful comparison. In addition, we found a total of 6 RNAs with recently solved structures (after March 9, 2019) that are not redundant according to CD-HIT-EST and BLAST-N to our training sets (TR0 and TR1) and test sets (TS1 and TS2). The prediction for a synthetic construct RNA (released on 26 June 2019, chain H in PDB ID 6dvk) 43 was compared with the native structure in Fig. 4a. For this synthetic RNA, SPOT-RNA yields a structural topology very Table 2 Performance of all the predictors according to base-pair types on the test set TS1. All base pairs Canonical only Watson\u2013Crick only Wobble only MCCa F1b Precision Sensitivity F1b Precision Sensitivity F1b Precision Sensitivity F1b Precision Sensitivity SPOT-RNA 0.700 0.690 0.849 0.582 0.773 0.858 0.703 0.790 0.857 0.733 0.592 0.865 0.450 mxfold 0.644 0.628 0.824 0.508 0.728 0.824 0.652 0.749 0.830 0.682 0.519 0.747 0.398 ContextFold 0.636 0.621 0.811 0.503 0.719 0.811 0.646 0.737 0.822 0.668 0.554 0.693 0.462 CONTRAfold 0.621 0.611 0.765 0.508 0.704 0.765 0.652 0.724 0.778 0.677 0.517 0.630 0.439 Knotty 0.611 0.603 0.742 0.508 0.694 0.742 0.652 0.713 0.755 0.676 0.519 0.611 0.450 IPknot 0.596 0.576 0.789 0.454 0.671 0.789 0.583 0.690 0.799 0.608 0.483 0.681 0.374 RNAfold 0.593 0.585 0.724 0.491 0.674 0.724 0.630 0.696 0.742 0.655 0.478 0.554 0.421 ProbKnot 0.582 0.576 0.705 0.486 0.662 0.705 0.624 0.684 0.725 0.648 0.466 0.522 0.421 CentroidFold 0.577 0.569 0.706 0.477 0.656 0.706 0.612 0.675 0.719 0.636 0.476 0.569 0.409 RNAstructure 0.570 0.562 0.702 0.469 0.648 0.702 0.602 0.670 0.719 0.627 0.451 0.532 0.392 RNAshapes 0.564 0.555 0.699 0.460 0.640 0.699 0.591 0.661 0.716 0.614 0.455 0.531 0.398 pkiss 0.543 0.538 0.660 0.454 0.619 0.660 0.582 0.643 0.682 0.608 0.403 0.453 0.363 CycleFold 0.461 0.466 0.476 0.456 0.546 0.551 0.540 0.565 0.566 0.564 0.368 0.403 0.339 aMatthews correlation coefficient b Harmonic mean of precision and sensitivity NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 5 similar to the native secondary structure with F1 score of 0.85, precision of 97%, and sensitivity of 77%. In particular, SPOT- RNA captures one noncanonical base pair between G46 and A49 correctly but missed others in pseudoknots. The SPOT-RNA predictions of Glutamine II Riboswitch (chain A in PDB ID 6qn3, released on June 12, 2019) 44 and Synthetic Construct Hatchet Ribozyme (chain U in PDB ID 6jq6, released on June 12, 2019)45 are compared with their respective native secondary structure in Fig. 4b, c, respectively. For these two RNAs, experimental evidence suggests strand swapping in dimerization44,45 . Thus, their monomeric native structures are obtained by replacing the swapped stand by its original stand. SPOT-RNA is able to predict both the stems and pseudoknot (in Blue) with an overall F1 score of 0.90 for Glutamine II Riboswitch. For Hatchet Ribozyme, SPOT-RNA is able to predict native-like structure with F1 score of 0.74 although it has missed noncanonical and pseudoknot base pairs. Three other RNAs are Pistol Ribozyme (chain A and B in PDB ID 6r47, released on July 3, 2019)46 , Mango Aptamer (chain B in PDB ID 6e8u, released on April 17, 2019) 47 , and Adenovirus Virus-associated RNA (chain C in PDB ID 6ol3, released on July 3, 2019)48 . SPOT-RNA achieves F1 score of 0.57, 0.41, and 0.63 on Pistol Ribozyme, Mango Aptamer, and adenovirus virus- associated RNA, respectively. For this level of performance, it is more illustrative to show a one-dimensional representation of RNA secondary structure (Fig. 5a\u2013c). The figures show that the relatively poor performance of Pistol Ribozyme and Mango Aptamer RNAs is in part due to the uncommon existence of a large number of noncanonical base pairs (in Green). For adenovirus virus-associated RNA (VA-I), SPOT-RNA\u2019s prediction is poor. It contains three false-positive stems with falsely predicted pseudoknots (Fig. 5c). Performance comparison on these 6 RNAs with 12 other secondary-structure predictors is shown in Fig. 6. SPOT-RNA outperforms all other predictors on Synthetic Construct RNA (Fig. 6a), Glutamine II Riboswitch (Fig. 6b), and Pistol Ribozyme (Fig. 6c). It is the co-first (same as mxfold) in Mango Aptamer (Fig. 6e) and the second best (behind mxfold only) in Hatchet Ribozyme (Fig. 6d). However, it did not do well on adenovirus virus-associated RNA (Fig. 6f), which was part of RNA puzzle- 2017, when compared with other methods. This poor prediction compared with other methods is likely because this densely contacted, base-pairing network without pseudoknots (except those due to noncanonical base pairs) is most suitable for folding-based algorithms that maximize the number of stacked canonical base pairs. Discussion This work developed RNA secondary-structure prediction method purely based on deep neural network learning from a single RNA sequence. Because only a small number of high- resolution RNA structures are available, deep-learning models have to be first trained by using a large database of RNA sec- ondary structures (bpRNA) annotated according to comparative analysis, followed by transfer learning to the precise secondary structures derived from 3D structures. Although the slightly noisy data in bpRNA lead to an upbound around 96% for the precision (Fig. 2a), the model generated from transfer learning yields a substantial improvement (30% in F1 score) over the model based on direct learning TS1. Without the need for folding-based optimization, the transfer-learning model yields a method that can predict not only canonical base pairs but also those base pairs often associated with tertiary interactions, including pseudoknots, lone, and noncanonical base pairs. By comparing with 12 current secondary-structure prediction techniques by using the inde- pendent test of 62 high-resolution X-ray structures of RNAs, the method (SPOT-RNA) achieved 93% in precision, which is a 13% improvement over the second-best method mxfold when the sensitivity for SPOT-RNA is set to 50.8% as in mxfold. One advantage of a pure machine-learning approach is that all base pairs can be trained and predicted, regardless if it is asso- ciated with local or nonlocal (tertiary) interactions. By compar- ison, a folding-based method has to have accurate energetic parameters to capture noncanonical base pairs and sophisticated algorithms for a global minimum search to account for pseu- doknots. SPOT-RNA represents a significant advancement in predicting noncanonical base pairs. Its F1 score improves over CycleFold by 47% from 17% to 26% although both methods have a low sensitivity at about 16% (Supplementary Table 5). SPOT- RNA can also achieve the best prediction of base pairs in pseu- doknots although the performance of all methods remains low with an F1 score of 0.239 for SPOT-RNA and 0.157 for the next- best (pkiss, Table 3). This is mainly because the number of base pairs in pseudoknots is low in the structural datasets (an average of 3\u20134 base pairs per pseudoknot RNA in TS1, see Supplementary Table 7). Moreover, a long stem of many stacked base pairs is easier to learn and predict than a few nonlocal base pairs in pseudoknot. As a reference for future method development, we Table 3 Performance of all the predictors on 40 pseudoknot RNAs in the test set TS1. All Base Pairs Base Pairs in Pseudoknots Base Pair not in Pseudoknots MCCa F1b Precision Sensitivity F1b Precision Sensitivity F1b Precision Sensitivity SPOT-RNA 0.769 0.764 0.875 0.679 0.239 0.550 0.153 0.797 0.872 0.734 mxfold 0.687 0.682 0.797 0.595 0.000 0.000 0.000 0.714 0.780 0.659 ContextFold 0.686 0.680 0.797 0.594 0.000 0.000 0.000 0.714 0.781 0.658 CONTRAfold 0.659 0.658 0.735 0.595 0.000 0.000 0.000 0.688 0.719 0.659 Knotty 0.678 0.678 0.740 0.625 0.108 0.134 0.090 0.707 0.761 0.660 IPknot 0.638 0.629 0.769 0.533 0.131 0.458 0.076 0.664 0.768 0.585 RNAfold 0.605 0.606 0.666 0.555 0.000 0.000 0.000 0.646 0.666 0.628 ProbKnot 0.610 0.611 0.669 0.562 0.118 0.256 0.076 0.632 0.663 0.603 CentroidFold 0.616 0.616 0.682 0.562 0.000 0.000 0.000 0.644 0.668 0.621 RNAstructure 0.585 0.584 0.650 0.531 0.000 0.000 0.000 0.621 0.647 0.598 RNAshapes 0.569 0.568 0.639 0.512 0.000 0.000 0.000 0.591 0.622 0.563 pkiss 0.564 0.565 0.619 0.520 0.157 0.180 0.139 0.566 0.616 0.523 CycleFold 0.455 0.458 0.423 0.499 0.000 0.000 0.000 0.482 0.422 0.563 aMatthews correlation coefficient b Harmonic mean of precision and sensitivity ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 6 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications also examined the ability of SPOT-RNA to capture triple inter- actions: one base paired with two other bases. Both precision and sensitivity are low (12% and 7%, respectively, Supplementary Table 5). This is mainly because there is a lack of data on base triples in bpRNA for pretraining and the number of both triplets and quartets is only 1194 in the structural training set TR1. To further confirm the performance, SPOT-RNA was applied to 39 RNA structures determined by NMR (TS2). Unlike X-ray structures, structures determined by NMRs resulted from mini- mization of experimental distance-based constraints. These 39 NMR structures, smaller with average length of 51 nucleotides, have only a total of 21 base pairs in pseudoknots. As a result, they are much easier to predict for all methods (MCC &lt; 0.7 except SPOT-RNA for TS1 but &gt;0.74 for most methods in TS2). Despite of this, SPOT-RNA continues to have the best performance (Fig. 3, Supplementary Table 6, and Supplementary Fig. 4) as compared with other 12 predictors. Furthermore, the perfor- mance of SPOT-RNA was tested on 6 recently released non- redundant (to TR0 and TR1) RNAs in PDB. SPOT-RNA performs the best or the same as the best in 4 and the second best in 1 of the 6 RNAs (Fig. 6). One limitation of SPOT-RNA is that it was trained by RNAs shorter than 500 nucleotides due to our hardware limitation. Within 500 nucleotides, SPOT-RNA provides a consistent improvement over existing techniques (Supplementary Fig. 1). However, for really long RNA chains (&gt;1000), a purely machine- learning-based technique is not as accurate as some of the folding-algorithm-based methods such as mxfold as shown in Supplementary Fig. 1. The lack of training for long RNAs is the main reason. Currently, even if there is no hardware limitation, the number of high-resolution RNA structures with &gt;500 nucleotides in PDB structures are too few to provide adequate training. Thus, at this stage, SPOT-RNA is most suitable for RNA length of &lt;500. In addition to prediction accuracy, high computational effi- ciency is necessary for RNA secondary-structure prediction because genome-scale studies are often needed. We found that the CPU time for predicting all 62 RNAs in the test set TS1 on a single thread of 32-core Intel Xenon(R) E5-2630v4 CPU is 540 s, which is faster than Knotty (2800 s) but slower than IPknot (1.2 s), ProbKnot (13 s), and pkiss (112 s). However, our distributed version can be easily run on multiple CPU threads or on GPUs. For example, by running SPOT-RNA on a single Nvidia GTX TITAN X GPU, the computation time for predicting all 62 RNAs would be reduced to 39 s. Thus, SPOT-RNA can feasibly be used for genome-scale studies. This work has used a single RNA sequence as the only input. It is quite remarkable that relying on a single sequence alone can obtain a more accurate method than existing folding methods in secondary-structure prediction. For protein contact map predic- tion, evolution profiles generated from PSIBLAST 40 and HHblits 49 as well as direct coupling analysis among homologous sequences 50 are the key input vectors responsible for the recent improvement in highly accurate prediction. Thus, one expects that a similar evolution-derived sequence profile generated from BLAST-N and direct/evolution-coupling analysis would further improve secondary-structure prediction for nonlocal base pairs in long RNAs, in particular. Indeed, recently, we have shown that using evolution-derived sequence profiles significantly improves the accuracy of predicting RNA solvent accessibility and flexibility 38,39 . For example, the correlation coefficient between predicted and actual solvent accessibility increases from 0.54 to 0.63 if a single sequence is replaced by a sequence profile from BLAST-N 38 . However, the generation of sequence profiles and evolution coupling is computationally time consuming. The resulting improvement (or lack of improvement) is strongly Table 4 Performance of all the predictors on secondary-structure motifs on the test set TS1. Stem (F1a) Stem (PR) Stem (SN) Hairpin loop (F1a) Hairpin loop (PR) Hairpin loop (SN) Bulge (F1a) Bulge (PR) Bulge (SN) Internal loop (F1a) Internal loop (PR) Internal loop (SN) Multiloop (F1a) Multiloop (PR) Multiloop (SN) SPOT-RNA 0.762 0.841 0.697 0.686 0.625 0.761 0.369 0.508 0.289 0.266 0.239 0.300 0.562 0.503 0.638 mxfold 0.717 0.769 0.671 0.625 0.525 0.771 0.213 0.360 0.152 0.329 0.270 0.422 0.526 0.465 0.607 ContextFold 0.706 0.755 0.663 0.633 0.513 0.825 0.286 0.539 0.194 0.214 0.170 0.289 0.574 0.544 0.607 CONTRAfold 0.688 0.705 0.671 0.624 0.553 0.715 0.331 0.378 0.294 0.279 0.241 0.331 0.469 0.587 0.391 Knotty 0.670 0.739 0.613 0.600 0.493 0.766 0.295 0.421 0.227 0.279 0.238 0.338 0.549 0.649 0.476 IPknot 0.665 0.754 0.595 0.602 0.510 0.735 0.201 0.474 0.128 0.218 0.202 0.236 0.417 0.339 0.542 RNAfold 0.671 0.686 0.657 0.617 0.539 0.722 0.313 0.500 0.227 0.270 0.218 0.354 0.514 0.555 0.478 ProbKnot 0.625 0.661 0.592 0.571 0.480 0.704 0.276 0.377 0.218 0.209 0.187 0.236 0.481 0.492 0.470 CentroidFold 0.646 0.662 0.632 0.579 0.467 0.761 0.293 0.395 0.232 0.179 0.211 0.156 0.433 0.379 0.506 RNAstructure 0.646 0.665 0.629 0.596 0.508 0.720 0.300 0.440 0.227 0.238 0.204 0.285 0.478 0.546 0.424 RNAshapes 0.627 0.650 0.605 0.574 0.507 0.663 0.310 0.432 0.242 0.238 0.193 0.308 0.433 0.507 0.378 pkiss 0.618 0.684 0.565 0.532 0.449 0.655 0.253 0.457 0.175 0.229 0.183 0.304 0.406 0.494 0.344 CycleFold 0.496 0.431 0.584 0.437 0.564 0.357 0.277 0.333 0.237 0.000 0.000 0.000 0.367 0.374 0.360 a Harmonic mean of precision (PR) and sensitivity (SN) NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 7 depending on the number of homologous sequences available in current RNA sequence databases. If the number of homologous sequences is too low (which is true for most RNAs), it may introduce more noise than the signal to prediction as demon- strated in protein secondary structure and intrinsic disorder prediction 51,52 . Moreover, synthetic RNAs will not have any homologous sequences. Thus, we present the method with single- sequence information as input in this study. Using sequence profiles and evolutionary coupling as input for RNA secondary- structure prediction is working in progress. Another possible method for further improving SPOT-RNA is to employ the predicted probability as a restraint for folding with an appropriate scoring function. Such a dual-approach method will likely improve SPOT-RNA as folding optimization may have a better capability to capture nonlocal interactions between WC pairs for long RNAs, in particular as shown in Supplementary Fig. 1. However, a simple integration may not yield a large improvement for shorter chains (&lt;500). In mxfold, combining machine-learning and thermodynamic models leads to 0.6% in one test set and 5% in another test set33 . Moreover, most ther- modynamic methods simply ignore noncanonical base pairs and many do not even account for pseudoknots. mxfold, for example, employs a pseudoknot-free thermodynamic method to combine with its machine-learning model. Thus, balancing the perfor- mance for canonical, noncanonical, and pseudoknots will require a careful selection of appropriate scoring schemes. A simple integration may lead to high performance in one type of base pair at the expense of other types of base pairs. Nevertheless, we found that if we simply keep only the base pair with the highest pre- dicted probability in predicted triple interactions, SPOT-RNA would be improved by another 3% in F1 score (from 0.69 to 0.71 in TS1), confirming that there is some room for improvement. We will defer this for future studies. The significantly improved performance in secondary- structure prediction should allow large improvement in mod- eling RNA 3D structures. This is because the method predicts not only canonical base pairs but also provides important ter- tiary contacts of noncanonical and non-nested base pairs. Thus, it can serve as a more accurate, quasi-three-dimensional frame to enable correct folding into the right RNA tertiary structure. The usefulness of 2D structure prediction for 3D structure modeling has been demonstrated in RNA Puzzles (blind RNA structure prediction) 53 . Moreover, improvement in predicting secondary structural motifs (stems, loops, and bulges, see Table 4) would allow better functional inference 54,55 , sequence alignment 56 , and RNA inhibitor design 57 . The method and datasets are available as a server and stand-alone software publicly at http://sparks-lab.org/jaswinder/server/SPOT-RNA/ and https://github.com/jaswindersingh2/SPOT-RNA/. Methods Datasets. The datasets for initial training were obtained from bpRNA-1m (Ver- sion 1.0) 34, which consists of 102,348 RNA sequences with annotated secondary structure. Sequences with sequence similarity of more than 80% were removed by using CD-HIT-EST 37. About 80% sequence-identity cutoff was the lowest cutoff allowed by CD-HIT-EST and has been used previously as an RNA nonredundancy cutoff 38,39 . After removing sequence similarity, 14,565 sequences remained. RNA sequences with RNA structures from the PDB5 available in this dataset were also removed as we prepared separate datasets based on RNAs with PDB structure only 5 . Moreover, due to hardware limitations for training on long sequences, the maximum sequence length was restricted to 500. After preprocessing, this dataset contains 13,419 sequences. These sequences were randomly split into 10,814 RNAs for training (TR0), 1300 for validation (VL0), and 1,305 for independent test (TS0). Supplementary Table 7 shows the number of RNA sequences and their Watson\u2013Crick (A\u2013U and G\u2013C), Wobble (G\u2013U), and noncanonical base-pair count as well as the number of base pairs associated with pseudoknots. The average sequence lengths in TR0, VL0, and TS0 are all roughly 130. Here, base pairs associated with pseudoknots are defined as the minimum number of base pairs that can be removed to result in a pseudoknot-free secondary structure. Pseudoknot labels were generated by using software bpRNA 34 (available at https://github.com/ hendrixlab/bpRNA). The datasets for transfer learning were obtained by downloading high- resolution (&lt;3.5 \u00c5) RNAs from PDB on March 2, 2019 5. Sequences with similarity of more than 80% among these sequences were removed with CD-HIT-EST 37. After removing sequence similarity, only 226 sequences remained. These sequences were randomly split into 120, 30, and 76 RNAs for training (TR1), validation (VL1), and independent test (TS1), respectively. Furthermore, any sequence in TS1 having sequence similarity of more than 80% with TR0 was also removed, which reduced TS1 to 69 RNAs. As CD-HIT-EST can only remove sequences with similarity more than 80%, we employed BLAST-N 40 to further remove potential sequence homologies with training data with a large e-value cutoff of 10. This procedure further decreased TS1 from 69 to 67 RNAs. To further benchmark RNA secondary-structure predictors, we employed 641 RNA structures solved by NMR. Using CD-HIT-EST with 80% identity cutoff followed by BLAST-N with e-value cutoff of 10 against TR0, TR1, and TS1, we obtained 39 NMR-solved structures as TS2. The secondary structure of all the PDB sets was derived from their respective structures by using DSSR 58 software. For NMR- solved structures, model 1 structure was used as it is considered as the most reliable structure among all. The numbers of canonical, noncanonical, and pseudoknot base pairs, and base multiplets (triplets and quartets) for all the sets are listed in Supplementary Table 7. These datasets along with annotated secondary structure are publicly available at http://sparks-lab.org/jaswinder/server/SPOT-RNA/ and https://github.com/ jaswindersingh2/SPOT-RNA. RNA secondary-structure types. For the classification of different RNA secondary-structure types, we used the same definitions as previously used by bpRNA 34. A stem is defined as a region of uninterrupted base pairs, with no intervening loops or bulge. A hairpin loop is a sequence of unpaired nucleotides with both ends meeting at the two strands of a stem region. An internal loop is defined as two unpaired strands flanked by closing base pairs on both sides. A 0 0.2 0.4 0.6 0.8 1 Sensitivity/recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision a SPOT_RNA SPOT-RNA-IT SPOT-RNA-DT Knotty RNAfold RNAshapes (MFE) RNAstructure CONTRAfold pkiss Probknot mxfold IPknot ContextFold CentroidFold CycleFold SPOT-RNA Knotty RNAfold RNAshapes (MFE) RNAstructure CONTRAfold pkiss Probknot mxfold IPknot ContextFold CentroidFold CycleFold 0 0.2 0.4 0.6 0.8 1 F1-Measure b Fig. 3 Performance comparison of SPOT-RNA with 12 other predictors by using PR curve and boxplot on the test set TS2. a Precision-recall curves on the independent test set TS2 by various methods as in Fig. 2a labeled. b Distribution of F1 score for individual RNAs on the independent test set TS2 given by various methods as in Fig. 2b labeled. ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 8 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications a b c Non-canonical base-pair 50 40 60 30 20 70 80 10 90 95 1 50 40 60 30 20 70 80 10 90 95 1 40 30 1 50 20 10 40 30 50 1 20 10 50 40 60 30 70 80 20 81 1 10 50 40 60 30 70 80 81 120 10 Fig. 4 Comparison of SPOT-RNA prediction with the native structure of a Synthetic Construct, Glutamine II Riboswitch, and Hatchet Ribozyme. The secondary structure of a synthetic construct RNA (chain H in PDB ID 6dvk), the Glutamine II Riboswitch RNA (chain A in PDB ID 6qn3), and Synthetic Construct Hatchet Ribozyme (chain U in PDB ID 6jq6) represented by 2D diagram with canonical base pair (BP) in black color, noncanonical BP in green color, pseduoknot BP and lone pair in blue color, and wrongly predicted BP in magenta color: a predicted structure by SPOT-RNA (at top), with 97% precision and 77% sensitivity, as compared with the native structure (at bottom) for the Synthetic Construct RNA, b the predicted structure by SPOT-RNA (at top) with 100% precision and 81% sensitivity, as compared with the native structure (at bottom) for the Riboswitch, c the predicted structure by SPOT- RNA (at top) with 100% precision and 59% sensitivity, as compared with the native structure (at bottom) for the synthetic construct Hatchet ribozyme. NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 9 bulge is a special case of the internal loop where one of the strands is of length zero. A multiloop consists of a cycle of more than two unpaired strands, connected by stems. The distribution of different secondary-structure types in TR1, VL1, and TS1 (excluding multiplet base pairs) is shown in Supplementary Table 8. These secondary-structure classifications were obtained by using a secondary-structure analysis program bpRNA 34. Deep neural networks. We employed an ensemble of deep-learning neural net- works for pretraining. The ensemble is made of 5 top-ranked models based on their performance on VL0 with the architecture shown in Fig. 1, similar to what was used previously for protein contact prediction in SPOT-Contact 29. The architecture of each model consists of ResNet blocks followed by a 2D- BLSTM layer and a fully connected (FC) block. An initial convolution layer for pre- activation was used before our ResNet blocks as proposed in He et al.30. The initial convolution layer is followed by N A ResNet blocks (Block A in Fig. 1). Each ResNet block consists of two convolutional layers with a kernel size of 3 \u00b4 3 and 5 \u00b4 5, respectively, and a depth of DRES . The exponential linear units (ELU) 59 activation function and the layer normalization technique 60 were used. A dropout rate of 25% was used before each convolution layer to avoid overfitting during training 61. In some models, we used dilated convolutions that are reported to better learn longer- range dependencies 62. For the dilated convolutional layers, the dilation factor was set to 2i%n, where i is the depth of the convolution layer, n is a fixed scalar, and % is the modulus operator. The next block in the architecture was a 2D-BLSTM 31,32. The output from the final ResNet block was activated (with ELU) and normalized (using layer normalization) before being given as an input to the 2D-BLSTM. The number of nodes in each LSTM direction cell was D BL. After the 2D-BLSTM, N B FC layers with D FC nodes were used, as per Block B in Fig. 1. The output of each FC layer was activated with the ELU function and normalized by using the layer normalization technique. A dropout rate of 50% was utilized for the hidden FC layers to avoid overtraining. The final stage of the architecture consisted of an output FC layer with one node and a sigmoidal activation function. The sigmoid function converts the output into the probability of each nucleotide being paired with other nucleotides. The number of outputs was equal to the number of elements in the upper triangular matrix of size L \u00b4 L, where L is the length of the sequence. Each model was implemented in Google\u2019s Tensorflow framework (v1.12) 63 and trained by using the ADAM optimization algorithm 64 with default parameters. All models were trained on Nvidia GTX TITAN X graphics processing unit (GPU) to speed up training 65. We trained multiple deep-learning models, based on the architecture shown in Fig. 1, on TR0 by performing a hyperparameter grid search over N A, DRES , DBL , N B, and DFC . N A , DRES , DBL , N B, DFC were searched from 16 to 32, 32 to 72, 128 to 256, 0 to 4, and 256 to 512, respectively. These models were optimized on VL0 and tested on TS0. Transfer learning was then used to further train these models on TR1. During transfer learning, VL1 was used as the validation set and TS1 was used as an independent test set. Transfer learning. Transfer learning 35 involves further training a large model that was trained on a large dataset for a specific task to some other related task with limited data. In this project, we used our large dataset bpRNA for initial training, and then transfer learning was employed by using the small PDB dataset as shown in Fig. 1. All of the weights/parameters that were learnt on TR0 were retrained for further training on TR1. During transfer learning, training and validation labels were formatted in exactly the same way as the initial training as a 2-dimensional (2D) L \u00b4 L upper triangular matrix where L is the length of the RNA sequence. All of the labels used during the transfer learning were derived from high-resolution X- ray structures in the PDB. Some approaches in transfer learning freeze weights for specific layers and train for other layers. Here, we trained all the weights of the models without freezing any layer, as this provided better results. Previous work on protein molecular recognition features (MoRFs) prediction 36 also showed that using transfer learning by retraining through all of the weights provides a better result than freezing some of the layers during retraining. During transfer learning on TS1, we used the same hyperparameters (number of layers, depth of layers, kernel size, dilation factor, and learning rate) that were used for the TS0-trained models. All the models were validated for VL1, and based on the performance of these models on VL1, the 5 best models were selected for the ensemble. The parameters of these models are shown in Supplementary Table 9. Input. The input to SPOT-RNA is an RNA sequence represented by a binary one- hot vector of size L \u00b4 4, where L is the length of the RNA sequence and 4 cor- responds to the number of base types (A, U, C, G). In one-hot encoding, a value of 1 was assigned to the corresponding base-type position in the vector and 0 else- where. A missing or invalid sequence in residue value of \u22121 was assigned in one- hot encoded vector. This one-dimensional (L \u00b4 4) input feature is converted into two dimensional (L \u00b4 L \u00b4 8) by the outer concatenation function as described in RaptorX- a b c 1 10 20 30 40 50 60 65 1 10 20 30 40 50 60 65 1 10 20 30 137 10 20 30 37 1 10 20 30 40 50 60 70 80 90 100 110 122 1 10 20 30 40 50 60 70 80 90 100 110 112 Fig. 5 Comparison of SPOT-RNA prediction with the native structure of a Pistol Ribozyme, Mango aptamer, and Adenovirus Virus-associated RNA. The secondary structure of a Pistol Ribozyme (chain A and B in PDB ID 6r47), the Mango Aptamer (chain B in PDB ID 6e8u), and the adenovirus virus- associated RNA (chain C in PDB ID 6ol3) represented by arc diagrams with canonical base pair (BP) in blue color, noncanonical, pseduoknot BP and lone pair in green color, and wrongly predicted BP in magenta color: a predicted structure by SPOT-RNA (on left), with 93% precision and 41% sensitivity, as compared with the native structure (on right) for the Pistol Ribozyme, b the predicted structure by SPOT-RNA (on left) with 100% precision and 26% sensitivity, as compared with the native structure (on right) for the Mango aptamer, c the predicted structure by SPOT-RNA (on left) with 66% precision and 60% sensitivity, as compared with the native structure (on right) for the adenovirus virus-associated RNA. ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 10 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications Contact 28. The input is standardized to have zero mean and unit variance (according to the training data) before being fed into the model. Output. The output of our model is a 2-dimensional (2D) L \u00b4 L upper triangular matrix where L is the length of the RNA sequence. This upper tri- angular matrix represents the likelihood of each nucleotide to be paired with any other nucleotide in a sequence. A single threshold value is used to decide whether a nucleotide is in pair with any other nucleotides. The value of the threshold was chosen in such a way that it optimizes the performance on the validation set. Performance measure. RNA secondary-structure prediction is a binary classifi- cation problem. We used sensitivity, precision, and F1 score for performance measure where sensitivity is the fraction of predicted base pairs in all native base pairs (SN \u00bc TP=\u00f0TP \u00fe FN\u00de), precision is the fraction of correctly predicted base pairs (PR \u00bc TP=\u00f0TP \u00fe FP\u00de), and F1 score is their harmonic mean (F1 \u00bc 2\u00f0PR  SN\u00de=\u00f0PR \u00fe SN\u00de). Here, TP, FN, and FP denote true positives, false negatives, and false positives, respectively. In addition to the above metrics that emphasize on positives, a balanced measure, Matthews correlation coefficient (MCC)66 was also used. MCC is calculated as MCC \u00bc TP \u00b4 TN  FP \u00b4 FN ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi \u00f0TP \u00fe FP\u00de\u00f0TP \u00fe FN\u00de\u00f0TN \u00fe FP\u00de\u00f0TN \u00fe FN\u00de p ; \u00f01\u00de where TN denotes true negatives. MCC measures the correlation between the expected class and the obtained class. Moreover, a precision-recall (sensitivity) curve is used to compare our model with currently available RNA secondary- structure predictors. To show the statistical significance of improvement by SPOT-RNA over the second-best predictor, a paired t test was used on F1 score to obtain P value 67 . The smaller the P value is, the more significant the difference between the two predictors. As the output of the SPOT-RNA is a base-pair probability, we can use the ensemble defect as an additional performance metric. The ensemble defect describes the similarity between predicted base-pair probability and target structure 68 . It can be calculated by appending an extra column to the predicted probability matrix and target matrix for unpaired bases. If P and S are predicted and target structures, respectively, and P\u2032 and S\u2032 are predicted and target structures after appending the extra column, the ensemble defect (ED) is given by ED \u00bc 1  1 L X i \u00bc 1 : L j \u00bc 1 : L \u00fe 1 P0 ij S0 ij; \u00f02\u00de where L is the length of the sequence. The smaller the value of ED is, the higher the structural similarity between predicted base-pair probability and target structure. Methods comparison. We compared SPOT-RNA with 12 best available pre- dictors. We downloaded the stand-alone version of mxfold 33 (available at https:// github.com/keio-bioinformatics/mxfold), ContextFold 16 (available at https://www. cs.bgu.ac.il/negevcb/contextfold/), CONTRAfold 14 (available at http://contra. stanford.edu/contrafold/), Knotty 24 (available at https://github.com/HosnaJabbari/ Knotty), IPknot 23 (available at http://rtips.dna.bio.keio.ac.jp/ipknot/), RNAfold 11 (available at https://www.tbi.univie.ac.at/RNA/), ProbKnot 22 (available at http:// rna.urmc.rochester.edu/RNAstructure.html), CentroidFold 15 (available at https:// github.com/satoken/centroid-rna-package), RNAstructure 12 (available at http:// rna.urmc.rochester.edu/RNAstructure.html), RNAshapes 13 (available at https:// bibiserv.cebitec.uni-bielefeld.de/rnashapes), pkiss 13 (available at https://bibiserv. cebitec.uni-bielefeld.de/pkiss), and CycleFold 27 (available at http://rna.urmc. rochester.edu/RNAstructure.html). In most of the cases, we used default para- meters for secondary-structure prediction except for pkiss. In pkiss, we used Strategy C that is slow but thorough in comparison with Strategies A and B that are fast but less accurate. For CONTRAfold and CentroidFold their performance metrics are derived from their predicted base-pair probabilities with threshold values from maximizing MCC. Reporting summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article. 0.86 0.82 0.84 0.82 0.82 0.85 0.82 0.81 0.85 0.82 0.82 0.68 0.43 0 0.2 0.4 0.6 0.8 1 F1-score a 0.90 0.86 0.67 0.18 0.65 0.65 0.61 0.61 0.53 0.63 0.31 0.42 0.63 0 0.2 0.4 0.6 0.8 1 F1-score b 0.57 0.16 0.46 0.20 0.48 0.55 0.44 0.56 0.55 0.52 0.24 0.41 0.21 0 0.2 0.4 0.6 0.8 1 F1-score c 0.74 0.59 0.79 0.56 0.68 0.64 0.63 0.63 0.49 0.64 0.64 0.59 0.69 0 0.2 0.4 0.6 0.8 1 F1-score d 0.41 0.36 0.41 0.28 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.34 0 0.2 0.4 0.6 0.8 1 F1-score e 0.63 0.91 0.73 0.74 0.71 0.88 0.91 0.88 0.82 0.91 0.91 0.69 0.51 0 0.2 0.4 0.6 0.8 1 F1-score f SPOT-RNA Knotty mxfold ContextFold CONTRAfold IPknot RNAfold ProbKnot CentroidFold RNAstructure RNAshapes pkiss CycleFold Fig. 6 Performance comparison of all predictors on 6 recently released (after March 9, 2019) crystal structures. a F1 score of predicted structure on a synthetic construct RNA (chain H in PDB ID 6dvk), b F1 score of predicted structure on the Glutamine II Riboswitch RNA (chain A in PDB ID 6qn3), c F1 score of predicted structure on a synthetic construct Hatchet Ribozyme (chain U in PDB ID 6jq6), d F1 score of predicted structure on a Pistol Ribozyme (chain A &amp; B in PDB ID 6r47), e F1 score of predicted structure on the Mango Aptamer (chain B in PDB ID 6e8u), f F1 score of predicted structure on the adenovirus virus-associated RNA (chain C in PDB ID 6ol3). NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 11 Data availability The data used by SPOT-RNA for initial training (bpRNA)34 and transfer learning (PDB)5 along with their annotated secondary structure are publicly available at http://sparks-lab. org/jaswinder/server/SPOT-RNA/ and https://github.com/jaswindersingh2/SPOT-RNA. Code availability SPOT-RNA predictor is available as a server at http://sparks-lab.org/jaswinder/server/ SPOT-RNA/ and stand-alone software at https://github.com/jaswindersingh2/SPOT- RNA to run on a local computer. The web server provides an arc diagram and a 2D diagram of predicted RNA secondary structure through Visualization Applet for RNA (VARNA) 69 tool along with a dot plot of SPOT-RNA-predicted base-pair probabilities. Received: 12 June 2019; Accepted: 1 November 2019; References 1. Tinoco, I. &amp; Bustamante, C. How RNA folds. J. Mol. Biol. 293, 271\u2013281 (1999). 2. Bevilacqua, P. C., Ritchey, L. E., Su, Z. &amp; Assmann, S. M. Genome-wide analysis of RNA secondary structure. Annu. Rev. Genet. 50, 235\u2013266 (2016). 3. Tian, S. &amp; Das, R. RNA structure through multidimensional chemical mapping. Q. Rev. Biophys. 49, e7 (2016). 4. RNAcentral: a comprehensive database of non-coding RNA sequences. Nucleic Acids Res. 45, D128\u2013D134 (2016). 5. Rose, P. W. et al. The RCSB protein data bank: integrative view of protein, gene and 3D structural information. Nucleic Acids Res. 45, D271\u2013D281 (2016). 6. Rivas, E. The four ingredients of single-sequence RNA secondary structure prediction. A unifying perspective. RNA Biol. 10, 1185\u20131196 (2013). 7. Gutell, R. R., Lee, J. C. &amp; Cannone, J. J. The accuracy of ribosomal RNA comparative structure models. Curr. Opin. Struct. Biol. 12, 301\u2013310 (2002). 8. Griffiths-Jones, S., Bateman, A., Marshall, M., Khanna, A. &amp; Eddy, S. R. Rfam: an RNA family database. Nucleic Acids Res. 31, 439\u2013441 (2003). 9. Zuker, M. &amp; Stiegler, P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information. Nucleic Acids Res. 9, 133\u2013148 (1981). 10. Schroeder, S. J. and Turner, D. H. Chapter 17\u2014Optical Melting Measurements of Nucleic Acid Thermodynamics. In Biophysical, Chemical, and Functional Probes of RNA Structure, Interactions and Folding: Part A, vol. 468 of Methods in Enzymology, 371\u2013387 (Academic Press, 2009). 11. Lorenz, R. et al. ViennaRNA Package 2.0. Algorithms Mol. Biol. 6, 26 (2011). 12. Reuter, J. S. &amp; Mathews, D. H. RNAstructure: software for RNA secondary structure prediction and analysis. BMC Bioinforma. 11, 129 (2010). 13. Janssen, S. &amp; Giegerich, R. The RNA shapes studio. Bioinformatics 31, 423\u2013425 (2014). 14. Do, C. B., Woods, D. A. &amp; Batzoglou, S. CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinformatics 22, e90\u2013e98 (2006). 15. Sato, K., Hamada, M., Asai, K. &amp; Mituyama, T. CentroidFold: a web server for RNA secondary structure prediction. Nucleic Acids Res. 37, W277\u2013W280 (2009). 16. Zakov, S., Goldberg, Y., Elhadad, M. &amp; Ziv-ukelson, M. Rich parameterization improves RNA structure prediction. J. Computational Biol. 18, 1525\u20131542 (2011). 17. Seetin, M. G. and Mathews, D. H. RNA Structure prediction: an overview of methods. In (ed Keiler, K. C.) Bacterial Regulatory RNA: Methods and Protocols, 99\u2013122 (Humana Press, Totowa, NJ, 2012). https://doi.org/10.1007/ 978-1-61779-949-5_8. 18. Xu, X. &amp; Chen, S.-J. Physics-based RNA structure prediction. Biophysics Rep. 1, 2\u201313 (2015). 19. Nowakowski, J. &amp; Tinoco, I. RNA structure and stability. Semin. Virol. 8, 153\u2013165 (1997). 20. Westhof, E. &amp; Fritsch, V. RNA folding: beyond Watson-Crick pairs. Structure 8, R55\u2013R65 (2000). 21. Reeder, J. &amp; Giegerich, R. Design, implementation and evaluation of a practical pseudoknot folding algorithm based on thermodynamics. BMC Bioinforma. 5, 104 (2004). 22. Bellaousov, S. &amp; Mathews, D. H. ProbKnot: fast prediction of RNA secondary structure including pseudoknots. RNA 16, 1870\u20131880 (2010). 23. Sato, K., Kato, Y., Hamada, M., Akutsu, T. &amp; Asai, K. IPknot: fast and accurate prediction of RNA secondary structures with pseudoknots using integer programming. Bioinformatics 27, i85\u2013i93 (2011). 24. Jabbari, H., Wark, I., Montemagno, C. &amp; Will, S. Knotty: efficient and accurate prediction of complex RNA pseudoknot structures. Bioinformatics 34, 3849\u20133856 (2018). 25. Parisien, M. &amp; Major, F. The MC-fold and MC-sym pipeline infers RNA structure from sequence data. Nature 452, 51\u201355 (2008). 26. zu Siederdissen, C. H., Bernhart, S. H., Stadler, P. F. &amp; Hofacker, I. L. A folding algorithm for extended RNA secondary structures. Bioinformatics 27, i129\u2013i136 (2011). 27. Sloma, M. F. &amp; Mathews, D. H. Base pair probability estimates improve the prediction accuracy of RNA non-canonical base pairs. PLOS Comput. Biol. 13, 1\u201323 (2017). 28. Wang, S., Sun, S., Li, Z., Zhang, R. &amp; Xu, J. Accurate de novo prediction of protein contact map by ultra-deep learning model. PLOS Comput. Biol. 13, 1\u201334 (2017). 29. Hanson, J., Paliwal, K., Litfin, T., Yang, Y. &amp; Zhou, Y. Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks. Bioinformatics 34, 4039\u20134045 (2018). 30. He, K., Zhang, X., Ren, S. and Sun, J. Identity mappings in deep residual networks. In (eds Leibe, B., Matas, J., Sebe, N. and Welling, M.) Computer Vision\u2014ECCV 2016, 630\u2013645 (Springer International Publishing, Cham, 2016). 31. Hochreiter, S. &amp; Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735\u20131780 (1997). 32. Schuster, M. &amp; Paliwal, K. K. Bidirectional recurrent neural networks. IEEE Trans. Signal Process. 45, 2673\u20132681 (1997). 33. Akiyama, M., Sato, K. &amp; Sakakibara, Y. A max-margin training of RNA secondary structure prediction integrated with the thermodynamic model. J. Bioinforma. Comput. Biol. 16, 1840025 (2018). 34. Danaee, P. et al. bpRNA: large-scale automated annotation and analysis of RNA secondary structure. Nucleic Acids Res. 46, 5381\u20135394 (2018). 35. Pan, S. J. &amp; Yang, Q. A Survey on Transfer Learning. IEEE Trans. Knowl. Data Eng. 22, 1345\u20131359 (2010). 36. Hanson, J., Litfin, T., Paliwal, K. and Zhou, Y. Identifying molecular recognition features in intrinsically disordered regions of proteins by transfer learning. Bioinformatics. https://doi.org/10.1093/bioinformatics/btz691 (2019). 37. Fu, L., Niu, B., Zhu, Z., Wu, S. &amp; Li, W. CD-HIT: accelerated for clustering the next-generation sequencing data. Bioinformatics 28, 3150\u20133152 (2012). 38. Yang, Y. et al. Genome-scale characterization of RNA tertiary structures and their functional impact by RNA solvent accessibility prediction. RNA 23, 14\u201322 (2017). 39. Guruge, I., Taherzadeh, G., Zhan, J., Zhou, Y. &amp; Yang, Y. B-factor profile prediction for RNA flexibility using support vector machines. J. Comput. Chem. 39, 407\u2013411 (2018). 40. Altschul, S. F. et al. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. Nucleic Acids Res. 25, 3389\u20133402 (1997). 41. Liberman, J. A., Salim, M., Krucinska, J. &amp; Wedekind, J. E. Structure of a class II preQ1 riboswitch reveals ligand recognition by a new fold. Nat. Chem. Biol. 9, 353 EP (2013). 42. Goto-Ito, S., Ito, T., Kuratani, M., Bessho, Y. &amp; Yokoyama, S. Tertiary structure checkpoint at anticodon loop modification in tRNA functional maturation. Nat. Struct. Amp; Mol. Biol. 16, 1109 EP (2009). 43. Yesselman, J. D. et al. Computational design of three-dimensional RNA structure and function. Nat. Nanotechnol. 14, 866\u2013873 (2019). 44. Huang, L., Wang, J., Watkins, A. M., Das, R. &amp; Lilley, D. M. J. Structure and ligand binding of the glutamine-II riboswitch. Nucleic Acids Res. 47, 7666\u20137675 (2019). 45. Zheng, L. et al. Hatchet ribozyme structure and implications for cleavage mechanism. Proc. Natl Acad. Sci. 116, 10783\u201310791 (2019). 46. Wilson, T. J. et al. Comparison of the structures and mechanisms of the Pistol and Hammerhead ribozymes. J. Am. Chem. Soc. 141, 7865\u20137875 (2019). 47. Trachman, R. J. et al. Structure and functional reselection of the Mango-III fluorogenic RNA aptamer. Nat. Chem. Biol. 15, 472\u2013479 (2019). 48. Hood, I. V. et al. Crystal structure of an adenovirus virus-associated RNA. Nat. Commun. 10, 2871 (2019). 49. Remmert, M., Biegert, A., Hauser, A. &amp; S\u00f6ding, J. HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment. Nat. Methods 9, 173\u2013175 (2011). 50. De Leonardis, E. et al. Direct-Coupling Analysis of nucleotide coevolution facilitates RNA secondary and tertiary structure prediction. Nucleic Acids Res. 43, 10444\u201310455 (2015). 51. Heffernan, R. et al. Single-sequence-based prediction of protein secondary structures and solvent accessibility by deep whole-sequence learning. J. Comput. Chem. 39, 2210\u20132216 (2018). 52. Hanson, J., Paliwal, K. &amp; Zhou, Y. Accurate single-sequence prediction of protein intrinsic disorder by an ensemble of deep recurrent and convolutional architectures. J. Chem. Inf. Model. 58, 2369\u20132376 (2018). 53. Miao, Z. et al. RNA-Puzzles Round III: 3D RNA structure prediction of five riboswitches and one ribozyme. RNA 23, 655\u2013672 (2017). ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 12 NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 54. Rabani, M., Kertesz, M. and Segal, E. Computational prediction of RNA structural motifs involved in post-transcriptional regulatory processes. In (ed Gerst, J. E.) RNA Detection and Visualization: Methods and Protocols, 467\u2013479 (Humana Press, 2011). 55. Achar, A. &amp; S\u00e6trom, P. RNA motif discovery: a computational overview. Biol. Direct 10, 61 (2015). 56. Nawrocki, E. P. &amp; Eddy, S. R. Infernal 1.1: 100-fold faster RNA homology searches. Bioinformatics 29, 2933\u20132935 (2013). 57. Schlick, T. &amp; Pyle, A. M. Opportunities and challenges in RNA structural modeling and design. Biophys. J. 113, 225\u2013234 (2017). 58. Lu, X.-J., Bussemaker, H. J. &amp; Olson, W. K. DSSR: an integrated software tool for dissecting the spatial structure of RNA. Nucleic Acids Res. 43, e142\u2013e142 (2015). 59. Clevert, D.-A., Unterthiner, T. and Hochreiter, S. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). Preprint at: https:// arxiv.org/abs/1511.07289 (2015). 60. Ba, J. L., Kiros, J. R. and Hinton, G. E. Layer Normalization. Preprint at: https://arxiv.org/abs/1607.06450 (2016). 61. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. &amp; Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929\u20131958 (2014). 62. Bai, S., Kolter, J. Z. and Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. CoRR abs/ 1803.01271 (2018). 63. Abadi, M. et al. TensorFlow: A System for Large-Scale Machine Learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 265\u2013283 (USENIX Association, Savannah, GA, 2016). https:// www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi. 64. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. Preprint at: https://arxiv.org/abs/1412.6980 (2014). 65. Oh, K.-S. &amp; Jung, K. GPU implementation of neural networks. Pattern Recognit. 37, 1311\u20131314 (2004). 66. Matthews, B. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochim. Biophys. Acta 405, 442\u2013451 (1975). 67. Lovric, M. (ed.) International Encyclopedia of Statistical Science (Springer, Berlin Heidelberg, 2011). https://doi.org/10.1007/978-3-642-04898-2 68. Martin, J. S. Describing the structural diversity within an RNAas ensemble. Entropy 16, 1331\u20131348 (2014). 69. Darty, K., Denise, A. &amp; Ponty, Y. VARNA: Interactive drawing and editing of the RNA secondary structure. Bioinformatics 25, 1974\u20131975 (2009). Acknowledgements This work was supported by Australia Research Council DP180102060 to Y.Z. and K.P. and in part by National Health and Medical Research Council (1,121,629) of Australia to Y.Z. We also gratefully acknowledge the use of the High Performance Computing Cluster Gowonda to complete this research, and the aid of the research cloud resources provided by the Queensland CyberInfrastructure Foundation (QCIF). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research. Author contributions J.S., J.H., and K.P. designed the network architectures, J.S. prepared the data, did the data analysis, and wrote the paper. J.S. and J.H. performed the training and testing of the algorithms. Y.Z. conceived of the study, participated in the initial design, assisted in data analysis, and drafted the whole paper. All authors read, contributed to the discussion, and approved the final paper. Competing interests The authors declare no competing interests. Additional information Supplementary information is available for this paper at https://doi.org/10.1038/s41467- 019-13395-9. Correspondence and requests for materials should be addressed to K.P. or Y.Z. Peer review information Nature Communications thanks the anonymous reviewer(s) for their contribution to the peer review of this work. Reprints and permission information is available at http://www.nature.com/reprints Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/. \u00a9 The Author(s) 2019 NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-019-13395-9 ARTICLE NATURE COMMUNICATIONS | (2019) 10:5407 | https://doi.org/10.1038/s41467-019-13395-9 | www.nature.com/naturecommunications 13 === 1 Briefings in Bioinformatics, 23(1), 2022, 1\u20139 https://doi.org/10.1093/bib/bbab395 Problem Solving Protocol Prediction of RNA secondary structure including pseudoknots for long sequences Kengo Sato and Yuki Kato Corresponding author: Kengo Sato, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama 223-8522, Japan. Tel.: +81-45-566-1511; E-mail: satoken@bio.keio.ac.jp Abstract RNA structural elements called pseudoknots are involved in various biological phenomena including ribosomal frameshifts. Because it is infeasible to construct an efficiently computable secondary structure model including pseudoknots, secondary structure prediction methods considering pseudoknots are not yet widely available. We developed IPknot, which uses heuristics to speed up computations, but it has remained difficult to apply it to long sequences, such as messenger RNA and viral RNA, because it requires cubic computational time with respect to sequence length and has threshold parameters that need to be manually adjusted. Here, we propose an improvement of IPknot that enables calculation in linear time by employing the LinearPartition model and automatically selects the optimal threshold parameters based on the pseudo-expected accuracy. In addition, IPknot showed favorable prediction accuracy across a wide range of conditions in our exhaustive benchmarking, not only for single sequences but also for multiple alignments. Key words: RNA secondary structure prediction; pseudoknots; integer programming Introduction Genetic information recorded in DNA is transcribed into RNA, which is then translated into protein to fulfill its function. In other words, RNA is merely an intermediate product for the transmission of genetic information. This type of RNA is called messenger RNA (mRNA). However, many RNAs that do not fit into this framework have been discovered more recently. For example, transfer RNA and ribosomal RNA, which play central roles in the translation mechanism, nucleolar small RNA, which guides the modification sites of other RNAs, and microRNA, which regulates gene expression, have been discovered. Thus, it has become clear that RNAs other than mRNAs are involved in various biological phenomena. Because these RNAs do not encode proteins, they are called non-coding RNAs. In contrast to DNA, which forms a double-stranded structure in vivo, RNA is often single-stranded and is thus unstable when intact. In the case of mRNA, the cap structure at the 5\u2032 end and the poly-A strand at the 3\u2032 end protect it from degradation. On the other Kengo Sato is an assistant professor at the Department of Biosciences and Informatics at Keio University, Japan. He received his PhD in Computer Science from Keio University, Japan, in 2003. His research interests include bioinformatics, computational linguistics and machine learning. Yuki Kato is an assistant professor at Department of RNA Biology and Neuroscience, Graduate School of Medicine, and at Integrated Frontier Research for Medical Science Division, Institute for Open and Transdisciplinary Research Initiatives, Osaka University, Japan. His research interests include biological sequence analysis and single-cell genomics. Submitted: 16 June 2021; Received (in revised form): 13 August 2021 \u00a9 The Author(s) 2021. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/ licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com hand, for other RNAs that do not have such structures, single- stranded RNA molecules bind to themselves to form three- dimensional structures and ensure their stability. Also, as in the case of proteins, RNAs with similar functions have similar three- dimensional structures, and it is known that there is a strong association between function and structure. The determination of RNA three-dimensional (3D) structure can be performed by X- ray crystallography, nuclear magnetic resonance, cryo-electron microscopy, and other techniques. However, it is difficult to apply these methods on a large scale owing to difficulties associated with sequence lengths, resolution and cost. Therefore, RNA sec- ondary structure, which is easier to model, is often computation- ally predicted instead. RNA secondary structure refers to the set of base pairs consisting of Watson\u2013Crick base pairs (A\u2013U, G\u2013C) and wobble base pairs (G\u2013U) that form the backbone of the 3D structure. RNA secondary structure prediction is conventionally based on thermodynamic models, which predict the secondary 2 Sato and Kato Figure 1. (A) A typical psudoknot structure. The dotted lines represent base pairs. (B) A linear presentation of the pseudoknot. structure with the minimum free energy (MFE) among all possible secondary structures. Popular methods based on thermodynamic models include mfold [1], RNAfold [2], and RNAstructure [3]. Recently, RNA secondary structure prediction methods based on machine learning have also been developed. These methods train alternative parameters to the thermody- namic parameters by taking a large number of pairs of RNA sequences and their reference secondary structures as training data. The following methods fall under the category of methods that use machine learning: CONTRAfold [4], ContextFold [5], SPOT-RNA [6] and MXfold2 [7]. However, from the viewpoint of computational complexity, most approaches do not support the prediction of secondary structures that include pseudoknot substructures. Pseudoknots are one of the key topologies occurring in RNA secondary structures. The pseudoknot structure is a structure in which some bases inside of a loop structure form base pairs with bases outside of the loop (e.g. Figure 1A). In other words, it is said to have a pseudoknot structure if there exist base pairs that are crossing each other by connecting bases of base pairs with arcs, as shown in Figure 1B. The pseudoknot structure is known to be involved in the regulation of translation and splicing, and riboso- mal frameshifts [8\u201310]. The results of sequence analysis suggest that the hairpin loops, which are essential building blocks of the pseudoknots, first appeared in the evolutionary timescale [11], and then the pseudoknots were configured, resulting in gaining those functions. We therefore conclude that pseudoknots should not be excluded from the modeling of RNA secondary structures. The computational complexity required for MFE predictions of an arbitrary pseudoknot structure has been proven to be NP-hard [12, 13]. To address this, dynamic programming-based methods that require polynomial time (O(n4 )\u2013O(n6 ) for sequence length n) to exactly compute the restricted complexity of pseu- doknot structures [12\u201316] and heuristics-based fast computation methods [17\u201320] have been developed. We previously developed IPknot [21], a fast heuristic-based method for predicting RNA secondary structures including pseu- doknots. IPknot decomposes a secondary structure with pseudo- knots into several pseudoknot-free substructures and predicts the optimal secondary structure using integer programming (IP) based on maximization of expected accuracy (MEA) under the constraints that each substructure must satisfy. The threshold cut technique, which is naturally derived from MEA, enables IPknot to perform much faster calculations with nearly com- parable prediction accuracy relative to other methods. How- ever, because the MEA-based score uses base pairing probability without considering pseudoknots, which requires a calculation time that increases cubically with sequence length, it is difficult to use for secondary structure prediction of sequences that exceed 1000 bases, even when applying a threshold cut tech- nique. Furthermore, as the prediction accuracy can drastically change depending on the thresholds determined in advance for each pseudoknot-free substructure, thresholds must be carefully determined. To address the limitations of IPknot, we implemented the following two improvements to the method. The first is the use of LinearPartition [22] to calculate base pairing probabili- ties. LinearPartition can calculate the base pairing probability, with linear computational complexity with respect to sequence length, using the beam search technique. By employing the LinearPartition model, IPknot is able to predict secondary struc- tures while considering pseudoknots for long sequences, includ- ing mRNA, lncRNA and viral RNA. The other improvement is the selection of thresholds based on pseudo-expected accuracy, which was originally developed by Hamada et al. [23]. We show that the pseudo-expected accuracy is correlated with the \u2018true\u2019 accuracy, and by choosing thresholds for each sequence based on the pseudo-expected accuracy, we can select a nearly optimal secondary structure prediction. Materials and Methods Given an RNA sequence x = x1 \u00b7 \u00b7 \u00b7 x n (x i \u2208 {A, C, G, U}), its sec- ondary structure is represented by a binary matrix y = (yij), where y ij = 1 if x i and x j form a base pair and otherwise y ij = 0. Let Y(x) be a set of all possible secondary structures of x including pseudoknots. We assume that y \u2208 Y(x) can be decomposed into a set of pseudoknot-free substructures y(1) , y(2) , . . . , y(m) , such that y = \u2211m p=1 y(p) . In order to guarantee the uniqueness of the decomposition, the following conditions should be satisfied: (i) y \u2208 Y(x) should be decomposed into mutually exclusive sets; that is, for all 1 \u2264 i &lt; j \u2264 |x|, \u2211m p=1 y(p) ij \u2264 1; (ii) every base pair in y(p) should be pseudoknotted with at least one base pair in y(q) for \u2200q &lt; p. Maximizing expected accuracy One of the most promising techniques for predicting RNA sec- ondary structures is the MEA-based approach [4, 24]. First, we define a gain function of prediction \u02c6y \u2208 Y(x) with regard to the correct secondary structure y \u2208 Y(x) as G\u03c4 (y, \u02c6y) = (1 \u2212 \u03c4 )TP(y, \u02c6y) + \u03c4 TN(y, \u02c6y), (1) where TP(y, \u02c6y) = \u2211 i&lt;j I(y ij = 1)I( \u02c6y ij = 1) is the number of true positive base pairs, TN(y, \u02c6y) = \u2211 i&lt;j I(y ij = 0)I( \u02c6y ij = 0) is the number of true negative base pairs, and \u03c4 \u2208 [0, 1] is a balancing parameter between true positives and true negatives. Here, I(condition) is the indicator function that takes a value of 1 or 0 depending on whether the condition is true or false. Our objective is to find a secondary structure that maximizes the expectation of the gain function (1) under a given probability distribution over the space Y(x) of pseudoknotted secondary structures, as follows: Ey|x [G\u03c4 (y, \u02c6y)] = \u2211 y\u2208Y(x) G\u03c4 (y, \u02c6y)P(y | x). (2) Here, P(y | x) is a probability distribution of RNA secondary structures including pseudoknots. Because the calculation of the expected gain function (2) is intractable for arbitrary pseudoknots, we approximate Eq. (2) by the sum of the expected gain function for decomposed pseudoknot-free substructures \u02c6y(1) , . . . , \u02c6y(m) for \u02c6y \u2208 Y(x) such that IPknot for long sequences 3 \u02c6y = \u2211m p=1 \u02c6y(p) , and thus, we find a pseudoknotted structure \u02c6y and its decomposition \u02c6y(1) , . . . , \u02c6y(m) that maximize m\u2211 p=1 \u2211 y\u2208Y\u2032 (x) G\u03c4 (p) (y, \u02c6y(p) )P\u2032(y | x) = m\u2211 p=1 \u2211 i&lt;j [ p ij \u2212 \u03c4 (p) ] \u02c6y(p) ij + C, (3) where \u03c4 (p) \u2208 [0, 1] is a balancing parameter between true positives and true negatives for a level p, and C is a constant independent of \u02c6y. The base pairing probability p ij is the probability that the bases x i and x j form a base pair, which is defined as p ij = \u2211 y\u2208Y\u2032 (x) I(y ij = 1)P\u2032(y | x). (4) See Section S1 in Supplementary Information for the derivation. Notably, it is no longer necessary to consider the base pairs whose probabilities are at most the threshold \u03c4 (p) , which we refer to as the threshold cut. We can choose P\u2032(y | x), a probability distribution over a set Y\u2032(x) of secondary structures without pseudoknots, from among several options. Instead of using a probability distribution with pseudoknots, we can employ a probability distribution without pseudoknots, such as the McCaskill model [25] and the CON- TRAfold model [4], whose computational complexity is O(|x|3 ) for time and O(|x|2 ) for space. Alternatively, the LinearPartition model [22], which is O(|x|) in both time and space, enables us to predict the secondary structure of sequences much longer than 1000 bases. IP formulation We can formulate our problem described in the previous section as the following IP problem: maximize m\u2211 p=1 \u2211 i&lt;j [ p ij \u2212 \u03c4 (p) ] y(p) ij (5) subject to y ij \u2208 {0, 1} (1 \u2264 \u2200i &lt; \u2200 &lt; j \u2264 n), (6) y(p) ij \u2208 {0, 1} (1 \u2264 \u2200p \u2264 m, 1 \u2264 \u2200i &lt; \u2200j \u2264 n), (7) yij = m\u2211 p=1 y(p) ij (1 \u2264 \u2200i &lt; \u2200j \u2264 n), (8) i\u22121\u2211 h=1 y hi + n\u2211 h=i+1 y ih \u2264 1 (1 \u2264 \u2200i \u2264 n), (9) y(p) ij + y(p) kl \u2264 1 (1 \u2264 p \u2264 m, 1 \u2264 \u2200i &lt; \u2200k &lt; \u2200j &lt; \u2200l \u2264 n), (10) \u2211 i&lt;k&lt;j&lt;l y(q) ij + \u2211 k&lt;i\u2032 &lt;l&lt;j\u2032 y(q) i\u2032 j\u2032 \u2265 y(p) kl (1 \u2264 q &lt; p \u2264 m, 1 \u2264 \u2200k &lt; \u2200l \u2264 n). (11) Because Equation (5) is an instantiation of the approximate estimator (3) and the threshold cut technique is applicable to Eq. (3), the base pairs y(p) ij whose base pairing probabilities p ij are larger than \u03c4 (p) need to be considered. The number of variables y(p) ij that should be considered is at most |x|/\u03c4 (p) because \u2211 ji p ij \u2264 1 for 1 \u2264 \u2200i \u2264 |x|. Constraint (9) means that each base x i is paired with at most one base. Constraint (10) disallows pseudoknots within the same level p. Constraint (11) ensures that each base pair at level p is pseudoknotted with at least one base pair at every lower level q &lt; p to guarantee the uniqueness of the decomposition y = \u2211m p=1 y(p) . Pseudo-expected accuracy To solve the IP problem (5)\u2013(11), we are required to choose the set of thresholds for each level \u03c4 (1) , . . . , \u03c4 (m) , each of which is a balancing parameter between true positives and true negatives. However, it is not easy to obtain the best set of \u03c4 values for any sequence beforehand. Therefore, we employ an approach originally proposed by Hamada et al. [23], which chooses a param- eter set for each sequence among several parameter sets that predicts the best secondary structure in terms of an approxima- tion of the expected accuracy (called pseudo-expected accuracy) and makes the prediction by the best parameter set the final prediction. The accuracy of a predicted RNA secondary structure \u02c6y against a reference structure y is evaluated using the following measures: PPV(y, \u02c6y) = TP(y, \u02c6y) TP(y, \u02c6y) + FP(y, \u02c6y) , (12) SEN(y, \u02c6y) = TP(y, \u02c6y) TP(y, \u02c6y) + FN(y, \u02c6y) , (13) F(y, \u02c6y) = 2 \u00b7 PPV(y, \u02c6y) \u00b7 SEN(y, \u02c6y) PPV(y, \u02c6y) + SEN(y, \u02c6y) . (14) Here, TP(y, \u02c6y) = \u2211 i&lt;j I(y ij = 1)I( \u02c6y ij = 1), FP(y, \u02c6y) = \u2211 i&lt;j I(y ij = 0)I( \u02c6y ij = 1) and FN(y, \u02c6y) = \u2211 i&lt;j I(y ij = 1)I( \u02c6y ij = 0). To estimate the accuracy of the predicted secondary structure \u02c6y without knowing the true secondary structure y, we take an expectation of F(y, \u02c6y) over the distribution of y: F( \u02c6y) = Ey|x [F(y, \u02c6y)] = \u2211 y\u2208Y(x) F(y, \u02c6y)P(y | x). (15) However, this calculation is intractable because the number of y \u2208 Y(x) increases exponentially with the length of sequence x. Alternatively, we first calculate expected TP, FP and FN as follows: TP( \u02c6y) = Ey|x [TP(y, \u02c6y)] = \u2211 i&lt;j p ij I( \u02c6y ij = 1), (16) 4 Sato and Kato FP( \u02c6y) = Ey|x [FP(y, \u02c6y)] = \u2211 i&lt;j (1 \u2212 p ij)I( \u02c6y ij = 1), (17) FN( \u02c6y) = Ey|x[FN(y, \u02c6y)] = \u2211 i&lt;j p ij I( \u02c6y ij = 0). (18) Then, we approximate F by calculating Equation (14) using TP, FP, and FN instead of TP, FP and FN, respectively. In addition to the original pseudo-expected accuracy described above, we introduce the pseudo-expected accuracy for crossing base pairs to predict pseudoknotted structures. Prediction of secondary structures including pseudoknots depends on both the conventional prediction accuracy of base pairs described above and the accuracy of crossing base pairs. A crossing base pair is a base pair xi and x j such that there exists another base pair x k and x l that is crossing the base pair xi and x j; that is, k &lt; i &lt; l &lt; j or i &lt; k &lt; j &lt; l. We define the expectations of true positives, false positives and false negatives for crossing base pairs as follows: TP cb( \u02c6y) = Ey|x [TP(cb(y), cb( \u02c6y))] \u2248 \u2211 i&lt;k&lt;j&lt;l p ij p kl I( \u02c6y ij = 1 \u2227 \u02c6y kl = 1), (19) FPcb( \u02c6y) = Ey|x [FP(cb(y), cb( \u02c6y))] \u2248 \u2211 i&lt;k&lt;j&lt;l (1 \u2212 p ij p kl)I( \u02c6y ij = 1 \u2227 \u02c6y kl = 1), (20) FNcb( \u02c6y) = Ey|x [FN(cb(y), cb( \u02c6y))] \u2248 \u2211 i&lt;k&lt;j&lt;l p ij p kl I( \u02c6y ij = 0 \u2228 \u02c6y kl = 0). (21) Here, cb(y) is an n\u00d7n binary matrix, whose (i, j)-element is yij itself if there exists k &lt; i &lt; l &lt; j or i &lt; k &lt; j &lt; l such that y kl = 1, and 0 otherwise. Then, we calculate the pseudo-expected F-value for crossing base pairs F cb using Equation (14) with TP cb, FPcb and FNcb instead of TP, FP and FN, respectively. Equations (19)\u2013(21) require O(n4 ) for naive calculations, but can be reduced to acceptable computational time by utilizing the threshold cut technique. We predict secondary structures \u02c6y t (t = 1, . . . , l) for several threshold parameters {(\u03c4 (1) t , . . . , \u03c4 (m) t ) | t = 1, . . . , l}. Then, we cal- culate their pseudo-expected accuracy F( \u02c6y t) + F cb( \u02c6y t) and choose the secondary structure \u02c6y t that maximizes the pseudo-expected accuracy as the final prediction. Common secondary structure prediction The average of the base pairing probability matrices for each sequence in an alignment has been used to predict the com- mon secondary structure for the alignment [26, 27]. Let A be an alignment of RNA sequences that contains k sequences and |A| denote the number of columns in A. We calculate the base pairing probabilities of an individual sequence x \u2208 A as p(x) ij = \u2211 y\u2208Y(x) I(y ij = 1)P(y | x). (22) The averaged base pairing probability matrix is defined as p(A) ij = 1 k \u2211 x\u2208A p(x) ij . (23) The common secondary structure of the alignment A can be calculated in the same way by replacing p ij in Equations (5) with p(A) ij . While the common secondary structure prediction based on the average base pairing probability matrix has been implemented in the previous version of IPknot [21], the present version employs the LinearPartition model, which enables the calculation linearly with respect to the alignment length. Implementation Our method has been implemented as the newest version of a program called IPknot. In addition to the McCaskil model [25] and CONTRAfold model [4], which were already integrated into the previous version of IPknot, the LinearPartition model [22] is also supported as a probability distribution for secondary structures. To solve IP problems, the GNU Linear Programming Kit (GLPK; http://www.gnu.org/software/glpk/), Gurobi Optimizer (http://gu robi.com/) or IBM CPLEX Optimizer (https://www.ibm.com/ana lytics/cplex-optimizer) can be employed. Datasets To evaluate our algorithm, we performed computational experiments on several datasets. We employed RNA sequences extracted from the bpRNA-1m dataset [28], which is based on Rfam 12.2 [29], and the comparative RNA web dataset [30] with 2588 families. In addition, we built a dataset that includes families from the most recent Rfam database, Rfam 14.5 [31]. Since the release of Rfam 12.2, the Rfam project has actively collected about 1400 RNA families, including families detected by newly developed techniques. We extracted these newly discovered families. To limit bias in the training data, sequences with higher than 80% sequence identity with the sequence subsets S-Processed-TRA from RNA STRAND [32] and TR0 from bpRNA-1m [28], which are the training datasets for CONTRAfold and SPOT-RNA, respectively, were removed using CD-HIT-EST-2D [33]. We then removed redundant sequences using CD-HIT-EST [33], with a cutoff threshold of 80% sequence identity. For the prediction of common secondary structures, the sequence selected by the above method was used as a seed, and 1\u20139 sequences of the same Rfam family and with high sequence identity (\u2265 80%) with the seed sequence were randomly selected to create an alignment. Common secondary structure prediction was performed on the reference alignments from Rfam and the alignments calculated by MAFFT [34]. Because there are sequences from bpRNA-1m that do not have Rfam reference alignments, only sequences from Rfam 14.5 were tested for common secondary structure prediction. To capture the accuracy of the common secondary structure prediction, the accuracy for the seed sequence is shown. A summary of the dataset created and utilized is shown in Table 1. Results Effectiveness of pseudo-expected accuracy First, to show the effectiveness of the automatic selection from among thresholds \u03c4 (1) , . . . , \u03c4 (m) based on the pseudo-expected IPknot for long sequences 5 Table 1. Datasets used in our experiments. Each element of the table shows the number of sequences Pseudoknot-free Pseudoknotted Short Medium Long Short Medium Long Length (nt) (12\u2013150) (151\u2013500) (501\u20134381) (12\u2013150) (151\u2013500) (501\u20134381) (Single) bpRNA-1m 1971 514 420 125 162 245 Rfam 14.5 6299 723 9 1692 477 151 (Multiple) Rfam 14.5 5118 554 4 1692 477 151 Figure 2. PPV\u2013SEN plot of IPknot and ThreshKnot for short RNA sequences (\u2264 150 nt). accuracy, Figure 2 and Table S1 in Supplementary Information show the prediction accuracy on the dataset of short sequences (\u2264 150 nt) using automatic selection and manual selection of the threshold \u03c4 values. For IPknot, we fixed the number of decom- posed sets of secondary substructures m = 2, and varied thresh- old parameters \u03c4 values for base pairing probability in such a way that {(\u03c4 (1) , \u03c4 (2) ) | \u03c4 (p) = 2\u2212t, p = 1, 2, t = 1, 2, 3, 4, \u03c4 (1) \u2265 \u03c4 (2)}. In IPknot with pseudo-expected accuracy, the best secondary structure in the sense of pseudo-expected F is selected from the same range of (\u03c4 (1) , \u03c4 (2) ) for each sequence. For these variants of IPknot, the LinearPartition model with CONTRAfold parameters (LinearPartition-C) was used to calculate base pairing probabili- ties. In addition, we compared the prediction accuracy of IPknot with that of ThreshKnot [35], which also calculates base pairing probabilities using LinearPartition-C. We used {2\u2212t | t = 1, 2, 3, 4}\u222a {0.3} as the threshold parameter \u03b8 for ThreshKnot because the default threshold parameter of ThreshKnot is \u03b8 = 0.3. IPknot with threshold parameters of \u03c4 (1) = 0.125 and \u03c4 (2) = 0.125 had the highest prediction accuracy of F = 0.659. IPknot with pseudo- expected accuracy has a prediction accuracy of F = 0.658, which is comparable to the highest accuracy obtained. ThreshKnot with a threshold of 0.25 has an accuracy of F = 0.656, which is also comparable to the best accuracy obtained. The pseudo-expected F-value and \u201ctrue\u201d F-value are relatively highly correlated (Spearman correlation coefficient \u03c1 = 0.639), indicating that the selection of predicted secondary structure using pseudo-expected accuracy works well. While the accuracy of the prediction of the entire secondary structure has already been considered, as shown in Figure 2, for the prediction of secondary structures with pseudoknots, it is necessary to evaluate the prediction accuracy focused on the crossing base pairs. In terms of prediction accuracy limited to only crossing base pairs, IPknot with pseudo-expected accu- racy yielded Fcb = 0.258, while the highest accuracy achieved by IPknot with the threshold parameters and ThreshKnot was considerably lower at F cb = 0.161 and 0.057, respectively (See Table S1 in Supplementary Information). We can observe the similar tendency to the above in Figures S1 and S2, and Tables S2 and S3 in Supplementary Information for medium (151\u2013 500 nt) and long (&gt; 500 nt) sequences. These results suggest that prediction of crossing base pairs is improved by selecting the predicted secondary structure while considering both the pseudo-expected accuracy of the entire secondary structure and the pseudo-expected accuracy of the crossing base pairs. Comparison with previous methods for single RNA sequences Using our dataset, we compared our algorithm with several pre- vious methods that can predict pseudoknots, including Thresh- Knot utilizing LinearPartition (committed on 17 March 2021) [22], Knotty (committed on Mar 28, 2018) [22] and SPOT-RNA (committed on 1 April 2021) [6], and those that can predict only pseudoknot-free structures, including CONTRAfold (ver- sion 2.02) [4] and RNAfold in the ViennaRNA package (version 2.4.17) [22]. IPknot has several options for the calculation model for base pairing probabilities, namely the LinearPartition model with CONTRAfold parameters (LinearPartition-C), the LinearPar- tition model with ViennaRNA parameters (LinearPartition-V), the CONTRAfold model and the ViennaRNA model. In addition, ThreshKnot has two possible LinearPartition models for calcu- lating base pairing probabilities. The other existing methods were tested using the default settings. We evaluated the prediction accuracy according to the F- value as defined by Equation (14) for pseudoknot-free sequences (PKF in Table 2), pseudoknotted sequences (PK in Table 2) and only crossing base pairs (CB in Table 2) by stratifying sequences by length: short (12\u2013150 nt), medium (151\u2013500 nt) and long (500\u20134381 nt). For short sequences, SPOT-RNA archived high accuracy, espe- cially for pseudoknotted sequences. However, a large difference in accuracy between the bpRNA-1m-derived and Rfam 14.5- derived sequences can be observed for SPOT-RNA compared with the other methods (See Tables S4\u2013S9 in Supplementary Informa- tion). Notably, bpRNA-1m contains many sequences in the same 6 Sato and Kato Table 2. A comparison of prediction accuracies (F-values) by sequence length for each method Length Short (12\u2013150 nt) Medium (151\u2013500 nt) Long (501\u20134381 nt) PKF PK CB PKF PK CB PKF PK CB IPknot (LinearPartition-C) 0.681 0.552 0.258 0.492 0.482 0.128 0.433 0.428 0.061 (LinearPartition-V) 0.669 0.499 0.143 0.478 0.461 0.091 0.380 0.370 0.038 (CONTRAfold) 0.678 0.550 0.259 0.495 0.505 0.154 0.426 0.413 0.066 (ViennaRNA) 0.669 0.500 0.144 0.480 0.461 0.091 0.212 0.317 0.041 ThreshKnot (LinearPartition-C) 0.681 0.501 0.027 0.493 0.475 0.019 0.439 0.431 0.008 (LinearPartition-V) 0.669 0.484 0.033 0.481 0.456 0.026 0.383 0.372 0.014 Knotty 0.641 0.550 0.315 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 SPOT-RNA 0.658 0.621 0.322 0.462 0.479 0.127 \u2014 \u2014 \u2014 CONTRAfold 0.682 0.519 0.000 0.500 0.497 0.000 0.425 0.415 0.000 RNAfold 0.668 0.472 0.000 0.474 0.442 0.000 0.361 0.347 0.000 PKF, F-value for pseudoknot-free sequences; PK, F-value for pseudoknotted sequences; CB, F-value of crossing base pairs. Figure 3. Computational time of each method as a function of sequence length. For SPOT-RNA with GPGPU, we used a Linux workstation with Intel Xeon Gold 6136 and NVIDIA Tesla V100. All other computations were performed on Linux workstations with AMD EPYC 7702. For IPknot, we employed IBM CPLEX Optimizer as the IP solver. family as the SPOT-RNA training data, and although we per- formed filtering based on sequence identity, there is still a con- cern of overfitting. Knotty can predict structures including pseu- doknots with an accuracy comparable to that of SPOT-RNA, but as shown in Figure 3, it can perform secondary structure predic- tion for only short sequences, owing to its huge computational complexity. Comparing IPknot using the LinearPartition-C and - V models with its counterparts, the original CONTRAfold model and ViennaRNA model achieved comparable accuracy. However, because the computational complexity of the original models is cubic with respect to sequence length, the computational time of the original models increases rapidly as the sequence length exceeds 1500 bases. On the other hand, the computational complexity of the LinearPartition models is linear with respect to sequence length, so the base pairing probabilities can be quickly calculated even when the sequence length exceeds 4000 bases. In addition to calculating the base pairing probabilities, IP calculations are required, but because the number of variables and constraints to be considered can be greatly reduced using the threshold cut technique, the overall execution time is not significantly affected if the sequence length is several thousand bases. Because ThreshKnot, like IPknot, uses the LinearPartition model, it is able to perform fast secondary structure prediction even for long sequences. However, for the prediction accuracy of crossing base pairs, ThreshKnot is even less accurate. Pseudoknots are found not only in cellular RNAs but also in viral RNAs, performing a variety of functions [8]. Tables S10\u2013S11 in Supplementary Information show the results of the secondary structure prediction by separating the datasets into cellular RNAs and viral RNAs, indicating that there is no significant difference in the prediction accuracy between cellular RNAs and viral RNAs. Prediction of common secondary structures with pseudoknots Few methods exist that can perform prediction of common secondary structures including pseudoknots for sequence alignments longer than 1000 bases. Table 3 and Tables S12\u2013S20 in Supplementary Information compare the accuracy of IPknot that employs the LinearPartition model, and RNAalifold in the Vien- naRNA package. We performed common secondary structure prediction for the Rfam reference alignment and the alignment calculated by MAFFT, as well as secondary structure prediction of single sequences only for the seed sequence included in the alignment, and evaluated the prediction accuracy for the seed sequence. In most cases, the prediction accuracy improved as the quality of the alignment increased (Single &lt; MAFFT &lt; Reference). IPknot predicts crossing base pairs based on IPknot for long sequences 7 Table 3. A comparison of prediction accuracies (F-values) of common secondary structure prediction by sequence alignments for each method Reference MAFFT Single PKF PK CB PKF PK CB PKF PK CB IPknot (LinearPartition-C) 0.765 0.616 0.220 0.732 0.585 0.218 0.718 0.548 0.227 (LinearPartition-V) 0.761 0.565 0.177 0.729 0.529 0.165 0.714 0.494 0.124 RNAalifold 0.804 0.611 0.000 0.745 0.540 0.000 0.716 0.474 0.000 PKF, F-value for pseudoknot-free sequences; PK, F-value for pseudoknotted sequences; CB, F-value of crossing base pairs. pseudo-expected accuracy, whereas RNAalifold is unable to predict pseudoknots. Discussion Both IPknot and ThreshKnot use the LinearPartition model to calculate base pairing probabilities, and then perform secondary structure prediction using different strategies. ThreshKnot pre- dicts the base pairs x i and x j that are higher than a predeter- mined threshold \u03b8 and have the largest p ij in terms of both i and j. IPknot predicts the pseudoknot structure with multiple thresholds \u03c4 (1) , . . . , \u03c4 (m) in a hierarchical manner based on IP (5)\u2013 (11), and then carefully selects from among these thresholds based on pseudo-expected accuracy. Because both the pseudo- expected accuracy of the entire secondary structure as well as the pseudo-expected accuracy of the crossing base pairs are taken into account, the prediction accuracy of the pseudoknot structure is inferred to be enhanced in IPknot. Because the LinearPartition model uses the same parameters as the CONTRAfold and ViennaRNA packages, there is no sig- nificant difference in accuracy between using LinearPartition-C and -V and their counterparts, the CONTRAfold and ViennaRNA models. It has been shown that LinearPartition has no significant effect on accuracy even though it ignores structures whose probability is extremely low owing to its use of beam search, which makes the calculation linear with respect to the sequence length [22]. The LinearPartition model enables IPknot to perform secondary structure prediction including pseudoknots of very long sequences, such as mRNA, lncRNA, and viral RNA. SPOT-RNA [6], which uses deep learning, showed notable prediction accuracy in our experiments, especially in short sequences containing pseudoknots, with F-value of 0.621, which is superior to other methods. However, SPOT-RNA requires considerable computing resources such as GPGPU and long computational time. Furthermore, SPOT-RNA showed a large difference in prediction accuracy between sequences that are close to the training data and those that are not compared with the other methods. Therefore, the situations in which SPOT-RNA can be used are considered to be limited. In contrast, IPknot uses CONTRAfold parameters, which is also based on machine learning, but we did not observe as much overfitting with IPknot as with SPOT-RNA. Approaches that provide an exact solution for limited- complexity pseudoknot structures, such as PKNOTS [14], pknot- sRG [15], and Knotty [16], can predict pseudoknot structures with high accuracy but demand a huge amount of computation O(n4 )\u2013O(n6 ) for sequence length n, limiting secondary structure prediction to sequences only up to about 150 bases. On the other hand, IPknot predicts the pseudoknot structure using a fast computational heuristic-based method with the linear time computation, which does not allow us to find an exact solution. Instead, IPknot improves the prediction accuracy of the pseudoknot structure by choosing the best solution from among several solutions based on the pseudo-expected accuracy. IPknot uses pseudoknot-free algorithms, such as CON- TRAfold and ViennaRNA, to calculate base pairing probabilities, and its prediction accuracy of the resulting secondary structure strongly depends on the algorithm used to calculate base pairing probabilities. Therefore, we can expect to improve the prediction accuracy of IPknot by calculating the base pairing probabilities based on state-of-the-art pseudoknot-free secondary structure prediction methods such as MXfold2 [7]. It is well known that common secondary structure prediction from sequence alignments improves the accuracy of secondary structure prediction. However, among the algorithms for predict- ing common secondary structure including pseudoknots, only IPknot can deal with sequence alignments longer than several thousand bases. In the RNA virus SARS-CoV-2, programmed -1 ribosomal frameshift (-1 PRF), in which a pseudoknot structure plays an important role, has been identified and is attracting attention as a drug target [10]. Because many closely related strains of SARS-CoV-2 have been sequenced, it is expected that structural motifs including pseudoknots, such as -1 PRF, can be found by predicting the common secondary structure from the alignment. Conclusions We have developed an improvement to IPknot that enables cal- culation in linear time by employing the LinearPartition model and automatically selects the optimal threshold parameters based on the pseudo-expected accuracy. LinearPartition can cal- culate the base pairing probability with linear computational complexity with respect to the sequence length. By employing LinearPartition, IPknot is able to predict the secondary structure considering pseudoknots for long sequences such as mRNA, lncRNA, and viral RNA. By choosing the thresholds for each sequence based on the pseudo-expected accuracy, we can select a nearly optimal secondary structure prediction. The LinearPartition model realized the predictiction of sec- ondary structures considering pseudoknots for long sequences. However, the prediction accuracy is still not sufficiently high, especially for crossing base pairs. We expect that by learn- ing parameters from long sequences [36], we can achieve high accuracy even for long sequences. Key Points \u2022 We reduced the computational time required by IPknot from cubic to linear with respect to the sequence length by employing the LinearPartition model and enabled the secondary structure prediction 8 Sato and Kato including pseudoknots for long RNA sequences such as mRNA, lncRNA, and viral RNA. \u2022 We improved the accuracy of secondary structure pre- diction including pseudoknots by introducing pseudo- expected accuracy not only for the entire base pairs but also for crossing base pairs. \u2022 To the best of our knowledge, IPknot is the only method that can perform RNA secondary structure prediction including pseudoknot not only for very long single sequence, but also for very long sequence alignments. Supplementary Data Supplementary data are available online at Briefings in Bioinformatics. Availability The IPknot source code is freely available at https://githu b.com/satoken/ipknot. IPknot is also available for use from a web server at http://rtips.dna.bio.keio.ac.jp/ipknot++/. The datasets used in our experiments are available at https:// doi.org/10.5281/zenodo.4923158. Author contributions statement K.S. conceived the study, implemented the algorithm, col- lected the datasets, conducted experiments, and drafted the manuscript. K.S. and Y.K. discussed the algorithm and designed the experiments. All authors read, contributed to the discussion of and approved the final manuscript. Funding This work was partially supported by a Grant-in-Aid for Scientific Research (B) (No. 19H04210) and Challenging Exploratory Research (No. 19K22897) from the Japan Society for the Promotion of Science (JSPS) to K.S. and a Grant-in- Aid for Scientific Research (C) (Nos. 18K11526 and 21K12109) from JSPS to Y.K. Acknowledgments The supercomputer system used for this research was made available by the National Institute of Genetics, Research Organization of Information and Systems. References 1. Zuker M. Mfold web server for nucleic acid folding and hybridization prediction. Nucleic Acids Res 2003;31(13):3406\u2013 15. 2. Lorenz R, Bernhart SH, H\u00f6ner Zu Siederdissen C, et al. Vien- naRNA package 2.0. Algorithms Mol Biol 2011;6:26. 3. Reuter JS, Mathews DH. RNAstructure: software for RNA sec- ondary structure prediction and analysis. BMC Bioinformatics 2010;11:129. 4. Do CB, Woods DA, Batzoglou S. CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinfor- matics 2006;22(14):e90\u20138. 5. Zakov S, Goldberg Y, Elhadad M, et al. Rich parameteri- zation improves RNA structure prediction. J Comput Biol 2011;18(11):1525\u201342. 6. Singh J, Hanson J, Paliwal K, et al. RNA secondary struc- ture prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nat Commun 2019;10(1):5407. 7. Sato K, Akiyama M, Sakakibara Y. RNA secondary struc- ture prediction using deep learning with thermodynamic integration. Nat Commun 2021;12(1):941. 8. Brierley I, Pennell S, Gilbert RJC. Viral RNA pseudoknots: versatile motifs in gene expression and replication. Nat Rev Microbiol 2007;5(8):598\u2013610. 9. Staple DW, Butcher SE. Pseudoknots: RNA structures with diverse functions. PLoS Biol 2005;3(6):e213. 10. Kelly JA, Olson AN, Neupane K, et al. Structural and functional conservation of the programmed -1 ribosomal frameshift signal of SARS coronavirus 2 (SARS-CoV-2). J Biol Chem 2020;295(31):10741\u20138. 11. Trifonov EN, Gabdank I, Barash D, et al. Primordia vita. deconvolution from modern sequences. Orig Life Evol Biosph December 2006;36(5\u20136):559\u201365. 12. Akutsu T. Dynamic programming algorithms for RNA sec- ondary structure prediction with pseudoknots. Discrete Appl Math 2000;104(1):45\u201362. 13. Lyngs\u00f8 RB, Pedersen CN. RNA pseudoknot prediction in energy-based models. J Comput Biol 2000;7(3\u20134):409\u201327. 14. Rivas E, Eddy SR. A dynamic programming algorithm for RNA structure prediction including pseudoknots. J Mol Biol 1999;285(5):2053\u201368. 15. Reeder J, Giegerich R. Design, implementation and evalua- tion of a practical pseudoknot folding algorithm based on thermodynamics. BMC Bioinformatics 2004;5:104. 16. Jabbari H, Wark I, Montemagno C, et al. Knotty: efficient and accurate prediction of complex RNA pseudoknot structures. Bioinformatics 2018;34(22):3849\u201356. 17. Ruan J, Stormo GD, Zhang W. An iterated loop matching approach to the prediction of RNA secondary structures with pseudoknots. Bioinformatics 2004;20(1):58\u201366. 18. Ren J, Rastegari B, Condon A, et al. HotKnots: heuristic pre- diction of RNA secondary structures including pseudoknots. RNA 2005;11(10):1494\u2013504. 19. Chen X, He S-M, Bu D, et al. FlexStem: improving predictions of RNA secondary structures with pseudoknots by reducing the search space. Bioinformatics 2008;24(18):1994\u20132001. 20. Bellaousov S, Mathews D. H. ProbKnot: fast prediction of RNA secondary structure including pseudoknots. RNA 2010;16(10):1870\u201380. 21. Sato K, Kato Y, Hamada M, et al. IPknot: fast and accurate pre- diction of RNA secondary structures with pseudoknots using integer programming. Bioinformatics 2011;27(13):i85\u201393. 22. Zhang H, Zhang L, Mathews DH, et al. LinearPartition: linear-time approximation of RNA folding partition function and base-pairing probabilities. Bioinformatics 2020;36(Supplement_1):i258\u201367. 23. Hamada M, Sato K, Asai K. Prediction of RNA secondary structure by maximizing pseudo-expected accuracy. BMC Bioinformatics 2010;11:586. 24. Hamada M, Kiryu H, Sato K, et al. Prediction of RNA secondary structure using generalized centroid estimators. Bioinformat- ics 2009;25(4):465\u201373. IPknot for long sequences 9 25. McCaskill JS. The equilibrium partition function and base pair binding probabilities for RNA secondary structure. Biopolymers 1990;29(6\u20137):1105\u201319. 26. Kiryu H, Kin T, Asai K. Robust prediction of consensus sec- ondary structures using averaged base pairing probability matrices. Bioinformatics 2007;23(4):434\u201341. 27. Hamada M, Sato K, Asai K. Improving the accuracy of predict- ing secondary structure for aligned RNA sequences. Nucleic Acids Res 2011;39(2):393\u2013402. 28. Danaee P, Rouches M, Wiley M, et al. bpRNA: large-scale auto- mated annotation and analysis of RNA secondary structure. Nucleic Acids Res 2018;46(11):5381\u201394. 29. Nawrocki EP, Burge SW, Bateman A, et al. Rfam 12.0: updates to the RNA families database. Nucleic Acids Res 2015;43(Database issue):D130\u20137. 30. Cannone JJ, Subramanian S, Schnare MN, et al. The compara- tive RNA web (CRW) site: an online database of comparative sequence and structure information for ribosomal, intron, and other RNAs. BMC Bioinformatics 2002;3:2. 31. Kalvari I, Nawrocki EP, Ontiveros-Palacios N, et al. Rfam 14: expanded coverage of metagenomic, viral and microRNA families. Nucleic Acids Res 2021;49(D1): D192\u2013200. 32. Andronescu M, Bereg V, Hoos HH, et al. RNA STRAND: the RNA secondary structure and statistical analysis database. BMC Bioinformatics 2008;9:340. 33. Fu L, Niu B, Zhu Z, et al. CD-HIT: accelerated for clus- tering the next-generation sequencing data. Bioinformatics 2012;28(23):3150\u20132. 34. Katoh K, Standley DM. MAFFT multiple sequence align- ment software version 7: improvements in performance and usability. Mol Biol Evol 2013;30(4):772\u201380. 35. Zhang L, Zhang H, Mathews DH, et al. ThreshKnot: Thresh- olded ProbKnot for improved RNA secondary structure pre- diction. arXiv:1912.12796v1 [q-bio.BM] 2019. 36. Rezaur Rahman F, Zhang H, Huang L. Learning to fold RNAs in linear time. bioRxiv. 2019. https://doi.org/10.1101/852871. === Published as a conference paper at ICLR 2020 RNA SECONDARY STRUCTURE PREDICTION BY LEARNING UNROLLED ALGORITHMS Xinshi Chen1\u2217 , Yu Li2 \u2217, Ramzan Umarov2, Xin Gao2,\u2020, Le Song1,3,\u2020 1Georgia Tech 2KAUST 3Ant Financial xinshi.chen@gatech.edu {yu.li;ramzan.umarov;xin.gao}@kaust.edu.sa lsong@cc.gatech.edu ABSTRACT In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly pre- dict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time. 1 INTRODUCTIONG G G A A A C G U U C C G G G 1 G 1 A 1 A A C G U U 1 C 1 C 1 G matrix representation Figure 1: Graph and matrix represen- tations of RNA secondary structure. Ribonucleic acid (RNA) is a molecule playing essential roles in numerous cellular processes and regulating expression of genes (Crick, 1970). It consists of an ordered sequence of nu- cleotides, with each nucleotide containing one of four bases: Adenine (A), Guanine (G), Cytosine (C) and Uracile (U). This sequence of bases can be represented as x := (x1, . . . , xL) where xi \u2208 {A, G, C, U }, which is known as the primary structure of RNA. The bases can bond with one another to form a set of base-pairs, which defines the secondary structure. A secondary structure can be represented by a binary matrix A\u2217 where A\u2217 ij = 1 if the i, j-th bases are paired (Fig 1). Discovering the secondary structure of RNA is important for understanding functions of RNA since the structure essentially affects the interaction and reaction between RNA and other cellular components. Although secondary structure can be determined by experimental assays (e.g. X-ray diffraction), it is slow, expensive and technically challenging. Therefore, compu- tational prediction of RNA secondary structure becomes an important task in RNA research and is useful in many applications such as drug design (Iorns et al., 2007).(ii) Pseudo-knot(i) Nested Structure Figure 2: Nested and non-nested structures. Research on computational prediction of RNA secondary structure from knowledge of primary structure has been carried out for decades. Most existing methods assume the secondary structure is a result of energy minimiza- tion, i.e., A\u2217 = arg minA Ex(A). The energy function is either estimated by physics-based thermodynamic ex- periments (Lorenz et al., 2011; Bellaousov et al., 2013; Markham &amp; Zuker, 2008) or learned from data (Do et al., 2006). These approaches are faced with a common problem that the search space of all valid sec- ondary structures is exponentially-large with respect to the length L of the sequence. To make the minimization tractable, it is often assumed the base-pairing has a nested structure (Fig 2 left), and \u2217Equal contribution. \u2020Co-corresponding. 1 arXiv:2002.05810v1 [cs.LG] 13 Feb 2020 Published as a conference paper at ICLR 2020 the energy function factorizes pairwisely. With this assumption, dynamic programming (DP) based algorithms can iteratively find the optimal structure for subsequences and thus consider an enormous number of structures in time O(L3). Although DP-based algorithms have dominated RNA structure prediction, it is notable that they restrict the search space to nested structures, which excludes some valid yet biologically important RNA secondary structures that contain \u2018pseudoknots\u2019, i.e., elements with at least two non-nested base-pairs (Fig 2 right). Pseudoknots make up roughly 1.4% of base-pairs (Mathews &amp; Turner, 2006), and are overrepresented in functionally important regions (Hajdin et al., 2013; Staple &amp; Butcher, 2005). Furthermore, pseudoknots are present in around 40% of the RNAs. They also assist folding into 3D structures (Fechter et al., 2001) and thus should not be ignored. To predict RNA structures with pseudoknots, energy-based methods need to run more computationally intensive algorithms to decode the structures. In summary, in the presence of more complex structured output (i.e., pseudoknots), it is challenging for energy-based approaches to simultaneously take into account the complex constraints while be- ing efficient. In this paper, we adopt a different viewpoint by assuming that the secondary structure is the output of a feed-forward function, i.e., A\u2217 = F\u03b8 (x), and propose to learn \u03b8 from data in an end-to-end fashion. It avoids the second minimization step needed in energy function based ap- proach, and does not require the output structure to be nested. Furthermore, the feed-forward model can be fitted by directly optimizing the loss that one is interested in. Despite the above advantages of using a feed-forward model, the architecture design is challenging. To be more concrete, in the RNA case, F\u03b8 is difficult to design for the following reasons: (i) RNA secondary structure needs to obey certain hard constraints (see details in Section 3), which means certain kinds of pairings cannot occur at all (Steeg, 1993). Ideally, the output of F\u03b8 needs to satisfy these constraints. (ii) The number of RNA data points is limited, so we cannot expect that a naive fully connected network can learn the predictive information and constraints directly from data. Thus, inductive biases need to be encoded into the network architecture. (iii) One may take a two-step approach, where a post-processing step can be carried out to enforce the constraints when F\u03b8 predicts an invalid structure. However, in this design, the deep network trained in the first stage is unaware of the post-processing stage, making less effective use of the potential prior knowledge encoded in the constraints.All Binary Structures Output Space of E2Efold All Valid Structures Nested Structures with constraints (DP applicable) Figure 3: Output space of E2Efold. In this paper, we present an end-to-end deep learning solution which integrates the two stages. The first part of the archi- tecture is a transformer-based deep model called Deep Score Network which represents sequence information useful for structure prediction. The second part is a multilayer network called Post-Processing Network which gradually enforces the constraints and restrict the output space. It is designed based on an unrolled algorithm for solving a constrained optimiza- tion. These two networks are coupled together and learned jointly in an end-to-end fashion. Therefore, we call our model E2Efold. By using an unrolled algorithm as the inductive bias to design Post-Processing Network, the output space of E2Efold is constrained (illustrated in Fig 3), which makes it easier to learn a good model in the case of limited data and also reduces the overfitting issue. Yet, the constraints encoded in E2Efold are flexible enough such that pseudoknots are not excluded. In summary, E2Efold strikes a nice balance between model biases for learning and expressiveness for valid RNA structures. We conduct extensive experiments to compare E2Efold with state-of-the-art (SOTA) methods on several RNA benchmark datasets, showing superior performance of E2Efold including: \u2022 being able to predict valid RNA secondary structures including pseudoknots; \u2022 running as efficient as the fastest algorithm in terms of inference time; \u2022 producing structures that are visually close to the true structure; \u2022 better than previous SOTA in terms of F1 score, precision and recall. Although in this paper we focus on RNA secondary structure prediction, which presents an impor- tant and concrete problem where E2Efold leads to significant improvements, our method is generic 2 Published as a conference paper at ICLR 2020 and can be applied to other problems where constraints need to be enforced or prior knowledge is provided. We imagine that our design idea of learning unrolled algorithm to enforce constraints can also be transferred to problems such as protein folding and natural language understanding problems (e.g., building correspondence structure between different parts in a document). 2 RELATED WORK Classical RNA folding methods identify candidate structures for an RNA sequence energy min- imization through DP and rely on thousands of experimentally-measured thermodynamic parame- ters. A few widely used methods such as RNAstructure (Bellaousov et al., 2013), Vienna RNAfold (Lorenz et al., 2011) and UNAFold (Markham &amp; Zuker, 2008) adpoted this approach. These meth- ods typically scale as O(L3) in time and O(L2) in storage (Mathews, 2006), making them slow for long sequences. A recent advance called LinearFold (Huang et al., 2019) achieved linear run time O(L) by applying beam search, but it can not handle pseudoknots in RNA structures. The prediction of lowest free energy structures with pseudoknots is NP-complete (Lyngs\u00f8 &amp; Pedersen, 2000), so pseudoknots are not considered in most algorithms. Heuristic algorithms such as HotKnots (An- dronescu et al., 2010) and Probknots (Bellaousov &amp; Mathews, 2010) have been made to predict structures with pseudoknots, but the predictive accuracy and efficiency still need to be improved. Learning-based RNA folding methods such as ContraFold (Do et al., 2006) and ContextFold (Za- kov et al., 2011) have been proposed for energy parameters estimation due to the increasing avail- ability of known RNA structures, resulting in higher prediction accuracies, but these methods still rely on the above DP-based algorithms for energy minimization. A recent deep learning model, CDPfold (Zhang et al., 2019), applied convolutional neural networks to predict base-pairings, but it adopts the dot-bracket representation for RNA secondary structure, which can not represent pseu- doknotted structures. Moreover, it requires a DP-based post-processing step whose computational complexity is prohibitive for sequences longer than a few hundreds. Learning with differentiable algorithms is a useful idea that inspires a series of works (Hershey et al., 2014; Belanger et al., 2017; Ingraham et al., 2018; Chen et al., 2018; Shrivastava et al., 2019), which shared similar idea of using differentiable unrolled algorithms as a building block in neural architectures. Some models are also applied to structured prediction problems (Hershey et al., 2014; Pillutla et al., 2018; Ingraham et al., 2018), but they did not consider the challenging RNA sec- ondary structure problem or discuss how to properly incorporating constraints into the architecture. OptNet (Amos &amp; Kolter, 2017) integrates constraints by differentiating KKT conditions, but it has cubic complexity in the number of variables and constraints, which is prohibitive for the RNA case. Dependency parsing in NLP is a different but related problem to RNA folding. It predicts the de- pendency between the words in a sentence. Similar to nested/non-nested structures, the correspond- ing terms in NLP are projective/non-projective parsing, where most works focus on the former and DP-based inference algorithms are commonly used (McDonald et al., 2005). Deep learning mod- els (Dozat &amp; Manning, 2016; Kiperwasser &amp; Goldberg, 2016) are proposed to proposed to score the dependency between words, which has a similar flavor to the Deep Score Network in our work. 3 RNA SECONDARY STRUCTURE PREDICTION PROBLEM In the RNA secondary structure prediction problem, the input is the ordered sequence of bases x = (x1, . . . , xL) and the output is the RNA secondary structure represented by a matrix A\u2217 \u2208 {0, 1}L\u00d7L. Hard constraints on the forming of an RNA secondary structure dictate that certain kinds of pairings cannot occur at all (Steeg, 1993). Formally, these constraints are: (i) Only three types of nucleotides combinations, B := {AU, U A}\u222a {GC, CG} \u222a {GU, U G}, can form base-pairs. \u2200i, j, if xixj /\u2208 B, then Aij = 0. (ii) No sharp loops are allowed. \u2200|i \u2212 j| &lt; 4, Aij = 0. (iii) There is no overlap of pairs, i.e., it is a matching. \u2200i, \u2211L j=1 Aij \u2264 1. (i) and (ii) prevent pairing of certain base-pairs based on their types and relative locations. Incorpo- rating these two constraints can help the model exclude lots of illegal pairs. (iii) is a global constraint among the entries of A\u2217. 3 Published as a conference paper at ICLR 2020 The space of all valid secondary structures contains all symmetric matrices A \u2208 {0, 1}L\u00d7L that satisfy the above three constraints. This space is much smaller than the space of all binary matrices {0, 1}L\u00d7L. Therefore, if we could incorporate these constraints in our deep model, the reduced output space could help us train a better predictive model with less training data. We do this by using an unrolled algorithm as the inductive bias to design deep architecture. 4 E2EFOLD: DEEP LEARNING MODEL BASED ON UNROLLED ALGORITHM In the literature on feed-forward networks for structured prediction, most models are designed using traditional deep learning architectures. However, for RNA secondary structure prediction, directly using these architectures does not work well due to the limited amount of RNA data points and the hard constraints on forming an RNA secondary structure. These challenges motivate the design of our E2Efold deep model, which combines a Deep Score Network with a Post-Processing Network based on an unrolled algorithm for solving a constrained optimization problem. 4.1 DEEP SCORE NETWORK The first part of E2Efold is a Deep Score Network U\u03b8 (x) whose output is an L \u00d7 L symmetric matrix. Each entry of this matrix, i.e., U\u03b8 (x)ij , indicates the score of nucleotides xi and xj being paired. The x input to the network here is the L \u00d7 4 dimensional one-hot embedding. The specific architecture of U\u03b8 is shown in Fig 4. It mainly consists of \u2022 a position embedding matrix P which distinguishes {xi}L i=1 by their exact and relative positions: Pi = MLP(\u03c81(i), . . . , \u03c8<code>(i), \u03c8</code>+1(i/L), . . . , \u03c8n(i/L)), where {\u03c8j } is a set of n feature maps such as sin(\u00b7), poly(\u00b7), sigmoid(\u00b7), etc, and MLP(\u00b7) denotes multi-layer perceptions. Such posi- tion embedding idea has been used in natural language modeling such as BERT (Devlin et al., 2018), but we adapted for RNA sequence representation; \u2022 a stack of Transformer Encoders (Vaswani et al., 2017) which encode the sequence information and the global dependency between nucleotides; \u2022 a 2D Convolution layers (Wang et al., 2017) for outputting the pairwise scores.input \ud835\udc3f\u00d74 position \ud835\udc3f\u00d72 \ud835\udc3f\u00d7\ud835\udc51 Multiply by W \u2208 \u211d\u00d7+ Position Embedding 0100 1000 1000 1000 0010 0100 0001 0001 0010 0010 \u2026 G A A A C G U U C C \u2026 0100 1000 1000 1000 0010 0100 0001 0001 0010 0010 \u2026 G A A A C G U U C C \u2026 1 2 3 4 5 \u2026 1/L 2/L 3/L 4/L 5/L \u2026 feature map \ud835\udf13- , \u2026 , \ud835\udf130 \ud835\udc3f\u00d7\ud835\udc5b MLP \ud835\udc3f\u00d7\ud835\udc51 Transformer Encoder Transformer Encoder Transformer Encoder \ud835\udc3f\u00d72\ud835\udc51 \ud835\udc3f\u00d72\ud835\udc51 Sequence Encoder concat \ud835\udc3f\u00d73\ud835\udc51 pairwise concat \ud835\udc3f\u00d7\ud835\udc3f\u00d76\ud835\udc51 2D Convolution 2D Convolution concat \ud835\udc3f\u00d7\ud835\udc3f\u00d71 Output Layers scores U symmetrization Figure 4: Architecture of Deep Score Network. With the representation power of neural networks, the hope is that we can learn an informative U\u03b8 such that higher scoring entries in U\u03b8 (x) correspond well to ac- tual paired bases in RNA structure. Once the score matrix U\u03b8 (x) is computed, a naive approach to use it is to choose an offset term s \u2208 R (e.g., s = 0) and let Aij = 1 if U\u03b8 (x)ij &gt; s. However, such entry-wise independent predictions of Aij may re- sult in a matrix A that violates the constraints for a valid RNA secondary structure. Therefore, a post- processing step is needed to make sure the predicted A is valid. This step could be carried out separately after U\u03b8 is learned. But such decoupling of base-pair scoring and post-processing for constraints may lead to sub-optimal results, where the errors in these two stages can not be considered together and tuned to- gether. Instead, we will introduce a Post-Processing Network which can be trained end-to-end together with U\u03b8 to enforce the constraints. 4.2 POST-PROCESSING NETWORK The second part of E2Efold is a Post-Processing Net- work PP\u03c6 which is an unrolled and parameterized al- gorithm for solving a constrained optimization prob- lem. We first present how we formulate the post-processing step as a constrained optimization prob- lem and the algorithm for solving it. After that, we show how we use the algorithm as a template to design deep architecture PP\u03c6. 4 Published as a conference paper at ICLR 2020 4.2.1 POST-PROCESSING WITH CONSTRAINED OPTIMIZATION Formulation of constrained optimization. Given the scores predicted by U\u03b8 (x), we define the total score 1 2 \u2211 i,j (U\u03b8 (x)ij \u2212 s)Aij as the objective to maximize, where s is an offset term. Clearly, without structure constraints, the optimal solution is to take Aij = 1 when U\u03b8 (x)ij &gt; s. Intu- itively, the objective measures the covariation between the entries in the scoring matrix and the A matrix. With constraints, the exact maximization becomes intractable. To make it tractable, we consider a convex relaxation of this discrete optimization to a continuous one by allowing Aij \u2208 [0, 1]. Consequently, the solution space that we consider to optimize over is A(x) :={A \u2208 [0, 1]L\u00d7L | A is symmetric and satisfies constraints (i)-(iii) in Section 3} . To further simplify the search space, we define a nonlinear transformation T on RL\u00d7L as T ( \u02c6A) := 1 2 ( \u02c6A \u25e6 \u02c6A + ( \u02c6A \u25e6 \u02c6A)&gt;) \u25e6 M (x), where \u25e6 denotes element-wise multiplication. Matrix M is defined as M (x)ij := 1 if xixj \u2208 B and also |i \u2212 j| \u2265 4, and M (x)ij := 0 otherwise. From this definition we can see that M (x) encodes both constraint (i) and (ii). With transformation T , the resulting matrix is non-negative, symmetric, and satisfies constraint (i) and (ii). Hence, by defining A := T ( \u02c6A), the solution space is simplified as A(x) = {A = T ( \u02c6A) | \u02c6A \u2208 RL\u00d7L, A1 \u2264 1}. Finally, we introduce a <code>1 penalty term \u2016 \u02c6A\u20161 := \u2211 i,j | \u02c6Aij | to make A sparse and formulate the post-processing step as: (\u3008\u00b7, \u00b7\u3009 denotes matrix inner product, i.e., sum of entry-wise multiplication) max \u02c6A\u2208RL\u00d7L 1 2 \u3008 U\u03b8 (x) \u2212 s, A := T ( \u02c6A) \u3009 \u2212 \u03c1\u2016 \u02c6A\u20161 s.t. A1 \u2264 1 (1) The advantages of this formulation are that the variables \u02c6Aij are free variables in R and there are only L inequality constraints A1 \u2264 1. This system of linear inequalities can be replaced by a set of nonlinear equalities relu(A1 \u2212 1) = 0 so that the constrained problem can be easily transformed into an unconstrained problem by introducing a Lagrange multiplier \u03bb \u2208 RL +: min \u03bb\u22650 max \u02c6A\u2208RL\u00d7L 1 2 \u3008U\u03b8 (x) \u2212 s, A\u3009 \u2212 \u3008\u03bb, relu(A1 \u2212 1)\u3009 \ufe38 \ufe37\ufe37 \ufe38 f \u2212\u03c1\u2016 \u02c6A\u20161. (2) Algorithm for solving it. We use a primal-dual method for solving Eq. 2 (derived in Appendix B). In each iteration, \u02c6A and \u03bb are updated alternatively by: (primal) gradient step: \u02d9At+1 \u2190 \u02c6At + \u03b1 \u00b7 \u03b3t \u03b1 \u00b7 \u02c6At \u25e6 M (x) \u25e6 ( \u2202f /\u2202At + (\u2202f /\u2202At)&gt;) , (3) where {\u2202f /\u2202At = 1 2 (U\u03b8 (x) \u2212 s) \u2212 (\u03bb \u25e6 sign(At1 \u2212 1)) 1&gt;, sign(c) := 1 when c &gt; 0 and 0 otherwise, (4) (primal) soft threshold: \u02c6At+1 \u2190 relu(| \u02d9At+1| \u2212 \u03c1 \u00b7 \u03b1 \u00b7 \u03b3t \u03b1), At+1 \u2190 T ( \u02c6At+1), (5) (dual) gradient step: \u03bbt+1 \u2190 \u03bbt+1 + \u03b2 \u00b7 \u03b3t \u03b2 \u00b7 relu(At+11 \u2212 1), (6) where \u03b1, \u03b2 are step sizes and \u03b3\u03b1, \u03b3\u03b2 are decaying coefficients. When it converges at T , an approx- imate solution Round(AT = T ( \u02c6AT )) is obtained. With this algorithm operated on the learned U\u03b8 (x), even if this step is disconnected to the training phase of U\u03b8 (x), the final prediction works much better than many other existing methods (as reported in Section 6). Next, we introduce how to couple this post-processing step with the training of U\u03b8 (x) to further improve the performance. 4.2.2 POST-PROCESSING NETWORK VIA AN UNROLLED ALGORITHM We design a Post-Processing Network, denoted by PP\u03c6, based on the above algorithm. After it is defined, we can connect it with the deep score network U\u03b8 and train them jointly in an end-to-end fashion, so that the training phase of U\u03b8 (x) is aware of the post-processing step. 5 Published as a conference paper at ICLR 2020 Algorithm 1: Post-Processing Network PP\u03c6(U, M ) Parameters \u03c6 := {w, s, \u03b1, \u03b2, \u03b3\u03b1, \u03b3\u03b2 , \u03c1} U \u2190 softsign(U \u2212 s) \u25e6 U \u02c6A0 \u2190 softsign(U \u2212 s) \u25e6 sigmoid(U ) A0 \u2190 T ( \u02c6A0); \u03bb0 \u2190 w \u00b7 relu(A01 \u2212 1) For t = 0, . . . , T \u2212 1 do \u03bbt+1, At+1, \u02c6At+1 = PPcell\u03c6(U, M, \u03bbt, At, \u02c6At, t) return {At}T t=1 Algorithm 2: Neural Cell PPcell\u03c6 Function PPcell\u03c6(U, M, \u03bb, A, \u02c6A, t): G \u2190 1 2 U \u2212 (\u03bb \u25e6 softsign(A1 \u2212 1)) 1&gt; \u02d9A \u2190 \u02c6A + \u03b1 \u00b7 \u03b3\u03b1t \u00b7 \u02c6A \u25e6 M \u25e6 (G + G&gt;) \u02c6A \u2190 relu(| \u02d9A| \u2212 \u03c1 \u00b7 \u03b1 \u00b7 \u03b3\u03b1t) \u02c6A \u2190 1 \u2212 relu(1 \u2212 \u02c6A) [i.e.,min( \u02c6A, 1)] A \u2190 T ( \u02c6A); \u03bb \u2190 \u03bb+\u03b2\u00b7\u03b3\u03b2 t \u00b7relu(A1\u22121) return \u03bb, A, \u02c6A The specific computation graph of PP\u03c6 is given in Algorithm 1, whose main component is a recurrent cell which we call PPcell\u03c6. The computation graph is almost the same as the iterative update from Eq. 3 to Eq. 6, except for several modifications: \u2022 (learnable hyperparameters) The hyperparameters including step sizes \u03b1, \u03b2, decaying rate \u03b3\u03b1, \u03b3\u03b2 , sparsity coefficient \u03c1 and the offset term s are treated as learnable parameters in \u03c6, so that there is no need to tune the hyperparameters by hand but automatically learn them from data instead. \u2022 (fixed # iterations) Instead of running the iterative updates until convergence, PPcell\u03c6 is applied recursively for T iterations where T is a manually fixed number. This is why in Fig 3 the output space of E2Efold is slightly larger than the true solution space. \u2022 (smoothed sign function) Resulted from the gradient of relu(\u00b7), the update step in Eq. 4 contains a sign(\u00b7) function. However, to push gradient through PP\u03c6, we require a differentiable update step. Therefore, we use a smoothed sign function defined as softsign(c) := 1/(1 + exp(\u2212kc)), where k is a temperature. \u2022 (clip \u02c6A) An additional step, \u02c6A \u2190 min( \u02c6A, 1), is included to make the output At at each itera- tion stay in the range [0, 1]L\u00d7L. This is useful for computing the loss over intermediate results {At}T t=1, for which we will explain more in Section 5. With these modifications, the Post-Processing Network PP\u03c6 is a tuning-free and differentiable un- rolled algorithm with meaningful intermediate outputs. Combining it with the deep score network, the final deep model is E2Efold : {At}T t=1 = Post-Process Network \ufe37 \ufe38\ufe38 \ufe37 PP\u03c6( U\u03b8 (x) \ufe38 \ufe37\ufe37 \ufe38 Deep Score Network , M (x)) . (7) 5 END-TO-END TRAINING ALGORITHM Given a dataset D containing examples of input-output pairs (x, A\u2217), the training procedure of E2Efold is similar to standard gradient-based supervised learning. However, for RNA secondary structure prediction problems, commonly used metrics for evaluating predictive performances are F1 score, precision and recall, which are non-differentiable. Differentiable F1 Loss. To directly optimize these metrics, we mimic true positive (TP), false posi- tive (FP), true negative (TN) and false negative (FN) by defining continuous functions on [0, 1]L\u00d7L: TP = \u3008A, A\u2217\u3009, FP = \u3008A, 1 \u2212 A\u2217\u3009, FN = \u30081 \u2212 A, A\u2217\u3009, TN = \u30081 \u2212 A, 1 \u2212 A\u2217\u3009. Since F1 = 2TP/(2TP + FP + FN), we define a loss function to mimic the negative of F1 score as: L\u2212F1(A, A\u2217) := \u22122\u3008A, A\u2217\u3009/ (2\u3008A, A\u2217\u3009 + \u3008A, 1 \u2212 A\u2217\u3009 + \u30081 \u2212 A, A\u2217\u3009) . (8) Assuming that \u2211 ij A\u2217 ij 6 = 0, this loss is well-defined and differentiable on [0, 1]L\u00d7L. Precision and recall losses can be defined in a similar way, but we optimize F1 score in this paper. It is notable that this F1 loss takes advantages over other differentiable losses including</code>2 and cross-entropy losses, because there are much more negative samples (i.e. Aij = 0) than positive samples (i.e. Aij = 1). A hand-tuned weight is needed to balance them while using `2 or cross- entropy losses, but F1 loss handles this issue automatically, which can be useful for a number of problems (Wang et al., 2016; Li et al., 2017). 6 Published as a conference paper at ICLR 2020 Overall Loss Function. As noted earlier, E2Efold outputs a matrix At \u2208 [0, 1]L\u00d7L in each itera- tion. This allows us to add auxiliary losses to regularize the intermediate results, guiding it to learn parameters which can generate a smooth solution trajectory. More specifically, we use an objective that depends on the entire trajectory of optimization: min \u03b8,\u03c6 1 |D| \u2211 (x,A\u2217)\u2208D 1 T T\u2211 t=1 \u03b3T \u2212tL\u2212F1(At, A\u2217), (9) where {At}T t=1 = PP\u03c6(U\u03b8 (x), M (x)) and \u03b3 \u2264 1 is a discounting factor. Empirically, we find it very useful to pre-train U\u03b8 using logistic regression loss. Also, it is helpful to add this additional loss to Eq. 9 as a regularization. 6 EXPERIMENTS We compare E2Efold with the SOTA and also the most commonly used methods in the RNA sec- ondary structure prediction field on two benchmark datasets. It is revealed from the experimental results that E2Efold achieves 29.7% improvement in terms of F1 score on RNAstralign dataset and it infers the RNA secondary structure as fast as the most efficient algorithm (LinearFold) among ex- isting ones. An ablation study is also conducted to show the necessity of pushing gradient through the post-processing step. The codes for reproducing the experimental results are released.1 Table 1: Dataset Statistics Type ArchiveII RNAStralign length #samples length #samples All 28\u223c2968 3975 30\u223c1851 30451 16SrRNA 73\u223c1995 110 54\u223c1851 11620 5SrRNA 102\u223c135 1283 104\u223c132 9385 tRNA 54\u223c93 557 59\u223c95 6443 grp1 210\u223c736 98 163\u223c615 1502 SRP 28\u223c533 928 30\u223c553 468 tmRNA 102\u223c437 462 102\u223c437 572 RNaseP 120\u223c486 454 189\u223c486 434 telomerase 382\u223c559 37 382\u223c559 37 23SrRNA 242\u223c2968 35 - - grp2 619\u223c780 11 - - Dataset. We use two benchmark datasets: (i) ArchiveII (Sloma &amp; Mathews, 2016), containing 3975 RNA struc- tures from 10 RNA types, is a widely used benchmark dataset for classical RNA folding methods. (ii) RNAS- tralign (Tan et al., 2017), composed of 37149 structures from 8 RNA types, is one of the most comprehensive col- lections of RNA structures in the market. After removing redundant sequences and structures, 30451 structures re- main. See Table 1 for statistics about these two datasets. Experiments On RNAStralign. We divide RNAStralign dataset into training, testing and validation sets by strat- ified sampling (see details in Table 7 and Fig 6), so that each set contains all RNA types. We compare the performance of E2Efold to six methods includ- ing CDPfold, LinearFold, Mfold, RNAstructure (ProbKnot), RNAfold and CONTRAfold. Both E2Efold and CDPfold are learned from the same training/validation sets. For other methods, we directly use the provided packages or web-servers to generate predicted structures. We evaluate the F1 score, Precision and Recall for each sequence in the test set. Averaged values are reported in Table 2. As suggested by Mathews (2019), for a base pair (i, j), the following predictions are also considered as correct: (i + 1, j), (i \u2212 1, j), (i, j + 1), (i, j \u2212 1), so we also reported the metrics when one-position shift is allowed. Table 2: Results on RNAStralign test set. \u201c(S)\u201d indi- cates the results when one-position shift is allowed. Method Prec Rec F1 Prec(S) Rec(S) F1(S) E2Efold 0.866 0.788 0.821 0.880 0.798 0.833 CDPfold 0.633 0.597 0.614 0.720 0.677 0.697 LinearFold 0.620 0.606 0.609 0.635 0.622 0.624 Mfold 0.450 0.398 0.420 0.463 0.409 0.433 RNAstructure 0.537 0.568 0.550 0.559 0.592 0.573 RNAfold 0.516 0.568 0.540 0.533 0.587 0.558 CONTRAfold 0.608 0.663 0.633 0.624 0.681 0.650 Figure 5: Distribution of F1 score. As shown in Table 2, traditional methods can achieve a F1 score ranging from 0.433 to 0.624, which is consistent with the performance reported with their original papers. The two learning-based methods, CONTRAfold and CDPfold, can outperform classical methods with reasonable margin on 1The codes for reproducing the experimental results are released at https://github.com/ml4bio/e2efold. 7 Published as a conference paper at ICLR 2020 some criteria. E2Efold, on the other hand, significantly outperforms all previous methods across all criteria, with at least 20% improvement. Notice that, for almost all the other methods, the recall is usually higher than precision, while for E2Efold, the precision is higher than recall. That can be the result of incorporating constraints during neural network training. Fig 5 shows the distributions of F1 scores for each method. It suggests that E2Efold has consistently good performance. To estimate the performance of E2Efold on long sequences, we also compute the F1 scores weighted by the length of sequences, such that the results are more dominated by longer sequences. Detailed results are given in Appendix D.3. Table 3: Performance comparison on ArchiveII Method Prec Rec F1 Prec(S) Rec(S) F1(S) E2Efold 0.734 0.66 0.686 0.758 0.676 0.704 CDPfold 0.557 0.535 0.545 0.612 0.585 0.597 LinearFold 0.641 0.617 0.621 0.668 0.644 0.647 Mfold 0.428 0.383 0.401 0.450 0.403 0.421 RNAstructure 0.563 0.615 0.585 0.590 0.645 0.613 RNAfold 0.565 0.627 0.592 0.586 0.652 0.615 CONTRAfold 0.607 0.679 0.638 0.629 0.705 0.662 Table 4: Inference time on RNAStralign Method total run time time per seq E2Efold (Pytorch) 19m (GPU) 0.40s CDPfold (Pytorch) 440m*32 threads 300.107s LinearFold (C) 20m 0.43s Mfold (C) 360m 7.65s RNAstructure (C) 3 days 142.02s RNAfold (C) 26m 0.55s CONTRAfold (C) 1 day 30.58s Test On ArchiveII Without Re-training. To mimic the real world scenario where the users want to predict newly discovered RNA\u2019s structures which may have a distribution different from the training dataset, we directly test the model learned from RNAStralign training set on the ArchiveII dataset, without re-training the model. To make the comparison fair, we exclude sequences that are over- lapped with the RNAStralign dataset. We then test the model on sequences in ArchiveII that have overlapping RNA types (5SrRNA, 16SrRNA, etc) with the RNAStralign dataset. Results are shown in Table 3. It is understandable that the performances of classical methods which are not learning- based are consistent with that on RNAStralign. The performance of E2Efold, though is not as good as that on RNAStralign, is still better than all the other methods across different evaluation crite- ria. In addition, since the original ArchiveII dataset contains domain sequences (subsequences), we remove the domains and report the results in Appendix D.4, which are similar to results in Table 3. Inference Time Comparison. We record the running time of all algorithms for predicting RNA secondary structures on the RNAStralign test set, which is summarized in Table 4. LinearFold is the most efficient among baselines because it uses beam pruning heuristic to accelerate DP. CDPfold, which achieves higher F1 score than other baselines, however, is extremely slow due to its DP post-processing step. Since we use a gradient-based algorithm which is simple to design the Post- Processing Network, E2Efold is fast. On GPU, E2Efold has similar inference time as LinearFold. Table 5: Evaluation of pseudoknot prediction Method Set F1 TP FP TN FN E2Efold 0.710 1312 242 1271 0 RNAstructure 0.472 1248 307 983 286 Pseudoknot Prediction. Even though E2Efold does not exclude pseudoknots, it is not sure whether it ac- tually generates pseudoknotted structures. Therefore, we pick all sequences containing pseudoknots and com- pute the averaged F1 score only on this set. Besides, we count the number of pseudoknotted sequences that are predicted as pseudoknotted and report this count as true positive (TP). Similarly we report TN, FP and FN in Table 5 along with the F1 score. Most tools exclude pseudoknots while RNAstructure is the most famous one that can predict pseudoknots, so we choose it for comparison.E2Efold RNAstructure CONTRAfold true structure RNAstructure CONTRAfoldE2Efoldtrue structuretrue structure E2Efoldtrue structure E2Efold Visualization. We visualize predicted structures of three RNA sequences in the main text. More examples are provided in appendix (Fig 8 to 14). In these figures, purple lines indicate edges of pseudoknotted elements. Although CDPfold has higher F1 score than other base- lines, its predictions are visually far from the ground- truth. Instead, RNAstructure and CONTRAfold produce comparatively more reasonable visualizations among all baselines, so we compare with them. These 8 Published as a conference paper at ICLR 2020 two methods can capture a rough sketch of the structure, but not good enough. For most cases, E2Efold produces structures most similar to the ground-truths. Moreover, it works surprisingly well for some RNA sequences that are long and very difficult to predict. Table 6: Ablation study (RNAStralign test set) Method Prec Rec F1 Prec(S) Rec(S) F1(S) E2Efold 0.866 0.788 0.821 0.880 0.798 0.833 U\u03b8 +PP 0.755 0.712 0.721 0.782 0.737 0.752 Ablation Study. To exam whether integrating the two stages by pushing gradient through the post- process is necessary for performance of E2Efold, we conduct an ablation study (Table 6). We test the per- formance when the post-processing step is discon- nected with the training of Deep Score Network U\u03b8 . We apply the post-processing step (i.e., for solving augmented Lagrangian) after U\u03b8 is learned (thus the notation \u201cU\u03b8 + PP\u201d in Table 6). Al- though \u201cU\u03b8 + PP\u201d performs decently well, with constraints incorporated into training, E2Efold still has significant advantages over it. Discussion. To better estimate the performance of E2Efold on different RNA types, we include the per-family F1 scores in Appendix D.5. E2Efold performs significantly better than other methods in 16S rRNA, tRNA, 5S RNA, tmRNA, and telomerase. These results are from a single model. In the future, we can view it as multi-task learning and further improve the performance by learning multiple models for different RNA families and learning an additional classifier to predict which model to use for the input sequence. 7 CONCLUSION We propose a novel DL model, E2Efold, for RNA secondary structure prediction, which incorpo- rates hard constraints in its architecture design. Comprehensive experiments are conducted to show the superior performance of E2Efold, no matter on quantitative criteria, running time, or visualiza- tion. Further studies need to be conducted to deal with the RNA types with less samples. Finally, we believe the idea of unrolling constrained programming and pushing gradient through post-processing can be generic and useful for other constrained structured prediction problems. ACKNOWLEDGEMENT We would like to thank anonymous reviewers for providing constructive feedbacks. This work is supported in part by NSF grants CDS&amp;E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CA- REER IIS-1350983 to L.S. and grants from King Abdullah University of Science and Technology, under award numbers BAS/1/1624-01, FCC/1/1976-18-01, FCC/1/1976-23-01, FCC/1/1976-25-01, FCC/1/1976-26-01, REI/1/0018-01-01, and URF/1/4098-01-01. REFERENCES Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 136\u2013 145. JMLR. org, 2017. Mirela S Andronescu, Cristina Pop, and Anne E Condon. Improved free energy parameters for RNA pseudoknotted secondary structure prediction. RNA, 16(1):26\u201342, 2010. David Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured prediction energy networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 429\u2013439. JMLR. org, 2017. Stanislav Bellaousov and David H Mathews. Probknot: fast prediction of RNA secondary structure including pseudoknots. RNA, 16(10):1870\u20131880, 2010. Stanislav Bellaousov, Jessica S Reuter, Matthew G Seetin, and David H Mathews. RNAstructure: web servers for RNA secondary structure prediction and analysis. Nucleic acids research, 41 (W1):W471\u2013W474, 2013. Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of un- folded ista and its practical weights and thresholds. In Advances in Neural Information Processing Systems, pp. 9061\u20139071, 2018. 9 Published as a conference paper at ICLR 2020 Francis Crick. Central dogma of molecular biology. Nature, 227(5258):561, 1970. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Chuong B Do, Daniel A Woods, and Serafim Batzoglou. Contrafold: RNA secondary structure prediction without physics-based models. Bioinformatics, 22(14):e90\u2013e98, 2006. Timothy Dozat and Christopher D Manning. Deep biaffine attention for neural dependency parsing. arXiv preprint arXiv:1611.01734, 2016. P Fechter, J Rudinger-Thirion, C Florentz, and R Giege. Novel features in the tRNA-like world of plant viral RNAs. Cellular and Molecular Life Sciences CMLS, 58(11):1547\u20131561, 2001. Christine E Hajdin, Stanislav Bellaousov, Wayne Huggins, Christopher W Leonard, David H Math- ews, and Kevin M Weeks. Accurate shape-directed RNA secondary structure modeling, including pseudoknots. Proceedings of the National Academy of Sciences, 110(14):5498\u20135503, 2013. John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014. Liang Huang, He Zhang, Dezhong Deng, Kai Zhao, Kaibo Liu, David A Hendrix, and David H Mathews. Linearfold: linear-time approximate RNA folding by 5\u2019-to-3\u2019dynamic programming and beam search. Bioinformatics, 35(14):i295\u2013i304, 2019. John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks. Learning protein structure with a differentiable simulator. 2018. Elizabeth Iorns, Christopher J Lord, Nicholas Turner, and Alan Ashworth. Utilizing RNA interfer- ence to enhance cancer drug discovery. Nature reviews Drug discovery, 6(7):556, 2007. Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirec- tional lstm feature representations. Transactions of the Association for Computational Linguistics, 4:313\u2013327, 2016. Yu Li, Sheng Wang, Ramzan Umarov, Bingqing Xie, Ming Fan, Lihua Li, and Xin Gao. Deepre: sequence-based enzyme ec number prediction by deep learning. Bioinformatics, 34(5):760\u2013769, 2017. Ronny Lorenz, Stephan H Bernhart, Christian H\u00a8oner Zu Siederdissen, Hakim Tafer, Christoph Flamm, Peter F Stadler, and Ivo L Hofacker. ViennaRNA package 2.0. Algorithms for molecular biology, 6(1):26, 2011. Rune B Lyngs\u00f8 and Christian NS Pedersen. RNA pseudoknot prediction in energy-based models. Journal of computational biology, 7(3-4):409\u2013427, 2000. NR Markham and M Zuker. Unafold: software for nucleic acid folding and hybridization in: Keith jm, editor.(ed.) bioinformatics methods in molecular biology, vol. 453, 2008. David H Mathews. Predicting RNA secondary structure by free energy minimization. Theoretical Chemistry Accounts, 116(1-3):160\u2013168, 2006. David H Mathews. How to benchmark RNA secondary structure prediction accuracy. Methods, 2019. David H Mathews and Douglas H Turner. Prediction of RNA secondary structure by free energy minimization. Current opinion in structural biology, 16(3):270\u2013278, 2006. Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji\u02c7c. Non-projective dependency pars- ing using spanning tree algorithms. In Proceedings of the conference on Human Language Tech- nology and Empirical Methods in Natural Language Processing, pp. 523\u2013530. Association for Computational Linguistics, 2005. 10 Published as a conference paper at ICLR 2020 Venkata Krishna Pillutla, Vincent Roulet, Sham M Kakade, and Zaid Harchaoui. A smoother way to train structured prediction models. In Advances in Neural Information Processing Systems, pp. 4766\u20134778, 2018. Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinvas Aluru, and Le Song. Glad: Learning sparse graph recovery. arXiv preprint arXiv:1906.00271, 2019. Michael F Sloma and David H Mathews. Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures. RNA, 22(12):1808\u20131818, 2016. David W Staple and Samuel E Butcher. Pseudoknots: RNA structures with diverse functions. PLoS biology, 3(6):e213, 2005. Evan W Steeg. Neural networks, adaptive optimization, and RNA secondary structure prediction. Artificial intelligence and molecular biology, pp. 121\u2013160, 1993. Zhen Tan, Yinghan Fu, Gaurav Sharma, and David H Mathews. Turbofold ii: RNA structural alignment and secondary structure prediction informed by multiple homologs. Nucleic acids research, 45(20):11570\u201311581, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998\u20136008, 2017. Sheng Wang, Siqi Sun, and Jinbo Xu. Auc-maximized deep convolutional neural fields for protein sequence labeling. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 1\u201316. Springer, 2016. Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, and Jinbo Xu. Accurate de novo prediction of protein contact map by ultra-deep learning model. PLoS computational biology, 13(1):e1005324, 2017. Shay Zakov, Yoav Goldberg, Michael Elhadad, and Michal Ziv-Ukelson. Rich parameterization improves RNA structure prediction. Journal of Computational Biology, 18(11):1525\u20131542, 2011. Hao Zhang, Chunhe Zhang, Zhi Li, Cong Li, Xu Wei, Borui Zhang, and Yuanning Liu. A new method of RNA secondary structure prediction based on convolutional neural network and dy- namic programming. Frontiers in genetics, 10, 2019. 11 Published as a conference paper at ICLR 2020 A MORE DISCUSSION ON RELATED WORKS Here we explain the difference between our approach and other works on unrolling optimization problems. First, our view of incorporating constraints to reduce output space and to reduce sample complexity is novel. Previous works (Hershey et al., 2014; Belanger et al., 2017; Ingraham et al., 2018) did not discuss these aspects. The most related work which also integrates constraints is OptNet (Amos &amp; Kolter, 2017), but its very expensive and can not scale to the RNA problem. Therefore, our proposed approach is a simple and effective one. Second, compared to (Chen et al., 2018; Shrivastava et al., 2019), our approach has a different purpose of using the algorithm. Their goal is to learn a better algorithm, so they commonly make their architecture more flexible than the original algorithm for the room of improvement. However, we aim at enforcing constraints. To ensure that constraints are nicely incorporated, we keep the original structure of the algorithm and only make the hyperparameters learnable. Finally, although all works consider end-to-end training, none of them can directly optimize the F1 score. We proposed a differentiable loss function to mimic the F1 score/precision/recall, which is effective and also very useful when negative samples are much fewer than positive samples (or the inverse). B DERIVATION OF THE PROXIMAL GRADIENT STEP The maximization step in Eq. 1 can be written as the following minimization: min \u02c6A\u2208RL\u00d7L \u2212 1 2 \u3008U\u03b8 (x) \u2212 s, A\u3009 + \u3008\u03bb, relu(A1 \u2212 1)\u3009 \ufe38 \ufe37\ufe37 \ufe38 \u2212f ( \u02c6A) +\u03c1\u2016 \u02c6A\u20161. (10) Consider the quadratic approximation of \u2212f ( \u02c6A) centered at \u02c6At: \u2212 \u02dcf\u03b1( \u02c6A) := \u2212 f ( \u02c6At) + \u3008\u2212 \u2202f \u2202 \u02c6At , \u02c6A \u2212 \u02c6At\u3009 + 1 2\u03b1 \u2016 \u02c6A \u2212 \u02c6At\u20162 F (11) = \u2212 f ( \u02c6At) + 1 2\u03b1 \u2225 \u2225 \u2225 \u02c6A \u2212 ( \u02c6At + \u03b1 \u2202f \u2202 \u02c6At ) \u2225 \u2225 \u22252 F , (12) and rewrite the optimization in Eq. 10 as min \u02c6A\u2208RL\u00d7L \u2212 f ( \u02c6At) + 1 2\u03b1 \u2225 \u2225 \u2225 \u02c6A \u2212 \u02d9At+1 \u2225 \u2225 \u22252 F + \u03c1\u2016 \u02c6A\u20161 (13) \u2261 min \u02c6A\u2208RL\u00d7L 1 2\u03b1 \u2225 \u2225 \u2225 \u02c6A \u2212 \u02d9At+1 \u2225 \u2225 \u22252 F + \u03c1\u2016 \u02c6A\u20161, (14) where \u02d9At+1 := \u02c6At + \u03b1 \u2202f \u2202 \u02c6At . (15) Next, we define proximal mapping as a function depending on \u03b1 as follows: prox\u03b1( \u02d9At+1) = arg min \u02c6A\u2208RL\u00d7L 1 2\u03b1 \u2225 \u2225 \u2225 \u02c6A \u2212 \u02d9At+1 \u2225 \u2225 \u22252 F + \u03c1\u2016 \u02c6A\u20161 (16) = arg min \u02c6A\u2208RL\u00d7L 1 2 \u2225 \u2225 \u2225 \u02c6A \u2212 \u02d9At+1 \u2225 \u2225 \u22252 F + \u03b1\u03c1\u2016 \u02c6A\u20161 (17) = sign( \u02d9At+1) max(| \u02d9At+1| \u2212 \u03b1\u03c1, 0) (18) = sign( \u02d9At+1)relu(| \u02d9At+1| \u2212 \u03b1\u03c1). (19) Since we always use \u02c6A \u25e6 \u02c6A instead of \u02c6A in our problem, we can take the absolute value |prox\u03b1( \u02d9At+1)| = relu(| \u02d9At+1| \u2212 \u03b1\u03c1) without loss of generality. Therefore, the proximal gradient 12 Published as a conference paper at ICLR 2020 step is \u02d9At+1 \u2190 \u02c6At + \u03b1 \u2202f \u2202 \u02c6At (correspond to Eq. 3) (20) \u02c6At+1 \u2190 relu(| \u02d9At+1| \u2212 \u03b1\u03c1) (correspond to Eq. 5). (21) More specifically, in the main text, we write \u2202f \u2202 \u02c6At as \u2202f \u2202 \u02c6At = 1 2 ( \u2202f \u2202At + \u2202f \u2202At <p>) \u25e6 \u2202At \u2202 \u02c6At (22) = ( 1 2 \u2202At \u2202 \u02c6At ) \u25e6 ( \u2202f \u2202At + \u2202f \u2202At ) (23) = ( 1 22 \u25e6 M \u25e6 (2 \u02c6At + 2 \u02c6A&gt; t ) ) \u25e6 ( \u2202f \u2202At + \u2202f \u2202At ) (24) = ( 1 22 \u25e6 M \u25e6 (2 \u02c6At + 2 \u02c6A&gt; t ) ) \u25e6 ( \u2202f \u2202At + \u2202f \u2202At ) (25) = M \u25e6 \u02c6At \u25e6 ( \u2202f \u2202At + \u2202f \u2202At ) . (26) The last equation holds since \u02c6At will remain symmetric in our algorithm if the initial \u02c6A0 is symmet- ric. Moreover, in the main text, \u03b1 is replaced by \u03b1 \u00b7 \u03b3t \u03b1. C IMPLEMENTATION AND TRAINING DETAILS We used Pytorch to implement the whole package of E2Efold. Deep Score Network. In the deep score network, we used a hyper-parameter, d, which was set as 10 in the final model, to control the model capacity. In the transformer encoder layers, we set the number of heads as 2, the dimension of the feed-forward network as 2048, the dropout rate as 0.1. As for the position encoding, we used 58 base functions to form the position feature map, which goes through a 3-layer fully-connected neural network (the number of hidden neurons is 5 \u2217 d) to generate the final position embedding, whose dimension is L by d. In the final output layer, the pairwise concatenation is carried out in the following way: Let X \u2208 RL\u00d73d be the input to the final output layers in Figure 4 (which is the concatenation of the sequence embedding and position embedding). The pairwise concatenation results in a tensor Y \u2208 RL\u00d7L\u00d76d defined as Y (i, j, :) = [X(i, :), X(j, :)], (27) where Y (i, j, :) \u2208 R6d, X(i, :) \u2208 R3d, and X(j, :) \u2208 R3d. In the 2D convolution layers, the the channel of the feature map gradually change from 6\u2217d to d , and finally to 1. We set the kernel size as 1 to translate the feature map into the final score matrix. Each 2D convolution layer is followed by a batch normalization layer. We used ReLU as the activation function within the whole score network. Post-Processing Network. In the PP network, we initialized w as 1, s as log(9), \u03b1 as 0.01, \u03b2 as 0.1, \u03b3\u03b1 as 0.99, \u03b3\u03b2 as 0.99, and \u03c1 as 1. We set T as 20. Training details. During training, we first pre-trained a deep score network and then fine-tuned the score network and the PP network together. To pre-train the score network, we used binary cross- entropy loss and Adam optimizer. Since, in the contact map, most entries are 0, we used weighted loss and set the positive sample weight as 300. The batch size was set to fully use the GPU memory, which was 20 for the Titan Xp card. We pre-train the score network for 100 epochs. As for the fine-tuning, we used binary cross-entropy loss for the score network and F1 loss for the PP network and summed up these two losses as the final loss. The user can also choose to only use the F1 loss or 13 Published as a conference paper at ICLR 2020 use another coefficient to weight the loss estimated on the score network U\u03b8 . Due to the limitation of the GPU memory, we set the batch size as 8. However, we updated the model\u2019s parameters every 30 steps to stabilize the training process. We fine-tuned the whole model for 20 epochs. Also, since the data for different RNA families are imbalanced, we up-sampled the data in the small RNA families based on their size. For the training of the score network U\u03b8 in the ablation study, it is exactly the same as the training of the above mentioned process. Except that during the fine-tune process, there is the unrolled number of iterations is set to be 0. D MORE EXPERIMENTAL DETAILS D.1 DATASET STATISTICS Figure 6: The RNAStralign length distribution. Table 7: RNAStralign dataset splits statistics RNA type All Training Validation Testing 16SrRNA 11620 9325 1145 1150 5SrRNA 9385 7687 819 879 tRNA 6443 5412 527 504 grp1 1502 1243 123 136 SRP 468 379 36 53 tmRNA 572 461 50 61 RNaseP 434 360 37 37 telomerase 37 28 4 5 RNAStralign 30451 24895 2702 2854 D.2 TWO-SAMPLE HYPOTHESIS TESTING To better understand the data distribution in different datasets, we provide statistical hypothesis test results in this section. We can assume that (i) Samples in RNAStralign training set are i.i.d. from the distribution P(RNAStrtrain); (ii) Samples in RNAStralign testing set are i.i.d. from the distribution P(RNAStrtest); (iii) Samples in ArchiveII dataset are i.i.d. from the distribution P(ArcII). To compare the differences among these data distributions, we can test the following hypothesis: (a) P(RNAStrtrain) = P(RNAStrtest) (b) P(RNAStrtrain) = P(ArchiveII) 14 Published as a conference paper at ICLR 2020 The approach that we adopted is the permutation test on the unbiased empirical Maximum Mean Discrepancy (MMD) estimator: MMDu(X, Y ) := ( N\u2211 i=1 N\u2211 j6 =i k(xi, xj ) + M\u2211 i=1 M\u2211 j6 =i k(yi, yj ) \u2212 2 mn N\u2211 i=1 M\u2211 j=1 k(xi, yj ) ) 1 2 , (28) where X = {xi}N i=1 contains N i.i.d. samples from a distribution P1, Y = {yi}M i=1 contains M i.i.d. samples from a distribution P2, and k(\u00b7, \u00b7) is a string kernel. Since we conduct stratified sampling to split the training and testing dataset, when we perform permutation test, we use stratified re-sampling as well (for both Hypothese (a) and (b)). The result of the permutation test (permuted 1000 times) is reported in Figure 7. Figure 7: Left: Distribution of MMDu under Hypothesis P(RNAStrtrain) = P(RNAStrtest). Right: Distribution of MMDu under Hypothesis P(RNAStrtrain) = P(ArchiveII). The result shows (a) Hypothesis P(RNAStrtrain) = P(RNAStrtest) can be accepted with significance level 0.1. (b) Hypothesis P(RNAStrtrain) = P(ArchiveII) is rejected since the p-value is 0. Therefore, the data distribution in ArchiveII is very different from the RNAStralign training set. A good performance on ArchiveII shows a significant generalization power of E2Efold. D.3 PERFORMANCE ON LONG SEQUENCES: WEIGHTED F1 SCORE For long sequences, E2Efold still performs better than other methods. We compute F1 scores weighted by the length of sequences (Table 8), such that the results are more dominated by longer sequences. Table 8: RNAStralign: F1 after a weighted average by sequence length. Method E2Efold CDPfold LinearFold Mfold RNAstructure RNAfold CONTRAfold non-weighted 0.821 0.614 0.609 0.420 0.550 0.540 0.633 weighted 0.720 0.691 0.509 0.366 0.471 0.444 0.542 change -12.3% +12.5% -16.4% -12.8% -14.3% -17.7% -14.3% The third row reports how much F1 score drops after reweighting. D.4 ARCHIVEII RESULTS AFTER DOMAIN SEQUENCES ARE REMOVED Since domain sequence (subsequences) in ArchiveII are explicitly labeled, we filter them out in ArchiveII and recompute the F1 scores (Table 9). The results do not change too much before or after filtering out subsequences. 15 Published as a conference paper at ICLR 2020 Table 9: ArchiveII: F1 after subsequences are filtered out. Method E2Efold CDPfold LinearFold Mfold RNAstructure RNAfold CONTRAfold original 0.704 0.597 0.647 0.421 0.613 0.615 0.662 filtered 0.723 0.605 0.645 0.419 0.611 0.615 0.659 D.5 PER-FAMILY PERFORMANCES To balance the performance among different families, during the training phase we conducted weighted sampling of the data based on their family size. With weighted sampling, the overall F1 score (S) is 0.83, which is the same as when we did equal-weighted sampling. The per-family results are shown in Table 10. Table 10: RNAStralign: per-family performances 16S rRNA tRNA 5S RNA SRP F1 F1(S) F1 F1(S) F1 F1(S) F1 F1(S) E2Efold 0.783 0.795 0.917 0.939 0.906 0.936 0.550 0.614 LinearFold 0.493 0.504 0.734 0.739 0.713 0.738 0.618 0.648 Mfold 0.362 0.373 0.662 0.675 0.356 0.367 0.350 0.378 RNAstructure 0.464 0.485 0.709 0.736 0.578 0.597 0.579 0.617 RNAfold 0.430 0.449 0.695 0.706 0.592 0.612 0.617 0.651 CONTRAfold 0.529 0.546 0.758 0.765 0.717 0.740 0.563 0.596 tmRNA Group I intron RNaseP telomerase F1 F1(S) F1 F1(S) F1 F1(S) F1 F1(S) E2Efold 0.588 0.653 0.387 0.428 0.565 0.604 0.954 0.961 LinearFold 0.393 0.412 0.565 0.579 0.567 0.578 0.515 0.531 Mfold 0.290 0.308 0.483 0.498 0.562 0.579 0.403 0.531 RNAstructure 0.400 0.423 0.566 0.599 0.589 0.616 0.512 0.545 RNAfold 0.411 0.430 0.589 0.599 0.544 0.563 0.471 0.496 CONTRAfold 0.463 0.482 0.603 0.620 0.645 0.662 0.529 0.548 16 Published as a conference paper at ICLR 2020 D.6 MORE VISUALIZATION RESULTSE2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 8: Visualization of 5S rRNA, B01865.E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 9: Visualization of 16S rRNA, DQ170870. 17 Published as a conference paper at ICLR 2020E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 10: Visualization of Group I intron, IC3, Kaf.c.trnL.E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 11: Visualization of RNaseP, A.salinestris-184.E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 12: Visualization of SRP, Homo.sapi. BU56690. 18 Published as a conference paper at ICLR 2020E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 13: Visualization of tmRNA, uncu.bact. AF389956.E2Efold RNAstructure CDPfoldtrue structure LinearFold MfoldCONTRAfold RNAfold Figure 14: Visualization of tRNA, tdbD00012019. 19 ===</p> <p>Skip to main content</p> <p>An official website of the United States government NCBI home page</p> <p>Primary site navigation Search PMC Full-Text Archive</p> <pre><code>Advanced Search\nJournal List\nUser Guide\n</code></pre> <p>As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer | PMC Copyright Notice Genes logo Genes (Basel) . 2022 Nov 18;13(11):2155. doi: 10.3390/genes13112155 Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots Manato Akiyama 1, Yasubumi Sakakibara 1, Kengo Sato 2,* Editors: Zihua Hu, Michel Ravelonandro, Lionel Benard</p> <p>PMCID: PMC9690657  PMID: 36421829 Abstract</p> <p>Existing approaches to predicting RNA secondary structures depend on how the secondary structure is decomposed into substructures, that is, the architecture, to define their parameter space. However, architecture dependency has not been sufficiently investigated, especially for pseudoknotted secondary structures. In this study, we propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that do not depend on the architecture of RNA secondary structures, and then implement this approach using two maximum expected accuracy (MEA)-based decoding algorithms: Nussinov-style decoding for pseudoknot-free structures and IPknot-style decoding for pseudoknotted structures. To train the neural networks connected to each base pair, we adopt a max-margin framework, called structured support vector machines (SSVM), as the output layer. Our benchmarks for predicting RNA secondary structures with and without pseudoknots show that our algorithm outperforms existing methods in prediction accuracy.</p> <p>Keywords: RNA secondary structure, deep learning, pseudoknots 1. Introduction</p> <p>The roles of functional non-coding RNAs (ncRNAs) in regulating transcription and guiding post-transcriptional modification have been recently shown to be critical in various biological processes, ranging from development and cell differentiation in healthy individuals to disease pathogenesis [1]. The well-established relationship between the primary sequence and structure of ncRNAs has motivated research aiming to elucidate the functions of ncRNAs by determining their structures.</p> <p>Yet, methods for experimentally determining RNA tertiary structures utilizing X-ray crystal structure analysis and nuclear magnetic resonance (NMR) are costly and labor-intensive, thus restricting their application. Accordingly, researchers often carry out computational prediction of RNA secondary structures based on the analysis of base pairs comprising nucleotides joined by hydrogen bonds.</p> <p>Computational approaches to RNA secondary structure prediction often utilize thermodynamic models (e.g., Turner\u2019s nearest neighbor model [2,3]) that define characteristic substructures, such as base-pair stacking and hairpin loops. In computational approaches, the free energy of each type of substructure is first empirically determined by methods such as optical melting experiments [2]. Then, the free energy of RNA secondary structures can be estimated as the sum of the free energy of their substructures. Dynamic programming can then be used to determine the optimal secondary structure that minimizes free energy for a given RNA sequence. This approach is employed by RNAfold [4], RNAstructure [5] and UNAfold [6], among other tools.</p> <p>As an alternative to experimental approaches, machine learning can be utilized to train scoring parameters based on the substructures constituting reference structures. This type of approach, as implemented in CONTRAfold [7,8], Simfold [9,10], ContextFold [11] and similar tools, has improved the accuracy of RNA secondary structure prediction. By integrating thermodynamic and machine-learning-based weighting approaches, MXfold avoided overfitting and achieved better performance than models based on either one alone [12]. Furthermore, interest in the use of deep learning for RNA secondary structure prediction is rapidly increasing [13,14,15]. MXfold2 used thermodynamic regularization to train a deep neural network so that the predicted folding score and free energy are as close as possible. This method showed robust prediction results in familywise cross validation, where the test dataset was structurally different from the training dataset.</p> <p>Another important aspect of RNA secondary structure prediction is the choice of the decoding algorithm used to find the optimal secondary structure from among all possible secondary structures. Two classic decoding algorithms are the minimum free energy (MFE) algorithm, which is used in thermodynamic approaches, and the maximum likelihood estimation (MLE) algorithm, which is used in machine-learning-based approaches. These algorithms find a secondary structure that minimizes the free energy and maximizes the probability or scoring function, respectively. Another option is a posterior decoding algorithm based on the maximum expected accuracy (MEA) principle, which is known to be an effective approach for many high-dimensional combinatorial optimization problems [16]. As researchers usually evaluate the prediction of RNA secondary structures using base-pair-wise accuracy measures, MEA-based decoding algorithms utilize posterior base-pairing probabilities that can be calculated by the McCaskill algorithm [17] or the inside\u2013outside algorithm for stochastic context-free grammars. CONTRAfold [18] and CentroidFold [19] both have MEA-based decoding algorithm implementations that successfully predict RNA secondary structures.</p> <p>Pseudoknots, an important structural element in RNA secondary structures, occur when at least two hydrogen bonds cross each other, and are typically drawn as two crossing arcs above a primary sequence (Figure 1). Figure 1.</p> <p>Figure 1 Open in a new tab</p> <p>An example of pseudoknots.</p> <p>Many RNAs, including rRNAs, tmRNAs and viral RNAs, form pseudoknotted secondary structures [20]. Pseudoknots are known to be involved in the regulation of translation and splicing as well as ribosomal frame shifting [21,22]. Furthermore, pseudoknots support folding into 3D structures in many cases [23]. Therefore, the impact of pseudoknots cannot be ignored in the structural and functional analysis of RNAs.</p> <p>However, all of the aforementioned algorithms cannot consider pseudoknotted secondary structures owing to computational complexity. It has been proven that the problem of finding MFE structures including arbitrary pseudoknots is NP-hard [24,25]. Therefore, practically available algorithms for predicting pseudoknotted RNA secondary structures fall into one of the following two approaches: exact algorithms for a limited class of pseudoknots, such as PKNOTS [26], NUPACK [27,28], pknotsRG [29] and Knotty [30]; and heuristic algorithms that do not guarantee that the optimal structure will be found, such as ILM [31], HotKnots [32,33], FlexStem [34] and ProbKnot [35].</p> <p>We previously developed IPknot, which enables fast and accurate prediction of RNA secondary structures with pseudoknots using integer programming [36,37]. IPknot adopts an MEA-based decoding algorithm that utilizes base-pairing probabilities combined with an approximate decomposition of a pseudoknotted structure into hierarchical pseudoknot-free structures. The prediction performance of IPknot is sufficient in terms of speed and accuracy compared with heuristic algorithms, and it is much faster than the exact algorithms.</p> <p>Both thermodynamic approaches and machine-learning-based approaches depend on the method by which a secondary structure is decomposed into substructures, that is, the architecture (as referred to in [38]), to define their parameter space. Turner\u2019s nearest neighbor model is the most well-studied architecture for predicting pseudoknot-free secondary structures, while the energy models for pseudoknotted secondary structures have not been sufficiently investigated, except for the Dirks\u2013Pierce model [27,28] and the Cao\u2013Chen model [39] for limited classes of pseudoknots. To our knowledge, an effective and efficient procedure to find a suitable architecture that can predict RNA secondary structures more accurately is still unknown.</p> <p>Here, we propose a novel algorithm to directly infer base-pairing probabilities with neural networks instead of the McCaskill algorithm or the inside\u2013outside algorithm, which both depend on the architecture of RNA secondary structures. Then, we employ the inferred base-pairing probabilities as part of a MEA-based scoring function for the two decoding algorithms: Nussinov-style decoding for pseudoknot-free structures, and IPknot-style decoding for pseudoknotted structures. To train the neural networks connected to each base pair, we adopt a max-margin framework, called structured support vector machines (SSVMs), as the output layer. We implement two types of neural networks connected to each base pair: bidirectional recursive neural networks (BiRNN) over tree structures and multilayer feedforward neural networks (FNN) with k-mer contexts around both bases in a pair. Our benchmarks for predicting RNA secondary structures with and without pseudoknots show that the prediction accuracy of our algorithm is superior to that of existing methods.</p> <p>The major advantages of our work are summarized as follows: (i) our algorithm enables us to accurately predict RNA secondary structures with and without pseudoknots; (ii) our algorithm assumes no prior knowledge of the architecture that defines the decomposition of RNA secondary structures and thus the corresponding parameter space. 2. Methods 2.1. Preliminaries</p> <p>The RNA sequence structure is modeled following the setup used by Akiyama et al. [12]. First, let , and let represent the set of all finite RNA sequences comprised of bases in . For a sequence , let represent the number of bases in x, referred to as the length of x. Let represent the set of all possible secondary structures formed by x. A secondary structure can be described as a binary-valued triangular matrix , in which if and only if bases and form a base pair linked by hydrogen bonds, including both canonical Watson\u2013Crick base pairs (i.e., G-C and A-U) and non-canonical wobble base pairs (e.g., G-U). 2.2. MEA-Based Scoring Function</p> <p>We employ the maximum expected accuracy (MEA)-based scoring function originally used for IPknot [36,37].</p> <p>A secondary structure is assumed to be decomposable into a set of pseudoknot-free substructures satisfying the following two conditions: (i) can be decomposed into a mutually-exclusive set, that is, for , ; and (ii) each base pair in can be pseudoknotted to at least one base pair in for . Each pseudoknot-free substructure is said to belong to level p. For each RNA secondary structure , there exists a positive integer m such that y is decomposable into m substructures without one or more pseudoknots (for more details, see the Supplementary Materials of [36]). Through the above decomposition, arbitrary pseudoknots can be modeled by our method.</p> <p>First, to construct an MEA-based scoring function, we define a gain function of with respect to the correct secondary structure as follows:     (1)</p> <p>Here, represents a base-pair weight parameter, and represent the numbers of true negatives (non-base pairs) and true positives (base pairs), respectively, and is an indicator function returning a value of either 1 or 0 depending on whether the is true or false.</p> <p>The objective is to identify a secondary structure that maximizes the expected value of the above gain function (1) under a given probability distribution over the space of pseudoknotted secondary structures:     (2)</p> <p>Here, is the probability distribution of RNA secondary structures including pseudoknots. The -centroid estimator (2) has been proven to allow us to decode secondary structures accurately based on a given probability distribution [18].</p> <p>Accordingly, the expected gain function (2) can be approximated as the sum of the expected gain functions for each level of pseudoknot-free substructures in the decomposed set of a pseudoknotted structure . Thus, a pseudoknotted structure and its decomposition can be found that maximize the following expected value:     (3)</p> <p>Here, is a weight parameter for level p base pairs and C is a constant that is independent of (for the derivation, see the Supplementary Material of [18]). The base-pairing probability represents the probability of base being paired with . As seen in Section 2.4, we employ one of three algorithms to calculate base-pairing probabilities.</p> <p>It should be noted that IPknot can be considered an extension of CentroidFold [18]. For the restricted case of a single decomposed level (i.e., ), the approximate expected gain function (3) of IPknot is equivalent to CentroidFold\u2019s -centroid estimator. 2.3. Decoding Algorithms 2.3.1. Nussinov-Style Decoding Algorithm for Pseudoknot-Free Structures</p> <p>For the prediction of pseudoknot-free secondary structures, we find that maximizes the expected gain (3) with under the following constraints on base pairs:     (4)     (5)     (6)</p> <p>The constraint defined by Equation (5) means that each base can be paired with at most one base. The constraint defined by Equation (6) disallows pseudoknot.</p> <p>This integer programming (IP) problem can be solved by dynamic programming as follows, similar to the Nussinov algorithm [40],     (7)</p> <p>and then tracing back from . 2.3.2. IPknot-Style Decoding Algorithm for Pseudoknotted Structures</p> <p>Maximization of the approximate expected gain (3) can be solved as the following IP problem:     (8)     (9)     (10)     (11)</p> <p>Note that Equation (3) requires the consideration of only base pairs with base-pairing probabilities being greater than . The constraint defined by Equation (9) means that each base can be paired with, at most, one base. The constraint defined by Equation (10) disallows pseudoknots within the same level p. The constraint defined by Equation (11) ensures that each level-p base pair is pseudoknotted to at least one base pair at each lower level . We set , which is IPknot\u2019s default setting. This suggests that the predicted structure can be decomposed into two pseudoknot-free secondary structures. 2.4. Inferring Base-Paring Probabilities</p> <p>Our scoring function (3) described in Section 2.2 is calculated by using base-pairing probabilities . In this section, we introduce two approaches for computing base-pairing probabilities. The first approach is a traditional one that is based on the probability distribution of RNA secondary structures, e.g., the McCaskill model [17] for pseudoknot-free structures and its extension to pseudoknotted structures, e.g., the Dirks\u2013Pierce model [27,28]. The second approach proposed in this paper directly calculates base-pairing probabilities using neural networks. 2.4.1. Traditional Models for Base-Pairing Probabilities</p> <p>The base-pairing probability is defined as     (12)</p> <p>from a probability distribution over a set of secondary structures with or without pseudoknots.</p> <p>For predicting pseudoknot-free structures, the McCaskill model [17] can be mostly used as combined with the Nussinov-style decoding algorithm described in Section 2.3.1. The computational complexity of calculating Equation (12) for the McCaskill model is for time and for space when using dynamic programming. This model was implemented previously as CentroidFold [18,19].</p> <p>For predicting pseudoknotted structures, we can select from among several models. A na\u00efve model could use the probability distribution with pseudoknots as well as Equation (2) in spite of high computational costs, e.g., the Dirks\u2013Pierce model [27,28] for a limited class of pseudoknots, with a computational complexity of for time and for space. Alternatively, we can employ a probability distribution without pseudoknots for each decomposed pseudoknot-free structure, such as the McCaskill model. Furthermore, to increase the prediction accuracy, we can utilize a heuristic algorithm with iterative refinement that refines the base-pairing probability matrix from the distribution without pseudoknots. See [36] for more details. These three models were implemented in IPknot [36]. 2.4.2. Neural Network Models</p> <p>In this research, we propose two neural network architectures for calculating base-pairing probabilities instead of the probability distribution over all RNA secondary structures.</p> <p>The first architecture is the bidirectional recursive neural network (BiRNN) over tree structures as shown in Figure 2. Stochastic context-free grammars (SCFG) can model RNA secondary structure without pseudoknots [7,41]. The layers of BiRNN over the tree structure are connected along grammatical trees derived from SCFG that models RNA secondary structures. The BiRNN consists of three matrices\u2014(a) the inside RNN matrix, (b) the outside RNN matrix and (c) the inside\u2013outside matrix\u2014for outputting base-pairing probabilities, each of whose elements contain a network layer (indicated by circles in Figure 2) with 80 hidden nodes. Each layer in the inside or outside matrix is recursively calculated from connected source layers as in the inside or outside algorithm, respectively, for stochastic context-free grammars (SCFG). The ReLU activation function is applied before being input to each recursive node. The base-pairing probability at each position is calculated from the corresponding layers in the inside and outside matrices with the sigmoid activation function. Our implementation of BiRNN assumes a simple RNA grammar</p> <p>where , a and represent the paired bases, represents the start non-terminal symbol, and represents the empty string. Figure 2.</p> <p>Figure 2 Open in a new tab</p> <p>A bidirectional recursive neural network for calculating base-pairing probabilities. A set of four dots above each base represents the one-hot representation of the base. Each circle indicates a network layer with 80 hidden nodes. Each solid arrow indicate a connection between layers along grammatical trees derived from the RNA grammar. Each dashed arrow represents a connection that aggregates the inside and outside layers to output base-pairing probabilities.</p> <p>The second architecture employs a simple multilayer feedforward neural network (FNN). To calculate the base-pairing probability , a FNN receives as input two k-mers around the i-th and j-th bases as shown in Figure 3. Figure 3.</p> <p>Figure 3 Open in a new tab</p> <p>A feedforward neural network with -mer contexts around and used to calculate the base-pairing probability . The end-of-loop nodes of the highlighted nucleotides are activated because they are beyond the paired bases.</p> <p>Each base is encoded by the one-hot encoding of nucleotides and an additional node that indicates the end of the loop, which should be active for s.t. in the left k-mer around or s.t. in the right k-mer around . This encoding can be expected to embed the length of loops and the contexts around the openings and closings of helices. We set for the k-mer context length default (for more details, see Section 3.4). We then construct two hidden layers consisting of 200 and 50 nodes, respectively, with the ReLU activation function and one output node with a sigmoid activation function to output base-pairing probabilities.</p> <p>Note that the FNN model depends on no assumption of RNA secondary structures, while the BiRNN model assumes an RNA grammar that considers no pseudoknots. Instead, the FNN model can take longer contexts around each base pair into consideration by using longer k-mers. 2.5. Learning Algorithm</p> <p>We optimize the network parameters by using a max-margin framework called a structured support vector machine (SSVM) [42]. For a training dataset , where represents the k-th RNA sequence and represents the correct secondary structure of the k-th sequence , we identify a that minimizes the objective function     (13)</p> <p>where is the scoring function of RNA secondary structure for a given RNA sequence , that is, Equation (4) for Nussinov-style decoding or Equation (8) for IPknot-style decoding. Here, is a loss function of for y defined as     (14)</p> <p>where and are tunable hyperparameters that can control the trade-off between sensitivity and specificity in learning the parameters. By default, we used . In this case, the first term of Equation (13) can be calculated using the Nussinov-style decoding algorithm or the IPknot-style decoding algorithm modified by loss-augmented inference [42].</p> <p>To minimize the objective function (13), stochastic subgradient descent (Algorithm 1) or one of its variants can be applied. We can calculate the gradients with regard to the network parameters for the objective function (13) using the gradients with regard to by the chain rule of differentiation. This means that the prediction errors occurred through the decoding algorithm backpropagating to the neural network that calculates base-pairing probabilities through the connected base pairs. Algorithm 1 The stochastic subgradient descent algorithm for structured support vector machines (SSVMs); is the predefined learning rate.</p> <pre><code>1:\n\ninitialize for all\n2:\n\nrepeat\n3:\n\n\u2003\u2003for all  do\n4:\n\n\u2003\u2003\u2003\u2003\n5:\n\n\u2003\u2003\u2003\u2003for all  do\n6:\n\n\u2003\u2003\u2003\u2003\u2003\u2003\n7:\n\n\u2003\u2003\u2003\u2003end for\n8:\n\n\u2003\u2003end for\n9:\n\nuntil all the parameters converge\n</code></pre> <p>Open in a new tab 3. Results 3.1. Implementation</p> <p>Our algorithm is implemented as the program Neuralfold, which is short for the neural network-based RNA folding algorithm. We employ Chainer [43] for the neural networks and the Python linear programming solver PuLP [44]. The source code for this implementation is available at https://github.com/keio-bioinformatics/neuralfold/, (accessed on 27 September 2022). 3.2. Datasets</p> <p>We evaluated our algorithm with the Nussinov-style decoding algorithm for predicting pseudoknot-free RNA secondary structures using four datasets, TrainSetA, TestSetA, TrainSetB and TestSetB, which were established by [45].</p> <p>TrainSetA and TestSetA are literature-based datasets [7,9,10,41,46] that were constructed to ensure sequence diversity. TrainSetA contains SSU and LSU domains, SRP RNAs, RNase P RNAs and tmRNAs comprising 3166 total sequences spanning 630,279 nt, with 333,466 forming base pairs (47.9%). The sequence lengths range from 10 to 734 nt, with an average length of 199 nt. TestSetA includes sequences from eight RNA families: 5S rRNA, group I and II introns, RNase P RNA, SRP RNA, tmRNA, tRNA, and telomerase RNA. TestSetA contains 697 sequences, with 51.7% of their bases forming base pairs. The sequence length ranges from 10 to 768 nt, with an average length of 195 nt. We excluded a number of sequences that contain pseudoknotted secondary structures in the original data sources from TestSetA. Thus, 593 sequences were selected as TestSetA.</p> <p>TrainSetB and TestSetB, which contain 22 families with 3D structures [38], were assembled from Rfam [47]. TrainSetB and TestSetB include sequences from Rfam seed alignments with no more than 70% shared identity between sequences. TrainSetB comprises 22 RNA families, and its specific composition is 145.8S rRNAs, 18 U1 spliceosomal RNAs, 45 U4 spliceosomal RNAs, 233 riboswitches (from seven different families), 116 cis-regulatory elements (from nine different families), 3 ribozymes and a single bacteriophage pRNA. TrainSetB was constructed by selecting sequences dissimilar to those in TestSetB. TrainSetB contains 1094 sequences, including 112,398 nt in all, of which 52,065 bases (46.3%) formed base pairs. The sequence length is in the range of 27 to 237 nt with an average length of 103 nt. TrainSetB contains 4.3% noncanonical base pairs. TestSetB also consists of the same 22 RNA families as TrainSetB, TestSetB contains 430 sequences, including 52,097 nt in all, of which 22,728 bases (43.6%) form base pairs. The sequence length is in the range of 27 to 244 nt, with an average length of 121 nt. TestSetB contains 8.3% noncanonical base pairs.</p> <p>We also evaluated our algorithm with the IPknot-style decoding algorithm for predicting pseudoknotted RNA secondary structures on two datasets. The first dataset is called the pk168 dataset [48], which was compiled from PseudoBase [20]. This dataset includes 16 categories of 168 pseudoknotted sequences with lengths &lt;140 nt.</p> <p>The second dataset is called RS-pk388, originally established by [36]. This dataset was obtained from the RNA STRAND database and contains 388 non-redundant sequences with lengths between 140 and 500 nt. 3.3. Prediction Performance</p> <p>We evaluated the accuracy of RNA secondary structure predictions based on sensitivity () and positive predictive value () as follows:</p> <p>Here, , and represent the numbers of true positives (i.e., the correctly predicted base pairs), false positives (i.e., incorrectly predicted base pairs), and false negatives (i.e., base pairs in the correct structure that were not predicted), respectively. As a balanced measure of and , we utilized their F-value, which is defined as their harmonic mean:</p> <p>We conducted computational experiments on the datasets described in the previous section using the Nussinov-style decoding algorithm with the McCaskill and neural network models as well as the BiRNN and FNN models. We employed CentroidFold as the Nussinov decoding algorithm with the McCaskill model. We performed experiments on TestSetB using the parameters trained from TrainSetB. As shown in Table 1, the neural network models achieved better accuracy compared with the traditional model. Hereafter, we adopt the FNN model with k-mer contexts as the default Neuralfold model since it yielded better prediction accuracy in this experiment. Table 1.</p> <p>Accuracy of inferred base-pairing probabilities for TestSetB. Implementation  Model   SEN     PPV     F Neuralfold  BiRNN   0.649   0.601   0.624 Neuralfold  FNN     0.600   0.700   0.646 CentroidFold    McCaskill   0.513   0.544   0.528 Open in a new tab</p> <p>The other computational experiments on the pseudoknotted dataset were conducted using the IPknot-style decoding algorithm with the McCaskill model with and without iterative refinement and with the Dirks\u2013Pierce model as well as using Neuralfold with the FNN model. Table 2 shows that the feedforward neural network (FNN) model with 10-fold cross validation is comparable to IPknot with the Dirks\u2013Pierce model for pseudoknots but superior to the McCaskill model both with and without iterative refinement. Table 2.</p> <p>Accuracy of inferred base-pairing probabilities for the pk168 dataset. Implementation  Model   SEN     PPV     F Neuralfold  FNN            IPknot  McCaskill w/o refine.   0.619   0.710   0.661 IPknot  McCaskill w/refine.     0.753   0.684   0.717 IPknot  Dirks\u2013Pierce    0.809   0.749   0.778 Open in a new tab</p> <p>Table 3 shows the computation time for of the following sequences, which vary in length: PKB229 and PKB134 in the pk168 dataset; ASE_00193, CRW_00614 and CRW_00774 in the RNA STRAND database [49]. Table 3.</p> <p>Computation time for calculating base-pairing probabilities of sequences of various lengths. ID  PKB229  PKB134  ASE_00193   CRW_00614   CRW_00774 Length (nt)     67  137     301     494     989 Neuralfold (FNN) IPknot  3.30 s  27.78 s     44.73 s     60.22 s     3 m 4.2 s   (w/o refine.)     0.01 s  0.05 s  0.18 s  0.55 s  2.64 s   (w/refine.)   0.03 s  0.08 s  0.31 s  1.03 s  5.86 s   (D&amp;P)     8.36 s  9 m 4.7 s   n/a     n/a     n/a Open in a new tab</p> <p>Computation time was measured on an Intel Xeon E5-2680 (2.80 GHz) computer with 64 GB of memory and running Linux OS v2.6.32. FNN, feedforward neural network; D&amp;P, Dirks\u2013Pierce. IPknot with D&amp;P failed to compute due to lack of memory for sequence lengths greater than 300.</p> <p>This shows that the computation time for predicting a pseudoknotted secondary structure using the FNN model is comparably fast to IPknot with the Dirks\u2013Pierce model. 3.4. Effects of Context Length</p> <p>We evaluated the prediction accuracy obtained with the FNN model on the TestSetB and pk168 datasets for several lengths of k-mers input to neural networks. The accuracy as measured by , , and their F-value for different k-mer lengths is summarized in Figure 4. This analysis indicates that the accuracy is essentially maximized when the k-mer length is 81, and the difference in the accuracy for is negligible. Figure 4.</p> <p>Figure 4 Open in a new tab</p> <p>The accuracy of the FNN model with different lengths of k-mers on the TestSetB dataset (left) and the pk168 dataset (right). , sensitivity; , positive predictive value; F, the F-value based on and . 3.5. Comparison with Previous Methods for Prediction of Pseudoknot-Free Secondary Structures</p> <p>We compared our algorithm with previous methods for predicting pseudoknot-free RNA secondary structures including CentroidFold [18,19], CONTRAfold [7,8], RNAfold in the Vienna RNA package [4] and ContextFold [29]. For the posterior decoding methods with the trade-off parameter in Equation (4), we used . We performed secondary structure prediction on TestSetA with parameters trained on TrainSetA as well as prediction on TestSetB with the parameters trained on TrainSetB. The PPV\u2013SEN plots for each method shown in Figure 5 indicate that our algorithm accurately predicts pseudoknot-free secondary structures in the datasets including famlilies similar with the training datasets. Figure 5.</p> <p>Figure 5 Open in a new tab</p> <p>Positive predictive value\u2013sensitivity (PPV\u2013SEN) plots comparing our algorithm with competitive methods on TestSetA (Left) and TestSetB (Right).</p> <p>On the other hand, to investigate the generalization ability of our method, another experiment in which our method was trained on TrainSetB and evaluated for accuracy on TestSetA showed that our method had very low accuracy (, , and ), which suggests that our method is severely overfitted. 3.6. Comparison with Alternative Methods for Predicting Pseudoknotted Secondary Structures</p> <p>We also compared our algorithm with competing methods for predicting pseudoknotted secondary structures, including IPknot [36], HotKnots [32,33], and pknotsRG [29], as well as methods for predicting pseudoknot-free secondary structures, including CentroidFold [19] and RNAfold [4]. Neuralfold performed 10-fold cross validation on the pk168 and RS-pk388 datasets. Figure 6 shows PPV\u2013SEN plots for each method, indicating that our algorithm works accurately on pseudoknotted datasets. Figure 6.</p> <p>Figure 6 Open in a new tab</p> <p>Positive predictive value\u2013sensitivity (PPV\u2013SEN) plots comparing our algorithm with competitive methods on the pk168 dataset (Left) and the RS-pk388 dataset (Right). For the pk168 dataset, we set , for Neuralfold; , for IPknot with the Dirks\u2013Pierce (D&amp;P) model; , for IPknot with/without refinement; for CentroidFold. For the RS-pk388 dataset, we set , for Neuralfold; , for IPknot without refinement; , for IPknot with refinement; for CentroidFold. 4. Discussion</p> <p>We propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that enables us to predict RNA secondary structures accurately. Sato et al. [36] previously proposed an iterative algorithm that refines the base-pairing probabilities calculated by the McCaskill algorithm so as to be appropriate for pseudoknotted secondary structure prediction. The direct inference of base-pairing probabilities with neural networks is an approach similar to the iterative refinement algorithm in the sense that both directly update base-pairing probabilities, and the IPknot-style decoding algorithm then uses the base-pairing probabilities. Although the iterative refinement algorithm can improve the prediction accuracy of IPknot to some extent, it should be noted that this is an ad hoc algorithm, as there is no theoretical guarantee of improvement. Meanwhile, the neural networks that infer base-pairing probabilities are trained on given reference secondary structures by the max-margin framework, meaning that we can theoretically expect that the neural network models improve the secondary structure prediction. Indeed, Table 2 shows that our algorithm achieved not only better accuracy than the iterative refinement algorithm, but is also comparable to that of the Dirks\u2013Pierce model, which can calculate exact base-pairing probabilities for a limited class of pseudoknots.</p> <p>Recently, several methods for predicting RNA secondary structure using deep learning were proposed [13,14,15]. Although most of them use deep learning to compute matrices (N is the sequence length), which can be regarded as base-pairing probability matrices, they do not directly address the constraints that the RNA secondary structure must satisfy (e.g., Equations (5) and (6) for pseudoknot-free structures, and Equations (9)\u2013(11) for pseudoknotted structures). On the other hand, MXfold2 [14] combines the Zuker-style dynamic programming [50] and deep learning to handle the constraints that pseudoknot-free RNA secondary structures must satisfy. UFold [15] predicts RNA secondary structure including pseudoknots using post-processing by linear programming, but does not directly address constraints on RNA secondary structure including pseudoknots when training deep learning models to predict base-pairing probabilities. By combining IPknot-style decoding with the max-margin training, the proposed Neuralfold can directly handle the constraints (9)\u2013(11) that pseudoknotted RNA secondary structure must satisfy, not only when predicting secondary structures, but also when training deep learning models.</p> <p>It has been pointed out that RNA secondary structure prediction based on machine learning and deep learning is prone to overfitting due to bias in the training data [14,45]. Several methods have been proposed to alleviate overfitting, such as using ensembles of multiple models [13], and integration with thermodynamic models [14]. UFold, on the other hand, employed artificially generated sequences and their predicted secondary structures for data augmentation, which were then used as additional training data to relax overfitting due to bias in the training data [15]. Our proposed method does not provide a strategy to counteract such overfitting and is therefore unsatisfactory at predicting sequences of families that are structurally distant from the training data, as shown in the results. However, by utilizing the ensembles of multiple models, as in SPOT-RNA, and the data augmentation strategy, as in UFold, it is expected to address to some extent the overfitting caused by bias in the training data.</p> <p>The FNN model takes two k-mers around each base pair as input to infer its base-pairing probability, where k is the context length to model the length of loops and the contexts around the openings and closings of helices. As can be seen in Figure 7, different k-mer context lengths affect the prediction of pseudoknotted secondary structures. For example, consider the input bases when calculating the base-pairing probability of the blue-highlighted base pair (AU) using the FNN model. The FNN model with the context length k = 11 takes as input five bases in both the upstream and downstream directions from bases i and j. As seen in Figure 7 (bottom), the distances from bases A and U are 10 and 13 to Stem 2, respectively. This means that all the bases comprising Stem 2 are not completely located within the context length k = 11 around the base pair AU. On the other hand, for the FNN model with context length k = 41, all the bases of Stem 2 are completely located within the context around the base pair AU. This leads the FNN model to correctly predict the base pair AU, suggesting that a longer context length enables consideration of the dependency between stems in pseudoknotted substructures. Figure 7.</p> <p>Figure 7 Open in a new tab</p> <p>(Top) Comparison between the reference structure of ID PKB189 (top-left) and the predicted structures with context lengths k = 11 (top-middle) and k = 41 (top-right). (Bottom) Distance between two stems (Stem 1 and Stem 2) in the pseudoknotted structure. 5. Conclusions</p> <p>We propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that enables us to accurately predict RNA secondary structures with pseudoknots. By combining IPknot-style decoding with the max-margin framework, our algorithm trains the model in the end-to-end manner to compute base-pairing probabilities under the constraints that RNA secondary structures, including pseudoknots, must satisfy. HotKnots 2.0 [32], on the other hand, finds a pseudoknotted secondary structure by using an MFE-based heuristic decoding algorithm with energy parameters of the Dirks\u2013Pierce model or the Cao\u2013Chen model trained on pseudoknotted reference structures. One of the advantages of our algorithm over HotKnots 2.0 is that no assumption about the architecture of RNA secondary structures is required. In other words, our model can be trained on arbitrary classes of pseudoknots, while HotKnots cannot be trained on more complicated classes of pseudoknots than the one assumed by the model. Furthermore, our algorithm can compute base-pairing probabilities, which can be used in various applications of RNA informatics, such as family classification [51,52], RNA\u2013RNA interaction prediction [53] and simultaneous aligning and folding [54]. Accurate base-pairing probabilities calculated by our algorithm can improve the quality of such applications. Acknowledgments</p> <p>The supercomputer system was provided by the National Institute of Genetics (NIG), Research Organization of Information and Systems (ROIS). Abbreviations</p> <p>The following abbreviations are used in this manuscript: BiRNN   bi-directional recurrent neural network FNN     feedforward neural network MEA     maximum expected accuracy MFE     minimum free energy ncRNA   non-coding RNA SSVM    structured support vector machine Open in a new tab Author Contributions</p> <p>Conceptualization, M.A. and K.S.; methodology, M.A. and K.S.; software, M.A.; validation, M.A. and K.S.; writing\u2014original draft preparation, M.A.; writing\u2014review and editing, K.S.; supervision, Y.S. and K.S.; project administration, K.S.; funding acquisition, K.S. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement</p> <p>Not applicable. Informed Consent Statement</p> <p>Not applicable. Data Availability Statement</p> <p>Not applicable. Conflicts of Interest</p> <p>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results. Funding Statement</p> <p>This work was supported in part by a Grant-in-Aid for Scientific Research (KAKENHI) (16K00404, 19H04210 and 19K22897) from the Japan Society for the Promotion of Science (JSPS) to K.S. Footnotes</p> <p>Publisher\u2019s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations. References</p> <pre><code>1.Hirose T., Mishima Y., Tomari Y. Elements and machinery of non-coding RNAs: Toward their taxonomy. EMBO Rep. 2014;15:489\u2013507. doi: 10.1002/embr.201338390. [DOI] [PMC free article] [PubMed] [Google Scholar]\n2.Schroeder S.J., Turner D.H. Optical melting measurements of nucleic acid thermodynamics. Meth. Enzymol. 2009;468:371\u2013387. doi: 10.1016/S0076-6879(09)68017-4. [DOI] [PMC free article] [PubMed] [Google Scholar]\n3.Turner D.H., Mathews D.H. NNDB: The nearest neighbor parameter database for predicting stability of nucleic acid secondary structure. Nucleic Acids Res. 2010;38:D280\u2013D282. doi: 10.1093/nar/gkp892. [DOI] [PMC free article] [PubMed] [Google Scholar]\n4.Lorenz R., Bernhart S.H., Honer Zu Siederdissen C., Tafer H., Flamm C., Stadler P.F., Hofacker I.L. ViennaRNA Package 2.0. Algorithms Mol. Biol. 2011;6:26. doi: 10.1186/1748-7188-6-26. [DOI] [PMC free article] [PubMed] [Google Scholar]\n5.Reuter J.S., Mathews D.H. RNAstructure: Software for RNA secondary structure prediction and analysis. BMC BioInform. 2010;11:129. doi: 10.1186/1471-2105-11-129. [DOI] [PMC free article] [PubMed] [Google Scholar]\n6.Zuker M. On finding all suboptimal foldings of an RNA molecule. Science. 1989;244:48\u201352. doi: 10.1126/science.2468181. [DOI] [PubMed] [Google Scholar]\n7.Do C.B., Woods D.A., Batzoglou S. CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinformatics. 2006;22:e90\u2013e98. doi: 10.1093/bioinformatics/btl246. [DOI] [PubMed] [Google Scholar]\n8.Do C.B., Foo C.S., Ng A. Efficient multiple hyperparameter learning for log-linear models; Proceedings of the 20th International Conference on Neural Information Processing Systems; Vancouver, BC, Canada. 3\u20136 December 2007; Red Hook, NY, USA: Curran Associates Inc.; 2007. Advances in Neural Information Processing Systems 20. [Google Scholar]\n9.Andronescu M., Condon A., Hoos H.H., Mathews D.H., Murphy K.P. Efficient parameter estimation for RNA secondary structure prediction. Bioinformatics. 2007;23:19\u201328. doi: 10.1093/bioinformatics/btm223. [DOI] [PubMed] [Google Scholar]\n10.Andronescu M., Condon A., Hoos H.H., Mathews D.H., Murphy K.P. Computational approaches for RNA energy parameter estimation. RNA. 2010;16:2304\u20132318. doi: 10.1261/rna.1950510. [DOI] [PMC free article] [PubMed] [Google Scholar]\n11.Zakov S., Goldberg Y., Elhadad M., Ziv-Ukelson M. Rich parameterization improves RNA structure prediction. J. Comput. Biol. 2011;18:1525\u20131542. doi: 10.1089/cmb.2011.0184. [DOI] [PubMed] [Google Scholar]\n12.Akiyama M., Sato K., Sakakibara Y. A max-margin training of RNA secondary structure prediction integrated with the thermodynamic model. J. Bioinform. Comput. Biol. 2018;16:1840025. doi: 10.1142/S0219720018400255. [DOI] [PubMed] [Google Scholar]\n13.Singh J., Hanson J., Paliwal K., Zhou Y. RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nat. Commun. 2019;10:5407. doi: 10.1038/s41467-019-13395-9. [DOI] [PMC free article] [PubMed] [Google Scholar]\n14.Sato K., Akiyama M., Sakakibara Y. RNA secondary structure prediction using deep learning with thermodynamic integration. Nat. Commun. 2021;12:941. doi: 10.1038/s41467-021-21194-4. [DOI] [PMC free article] [PubMed] [Google Scholar]\n15.Fu L., Cao Y., Wu J., Peng Q., Nie Q., Xie X. UFold: Fast and accurate RNA secondary structure prediction with deep learning. Nucleic Acids Res. 2022;50:e14. doi: 10.1093/nar/gkab1074. [DOI] [PMC free article] [PubMed] [Google Scholar]\n16.Carvalho L.E., Lawrence C.E. Centroid estimation in discrete high-dimensional spaces with applications in biology. Proc. Natl. Acad. Sci. USA. 2008;105:3209\u20133214. doi: 10.1073/pnas.0712329105. [DOI] [PMC free article] [PubMed] [Google Scholar]\n17.McCaskill J.S. The equilibrium partition function and base pair binding probabilities for RNA secondary structure. Biopolymers. 1990;29:1105\u20131119. doi: 10.1002/bip.360290621. [DOI] [PubMed] [Google Scholar]\n18.Hamada M., Kiryu H., Sato K., Mituyama T., Asai K. Prediction of RNA secondary structure using generalized centroid estimators. Bioinformatics. 2009;25:465\u2013473. doi: 10.1093/bioinformatics/btn601. [DOI] [PubMed] [Google Scholar]\n19.Sato K., Hamada M., Asai K., Mituyama T. CENTROIDFOLD: A web server for RNA secondary structure prediction. Nucleic Acids Res. 2009;37:W277\u2013W280. doi: 10.1093/nar/gkp367. [DOI] [PMC free article] [PubMed] [Google Scholar]\n20.van Batenburg F.H., Gultyaev A.P., Pleij C.W. PseudoBase: Structural information on RNA pseudoknots. Nucleic Acids Res. 2001;29:194\u2013195. doi: 10.1093/nar/29.1.194. [DOI] [PMC free article] [PubMed] [Google Scholar]\n21.Staple D.W., Butcher S.E. Pseudoknots: RNA structures with diverse functions. PLoS Biol. 2005;3:e213. doi: 10.1371/journal.pbio.0030213. [DOI] [PMC free article] [PubMed] [Google Scholar]\n22.Brierley I., Pennell S., Gilbert R.J. Viral RNA pseudoknots: Versatile motifs in gene expression and replication. Nat. Rev. Microbiol. 2007;5:598\u2013610. doi: 10.1038/nrmicro1704. [DOI] [PMC free article] [PubMed] [Google Scholar]\n23.Fechter P., Rudinger-Thirion J., Florentz C., Giege R. Novel features in the tRNA-like world of plant viral RNAs. Cell. Mol. Life Sci. 2001;58:1547\u20131561. doi: 10.1007/PL00000795. [DOI] [PMC free article] [PubMed] [Google Scholar]\n24.Akutsu T. Dynamic programming algorithms for RNA secondary structure prediction with pseudoknots. Discret. Appl. Math. 2000;104:45\u201362. doi: 10.1016/S0166-218X(00)00186-4. [DOI] [Google Scholar]\n25.Lyngs\u00f8 R.B., Pedersen C.N. RNA pseudoknot prediction in energy-based models. J. Comput. Biol. 2000;7:409\u2013427. doi: 10.1089/106652700750050862. [DOI] [PubMed] [Google Scholar]\n26.Rivas E., Eddy S.R. A dynamic programming algorithm for RNA structure prediction including pseudoknots. J. Mol. Biol. 1999;285:2053\u20132068. doi: 10.1006/jmbi.1998.2436. [DOI] [PubMed] [Google Scholar]\n27.Dirks R.M., Pierce N.A. A partition function algorithm for nucleic acid secondary structure including pseudoknots. J. Comput. Chem. 2003;24:1664\u20131677. doi: 10.1002/jcc.10296. [DOI] [PubMed] [Google Scholar]\n28.Dirks R.M., Pierce N.A. An algorithm for computing nucleic acid base-pairing probabilities including pseudoknots. J. Comput. Chem. 2004;25:1295\u20131304. doi: 10.1002/jcc.20057. [DOI] [PubMed] [Google Scholar]\n29.Reeder J., Giegerich R. Design, implementation and evaluation of a practical pseudoknot folding algorithm based on thermodynamics. BMC Bioinform. 2004;5:104. doi: 10.1186/1471-2105-5-104. [DOI] [PMC free article] [PubMed] [Google Scholar]\n30.Jabbari H., Wark I., Montemagno C., Will S. Knotty: Efficient and Accurate Prediction of Complex RNA Pseudoknot Structures. Bioinformatics. 2018;34:3849\u20133856. doi: 10.1093/bioinformatics/bty420. [DOI] [PubMed] [Google Scholar]\n31.Ruan J., Stormo G.D., Zhang W. An iterated loop matching approach to the prediction of RNA secondary structures with pseudoknots. Bioinformatics. 2004;20:58\u201366. doi: 10.1093/bioinformatics/btg373. [DOI] [PubMed] [Google Scholar]\n32.Andronescu M.S., Pop C., Condon A.E. Improved free energy parameters for RNA pseudoknotted secondary structure prediction. RNA. 2010;16:26\u201342. doi: 10.1261/rna.1689910. [DOI] [PMC free article] [PubMed] [Google Scholar]\n33.Ren J., Rastegari B., Condon A., Hoos H.H. HotKnots: Heuristic prediction of RNA secondary structures including pseudoknots. RNA. 2005;11:1494\u20131504. doi: 10.1261/rna.7284905. [DOI] [PMC free article] [PubMed] [Google Scholar]\n34.Chen X., He S.M., Bu D., Zhang F., Wang Z., Chen R., Gao W. FlexStem: Improving predictions of RNA secondary structures with pseudoknots by reducing the search space. Bioinformatics. 2008;24:1994\u20132001. doi: 10.1093/bioinformatics/btn327. [DOI] [PubMed] [Google Scholar]\n35.Bellaousov S., Mathews D.H. ProbKnot: Fast prediction of RNA secondary structure including pseudoknots. RNA. 2010;16:1870\u20131880. doi: 10.1261/rna.2125310. [DOI] [PMC free article] [PubMed] [Google Scholar]\n36.Sato K., Kato Y., Hamada M., Akutsu T., Asai K. IPknot: Fast and accurate prediction of RNA secondary structures with pseudoknots using integer programming. Bioinformatics. 2011;27:85\u201393. doi: 10.1093/bioinformatics/btr215. [DOI] [PMC free article] [PubMed] [Google Scholar]\n37.Sato K., Kato Y. Prediction of RNA secondary structure including pseudoknots for long sequences. Brief. Bioinform. 2022;23:bbab395. doi: 10.1093/bib/bbab395. [DOI] [PMC free article] [PubMed] [Google Scholar]\n38.Rivas E. The four ingredients of single-sequence RNA secondary structure prediction. A unifying perspective. RNA Biol. 2013;10:1185\u20131196. doi: 10.4161/rna.24971. [DOI] [PMC free article] [PubMed] [Google Scholar]\n39.Cao S., Chen S.J. Predicting RNA pseudoknot folding thermodynamics. Nucleic Acids Res. 2006;34:2634\u20132652. doi: 10.1093/nar/gkl346. [DOI] [PMC free article] [PubMed] [Google Scholar]\n40.Nussinov R., Pieczenick G., Griggs J., Kleitman D. Algorithms for loop matching. SIAM J. Appl. Math. 1978;35:68\u201382. doi: 10.1137/0135006. [DOI] [Google Scholar]\n41.Dowell R.D., Eddy S.R. Evaluation of several lightweight stochastic context-free grammars for RNA secondary structure prediction. BMC Bioinform. 2004;5:71. doi: 10.1186/1471-2105-5-71. [DOI] [PMC free article] [PubMed] [Google Scholar]\n42.Tsochantaridis I., Joachims T., Hofmann T., Altun Y. Large Margin Methods for Structured and Interdependent Output Variables. J. Mach. Learn. Res. 2005;6:1453\u20131484. [Google Scholar]\n43.Tokui S., Oono K., Hido S., Clayton J. Chainer: A Next-Generation Open Source Framework for Deep Learning; Proceedings of the Workshop on Machine Learning Systems (LearningSys) in The Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS); Montr\u00e9al, QC, Canada. 11\u201312 December 2015. [Google Scholar]\n44.Mitchell S., Consulting S.M., O\u2019sullivan M., Dunning I. PuLP: A Linear Programming Toolkit for Python. 2011. [(accessed on 27 September 2022)]. Available online: https://optimization-online.org/2011/09/3178/\n45.Rivas E., Lang R., Eddy S.R. A range of complex probabilistic models for RNA secondary structure prediction that includes the nearest-neighbor model and more. RNA. 2012;18:193\u2013212. doi: 10.1261/rna.030049.111. [DOI] [PMC free article] [PubMed] [Google Scholar]\n46.Lu Z.J., Gloor J.W., Mathews D.H. Improved RNA secondary structure prediction by maximizing expected pair accuracy. RNA. 2009;15:1805\u20131813. doi: 10.1261/rna.1643609. [DOI] [PMC free article] [PubMed] [Google Scholar]\n47.Gardner P.P., Daub J., Tate J., Moore B.L., Osuch I.H., Griffiths-Jones S., Finn R.D., Nawrocki E.P., Kolbe D.L., Eddy S.R., et al. Rfam: Wikipedia, clans and the \u201cdecimal\u201d release. Nucleic Acids Res. 2011;39:D141\u2013D145. doi: 10.1093/nar/gkq1129. [DOI] [PMC free article] [PubMed] [Google Scholar]\n48.Huang X., Ali H. High sensitivity RNA pseudoknot prediction. Nucleic Acids Res. 2007;35:656\u2013663. doi: 10.1093/nar/gkl943. [DOI] [PMC free article] [PubMed] [Google Scholar]\n49.Andronescu M., Bereg V., Hoos H.H., Condon A. RNA STRAND: The RNA secondary structure and statistical analysis database. BMC Bioinform. 2008;9:340. doi: 10.1186/1471-2105-9-340. [DOI] [PMC free article] [PubMed] [Google Scholar]\n50.Zuker M., Stiegler P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information. Nucleic Acids Res. 1981;9:133\u2013148. doi: 10.1093/nar/9.1.133. [DOI] [PMC free article] [PubMed] [Google Scholar]\n51.Sato K., Mituyama T., Asai K., Sakakibara Y. Directed acyclic graph kernels for structural RNA analysis. BMC Bioinform. 2008;9:318. doi: 10.1186/1471-2105-9-318. [DOI] [PMC free article] [PubMed] [Google Scholar]\n52.Morita K., Saito Y., Sato K., Oka K., Hotta K., Sakakibara Y. Genome-wide searching with base-pairing kernel functions for noncoding RNAs: Computational and expression analysis of snoRNA families in Caenorhabditis elegans. Nucleic Acids Res. 2009;37:999\u20131009. doi: 10.1093/nar/gkn1054. [DOI] [PMC free article] [PubMed] [Google Scholar]\n53.Kato Y., Sato K., Hamada M., Watanabe Y., Asai K., Akutsu T. RactIP: Fast and accurate prediction of RNA-RNA interaction using integer programming. Bioinformatics. 2010;26:i460\u2013i466. doi: 10.1093/bioinformatics/btq372. [DOI] [PMC free article] [PubMed] [Google Scholar]\n54.Sato K., Kato Y., Akutsu T., Asai K., Sakakibara Y. DAFS: Simultaneous aligning and folding of RNA sequences via dual decomposition. Bioinformatics. 2012;28:3218\u20133224. doi: 10.1093/bioinformatics/bts612. [DOI] [PubMed] [Google Scholar]\n</code></pre> <p>Associated Data</p> <p>This section collects any data citations, data availability statements, or supplementary materials included in this article. Data Availability Statement</p> <p>Not applicable.</p> <p>Articles from Genes are provided here courtesy of Multidisciplinary Digital Publishing Institute (MDPI) ACTIONS</p> <pre><code>View on publisher site\nPDF (556.0 KB)\n</code></pre> <p>RESOURCES On this page</p> <pre><code>Abstract\n1. Introduction\n2. Methods\n3. Results\n4. Discussion\n5. Conclusions\nAcknowledgments\nAbbreviations\nAuthor Contributions\nInstitutional Review Board Statement\nInformed Consent Statement\nData Availability Statement\nConflicts of Interest\nFunding Statement\nFootnotes\nReferences\nAssociated Data\n</code></pre> <p>Follow NCBI NCBI on X (formerly known as Twitter) NCBI on Facebook NCBI on LinkedIn NCBI on GitHub NCBI RSS feed</p> <p>Connect with NLM NLM on X (formerly known as Twitter) NLM on Facebook NLM on YouTube</p> <p>National Library of Medicine 8600 Rockville Pike Bethesda, MD 20894</p> <pre><code>Web Policies\nFOIA\nHHS Vulnerability Disclosure\n\nHelp\nAccessibility\nCareers\n\nNLM\nNIH\nHHS\nUSA.gov\n</code></pre> <p>====</p> <p>Loading web-font Gyre-Pagella/Normal/Regular [MDPI Open Access Journals]</p> <pre><code>Journals Topics Information Author Services Initiatives About\n</code></pre> <p>Sign In / Sign Up Submit</p> <p>Search for Articles: Advanced</p> <p>Journals Genes Volume 13 Issue 11 10.3390/genes13112155 genes-logo Submit to this Journal Review for this Journal Propose a Special Issue Article Menu</p> <pre><code>Academic Editors\nZihua Hu\nMichel Ravelonandro\nLionel Benard\nSubscribe SciFeed\nRecommended Articles\nRelated Info Links\nMore by Authors Links\n</code></pre> <p>Article Views 2376 Citations 4</p> <pre><code>Table of Contents\n    Abstract\n    Introduction\n    Methods\n    Results\n    Discussion\n    Conclusions\n    Author Contributions\n    Funding\n    Institutional Review Board Statement\n    Informed Consent Statement\n    Data Availability Statement\n    Acknowledgments\n    Conflicts of Interest\n    Abbreviations\n    References\n</code></pre> <p>share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles first_page settings Order Article Reprints Open AccessArticle Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots by Manato Akiyama 1, Yasubumi Sakakibara 1 [ORCID] and Kengo Sato 2,* [ORCID] 1 Department of Biosciences and Informatics, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama 223-8522, Japan 2 School of System Design and Technology, Tokyo Denki University, 5 Senju Asahi-cho, Adachi-ku, Tokyo 120-8551, Japan * Author to whom correspondence should be addressed. Genes 2022, 13(11), 2155; https://doi.org/10.3390/genes13112155 Submission received: 28 September 2022 / Revised: 15 November 2022 / Accepted: 16 November 2022 / Published: 18 November 2022 (This article belongs to the Special Issue Feature Papers in RNA) Download keyboard_arrow_down Browse Figures Versions Notes</p> <p>Abstract Existing approaches to predicting RNA secondary structures depend on how the secondary structure is decomposed into substructures, that is, the architecture, to define their parameter space. However, architecture dependency has not been sufficiently investigated, especially for pseudoknotted secondary structures. In this study, we propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that do not depend on the architecture of RNA secondary structures, and then implement this approach using two maximum expected accuracy (MEA)-based decoding algorithms: Nussinov-style decoding for pseudoknot-free structures and IPknot-style decoding for pseudoknotted structures. To train the neural networks connected to each base pair, we adopt a max-margin framework, called structured support vector machines (SSVM), as the output layer. Our benchmarks for predicting RNA secondary structures with and without pseudoknots show that our algorithm outperforms existing methods in prediction accuracy. Keywords: RNA secondary structure; deep learning; pseudoknots</p> <ol> <li>Introduction The roles of functional non-coding RNAs (ncRNAs) in regulating transcription and guiding post-transcriptional modification have been recently shown to be critical in various biological processes, ranging from development and cell differentiation in healthy individuals to disease pathogenesis [1]. The well-established relationship between the primary sequence and structure of ncRNAs has motivated research aiming to elucidate the functions of ncRNAs by determining their structures. Yet, methods for experimentally determining RNA tertiary structures utilizing X-ray crystal structure analysis and nuclear magnetic resonance (NMR) are costly and labor-intensive, thus restricting their application. Accordingly, researchers often carry out computational prediction of RNA secondary structures based on the analysis of base pairs comprising nucleotides joined by hydrogen bonds. Computational approaches to RNA secondary structure prediction often utilize thermodynamic models (e.g., Turner\u2019s nearest neighbor model [2,3]) that define characteristic substructures, such as base-pair stacking and hairpin loops. In computational approaches, the free energy of each type of substructure is first empirically determined by methods such as optical melting experiments [2]. Then, the free energy of RNA secondary structures can be estimated as the sum of the free energy of their substructures. Dynamic programming can then be used to determine the optimal secondary structure that minimizes free energy for a given RNA sequence. This approach is employed by RNAfold [4], RNAstructure [5] and UNAfold [6], among other tools. As an alternative to experimental approaches, machine learning can be utilized to train scoring parameters based on the substructures constituting reference structures. This type of approach, as implemented in CONTRAfold [7,8], Simfold [9,10], ContextFold [11] and similar tools, has improved the accuracy of RNA secondary structure prediction. By integrating thermodynamic and machine-learning-based weighting approaches, MXfold avoided overfitting and achieved better performance than models based on either one alone [12]. Furthermore, interest in the use of deep learning for RNA secondary structure prediction is rapidly increasing [13,14,15]. MXfold2 used thermodynamic regularization to train a deep neural network so that the predicted folding score and free energy are as close as possible. This method showed robust prediction results in familywise cross validation, where the test dataset was structurally different from the training dataset. Another important aspect of RNA secondary structure prediction is the choice of the decoding algorithm used to find the optimal secondary structure from among all possible secondary structures. Two classic decoding algorithms are the minimum free energy (MFE) algorithm, which is used in thermodynamic approaches, and the maximum likelihood estimation (MLE) algorithm, which is used in machine-learning-based approaches. These algorithms find a secondary structure that minimizes the free energy and maximizes the probability or scoring function, respectively. Another option is a posterior decoding algorithm based on the maximum expected accuracy (MEA) principle, which is known to be an effective approach for many high-dimensional combinatorial optimization problems [16]. As researchers usually evaluate the prediction of RNA secondary structures using base-pair-wise accuracy measures, MEA-based decoding algorithms utilize posterior base-pairing probabilities that can be calculated by the McCaskill algorithm [17] or the inside\u2013outside algorithm for stochastic context-free grammars. CONTRAfold [18] and CentroidFold [19] both have MEA-based decoding algorithm implementations that successfully predict RNA secondary structures. Pseudoknots, an important structural element in RNA secondary structures, occur when at least two hydrogen bonds cross each other, and are typically drawn as two crossing arcs above a primary sequence (Figure 1). Genes 13 02155 g001 550 Figure 1. An example of pseudoknots. Many RNAs, including rRNAs, tmRNAs and viral RNAs, form pseudoknotted secondary structures [20]. Pseudoknots are known to be involved in the regulation of translation and splicing as well as ribosomal frame shifting [21,22]. Furthermore, pseudoknots support folding into 3D structures in many cases [23]. Therefore, the impact of pseudoknots cannot be ignored in the structural and functional analysis of RNAs. However, all of the aforementioned algorithms cannot consider pseudoknotted secondary structures owing to computational complexity. It has been proven that the problem of finding MFE structures including arbitrary pseudoknots is NP-hard [24,25]. Therefore, practically available algorithms for predicting pseudoknotted RNA secondary structures fall into one of the following two approaches: exact algorithms for a limited class of pseudoknots, such as PKNOTS [26], NUPACK [27,28], pknotsRG [29] and Knotty [30]; and heuristic algorithms that do not guarantee that the optimal structure will be found, such as ILM [31], HotKnots [32,33], FlexStem [34] and ProbKnot [35]. We previously developed IPknot, which enables fast and accurate prediction of RNA secondary structures with pseudoknots using integer programming [36,37]. IPknot adopts an MEA-based decoding algorithm that utilizes base-pairing probabilities combined with an approximate decomposition of a pseudoknotted structure into hierarchical pseudoknot-free structures. The prediction performance of IPknot is sufficient in terms of speed and accuracy compared with heuristic algorithms, and it is much faster than the exact algorithms. Both thermodynamic approaches and machine-learning-based approaches depend on the method by which a secondary structure is decomposed into substructures, that is, the architecture (as referred to in [38]), to define their parameter space. Turner\u2019s nearest neighbor model is the most well-studied architecture for predicting pseudoknot-free secondary structures, while the energy models for pseudoknotted secondary structures have not been sufficiently investigated, except for the Dirks\u2013Pierce model [27,28] and the Cao\u2013Chen model [39] for limited classes of pseudoknots. To our knowledge, an effective and efficient procedure to find a suitable architecture that can predict RNA secondary structures more accurately is still unknown. Here, we propose a novel algorithm to directly infer base-pairing probabilities with neural networks instead of the McCaskill algorithm or the inside\u2013outside algorithm, which both depend on the architecture of RNA secondary structures. Then, we employ the inferred base-pairing probabilities as part of a MEA-based scoring function for the two decoding algorithms: Nussinov-style decoding for pseudoknot-free structures, and IPknot-style decoding for pseudoknotted structures. To train the neural networks connected to each base pair, we adopt a max-margin framework, called structured support vector machines (SSVMs), as the output layer. We implement two types of neural networks connected to each base pair: bidirectional recursive neural networks (BiRNN) over tree structures and multilayer feedforward neural networks (FNN) with k-mer contexts around both bases in a pair. Our benchmarks for predicting RNA secondary structures with and without pseudoknots show that the prediction accuracy of our algorithm is superior to that of existing methods. The major advantages of our work are summarized as follows: (i) our algorithm enables us to accurately predict RNA secondary structures with and without pseudoknots; (ii) our algorithm assumes no prior knowledge of the architecture that defines the decomposition of RNA secondary structures and thus the corresponding parameter space.</li> <li>Methods 2.1. Preliminaries The RNA sequence structure is modeled following the setup used by Akiyama et al. [12]. First, let \u03a3 = { A , C , G , U } , and let \u03a3 * represent the set of all finite RNA sequences comprised of bases in \u03a3 . For a sequence x = x 1 x 2 \u22ef x n \u2208 \u03a3 * , let | x | represent the number of bases in x, referred to as the length of x. Let S ( x ) represent the set of all possible secondary structures formed by x. A secondary structure y \u2208 S ( x ) can be described as a | x | \u00d7 | x | binary-valued triangular matrix y = ( y i j ) i &lt; j , in which y i j = 1 if and only if bases x i and x j form a base pair linked by hydrogen bonds, including both canonical Watson\u2013Crick base pairs (i.e., G-C and A-U) and non-canonical wobble base pairs (e.g., G-U). 2.2. MEA-Based Scoring Function We employ the maximum expected accuracy (MEA)-based scoring function originally used for IPknot [36,37]. A secondary structure y \u2208 S ( x ) is assumed to be decomposable into a set of pseudoknot-free substructures ( y ( 1 ) , y ( 2 ) , \u2026 , y ( m ) ) satisfying the following two conditions: (i) y \u2208 S ( x ) can be decomposed into a mutually-exclusive set, that is, for 1 \u2264 i &lt; j \u2264 | x | , \u2211 1 \u2264 p \u2264 m y i j ( p ) \u2264 1 ; and (ii) each base pair in y ( p ) can be pseudoknotted to at least one base pair in y ( q ) for \u2200 q &lt; p . Each pseudoknot-free substructure y ( p ) is said to belong to level p. For each RNA secondary structure y \u2208 S ( x ) , there exists a positive integer m such that y is decomposable into m substructures without one or more pseudoknots (for more details, see the Supplementary Materials of [36]). Through the above decomposition, arbitrary pseudoknots can be modeled by our method. First, to construct an MEA-based scoring function, we define a gain function of y ^ \u2208 S ( x ) with respect to the correct secondary structure y \u2208 S ( x ) as follows: G \u03b3 ( y , y ^ ) = \u03b3 T P ( y , y ^ ) + T N ( y , y ^ ) = \u2211 i &lt; j \u03b3 I ( y i j = 1 ) I ( y ^ i j = 1 ) + I ( y i j = 0 ) I ( y ^ i j = 0 ) . (1) Here, \u03b3 &gt; 0 represents a base-pair weight parameter, T N and T P represent the numbers of true negatives (non-base pairs) and true positives (base pairs), respectively, and I ( c o n d i t i o n ) is an indicator function returning a value of either 1 or 0 depending on whether the c o n d i t i o n is true or false. The objective is to identify a secondary structure y ^ that maximizes the expected value of the above gain function (1) under a given probability distribution over the space S ( x ) of pseudoknotted secondary structures: E y \u2223 x [ G \u03b3 ( y , y ^ ) ] = \u2211 y \u2208 S ( x ) G \u03b3 ( y , y ^ ) P ( y \u2223 x ) . (2) Here, P ( y \u2223 x ) is the probability distribution of RNA secondary structures including pseudoknots. The \u03b3 -centroid estimator (2) has been proven to allow us to decode secondary structures accurately based on a given probability distribution [18]. Accordingly, the expected gain function (2) can be approximated as the sum of the expected gain functions for each level of pseudoknot-free substructures ( y ^ ( 1 ) , \u2026 , y ^ ( m ) ) in the decomposed set of a pseudoknotted structure y ^ \u2208 S ( x ) . Thus, a pseudoknotted structure y ^ and its decomposition ( y ^ ( 1 ) , \u2026 , y ^ ( m ) ) can be found that maximize the following expected value: E y \u2223 x [ G \u03b3 ( y , y ^ ) ] \u2243 \u2211 1 \u2264 p \u2264 m \u2211 y \u2208 S ( x ) G \u03b3 ( p ) ( y , y ^ ( p ) ) P ( y \u2223 x ) = \u2211 1 \u2264 p \u2264 m \u2211 i &lt; j ( \u03b3 ( p ) + 1 ) p i j \u2212 1 y ^ i j ( p ) + C . (3) Here, \u03b3 ( p ) &gt; 0 is a weight parameter for level p base pairs and C is a constant that is independent of y ^ (for the derivation, see the Supplementary Material of [18]). The base-pairing probability p i j represents the probability of base x i being paired with x j . As seen in Section 2.4, we employ one of three algorithms to calculate base-pairing probabilities. It should be noted that IPknot can be considered an extension of CentroidFold [18]. For the restricted case of a single decomposed level (i.e., m = 1 ), the approximate expected gain function (3) of IPknot is equivalent to CentroidFold\u2019s \u03b3 -centroid estimator. 2.3. Decoding Algorithms 2.3.1. Nussinov-Style Decoding Algorithm for Pseudoknot-Free Structures For the prediction of pseudoknot-free secondary structures, we find y ^ that maximizes the expected gain (3) with m = 1 under the following constraints on base pairs: maximize \u2211 i &lt; j ( \u03b3 + 1 ) p i j \u2212 1 y ^ i j (4) subject to \u2211 j = 1 i \u2212 1 y j i + \u2211 j = i + 1 | x | y i j \u2264 1 ( 1 \u2264 \u2200 i \u2264 | x | ) , (5) y i j + y k l \u2264 1 ( 1 \u2264 \u2200 i &lt; \u2200 k &lt; \u2200 j &lt; \u2200 l \u2264 | x | ) . (6) The constraint defined by Equation (5) means that each base x i can be paired with at most one base. The constraint defined by Equation (6) disallows pseudoknot. This integer programming (IP) problem can be solved by dynamic programming as follows, similar to the Nussinov algorithm [40], M i , j = max M i + 1 , j M i , j \u2212 1 M i + 1 , j \u2212 1 + ( \u03b3 + 1 ) p i j \u2212 1 max i &lt; k &lt; j M i , k + M k + 1 , j , (7) and then tracing back from M 1 , | x | . 2.3.2. IPknot-Style Decoding Algorithm for Pseudoknotted Structures Maximization of the approximate expected gain (3) can be solved as the following IP problem: maximize \u2211 1 \u2264 p \u2264 m \u2211 i &lt; j ( \u03b3 ( p ) + 1 ) p i j \u2212 1 y ^ i j ( p ) (8) subject to \u2211 1 \u2264 p \u2264 m \u2211 j = 1 i \u2212 1 y j i ( p ) + \u2211 j = i + 1 | x | y i j ( p ) \u2264 1 ( 1 \u2264 \u2200 i \u2264 | x | ) , y i j ( p ) + y k l ( p ) \u2264 1 (9) ( 1 \u2264 \u2200 p \u2264 m , 1 \u2264 \u2200 i &lt; \u2200 k &lt; \u2200 j &lt; \u2200 l \u2264 | x | ) , \u2211 i &lt; k &lt; j &lt; l y i j ( q ) + \u2211 k &lt; i \u2032 &lt; l &lt; j \u2032 y i \u2032 j \u2032 ( q ) \u2265 y k l ( p ) (10) ( 1 \u2264 \u2200 q &lt; \u2200 p \u2264 m , 1 \u2264 \u2200 k &lt; \u2200 l \u2264 | x | ) . (11) Note that Equation (3) requires the consideration of only base pairs y i j ( p ) with base-pairing probabilities p i j being greater than \u03b8 ( p ) = 1 / ( \u03b3 ( p ) + 1 ) . The constraint defined by Equation (9) means that each base x i can be paired with, at most, one base. The constraint defined by Equation (10) disallows pseudoknots within the same level p. The constraint defined by Equation (11) ensures that each level-p base pair is pseudoknotted to at least one base pair at each lower level q &lt; p . We set m = 2 , which is IPknot\u2019s default setting. This suggests that the predicted structure can be decomposed into two pseudoknot-free secondary structures. 2.4. Inferring Base-Paring Probabilities Our scoring function (3) described in Section 2.2 is calculated by using base-pairing probabilities p i j . In this section, we introduce two approaches for computing base-pairing probabilities. The first approach is a traditional one that is based on the probability distribution of RNA secondary structures, e.g., the McCaskill model [17] for pseudoknot-free structures and its extension to pseudoknotted structures, e.g., the Dirks\u2013Pierce model [27,28]. The second approach proposed in this paper directly calculates base-pairing probabilities using neural networks. 2.4.1. Traditional Models for Base-Pairing Probabilities The base-pairing probability p i j is defined as p i j = \u2211 y \u2208 S ( x ) I ( y i j = 1 ) P ( y \u2223 x ) (12) from a probability distribution P ( y \u2223 x ) over a set S ( x ) of secondary structures with or without pseudoknots. For predicting pseudoknot-free structures, the McCaskill model [17] can be mostly used as P ( y \u2223 x ) combined with the Nussinov-style decoding algorithm described in Section 2.3.1. The computational complexity of calculating Equation (12) for the McCaskill model is O ( | x | 3 ) for time and O ( | x | 2 ) for space when using dynamic programming. This model was implemented previously as CentroidFold [18,19]. For predicting pseudoknotted structures, we can select P ( y \u2223 x ) from among several models. A na\u00efve model could use the probability distribution with pseudoknots as well as Equation (2) in spite of high computational costs, e.g., the Dirks\u2013Pierce model [27,28] for a limited class of pseudoknots, with a computational complexity of O ( | x | 5 ) for time and O ( | x | 4 ) for space. Alternatively, we can employ a probability distribution without pseudoknots for each decomposed pseudoknot-free structure, such as the McCaskill model. Furthermore, to increase the prediction accuracy, we can utilize a heuristic algorithm with iterative refinement that refines the base-pairing probability matrix from the distribution without pseudoknots. See [36] for more details. These three models were implemented in IPknot [36]. 2.4.2. Neural Network Models In this research, we propose two neural network architectures for calculating base-pairing probabilities instead of the probability distribution over all RNA secondary structures. The first architecture is the bidirectional recursive neural network (BiRNN) over tree structures as shown in Figure 2. Stochastic context-free grammars (SCFG) can model RNA secondary structure without pseudoknots [7,41]. The layers of BiRNN over the tree structure are connected along grammatical trees derived from SCFG that models RNA secondary structures. The BiRNN consists of three matrices\u2014(a) the inside RNN matrix, (b) the outside RNN matrix and (c) the inside\u2013outside matrix\u2014for outputting base-pairing probabilities, each of whose elements contain a network layer (indicated by circles in Figure 2) with 80 hidden nodes. Each layer in the inside or outside matrix is recursively calculated from connected source layers as in the inside or outside algorithm, respectively, for stochastic context-free grammars (SCFG). The ReLU activation function is applied before being input to each recursive node. The base-pairing probability at each position is calculated from the corresponding layers in the inside and outside matrices with the sigmoid activation function. Our implementation of BiRNN assumes a simple RNA grammar S \u2192 a S a ^ \u2223 a S \u2223 S a \u2223 S S \u2223 \u03f5 , where a \u2208 \u03a3 , a and a ^ represent the paired bases, S represents the start non-terminal symbol, and \u03f5 represents the empty string. Genes 13 02155 g002 550 Figure 2. A bidirectional recursive neural network for calculating base-pairing probabilities. A set of four dots above each base represents the one-hot representation of the base. Each circle indicates a network layer with 80 hidden nodes. Each solid arrow indicate a connection between layers along grammatical trees derived from the RNA grammar. Each dashed arrow represents a connection that aggregates the inside and outside layers to output base-pairing probabilities. The second architecture employs a simple multilayer feedforward neural network (FNN). To calculate the base-pairing probability p i j , a FNN receives as input two k-mers around the i-th and j-th bases as shown in Figure 3. Genes 13 02155 g003 550 Figure 3. A feedforward neural network with k ( = 9 ) -mer contexts around x i and x j used to calculate the base-pairing probability p i j . The end-of-loop nodes of the highlighted nucleotides are activated because they are beyond the paired bases. Each base is encoded by the one-hot encoding of nucleotides and an additional node that indicates the end of the loop, which should be active for x l s.t. l \u2265 j in the left k-mer around x i or x l s.t. l \u2264 i in the right k-mer around x j . This encoding can be expected to embed the length of loops and the contexts around the openings and closings of helices. We set k = 81 for the k-mer context length default (for more details, see Section 3.4). We then construct two hidden layers consisting of 200 and 50 nodes, respectively, with the ReLU activation function and one output node with a sigmoid activation function to output base-pairing probabilities. Note that the FNN model depends on no assumption of RNA secondary structures, while the BiRNN model assumes an RNA grammar that considers no pseudoknots. Instead, the FNN model can take longer contexts around each base pair into consideration by using longer k-mers. 2.5. Learning Algorithm We optimize the network parameters \u03bb by using a max-margin framework called a structured support vector machine (SSVM) [42]. For a training dataset D = { ( x ( k ) , y ( k ) ) } k = 1 K , where x ( k ) represents the k-th RNA sequence and y ( k ) \u2208 S ( x ( k ) ) represents the correct secondary structure of the k-th sequence x ( k ) , we identify a \u03bb that minimizes the objective function L ( \u03bb ) = \u2211 ( x , y ) \u2208 D max y ^ \u2208 S ( x ) f ( x , y ^ ) + \u0394 ( y , y ^ ) \u2212 f ( x , y ) , (13) where f ( x , y ) is the scoring function of RNA secondary structure y \u2208 S ( x ) for a given RNA sequence x \u2208 \u03a3 * , that is, Equation (4) for Nussinov-style decoding or Equation (8) for IPknot-style decoding. Here, \u0394 ( y , y ^ ) is a loss function of y ^ for y defined as \u0394 ( y , y ^ ) = \u03b4 FN \u00d7 ( # of false negative base pairs ) + \u03b4 FP \u00d7 ( # of false positive base pairs ) = \u03b4 FN \u2211 i &lt; j I ( y i j = 1 ) I ( y ^ i j = 0 ) + \u03b4 FP \u2211 i &lt; j I ( y i j = 0 ) I ( y ^ i j = 1 ) , (14) where \u03b4 FN and \u03b4 FP are tunable hyperparameters that can control the trade-off between sensitivity and specificity in learning the parameters. By default, we used \u03b4 FN = \u03b4 FP = 0.1 . In this case, the first term of Equation (13) can be calculated using the Nussinov-style decoding algorithm or the IPknot-style decoding algorithm modified by loss-augmented inference [42]. To minimize the objective function (13), stochastic subgradient descent (Algorithm 1) or one of its variants can be applied. We can calculate the gradients with regard to the network parameters \u03bb for the objective function (13) using the gradients with regard to p i j by the chain rule of differentiation. This means that the prediction errors occurred through the decoding algorithm backpropagating to the neural network that calculates base-pairing probabilities through the connected base pairs. Algorithm 1 The stochastic subgradient descent algorithm for structured support vector machines (SSVMs); \u03b7 &gt; 0 is the predefined learning rate.</li> </ol> <p>1:     initialize \u03bb k for all \u03bb k \u2208 \u03bb  2:     repeat 3:     \u2003\u2003for all ( x , y ) \u2208 D  do 4:     \u2003\u2003\u2003\u2003 y ^ \u2190 arg max y ^ f ( x , y ^ ) + \u0394 ( y , y ^ )  5:     \u2003\u2003\u2003\u2003for all  \u03bb k \u2208 \u03bb  do 6:     \u2003\u2003\u2003\u2003\u2003\u2003 \u03bb k \u2190 \u03bb k \u2212 \u03b7 ( \u03b3 + 1 ) \u2211 i &lt; j \u2202 p i j \u2202 \u03bb k ( y ^ i j \u2212 y i j )  7:     \u2003\u2003\u2003\u2003end for 8:     \u2003\u2003end for 9:     until all the parameters converge</p> <ol> <li>Results 3.1. Implementation Our algorithm is implemented as the program Neuralfold, which is short for the neural network-based RNA folding algorithm. We employ Chainer [43] for the neural networks and the Python linear programming solver PuLP [44]. The source code for this implementation is available at https://github.com/keio-bioinformatics/neuralfold/, (accessed on 27 September 2022). 3.2. Datasets We evaluated our algorithm with the Nussinov-style decoding algorithm for predicting pseudoknot-free RNA secondary structures using four datasets, TrainSetA, TestSetA, TrainSetB and TestSetB, which were established by [45]. TrainSetA and TestSetA are literature-based datasets [7,9,10,41,46] that were constructed to ensure sequence diversity. TrainSetA contains SSU and LSU domains, SRP RNAs, RNase P RNAs and tmRNAs comprising 3166 total sequences spanning 630,279 nt, with 333,466 forming base pairs (47.9%). The sequence lengths range from 10 to 734 nt, with an average length of 199 nt. TestSetA includes sequences from eight RNA families: 5S rRNA, group I and II introns, RNase P RNA, SRP RNA, tmRNA, tRNA, and telomerase RNA. TestSetA contains 697 sequences, with 51.7% of their bases forming base pairs. The sequence length ranges from 10 to 768 nt, with an average length of 195 nt. We excluded a number of sequences that contain pseudoknotted secondary structures in the original data sources from TestSetA. Thus, 593 sequences were selected as TestSetA. TrainSetB and TestSetB, which contain 22 families with 3D structures [38], were assembled from Rfam [47]. TrainSetB and TestSetB include sequences from Rfam seed alignments with no more than 70% shared identity between sequences. TrainSetB comprises 22 RNA families, and its specific composition is 145.8S rRNAs, 18 U1 spliceosomal RNAs, 45 U4 spliceosomal RNAs, 233 riboswitches (from seven different families), 116 cis-regulatory elements (from nine different families), 3 ribozymes and a single bacteriophage pRNA. TrainSetB was constructed by selecting sequences dissimilar to those in TestSetB. TrainSetB contains 1094 sequences, including 112,398 nt in all, of which 52,065 bases (46.3%) formed base pairs. The sequence length is in the range of 27 to 237 nt with an average length of 103 nt. TrainSetB contains 4.3% noncanonical base pairs. TestSetB also consists of the same 22 RNA families as TrainSetB, TestSetB contains 430 sequences, including 52,097 nt in all, of which 22,728 bases (43.6%) form base pairs. The sequence length is in the range of 27 to 244 nt, with an average length of 121 nt. TestSetB contains 8.3% noncanonical base pairs. We also evaluated our algorithm with the IPknot-style decoding algorithm for predicting pseudoknotted RNA secondary structures on two datasets. The first dataset is called the pk168 dataset [48], which was compiled from PseudoBase [20]. This dataset includes 16 categories of 168 pseudoknotted sequences with lengths &lt;140 nt. The second dataset is called RS-pk388, originally established by [36]. This dataset was obtained from the RNA STRAND database and contains 388 non-redundant sequences with lengths between 140 and 500 nt. 3.3. Prediction Performance We evaluated the accuracy of RNA secondary structure predictions based on sensitivity ( S E N ) and positive predictive value ( P P V ) as follows: S E N = T P T P + F N , P P V = T P T P + F P . Here, T P , F P and F N represent the numbers of true positives (i.e., the correctly predicted base pairs), false positives (i.e., incorrectly predicted base pairs), and false negatives (i.e., base pairs in the correct structure that were not predicted), respectively. As a balanced measure of S E N and P P V , we utilized their F-value, which is defined as their harmonic mean: F = 2 \u00d7 S E N \u00d7 P P V S E N + P P V . We conducted computational experiments on the datasets described in the previous section using the Nussinov-style decoding algorithm with the McCaskill and neural network models as well as the BiRNN and FNN models. We employed CentroidFold as the Nussinov decoding algorithm with the McCaskill model. We performed experiments on TestSetB using the parameters trained from TrainSetB. As shown in Table 1, the neural network models achieved better accuracy compared with the traditional model. Hereafter, we adopt the FNN model with k-mer contexts as the default Neuralfold model since it yielded better prediction accuracy in this experiment. Table 1. Accuracy of inferred base-pairing probabilities for TestSetB. Table The other computational experiments on the pseudoknotted dataset were conducted using the IPknot-style decoding algorithm with the McCaskill model with and without iterative refinement and with the Dirks\u2013Pierce model as well as using Neuralfold with the FNN model. Table 2 shows that the feedforward neural network (FNN) model with 10-fold cross validation is comparable to IPknot with the Dirks\u2013Pierce model for pseudoknots but superior to the McCaskill model both with and without iterative refinement. Table 2. Accuracy of inferred base-pairing probabilities for the pk168 dataset. Table Table 3 shows the computation time for of the following sequences, which vary in length: PKB229 and PKB134 in the pk168 dataset; ASE_00193, CRW_00614 and CRW_00774 in the RNA STRAND database [49]. Table 3. Computation time for calculating base-pairing probabilities of sequences of various lengths. Table This shows that the computation time for predicting a pseudoknotted secondary structure using the FNN model is comparably fast to IPknot with the Dirks\u2013Pierce model. 3.4. Effects of Context Length We evaluated the prediction accuracy obtained with the FNN model on the TestSetB and pk168 datasets for several lengths of k-mers input to neural networks. The accuracy as measured by S E N , P P V , and their F-value for different k-mer lengths k = { 3 , 7 , 11 , 15 , 19 , 21 , 41 , 61 , 81 , 101 , 121 } is summarized in Figure 4. This analysis indicates that the accuracy is essentially maximized when the k-mer length is 81, and the difference in the accuracy for k \u2265 81 is negligible. Genes 13 02155 g004 550 Figure 4. The accuracy of the FNN model with different lengths of k-mers on the TestSetB dataset (left) and the pk168 dataset (right). S E N , sensitivity; P P V , positive predictive value; F, the F-value based on S E N and P P V . 3.5. Comparison with Previous Methods for Prediction of Pseudoknot-Free Secondary Structures We compared our algorithm with previous methods for predicting pseudoknot-free RNA secondary structures including CentroidFold [18,19], CONTRAfold [7,8], RNAfold in the Vienna RNA package [4] and ContextFold [29]. For the posterior decoding methods with the trade-off parameter \u03b3 in Equation (4), we used \u03b3 \u2208 { 2 n \u2223 n \u2208 Z , \u2212 5 \u2264 n \u2264 10 } . We performed secondary structure prediction on TestSetA with parameters trained on TrainSetA as well as prediction on TestSetB with the parameters trained on TrainSetB. The PPV\u2013SEN plots for each method shown in Figure 5 indicate that our algorithm accurately predicts pseudoknot-free secondary structures in the datasets including famlilies similar with the training datasets. Genes 13 02155 g005 550 Figure 5. Positive predictive value\u2013sensitivity (PPV\u2013SEN) plots comparing our algorithm with competitive methods on TestSetA (Left) and TestSetB (Right). On the other hand, to investigate the generalization ability of our method, another experiment in which our method was trained on TrainSetB and evaluated for accuracy on TestSetA showed that our method had very low accuracy ( S E N = 0.232 , P P V = 0.160 , and F = 0.189 ), which suggests that our method is severely overfitted. 3.6. Comparison with Alternative Methods for Predicting Pseudoknotted Secondary Structures We also compared our algorithm with competing methods for predicting pseudoknotted secondary structures, including IPknot [36], HotKnots [32,33], and pknotsRG [29], as well as methods for predicting pseudoknot-free secondary structures, including CentroidFold [19] and RNAfold [4]. Neuralfold performed 10-fold cross validation on the pk168 and RS-pk388 datasets. Figure 6 shows PPV\u2013SEN plots for each method, indicating that our algorithm works accurately on pseudoknotted datasets. Genes 13 02155 g006 550 Figure 6. Positive predictive value\u2013sensitivity (PPV\u2013SEN) plots comparing our algorithm with competitive methods on the pk168 dataset (Left) and the RS-pk388 dataset (Right). For the pk168 dataset, we set \u03b3 ( 1 ) = 1 , \u03b3 ( 2 ) = 3 for Neuralfold; \u03b3 ( 1 ) = 2 , \u03b3 ( 2 ) = 4 for IPknot with the Dirks\u2013Pierce (D&amp;P) model; \u03b3 ( 1 ) = 2 , \u03b3 ( 2 ) = 16 for IPknot with/without refinement; \u03b3 = 2 for CentroidFold. For the RS-pk388 dataset, we set \u03b3 ( 1 ) = 1 , \u03b3 ( 2 ) = 3 for Neuralfold; \u03b3 ( 1 ) = 2 , \u03b3 ( 2 ) = 2 for IPknot without refinement; \u03b3 ( 1 ) = 1 , \u03b3 ( 2 ) = 1 for IPknot with refinement; \u03b3 = 2 for CentroidFold.</li> <li>Discussion We propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that enables us to predict RNA secondary structures accurately. Sato et al. [36] previously proposed an iterative algorithm that refines the base-pairing probabilities calculated by the McCaskill algorithm so as to be appropriate for pseudoknotted secondary structure prediction. The direct inference of base-pairing probabilities with neural networks is an approach similar to the iterative refinement algorithm in the sense that both directly update base-pairing probabilities, and the IPknot-style decoding algorithm then uses the base-pairing probabilities. Although the iterative refinement algorithm can improve the prediction accuracy of IPknot to some extent, it should be noted that this is an ad hoc algorithm, as there is no theoretical guarantee of improvement. Meanwhile, the neural networks that infer base-pairing probabilities are trained on given reference secondary structures by the max-margin framework, meaning that we can theoretically expect that the neural network models improve the secondary structure prediction. Indeed, Table 2 shows that our algorithm achieved not only better accuracy than the iterative refinement algorithm, but is also comparable to that of the Dirks\u2013Pierce model, which can calculate exact base-pairing probabilities for a limited class of pseudoknots. Recently, several methods for predicting RNA secondary structure using deep learning were proposed [13,14,15]. Although most of them use deep learning to compute N \u00d7 N matrices (N is the sequence length), which can be regarded as base-pairing probability matrices, they do not directly address the constraints that the RNA secondary structure must satisfy (e.g., Equations (5) and (6) for pseudoknot-free structures, and Equations (9)\u2013(11) for pseudoknotted structures). On the other hand, MXfold2 [14] combines the Zuker-style dynamic programming [50] and deep learning to handle the constraints that pseudoknot-free RNA secondary structures must satisfy. UFold [15] predicts RNA secondary structure including pseudoknots using post-processing by linear programming, but does not directly address constraints on RNA secondary structure including pseudoknots when training deep learning models to predict base-pairing probabilities. By combining IPknot-style decoding with the max-margin training, the proposed Neuralfold can directly handle the constraints (9)\u2013(11) that pseudoknotted RNA secondary structure must satisfy, not only when predicting secondary structures, but also when training deep learning models. It has been pointed out that RNA secondary structure prediction based on machine learning and deep learning is prone to overfitting due to bias in the training data [14,45]. Several methods have been proposed to alleviate overfitting, such as using ensembles of multiple models [13], and integration with thermodynamic models [14]. UFold, on the other hand, employed artificially generated sequences and their predicted secondary structures for data augmentation, which were then used as additional training data to relax overfitting due to bias in the training data [15]. Our proposed method does not provide a strategy to counteract such overfitting and is therefore unsatisfactory at predicting sequences of families that are structurally distant from the training data, as shown in the results. However, by utilizing the ensembles of multiple models, as in SPOT-RNA, and the data augmentation strategy, as in UFold, it is expected to address to some extent the overfitting caused by bias in the training data. The FNN model takes two k-mers around each base pair as input to infer its base-pairing probability, where k is the context length to model the length of loops and the contexts around the openings and closings of helices. As can be seen in Figure 7, different k-mer context lengths affect the prediction of pseudoknotted secondary structures. For example, consider the input bases when calculating the base-pairing probability of the blue-highlighted base pair (AU) using the FNN model. The FNN model with the context length k = 11 takes as input five bases in both the upstream and downstream directions from bases i and j. As seen in Figure 7 (bottom), the distances from bases A and U are 10 and 13 to Stem 2, respectively. This means that all the bases comprising Stem 2 are not completely located within the context length k = 11 around the base pair AU. On the other hand, for the FNN model with context length k = 41, all the bases of Stem 2 are completely located within the context around the base pair AU. This leads the FNN model to correctly predict the base pair AU, suggesting that a longer context length enables consideration of the dependency between stems in pseudoknotted substructures. Genes 13 02155 g007 550 Figure 7. (Top) Comparison between the reference structure of ID PKB189 (top-left) and the predicted structures with context lengths k = 11 (top-middle) and k = 41 (top-right). (Bottom) Distance between two stems (Stem 1 and Stem 2) in the pseudoknotted structure.</li> <li> <p>Conclusions We propose a novel algorithm for directly inferring base-pairing probabilities with neural networks that enables us to accurately predict RNA secondary structures with pseudoknots. By combining IPknot-style decoding with the max-margin framework, our algorithm trains the model in the end-to-end manner to compute base-pairing probabilities under the constraints that RNA secondary structures, including pseudoknots, must satisfy. HotKnots 2.0 [32], on the other hand, finds a pseudoknotted secondary structure by using an MFE-based heuristic decoding algorithm with energy parameters of the Dirks\u2013Pierce model or the Cao\u2013Chen model trained on pseudoknotted reference structures. One of the advantages of our algorithm over HotKnots 2.0 is that no assumption about the architecture of RNA secondary structures is required. In other words, our model can be trained on arbitrary classes of pseudoknots, while HotKnots cannot be trained on more complicated classes of pseudoknots than the one assumed by the model. Furthermore, our algorithm can compute base-pairing probabilities, which can be used in various applications of RNA informatics, such as family classification [51,52], RNA\u2013RNA interaction prediction [53] and simultaneous aligning and folding [54]. Accurate base-pairing probabilities calculated by our algorithm can improve the quality of such applications. Author Contributions Conceptualization, M.A. and K.S.; methodology, M.A. and K.S.; software, M.A.; validation, M.A. and K.S.; writing\u2014original draft preparation, M.A.; writing\u2014review and editing, K.S.; supervision, Y.S. and K.S.; project administration, K.S.; funding acquisition, K.S. All authors have read and agreed to the published version of the manuscript. Funding This work was supported in part by a Grant-in-Aid for Scientific Research (KAKENHI) (16K00404, 19H04210 and 19K22897) from the Japan Society for the Promotion of Science (JSPS) to K.S. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement Not applicable. Acknowledgments The supercomputer system was provided by the National Institute of Genetics (NIG), Research Organization of Information and Systems (ROIS). Conflicts of Interest The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results. Abbreviations The following abbreviations are used in this manuscript: BiRNN   bi-directional recurrent neural network FNN feedforward neural network MEA maximum expected accuracy MFE minimum free energy ncRNA   non-coding RNA SSVM    structured support vector machine References</p> <p>Hirose, T.; Mishima, Y.; Tomari, Y. Elements and machinery of non-coding RNAs: Toward their taxonomy. EMBO Rep. 2014, 15, 489\u2013507. [Google Scholar] [CrossRef] [PubMed] [Green Version] Schroeder, S.J.; Turner, D.H. Optical melting measurements of nucleic acid thermodynamics. Meth. Enzymol. 2009, 468, 371\u2013387. [Google Scholar] Turner, D.H.; Mathews, D.H. NNDB: The nearest neighbor parameter database for predicting stability of nucleic acid secondary structure. Nucleic Acids Res. 2010, 38, D280\u2013D282. [Google Scholar] [CrossRef] Lorenz, R.; Bernhart, S.H.; Honer Zu Siederdissen, C.; Tafer, H.; Flamm, C.; Stadler, P.F.; Hofacker, I.L. ViennaRNA Package 2.0. Algorithms Mol. Biol. 2011, 6, 26. [Google Scholar] [CrossRef] [PubMed] Reuter, J.S.; Mathews, D.H. RNAstructure: Software for RNA secondary structure prediction and analysis. BMC BioInform. 2010, 11, 129. [Google Scholar] [CrossRef] [PubMed] [Green Version] Zuker, M. On finding all suboptimal foldings of an RNA molecule. Science 1989, 244, 48\u201352. [Google Scholar] [CrossRef] [PubMed] [Green Version] Do, C.B.; Woods, D.A.; Batzoglou, S. CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinformatics 2006, 22, e90\u2013e98. [Google Scholar] [CrossRef] [Green Version] Do, C.B.; Foo, C.S.; Ng, A. Efficient multiple hyperparameter learning for log-linear models. In Proceedings of the 20th International Conference on Neural Information Processing Systems, Vancouver, BC, Canada, 3\u20136 December 2007; Advances in Neural Information Processing Systems 20. Curran Associates Inc.: Red Hook, NY, USA, 2007. [Google Scholar] Andronescu, M.; Condon, A.; Hoos, H.H.; Mathews, D.H.; Murphy, K.P. Efficient parameter estimation for RNA secondary structure prediction. Bioinformatics 2007, 23, 19\u201328. [Google Scholar] [CrossRef] [Green Version] Andronescu, M.; Condon, A.; Hoos, H.H.; Mathews, D.H.; Murphy, K.P. Computational approaches for RNA energy parameter estimation. RNA 2010, 16, 2304\u20132318. [Google Scholar] [CrossRef] [Green Version] Zakov, S.; Goldberg, Y.; Elhadad, M.; Ziv-Ukelson, M. Rich parameterization improves RNA structure prediction. J. Comput. Biol. 2011, 18, 1525\u20131542. [Google Scholar] [CrossRef] [PubMed] [Green Version] Akiyama, M.; Sato, K.; Sakakibara, Y. A max-margin training of RNA secondary structure prediction integrated with the thermodynamic model. J. Bioinform. Comput. Biol. 2018, 16, 1840025. [Google Scholar] [CrossRef] [PubMed] Singh, J.; Hanson, J.; Paliwal, K.; Zhou, Y. RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nat. Commun. 2019, 10, 5407. [Google Scholar] [CrossRef] [Green Version] Sato, K.; Akiyama, M.; Sakakibara, Y. RNA secondary structure prediction using deep learning with thermodynamic integration. Nat. Commun. 2021, 12, 941. [Google Scholar] [CrossRef] Fu, L.; Cao, Y.; Wu, J.; Peng, Q.; Nie, Q.; Xie, X. UFold: Fast and accurate RNA secondary structure prediction with deep learning. Nucleic Acids Res. 2022, 50, e14. [Google Scholar] [CrossRef] [PubMed] Carvalho, L.E.; Lawrence, C.E. Centroid estimation in discrete high-dimensional spaces with applications in biology. Proc. Natl. Acad. Sci. USA 2008, 105, 3209\u20133214. [Google Scholar] [CrossRef] [PubMed] [Green Version] McCaskill, J.S. The equilibrium partition function and base pair binding probabilities for RNA secondary structure. Biopolymers 1990, 29, 1105\u20131119. [Google Scholar] [CrossRef] Hamada, M.; Kiryu, H.; Sato, K.; Mituyama, T.; Asai, K. Prediction of RNA secondary structure using generalized centroid estimators. Bioinformatics 2009, 25, 465\u2013473. [Google Scholar] [CrossRef] [Green Version] Sato, K.; Hamada, M.; Asai, K.; Mituyama, T. CENTROIDFOLD: A web server for RNA secondary structure prediction. Nucleic Acids Res. 2009, 37, W277\u2013W280. [Google Scholar] [CrossRef] [Green Version] van Batenburg, F.H.; Gultyaev, A.P.; Pleij, C.W. PseudoBase: Structural information on RNA pseudoknots. Nucleic Acids Res. 2001, 29, 194\u2013195. [Google Scholar] [CrossRef] [Green Version] Staple, D.W.; Butcher, S.E. Pseudoknots: RNA structures with diverse functions. PLoS Biol. 2005, 3, e213. [Google Scholar] [CrossRef] [Green Version] Brierley, I.; Pennell, S.; Gilbert, R.J. Viral RNA pseudoknots: Versatile motifs in gene expression and replication. Nat. Rev. Microbiol. 2007, 5, 598\u2013610. [Google Scholar] [CrossRef] [PubMed] Fechter, P.; Rudinger-Thirion, J.; Florentz, C.; Giege, R. Novel features in the tRNA-like world of plant viral RNAs. Cell. Mol. Life Sci. 2001, 58, 1547\u20131561. [Google Scholar] [CrossRef] [PubMed] Akutsu, T. Dynamic programming algorithms for RNA secondary structure prediction with pseudoknots. Discret. Appl. Math. 2000, 104, 45\u201362. [Google Scholar] [CrossRef] [Green Version] Lyngs\u00f8, R.B.; Pedersen, C.N. RNA pseudoknot prediction in energy-based models. J. Comput. Biol. 2000, 7, 409\u2013427. [Google Scholar] [CrossRef] Rivas, E.; Eddy, S.R. A dynamic programming algorithm for RNA structure prediction including pseudoknots. J. Mol. Biol. 1999, 285, 2053\u20132068. [Google Scholar] [CrossRef] Dirks, R.M.; Pierce, N.A. A partition function algorithm for nucleic acid secondary structure including pseudoknots. J. Comput. Chem. 2003, 24, 1664\u20131677. [Google Scholar] [CrossRef] [Green Version] Dirks, R.M.; Pierce, N.A. An algorithm for computing nucleic acid base-pairing probabilities including pseudoknots. J. Comput. Chem. 2004, 25, 1295\u20131304. [Google Scholar] [CrossRef] Reeder, J.; Giegerich, R. Design, implementation and evaluation of a practical pseudoknot folding algorithm based on thermodynamics. BMC Bioinform. 2004, 5, 104. [Google Scholar] [CrossRef] [Green Version] Jabbari, H.; Wark, I.; Montemagno, C.; Will, S. Knotty: Efficient and Accurate Prediction of Complex RNA Pseudoknot Structures. Bioinformatics 2018, 34, 3849\u20133856. [Google Scholar] [CrossRef] Ruan, J.; Stormo, G.D.; Zhang, W. An iterated loop matching approach to the prediction of RNA secondary structures with pseudoknots. Bioinformatics 2004, 20, 58\u201366. [Google Scholar] [CrossRef] [Green Version] Andronescu, M.S.; Pop, C.; Condon, A.E. Improved free energy parameters for RNA pseudoknotted secondary structure prediction. RNA 2010, 16, 26\u201342. [Google Scholar] [CrossRef] [PubMed] Ren, J.; Rastegari, B.; Condon, A.; Hoos, H.H. HotKnots: Heuristic prediction of RNA secondary structures including pseudoknots. RNA 2005, 11, 1494\u20131504. [Google Scholar] [CrossRef] [PubMed] [Green Version] Chen, X.; He, S.M.; Bu, D.; Zhang, F.; Wang, Z.; Chen, R.; Gao, W. FlexStem: Improving predictions of RNA secondary structures with pseudoknots by reducing the search space. Bioinformatics 2008, 24, 1994\u20132001. [Google Scholar] [CrossRef] [Green Version] Bellaousov, S.; Mathews, D.H. ProbKnot: Fast prediction of RNA secondary structure including pseudoknots. RNA 2010, 16, 1870\u20131880. [Google Scholar] [CrossRef] [Green Version] Sato, K.; Kato, Y.; Hamada, M.; Akutsu, T.; Asai, K. IPknot: Fast and accurate prediction of RNA secondary structures with pseudoknots using integer programming. Bioinformatics 2011, 27, 85\u201393. [Google Scholar] [CrossRef] [Green Version] Sato, K.; Kato, Y. Prediction of RNA secondary structure including pseudoknots for long sequences. Brief. Bioinform. 2022, 23, bbab395. [Google Scholar] [CrossRef] [PubMed] Rivas, E. The four ingredients of single-sequence RNA secondary structure prediction. A unifying perspective. RNA Biol. 2013, 10, 1185\u20131196. [Google Scholar] [CrossRef] [Green Version] Cao, S.; Chen, S.J. Predicting RNA pseudoknot folding thermodynamics. Nucleic Acids Res. 2006, 34, 2634\u20132652. [Google Scholar] [CrossRef] [PubMed] Nussinov, R.; Pieczenick, G.; Griggs, J.; Kleitman, D. Algorithms for loop matching. SIAM J. Appl. Math. 1978, 35, 68\u201382. [Google Scholar] [CrossRef] Dowell, R.D.; Eddy, S.R. Evaluation of several lightweight stochastic context-free grammars for RNA secondary structure prediction. BMC Bioinform. 2004, 5, 71. [Google Scholar] [CrossRef] [Green Version] Tsochantaridis, I.; Joachims, T.; Hofmann, T.; Altun, Y. Large Margin Methods for Structured and Interdependent Output Variables. J. Mach. Learn. Res. 2005, 6, 1453\u20131484. [Google Scholar] Tokui, S.; Oono, K.; Hido, S.; Clayton, J. Chainer: A Next-Generation Open Source Framework for Deep Learning. In Proceedings of the Workshop on Machine Learning Systems (LearningSys) in The Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS), Montr\u00e9al, QC, Canada, 11\u201312 December 2015. [Google Scholar] Mitchell, S.; Consulting, S.M.; O\u2019sullivan, M.; Dunning, I. PuLP: A Linear Programming Toolkit for Python. 2011. Available online: https://optimization-online.org/2011/09/3178/ (accessed on 27 September 2022). Rivas, E.; Lang, R.; Eddy, S.R. A range of complex probabilistic models for RNA secondary structure prediction that includes the nearest-neighbor model and more. RNA 2012, 18, 193\u2013212. [Google Scholar] [CrossRef] [PubMed] Lu, Z.J.; Gloor, J.W.; Mathews, D.H. Improved RNA secondary structure prediction by maximizing expected pair accuracy. RNA 2009, 15, 1805\u20131813. [Google Scholar] [CrossRef] [PubMed] [Green Version] Gardner, P.P.; Daub, J.; Tate, J.; Moore, B.L.; Osuch, I.H.; Griffiths-Jones, S.; Finn, R.D.; Nawrocki, E.P.; Kolbe, D.L.; Eddy, S.R.; et al. Rfam: Wikipedia, clans and the \u201cdecimal\u201d release. Nucleic Acids Res. 2011, 39, D141\u2013D145. [Google Scholar] [CrossRef] [Green Version] Huang, X.; Ali, H. High sensitivity RNA pseudoknot prediction. Nucleic Acids Res. 2007, 35, 656\u2013663. [Google Scholar] [CrossRef] [Green Version] Andronescu, M.; Bereg, V.; Hoos, H.H.; Condon, A. RNA STRAND: The RNA secondary structure and statistical analysis database. BMC Bioinform. 2008, 9, 340. [Google Scholar] [CrossRef] [Green Version] Zuker, M.; Stiegler, P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information. Nucleic Acids Res. 1981, 9, 133\u2013148. [Google Scholar] [CrossRef] Sato, K.; Mituyama, T.; Asai, K.; Sakakibara, Y. Directed acyclic graph kernels for structural RNA analysis. BMC Bioinform. 2008, 9, 318. [Google Scholar] [CrossRef] [Green Version] Morita, K.; Saito, Y.; Sato, K.; Oka, K.; Hotta, K.; Sakakibara, Y. Genome-wide searching with base-pairing kernel functions for noncoding RNAs: Computational and expression analysis of snoRNA families in Caenorhabditis elegans. Nucleic Acids Res. 2009, 37, 999\u20131009. [Google Scholar] [CrossRef] [Green Version] Kato, Y.; Sato, K.; Hamada, M.; Watanabe, Y.; Asai, K.; Akutsu, T. RactIP: Fast and accurate prediction of RNA-RNA interaction using integer programming. Bioinformatics 2010, 26, i460\u2013i466. [Google Scholar] [CrossRef] [Green Version] Sato, K.; Kato, Y.; Akutsu, T.; Asai, K.; Sakakibara, Y. DAFS: Simultaneous aligning and folding of RNA sequences via dual decomposition. Bioinformatics 2012, 28, 3218\u20133224. [Google Scholar] [CrossRef] [PubMed]</p> </li> </ol> <p>Publisher\u2019s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p> <p>\u00a9 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style</p> <p>Akiyama, M.; Sakakibara, Y.; Sato, K. Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots. Genes 2022, 13, 2155. https://doi.org/10.3390/genes13112155 AMA Style</p> <p>Akiyama M, Sakakibara Y, Sato K. Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots. Genes. 2022; 13(11):2155. https://doi.org/10.3390/genes13112155 Chicago/Turabian Style</p> <p>Akiyama, Manato, Yasubumi Sakakibara, and Kengo Sato. 2022. \"Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots\" Genes 13, no. 11: 2155. https://doi.org/10.3390/genes13112155 APA Style</p> <p>Akiyama, M., Sakakibara, Y., &amp; Sato, K. (2022). Direct Inference of Base-Pairing Probabilities with Neural Networks Improves Prediction of RNA Secondary Structures with Pseudoknots. Genes, 13(11), 2155. https://doi.org/10.3390/genes13112155 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. Article Metrics Citations Crossref</p> <p>4 Web of Science</p> <p>3 PMC</p> <p>1 Scopus</p> <p>4 PubMed</p> <p>1 Google Scholar</p> <p>[click to view] Article Access Statistics Article access statisticsArticle Views17. Dec18. Dec19. Dec20. Dec21. Dec22. Dec23. Dec24. Dec25. Dec26. Dec27. Dec28. Dec29. Dec30. Dec31. Dec1. Jan2. Jan3. Jan4. Jan5. Jan6. Jan7. Jan8. Jan9. Jan10. Jan11. Jan12. Jan13. Jan14. Jan15. Jan16. Jan17. Jan18. Jan19. Jan20. Jan21. Jan22. Jan23. Jan24. Jan25. Jan26. Jan27. Jan28. Jan29. Jan30. Jan31. Jan1. Feb2. Feb3. Feb4. Feb5. Feb6. Feb7. Feb8. Feb9. Feb10. Feb11. Feb12. Feb13. Feb14. Feb15. Feb16. Feb17. Feb18. Feb19. Feb20. Feb21. Feb22. Feb23. Feb24. Feb25. Feb26. Feb27. Feb28. Feb1. Mar2. Mar3. Mar4. Mar5. Mar6. Mar7. Mar8. Mar9. Mar10. Mar11. Mar12. Mar13. Mar14. Mar15. Mar16. Mar050010001500200025003000 For more information on the journal statistics, click here. Multiple requests from the same IP address are counted as one view. Genes, EISSN 2073-4425, Published by MDPI RSS Content Alert Further Information Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter MDPI</p> <p>Subscribe to receive issue release notifications and newsletters from MDPI journals \u00a9 1996-2025 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy =====</p> <p>Skip to main content Skip to article Elsevier logo</p> <pre><code>Journals &amp; Books\n</code></pre> <p>Help</p> <pre><code>Search\n</code></pre> <p>My account Ripon College</p> <p>Ripon College does not subscribe to this content on ScienceDirect. Article preview</p> <pre><code>Abstract\nIntroduction\nSection snippets\nReferences (71)\n</code></pre> <p>Elsevier Computers in Biology and Medicine Volume 182, November 2024, 109207 Computers in Biology and Medicine Wfold: A new method for predicting RNA secondary structure with deep learning Author links open overlay panelYongna Yuan , Enjie Yang, Ruisheng Zhang Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.compbiomed.2024.109207 Get rights and content Highlights</p> <pre><code>\u2022\nWe develop a deep learning model Wfold for RNA secondary structure prediction.\n\u2022\nThe \u2018image\u2019 representation makes all possible long-range interactions explicitly.\n\u2022\nThe model considers all possible base pairing patterns.\n\u2022\nWfold combines Unet and transformer, which can effectively mine long-range and local information from input data.\n</code></pre> <p>Abstract Precise estimations of RNA secondary structures have the potential to reveal the various roles that non-coding RNAs play in regulating cellular activity. However, the mainstay of traditional RNA secondary structure prediction methods relies on thermos-dynamic models via free energy minimization, a laborious process that requires a lot of prior knowledge. Here, RNA secondary structure prediction using Wfold, an end-to-end deep learning-based approach, is suggested. Wfold is trained directly on annotated data and base-pairing criteria. It makes use of an image-like representation of RNA sequences, which an enhanced U-net incorporated with a transformer encoder can process effectively. Wfold eventually increases the accuracy of RNA secondary structure prediction by combining the benefits of self-attention mechanism's mining of long-range information with U-net's ability to gather local information. We compare Wfold's performance using RNA datasets that are within and across families. When trained and evaluated on different RNA families, it achieves a similar performance as the traditional methods, but dramatically outperforms the state-of-the-art methods on within-family datasets. Moreover, Wfold can also reliably forecast pseudoknots. The findings imply that Wfold may be useful for improving sequence alignment, functional annotations, and RNA structure modeling. Graphical abstract 9.27 cm high and 8.58 cm wide. Image 1</p> <pre><code>Download: Download high-res image (259KB)\nDownload: Download full-size image\n</code></pre> <p>Introduction The biology of RNA is diverse and complex. The foundations of RNA ranging from catalysis, cell-signalling, to transcriptional regulation [1] are deeply related to their structures rather than their primary sequences. The determination of RNA 3D structures can be performed by X-ray crystallography [2], NMR [3], cryo-electron microscopy [4], and other techniques. To date, there are more than 33 million RNA sequences [5] while less than 0.01 % have experimentally determined structures [6,7]. However, the major difficulties in computing RNA 3D structures have not yet been completely overcome such as high computational complexity, economic cost, and performance limits. It is tough to apply experimental methods on a large scale. RNA secondary structures, which are the backbones of 3D structures and are easier to model, are often computationally predicted. Therefore, accurate and cost-effective computational methods are highly desirable to be developed for direct prediction of RNA secondary structures from sequences, defined as a list of nucleotide bases paired by hydrogen bonding consisting of Watson-Crick base pairs (A-U, G-C) and wobble base pairs (G-U). Many RNA secondary structure prediction methods have been developed since the 1970s. These methods can be roughly divided into two categories: (i) single sequence prediction methods and (ii) comparative methods. The primary idea behind the first category is to use the minimal free energy (MFE) to look for thermodynamically stable states. Dynamic programming (DP) can effectively address the energy reduction problem if the secondary structure solely includes nested base pairing, such as those implemented in Vienna RNAfold [8], MFold [9], RNAstructure [10] and CONTRAfold [11]. Methods, such as Rfold [12], Vienna RNAplfold [13], LocalFold [14], LinearFold [15] and StemP [16], improved the speed of DP and the time efficiency is reduced from to , and then to O(n) [15]. Efficient DP algorithms that sample suboptimal secondary structures from the Boltzmann ensembles of structures have also been proposed, for example, CentroidFold [17]. Some DP-based techniques, however, fail when base pairs contain pseudoknots, which are non-nested patterns that help fold into three-dimensional structures. Pseudoknots are two stem\u2013loop structures in which half of one stem intercalates between the two halves of another stem. Predicting secondary structures with pseudoknots is hard and has shown to be NP-complete under the energy minimization framework [18]. Methods in the second category utilize covariance methods by aligning related RNA sequences and identifying correlated compensatory mutations [[19], [20], [21]]. By analyzing several sequences, these algorithms attempt to anticipate conserved structures and identify base pair placements by identifying points of base covariance within the sequences. Although the list of proposed methods in each of the two categories is long and diverse [22], the performance of these methods has not been significantly improved over time, reaching a performance ceiling of about 80 % [23]. It is probably because they fail to account for base pairing resulting from tertiary interactions [24], unstacked base pairs, pseudoknot, non-canonical base pairing, or other unknown factors [25]. Comprehending the RNA folding mechanism in its whole is really challenging. Machine learning (ML) techniques, on the other hand, are data-driven and do not require any prior knowledge of such mechanisms. Massive training data sets allow machine learning algorithms to discover the fundamental folding patterns. To increase computational performance, machine learning techniques have been used to several facets of RNA secondary structure prediction in the past few decades. Early ML-based methods usually trained basic machine learning models to predict RNA secondary structures [[26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39]], such as support vector machine (SVM) [40], multilayer perceptron (MLP) [[41], [42], [43]]. However, because of their poor accuracy, these methods did not attract much attention. The main causes of that were the tiny training dataset sizes and the shortcomings of the simpler ML models. Currently, because of the recent explosion of RNA sequence data and the rapid rise of deep learning (DL) techniques, the DL-based methods become the current mainstream methods in terms of accuracy and applicability [[44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62]]. Convolutional neural networks (CNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), Transformer, and their variations or combinations are the primary deep learning (DL) approaches utilized for RNA structure prediction. For example, SPOT-RNA [50], the first end-to-end DL model which processed and handled tasks from input to output in a seamless, automated manner, without requiring human intervention in intermediary steps, treated the RNA secondary structure as a contact table and employed an ensemble of ultra-deep hybrid networks of ResNets and 2D-BLSTMs for the prediction. SPOT-RNA showed superior performance with two RNA benchmark datasets. Recently, the SPOT-RNA2 model [53] employed evolution-derived sequence profiles and mutational coupling as inputs and outperformed SPOT-RNA for all types of base pairs using the same transfer learning approach. E2Efold [51] was another DL model for RNA secondary structure prediction. It integrated 2 coupled parts, i.e., a transformer-based deep model that encoded sequence information, and a multilayer network based on an unrolled algorithm that gradually enforced the constraints and restricted the output space. UFold [54], a DL-based method proposed for RNA secondary structure prediction, trained directly on annotated data and base-pairing rules. It applied a novel image-like representation of RNA sequences, which can be efficiently processed by Fully Convolutional Networks (FCNs). RNAformer [60] improved prediction accuracy by a two-dimensional latent space, axial attention, and recycling in the latent space. Nevertheless, current deep learning methodologies encounter multiple obstacles: Initially, the LSTM and transformer encoder components encompass an extensive array of model parameters, culminating in significant computational expenses and diminished efficiency. Secondly, amalgamating these with thermodynamic optimization techniques compels the models to adopt the presumptions intrinsic to conventional methods, potentially impairing their effectiveness. Thirdly, given that the efficacy of deep learning models is profoundly reliant on the training data distribution, it becomes crucial to devise strategies to enhance their performance across novel categories of RNA structures previously unencountered. Inspired by E2Efold and UFold, the former uses a coupled transformer model and a CNN model and the latter applies U-net model based on pure CNN, in this study, we propose an end-to-end deep learning model for predicting RNA secondary structures embedding transformer into U-net. The U-net model is good at capturing local information, while the transformer model is skilled in retaining long-term information. Ultimately, our model obtains an outstanding performance. As the model architecture presents a \u201cW\" shape, it is named Wfold. Similar to Ufold, the input sequence is handled as a 17-channel, 2D \u201cimage,\u201d which enables the model to explicitly take into account all potential base pairings and long-range interactions rather than relying only on the nucleotide sequence. We test Wfold's performance against benchmark models on RNAStralign, ArchiveII, bpRNA-1m, bpRNA-new, and within-family and cross-family datasets. Wfold performs best in terms of prediction accuracy. In summary, the advantages of Wfold are as follows:</p> <pre><code>i)\nWfold explicitly models every potential long-range interaction using an \u201cimage\u201d representation. Local base pairing appears in the image-like representation between distant sequence segments.\nii)\nWfold does not distinguish between canonical and non-canonical base pairs; instead, it takes all 16 conceivable base pairing patterns into account.\niii)\nWfold combines CNN (U-net) and transformer (multi-head self-attention) which can handle variable sequence length, eliminating the need of padding the input sequence to a fixed length.\n</code></pre> <p>Section snippets Datasets To evaluate Wfold, we perform computational experiments on four benchmark datasets: (1) RNAStralign [63], which contains 33,277 unique sequences from 8 RNA families, (2) ArchiveII [64], which contains 3966 sequences from 10 RNA families. These two datasets are the most widely used datasets for benchmarking RNA structure prediction performance recently. (3) bpRNA-1m [65], which is based on Rfam 12.2 [66] with 2588 families. After removing redundant sequences based on sequence identity, the Overview of wfold In this study, Wfold, as an end-to-end DL solution, integrates two parts mentioned in the Materials and methods section. The first part of the architecture is a model called Deep Score Network to represent RNA sequence information which is useful for RNA secondary structure prediction. The second part is a multilayer network called Post-Processing Network which gradually enforces the constraints and restricts the output space. Deep Score Network, with the combination of U-net and transformer, Conclusion This paper creates a deep neural network-based approach for RNA secondary structure prediction based just on a single RNA sequence. We present a unique DL model for RNA secondary structure prediction called Wfold, which includes strict limitations in its architecture design. Through independent testing and comparison with SOTA RNA secondary-structure prediction algorithms, Wfold obtains satisfactory results in terms of precision, recall, and F1 score. Extensive tests are carried out to Funding This study is supported by the National Natural Science Foundation of China (22373043), the Science and Technology Project of Gansu (21YF5GA102, 21YF5GA006, 21ZD8RA008, 22ZD6GA029), the Science and Technology Project of Lanzhou (2023332), the Science and Technology Plan Project of Chengguan District of Lanzhou (2023RCCX0005), and the Super Computing Center of Lanzhou University. Data and availability The main codes of methods mentioned in the study are available at https://github.com/EnjieYang/Wfold . CRediT authorship contribution statement Yongna Yuan: Supervision, Conceptualization. Enjie Yang: Writing \u2013 original draft, Data curation. Ruisheng Zhang: Writing \u2013 original draft, Supervision, Formal analysis. Declaration of competing interest None Declared. References (71)</p> <pre><code>X.D. Wang et al.\nDynamic programming for NP-hard problems\nProcess Eng.\n(2011)\nJ. Nowakowski et al.\nRNA structure and stability\nSemin. Virol.\n(1997)\nE. Westhof et al.\nRNA folding: beyond Watson\u2013Crick pairs\nStructure\n(2000)\nX. Tang\nSimulating RNA folding kinetics on approximated energy landscapes\nJ. Mol. Biol.\n(2008)\nH. Yonemoto et al.\nA semi-supervised learning approach for RNA secondary structure prediction\nComput. Biol. Chem.\n(2015)\nL. Quan\nDeveloping parallel ant colonies filtered by deep learned constrain for predicting RNA secondary structure with pseudo-knots\nNeurocomputing\n(2020)\nH. Zhang\nA new method of RNA secondary structure prediction based on convolutional neural network and dynamic programming\nFront. Genet.\n(2019)\nC. Shen\nBAT-Net: an enhanced RNA Secondary Structure prediction via bidirectional GRU-based network with attention mechanism\nComput. Biol. Chem.\n(2022)\nS. Geisler et al.\nRNA in unexpected places: long non-coding RNA functions in diverse cellular contexts\nNat. Rev. Mol. Cell Biol.\n(2013)\nE. Westhof\nTwenty years of RNA crystallography\nRNA\n(2015)\n</code></pre> <p>View more references Cited by (0) View full text \u00a9 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. Recommended articles</p> <pre><code>An efficient approach for EMG controlled pattern recognition system based on MUAP identification and segregation\nComputers in Biology and Medicine, Volume 182, 2024, Article 109169\nAnil Sharma, \u2026, Anil Kumar\nDrug-induced torsadogenicity prediction model: An explainable machine learning-driven quantitative structure-toxicity relationship approach\nComputers in Biology and Medicine, Volume 182, 2024, Article 109209\nFeyza Kelleci \u00c7elik, \u2026, G\u00fcl Karaduman\nDynamics of sit-to-stand and stand-to-sit motions based on the trajectory control of the centre of mass of the body: A bond graph approach\nComputers in Biology and Medicine, Volume 182, 2024, Article 109117\nVivek Soni, Anand Vaz\n</code></pre> <p>Show 3 more articles Article Metrics Captures</p> <pre><code>Mendeley Readers3\n</code></pre> <p>PlumX Metrics Logo View details Elsevier logo with wordmark</p> <pre><code>About ScienceDirect\n</code></pre> <p>Remote access Advertise Contact and support Terms and conditions Privacy policy</p> <p>Cookies are used by this site. Cookie settings</p> <p>All content on this site: Copyright \u00a9 2025 or its licensors and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply. RELX group home page ==== Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Cheng Tan 1 2 * Zhangyang Gao 2 1 * Hanqun Cao 3 Xingran Chen 4 Ge Wang 2 Lirong Wu 2 Jun Xia 2 Jiangbin Zheng 2 Stan Z. Li 2 Abstract The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we reformulate the RNA secondary structure prediction as a K-Rook prob- lem, thereby simplifying the prediction process into probabilistic matching within a finite solution space. Building on this innovative perspective, we introduce RFold, a simple yet effective method that learns to predict the most matching K-Rook solution from the given sequence. RFold em- ploys a bi-dimensional optimization strategy that decomposes the probabilistic matching problem into row-wise and column-wise components to reduce the matching complexity, simplifying the solving process while guaranteeing the validity of the output. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art approaches. The code is avail- able at github.com/A4Bio/RFold. 1. Introduction The functions of RNA molecules are determined by their structure (Sloma &amp; Mathews, 2016). The secondary struc- ture, which contains the nucleotide base pairing information, as shown in Figure 1, is crucial for the correct functions of RNA molecules (Fallmann et al., 2017). Although experi- mental assays such as X-ray crystallography (Cheong et al., 2004), nuclear magnetic resonance (F \u00a8urtig et al., 2003), and *Equal contribution 1Zhejiang University 2Westlake University 3The Chinese University of Hong Kong 4University of Michigan. Correspondence to: Stan Z. Li Stan.ZQ.Li@westlake.edu.cn. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).A A C C U G G U C A G G C C C G G A A G G G A G C A G C C A A A C C U G G U C A G G C C C G G A A G G G A G C A G C C A graph representation matrix representation (contact map) Figure 1. The graph and matrix representation of an RNA sec- ondary structure example. cryogenic electron microscopy (Fica &amp; Nagai, 2017) can be implemented to determine RNA secondary structure, they suffer from low throughput and expensive cost. Computational RNA secondary structure prediction meth- ods have been favored for their high efficiency in recent years (Iorns et al., 2007). Currently, mainstream meth- ods can be broadly classified into two categories (Rivas, 2013; Szikszai et al., 2022): (i) comparative sequence anal- ysis and (ii) single sequence folding algorithm. Compara- tive sequence analysis determines the secondary structure conserved among homologous sequences but the limited known RNA families hinder its development (Gutell et al., 2002; Griffiths-Jones et al., 2003; Gardner et al., 2009; Nawrocki et al., 2015). Researchers thus resort to single RNA sequence folding algorithms that do not need mul- tiple sequence alignment information. A classical cate- gory of computational RNA folding algorithms is to use dynamic programming (DP) that assumes the secondary structure is a result of energy minimization (Bellaousov et al., 2013; Nicholas &amp; Zuker, 2008; Lorenz et al., 2011; Zuker, 2003; Mathews &amp; Turner, 2006; Do et al., 2006). However, energy-based approaches usually require the base pairs have a nested structure while ignoring some valid yet biologically essential structures such as pseudoknots, i.e., non-nested base pairs (Chen et al., 2019; Seetin &amp; Math- ews, 2012; Xu &amp; Chen, 2015), as shown in Figure 2. Since predicting secondary structures with pseudoknots under the energy minimization framework has shown to be hard and 1 arXiv:2212.14041v5 [q-bio.BM] 19 Jun 2024 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective NP-complete (Wang &amp; Tian, 2011; Fu et al., 2022), deep learning techniques are introduced as an alternative.A AC U G U A AC U G U nested structure non-nested structure Figure 2. Examples of nested and non-nested secondary structures. Attempts to overcome the limitations of energy-based meth- ods have motivated deep learning methods that predict RNA secondary structures in the absence of DP. SPOT- RNA (Singh et al., 2019) is a seminal work that ensembles ResNet (He et al., 2016) and LSTM (Hochreiter &amp; Schmid- huber, 1997) and applies transfer learning to identify molec- ular recognition features. SPOT-RNA does not constrain the output space into valid RNA secondary structures, which de- grades its generalization ability on new datasets (Jung et al.). E2Efold (Chen et al., 2019) employs an unrolled algorithm for constrained programming that post-processes the net- work output to satisfy the constraints. E2Efold introduces a convex relaxation to make the constrained optimization tractable, leading to possible structural constraint violations and poor generalization ability (Sato et al., 2021; Fu et al., 2022; Franke et al., 2023; 2022). RTfold (Jung et al.) uti- lizes the Fenchel-Young loss (Berthet et al., 2020) to en- able differentiable discrete optimization with perturbations, but the approximation cannot guarantee the satisfaction of constraints. Developing an appropriate optimization that enforces the output to be valid becomes a crucial concern. Since deep learning-based approaches cannot directly out- put valid RNA secondary structures, existing approaches usually formulate the problem into a constrained optimiza- tion problem and optimize the output of the model to fulfill specific constraints as closely as possible. However, these methods typically necessitate iterative optimization, leading to reduced efficiency. Moreover, the extensive optimization space involved does not ensure the complete satisfaction of these constraints. In this study, we introduce a novel perspective for predicting RNA secondary structures by re- framing the challenge as a K-Rook problem. Recognizing the alignment between the solution spaces of the K-Rook problem and RNA secondary structure prediction, our objec- tive is to identify the most compatible K-Rook solution for each RNA sequence. This is achieved by training the deep learning model on prior data to learn matching patterns. Considering the high complexity of the solution space in the symmetric K-Rook problem, we introduced RFold, an inno- vative approach. This method utilizes a bi-dimensional opti- mization strategy, effectively decomposing the problem into separate row-wise and column-wise components. This de- composition significantly reduces the matching complexity, thereby simplifying the solving process while guaranteeing the validity of the output. We conduct extensive experiments to compare RFold with state-of-the-art methods on several benchmark datasets and show the superior performance of our proposed method. Moreover, RFold has faster inference efficiency than those methods due to its simplicity. 2. Related work Comparative Sequence Analysis Comparative sequence analysis determines base pairs conserved among homolo- gous sequences (Gardner &amp; Giegerich, 2004; Knudsen &amp; Hein, 2003; Gorodkin et al., 2001). ILM (Ruan et al., 2004) combines thermodynamic and mutual information content scores. Sankoff (Hofacker et al., 2004) merges the sequence alignment and maximal-pairing folding methods (Nussinov et al., 1978). Dynalign (Mathews &amp; Turner, 2002) and Carnac (Touzet &amp; Perriquet, 2004) are the subsequent vari- ants of Sankoff algorithms. RNA forester (Hochsmann et al., 2003) introduces a tree alignment model for global and local alignments. However, the limited number of known RNA families (Nawrocki et al., 2015) impedes the development. Energy-based Folding Algorithms When the structures consist only of nested base pairing, dynamic programming can predict the structure by minimizing energy. Early works include Vienna RNAfold (Lorenz et al., 2011), Mfold (Zuker, 2003), RNAstructure (Mathews &amp; Turner, 2006), and CONTRAfold (Do et al., 2006). Faster imple- mentations that speed up dynamic programming have been proposed, such as Vienna RNAplfold (Bernhart et al., 2006), LocalFold (Lange et al., 2012), and LinearFold (Huang et al., 2019). However, they cannot accurately predict structures with pseudoknots, as predicting the lowest free energy struc- tures with pseudoknots is NP-complete (Lyngs\u00f8 &amp; Pedersen, 2000), making it difficult to improve performance. Learning-based Folding Algorithms Deep learning methods have inspired approaches in bioengineering appli- cations (Wu et al., 2024a;b; Lin et al., 2022; 2023; Tan et al., 2024; 2023). SPOT-RNA (Singh et al., 2019) is a seminal work that employs deep learning for RNA secondary struc- ture prediction. SPOT-RNA2 (Singh et al., 2021) improves its predecessor by using evolution-derived sequence pro- files and mutational coupling. Inspired by Raptor-X (Wang et al., 2017) and SPOT-Contact (Hanson et al., 2018), SPOT- RNA uses ResNet and bidirectional LSTM with a sigmoid function. MXfold (Akiyama et al., 2018) combines sup- port vector machines and thermodynamic models. CDP- fold (Zhang et al., 2019), DMFold (Wang et al., 2019), and MXFold2 (Sato et al., 2021) integrate deep learning tech- niques with energy-based methods. E2Efold (Chen et al., 2019) constrains the output to be valid by learning unrolled algorithms. However, its relaxation for making the optimiza- tion tractable may violate the constraints. UFold (Fu et al., 2022) introduces U-Net model to improve performance. 2 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective 3. Background 3.1. Preliminaries The primary structure of RNA is a sequence of nucleotide bases A, U, C, and G, arranged in order and represented as X \u201c px1, ..., xLq, where each xi denotes one of these bases. The secondary structure is the set of base pairings within the sequence, modeled as a sparse matrix M P t0, 1uL\u02c6L, where M ij \u201c 1 indicates a bond between bases i and j. The key challenges include (i) designing a model, characterized by parameters \u0398, that captures the complex transformations from the sequence X to the pairing matrix M and (ii) cor- rectly identifying the sparsity of the secondary structure, which is determined by the nature of RNA. Thus, the trans- formation F\u0398 : X \u00de \u00d1 M is usually decomposed into two stages for capturing the sequence-to-structure relationship and optimizing the sparsity of the target matrix: F\u0398 :\u201c G\u03b8g \u02dd H\u03b8h , (1) where H\u03b8h : X \u00de \u00d1 H represents the initial processing step, transforming the RNA sequence into an intermediate, unconstrained representation H P RL\u02c6L. Subsequently, G\u03b8g : H \u00de \u00d1 M parameterizes the optimization stage for the intermediate distribution into the final sparse matrix M . 3.2. Constrained Optimization-based Approaches The core problem of secondary structure prediction lies in sparsity identification. Numerous studies regard this task as a constrained optimization problem, seeking the optimal refinement mappings by gradient descent. Besides, keeping the hard constraints on RNA secondary structures is also essential, which ensures valid biological functions (Steeg, 1993). These constraints can be formally described as: \u2022 (a) Only three types of nucleotide combinations can form base pairs: B :\u201c tAU, UAu Y tGC, CGu Y tGU, UGu. For any base pair xixj where xixj R B, M ij \u201c 0. \u2022 (b) No sharp loop within three bases. For any adjacent bases within a distance of three nucleotides, they cannot form pairs with each other. For all |i \u00b4 j| \u0103 4, M ij \u201c 0. \u2022 (c) There can be at most one pair for each base. For all i and j, \u0159L j\u201c1 M ij \u010f 1, \u0159L i\u201c1 M ij \u010f 1. The search for valid secondary structures is thus a quest for symmetric sparse matrices P t0, 1uL\u02c6L that adhere to the constraints above. The first two constraints can be sat- isfied by defining a constraint matrix \u010eM as: \u010eM ij :\u201c 1 if xixj P B and |i \u00b4 j| \u011b 4, and \u010eM ij :\u201c 0 otherwise. Addressing the third constraint is critical as it necessitates employing sparse optimization techniques. Therefore, our primary objective is to devise an effective sparse optimiza- tion strategy. This strategy is based on the symmetric in- herent distribution H and M , which support constraints (a) and (b), and additionally addresses constraint (c). SPOT-RNA subtly enforces the principles of sparsity. It streamlines the pathway from the raw neural network output H by harnessing the Sigmoid function to distill a sparse pattern. The transformation applies a threshold to yield a binary sparse matrix. This process can be represented as: GpHq \u201c 1rSigmoidpHq\u0105ss. (2) In this approach, a fixed threshold s of 0.5 is applied, typical for inducing sparsity. It omits complex constraints or extra parameters \u03b8g , simply using this cutoff to achieve sparse structure representations. E2Efold introduces a non-linear transformation to the in- termediate value xM P RL\u02c6L and an additional regulariza- tion term } xM }1. 1 2 A H \u00b4 s, T p xM q E \u00b4 \u03c1} xM }1, (3) where T p xM q \u201c 1 2 p xM d xM <code>p xM d xM qT q d \u010eM ensures symmetry and adherence to RNA base-pairing constraints (a) and (b), s is the log-ratio bias term set to logp9.0q, and the \u21131 penalty \u03c1} xM }1 promotes sparsity. To fulfill constraint (c), the objective is combined with conditions T p xM q1 \u010f 1. Denote \u03bb P RL</code> as the Lagrange multiplier, the formulation for the sparse optimization is expressed as: min \u03bb\u011b0 max xM 1 2 A H \u00b4 s, T p xM q E \u00b4 \u03c1} xM }1 \u00b4 A \u03bb, ReLUpT p xM q1 \u00b4 1q E , (4) In the training stage, the optimization objective is the output of score function S dependent on xM and H. It can be regarded as an optimization function G parameterized by \u03b8g : G\u03b8g pHq \u201c T parg max xM PRL\u02c6L Sp xM , Hqq. (5) Although the complicated design to the constraints is ex- plicitly formulated, the iterative updates may fall into sub- optimal or invalid solutions. Besides, it requires additional parameters \u03b8g , making the model training complicated. RTfold introduces a differentiable function that incorpo- rates an additional Gaussian perturbation W . The objective function is expressed as: min xM 1 N N\u00ff i\u201c1 T \u00b4 H <code>\u03f5W piq\u00af \u00b4 xM (6) where T denotes the non-linear transformation to constrain the initial output H, and N is the number of random sam- ples. The random perturbation W piq adjusts the distribution by leveraging the gradient during the optimization process. 3 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective While RTFold designs an efficient differential objective function, the constraints imposed by the non-linear transfor- mation on a noisy hidden distribution may lead to biologi- cally implausible structures. 4. RFold 4.1. Probabilistic K-Rook Matching The symmetric K-Rook arrangement (Riordan, 2014; Elkies &amp; Stanley, 2011) is a classic combinatorial problem in- volving the placement of KpK \u010f Lq non-attacking Rooks on an L \u02c6 L chessboard, where the goal is to arrange the Rooks such that they form a symmetric pattern. The term \u2019non-attacking\u2019 means that no two Rooks are positioned in the same row or column. An interesting parallel can be drawn between this combinatorial scenario and the domain of RNA secondary structure prediction, as illustrated in Fig- ure 3. This analogy stems from the conceptual similarity in the arrangement patterns required in both cases. The RNA sequence can be regarded as a chessboard of size L and the base pairs are the Rooks. The core problem is to determine an optimal arrangement of these base pairs.a d heb c f g 8 4 1 3 2 7 6 5 A C AUA C f C U G C A A A C C (a) symmetric Rooks arrangement (b) RNA secondary structure Figure 3. The analogy between the symmetric K-Rook arrange- ment and the RNA secondary structure prediction. Given a finite solution space M defined by the symmetric K-Rook arrangement, we reformulate our objective as a probabilistic matching problem. The goal is to find the most matching solution M \u02da P M for the given sequence X. The optimal solution M \u02da is defined as: M \u02da \u201c arg max M PM PpM |Xq. (7) According to Bayes\u2019 theorem, the posterior probability can be represented as PpM |Xq \u201c PpX|M qPpM q PpXq . Since the denominator P pXq is constant for all M , and assuming that the solution space is finite and each solution within it is equally likely, we can adopt a uniform prior PpM q in this context. Therefore, maximizing the posterior probability is equivalent to maximizing the likelihood PpX|M q. This leads to the following equation: M \u02da \u201c arg max M PM PpX|M q. (8) Therefore, our primary task becomes computing the like- lihood PpX|M q for the given sequence X under each possible solution M . 4.2. Bi-dimensional Optimization Computing the likelihood PpX|M q directly poses sig- nificant challenges. To address this, we propose a bi- dimensional optimization strategy that simplifies the prob- lem by decomposing it into row-wise and column-wise com- ponents. This approach is mathematically represented as: PpX|M q \u201c PpX|RqPpX|Cq, (9) where M is the product of the row-wise component R P RL\u02c6L and the column-wise component C P RL\u02c6L, i.e., M \u201c R d C. Each component represents the optimal solu- tion for the row-wise and column-wise matching problems, respectively. Importantly, the row-wise and column-wise components are independent, and the comprehensive solu- tion for the entire problem is derived from the product of the optimal solutions for these two sub-problems. Applying Bayes\u2019 theorem, for the row-wise component, we have PpR|Xq \u201c PpX|RqPpRq PpXq . Given that the solution space of R is both finite and valid, we can regard it as a uniform distribution. The analysis for the column-wise com- ponent, PpC|Xq, follows a similar approach. Therefore, the optimal solution M \u02da can be represented as: M \u02da \u201c arg max R,C PpR|XqPpC|Xq \u201c arg max R PpR|Xq arg max C PpC|Xq (10) The next phase involves establishing proxies for PpR|Xq and PpC|Xq. To this end, we introduce the basic sym- metric hidden distribution, xH \u201c pH d HT q d \u010eM . The row-wise and column-wise components are then derived by applying Softmax functions to xH, resulting in their respec- tive probability distributions: RpxHq \u201c exppxHij q \u0159L k\u201c1 exppxHikq , CpxHq \u201c exppxHij q \u0159L k\u201c1 exppxHkj q . (11) The final output is the element-wise product of the row-wise component RpxHq and the column-wise component CpxHq. This operation integrates the individual insights from both dimensions, leading to the optimized matrix M \u02da: M \u02da \u201c arg max RpxHq d arg max CpxHq. (12) As illustrated in Figure 4, we consider a random symmetric 6 \u02c6 6 matrix as an example. For simplicity, we disregard 4 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspectiveargmax \u211b \ud835\udc6f) \u2299 argmax \ud835\udc9e(\ud835\udc6f)\u0302 ) symmetric matrix \ud835\udc6f) argmax \u211b(\ud835\udc6f) ) argmax \ud835\udc9e(\ud835\udc6f) ) Figure 4. The visualization of arg max Rp xHq d arg max Cp xHq. the constraints (a-b) from \u010eM . This example demonstrates the outputs of Rp\u00a8q, Cp\u00a8q, and their element-wise product Rp\u00a8q d Cp\u00a8q. The row-wise and column-wise components jointly select the value that has the maximum in both its row and column while keeping the output matrix symmetric. Given the definition of xH \u201c pH d HT q d \u010eM , it is evident that xH inherently forms a symmetric and non- negative matrix. Regarding optimization, the operation RpxHq d CpxHq can be equivalently simplified to optimizing 1 2 pRpxHq</code> CpxHqq. This is because both approaches fun- damentally aim to maximize the congruence between the row-wise and column-wise components of xH. The underly- ing reason for this equivalence is that both optimizing the Hadamard product and the arithmetic mean of RpxHq and CpxHq focus on reinforcing the alignment and coherence across the various dimensions of the matrix. Moreover, examining the gradients of these operations sheds light on their computational efficiencies. The gradient of RpxHqdCpxHq entails a blend of partial derivatives intercon- nected via element-wise multiplication. It can be formally expressed as follows: BpRpxHq d CpxHqqij B xHij \u201cCpxHqij \u00a8 BRpxHqij B xHij <code>RpxHqij \u00a8 BCpxHqij B xHij . (13) In contrast, the gradient of 1 2 pRpxHq</code> CpxHqq is character- ized by a straightforward sum of partial derivatives: BpRpxHq <code>CpxHqqij B xHij \u201c BRpxHqij B xHij</code> BCpxHqij B xHij . (14) Element-wise addition, as used in the latter, tends to be numerically more stable and less susceptible to issues like floating-point precision errors, which are more common in element-wise multiplication operations. This stability is particularly beneficial when dealing with large-scale matri- ces or when the gradients involve extreme values, where numerical instability can pose significant challenges. The proposed simplification not only maintains the math- ematical integrity of the optimization problem but also provides computational advantages, making it a desirable strategy in practical scenarios involving large and intricate datasets. Consequently, we define the overall loss function as the mean square error (MSE) between the averaged row- wise and column-wise components of xH and the ground truth secondary structure M : LpM \u02da, M q \u201c 1 L2 \u203a \u203a \u203a 1 2 pRpxHq <code>CpxHqq \u00b4 M \u203a \u203a \u203a2 . (15) 4.3. Practical Implementation We identify the problem of predicting H P RL\u02c6L from the given sequence attention map pZ P RL\u02c6L as an image-to- image segmentation problem and apply the U-Net model to extract pair-wise information, as shown in Figure 5.sequence one-hot \ud835\udc7f: \ud835\udc3f\u00d74 Token embedding \ud835\udc3f\u00d7\ud835\udc37 Seq2map Attention \ud835\udc3f\u00d7\ud835\udc3f\u00d71 + Positional embedding \ud835\udc3f\u00d7\ud835\udc37 1 32 3264 128 64 256 512 256 128 \ud835\udc3f\u00d7\ud835\udc3f (\ud835\udc3f/2)\u00d7(\ud835\udc3f/2) (\ud835\udc3f/4)\u00d7(\ud835\udc3f/4) (\ud835\udc3f/8)\u00d7(\ud835\udc3f/8) (\ud835\udc3f/16)\u00d7(\ud835\udc3f/16) 1 \ud835\udc3f\u00d7\ud835\udc3f \ud835\udc6f \ud835\udc81+ \ud835\udc81 Figure 5. The overview model architecture of RFold. To automatically learn informative representations from sequences, we propose a Seq2map attention module. Given a sequence in one-hot form X P RL\u02c64, we first obtain the sum of the token embedding and positional embedding as the input of the Seq2map attention. We denote the input as Z P RL\u02c6D for convenience, where D is the hidden layer size of the token and positional embeddings. Motivated by the recent progress in attention mecha- nisms (Vaswani et al., 2017; Choromanski et al., 2020; Katharopoulos et al., 2020; Hua et al., 2022), we aim to develop a highly effective sequence-to-map transforma- tion based on pair-wise attention. We obtain the query Q P RL\u02c6D and key K P RL\u02c6D by applying per-dim scalars and offsets to Z: Q \u201c \u03b3QZ</code> \u03b2Q, K \u201c \u03b3K Z <code>\u03b2K , (16) where \u03b3Q, \u03b3K , \u03b2Q, \u03b2K P RL\u02c6D are learnable parameters. Then, the pair-wise attention map is obtained by: sZ \u201c ReLU2pQKT {Lq, (17) 5 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective where ReLU2 is an activation function that can be recog- nized as a simplified Softmax function in vanilla Transform- ers (So et al., 2021). The output of Seq2map is the gated representation of sZ: pZ \u201c sZ d \u03c3p sZq, (18) where \u03c3p\u00a8q is the Sigmoid function that performs as a gate. 5. Experiments We conduct experiments to compare our proposed RFold with state-of-the-art and commonly used approaches. Mul- tiple experimental settings are taken into account, includ- ing standard structure prediction, generalization evaluation, large-scale benchmark evaluation, cross-family evaluation, pseudoknot prediction and inference time comparison. De- tailed experimental setups can be found in the Appendix B. 5.1. Standard RNA Secondary Structure Prediction Following (Chen et al., 2019), we split the RNAStralign dataset into train, validation, and test sets by stratified sam- pling. We report the results in Table 1. Energy-based meth- ods achieve relatively weak F1 scores ranging from 0.420 to 0.633. Learning-based folding algorithms like E2Efold and UFold significantly improve performance by large mar- gins, while RFold obtains even better performance among all the metrics. Moreover, RFold obtains about 8% higher precision than the state-of-the-art method. This suggests that our optimization strategy is strict to satisfy all the hard constraints for predicting valid structures. Table 1. Results on RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.450 0.398 0.420 RNAfold 0.516 0.568 0.540 RNAstructure 0.537 0.568 0.550 CONTRAfold 0.608 0.663 0.633 LinearFold 0.620 0.606 0.609 CDPfold 0.633 0.597 0.614 E2Efold 0.866 0.788 0.821 UFold 0.905 0.927 0.915 RFold 0.981 0.973 0.977 5.2. Generalization Evaluation To verify the generalization ability of our proposed RFold, we directly evaluate the performance on another benchmark dataset ArchiveII using the pre-trained model on the RNAS- tralign training dataset. Following (Chen et al., 2019), we exclude RNA sequences in ArchiveII that have overlapping RNA types with the RNAStralign dataset for a fair compari- son. The results are reported in Table 2. It can be seen that traditional methods achieve F1 scores in the range of 0.545 to 0.842. A recent learning-based method, MXfold2, obtains an F1 score of 0.768, which is even lower than some energy-based methods. Another state- of-the-art learning-based method improves the performance to the F1 score of 0.905. RFold further improves the F1 score to 0.921, even higher than UFold. It is worth noting that RFold has a relatively lower result in the recall metric and a significantly higher result in the precision metric. The reason for this phenomenon may be the strict constraints of RFold. While none of the current learning-based methods can satisfy all the constraints we introduced in Sec. 3.2, the predictions of RFold are guaranteed to be valid. Thus, RFold may cover fewer pair-wise interactions, leading to a lower recall metric. However, the highest F1 score still suggests the great generalization ability of RFold. Table 2. Results on ArchiveII dataset. Results in bold and under- lined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.668 0.590 0.621 CDPfold 0.557 0.535 0.545 RNAfold 0.663 0.613 0.631 RNAstructure 0.664 0.606 0.628 CONTRAfold 0.696 0.651 0.665 LinearFold 0.724 0.605 0.647 RNAsoft 0.665 0.594 0.622 Eternafold 0.667 0.622 0.636 E2Efold 0.734 0.660 0.686 SPOT-RNA 0.743 0.726 0.711 MXfold2 0.788 0.760 0.768 Contextfold 0.873 0.821 0.842 RTfold 0.891 0.789 0.814 UFold 0.887 0.928 0.905 RFold 0.938 0.910 0.921 5.3. Large-scale Benchmark Evaluation The bpRNA dataset is a large-scale benchmark, comprises fixed training (TR0), evaluation (VL0), and testing (TS0) sets. Following previous works (Singh et al., 2019; Sato et al., 2021; Fu et al., 2022), we train the model in bpRNA- TR0 and evaluate the performance on bpRNA-TS0 by using the best model learned from bpRNA-VL0. The detailed results can be found in Table 3. RFold outperforms the prior state-of-the-art method, SPOT- RNA, by a notable 4.0% in terms of the F1 score. This improvement in the F1 score can be attributed to the con- sistently superior performance of RFold in the precision metric when compared to baseline models. However, it is important to note that the recall metric remains constrained, 6 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 3. Results on bpRNA-TS0 set. Method Precision Recall F1 E2Efold 0.140 0.129 0.130 RNAstructure 0.494 0.622 0.533 RNAsoft 0.497 0.626 0.535 RNAfold 0.494 0.631 0.536 Mfold 0.501 0.627 0.538 Contextfold 0.529 0.607 0.546 LinearFold 0.561 0.581 0.550 MXfold2 0.519 0.646 0.558 Externafold 0.516 0.666 0.563 CONTRAfold 0.528 0.655 0.567 SPOT-RNA 0.594 0.693 0.619 UFold 0.521 0.588 0.553 RFold 0.692 0.635 0.644 likely due to stringent constraints applied during prediction. Table 4. Results on long-range bpRNA-TS0 set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 Mfold 0.315 0.450 0.356 RNAfold 0.304 0.448 0.350 RNAstructure 0.299 0.428 0.339 CONTRAfold 0.306 0.439 0.349 LinearFold 0.281 0.355 0.305 RNAsoft 0.310 0.448 0.353 Externafold 0.308 0.458 0.355 SPOT-RNA 0.361 0.492 0.403 MXfold2 0.318 0.450 0.360 Contextfold 0.332 0.432 0.363 UFold 0.543 0.631 0.584 RFold 0.803 0.765 0.701 Following (Fu et al., 2022), we conduct an experiment on long-range interactions. Given a sequence of length L, the long-range base pairing is defined as the paired and unpaired bases with intervals longer than L{2. As shown in Table 4, RFold performs unexpectedly well on these long-range base pairing predictions and improves UFold in all metrics by large margins, demonstrating its strong predictive ability. 5.4. Cross-family Evaluation The bpRNA-new dataset is a cross-family benchmark dataset comprising 1,500 RNA families, presenting a signifi- cant challenge for pure deep learning approaches. UFold, for instance, relies on the thermodynamic method Contrafold for data augmentation to achieve satisfactory results. As shown in Table 5, the standard UFold achieves an F1 score of 0.583, while RFold reaches 0.616. When the same data augmentation technique is applied, UFold\u2019s performance increases to 0.636, whereas RFold yields a score of 0.651. This places RFold second only to the thermodynamics-based method, Contrafold, in terms of F1 score. Table 5. Results on bpRNA-new. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Precision Recall F1 E2Efold 0.047 0.031 0.036 SPOT-RNA 0.635 0.641 0.620 Contrafold 0.620 0.736 0.661 UFold 0.500 0.736 0.583 UFold + aug 0.570 0.742 0.636 RFold 0.614 0.619 0.616 RFold + aug 0.618 0.687 0.651 5.5. Predict with Pseudoknots Following E2Efold (Chen et al., 2019), we consider a se- quence to be a true positive if it is correctly identified as containing a pseudoknot. For this analysis, we extracted all sequences featuring pseudoknots from the RNAStralign test dataset and assessed their predictive accuracy. The results of this analysis are summarized in the following table: Table 6. Results on RNA structures with pseudoknots. Method Precision Recall F1 Score RNAstructure 0.778 0.761 0.769 SPOT-RNA 0.677 0.978 0.800 E2Efold 0.844 0.990 0.911 UFold 0.962 0.990 0.976 RFold 0.971 0.993 0.982 RFold demonstrates superior performance compared to its counterparts across all evaluated metrics, i.e., precision, recall, and F1 score. This consistent outperformance across multiple dimensions of accuracy underscores the efficacy and robustness of the RFold approach in predicting RNA structures with pseudoknots. 5.6. Inference Time Comparison We compared the running time of various methods for pre- dicting RNA secondary structures using the RNAStralign testing set with the same experimental setting and the hard- ware environment as in (Fu et al., 2022). The results are pre- sented in Table 7, which shows the average inference time per sequence. The fastest energy-based method, LinearFold, takes about 0.43s for each sequence. The learning-based baseline, UFold, takes about 0.16s. RFold has the highest inference speed, costing only about 0.02s per sequence. In particular, RFold is about eight times faster than UFold and sixteen times faster than MXfold2. 5.7. Ablation Study Bi-dimensional Optimization To validate the effective- ness of our proposed bi-dimensional optimization strategy, we conduct an experiment that replaces them with other op- 7 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 7. Inference time on the RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively. Method Time CDPfold (Tensorflow) 300.11 s RNAstructure (C) 142.02 s CONTRAfold (C++) 30.58 s Mfold (C) 7.65 s Eternafold (C++) 6.42 s RNAsoft (C++) 4.58 s RNAfold (C) 0.55 s LinearFold (C++) 0.43 s SPOT-RNA(Pytorch) 77.80 s (GPU) E2Efold (Pytorch) 0.40 s (GPU) MXfold2 (Pytorch) 0.31 s (GPU) UFold (Pytorch) 0.16 s (GPU) RFold (Pytorch) 0.02 s (GPU) timization methods. The results are summarized in Table 8, where RFold-E and RFold-S denote our model with the opti- mization strategies of E2Efold and SPOT-RNA, respectively. While precision, recall, and F1 score are evaluated at base- level, we report the validity which is a sample-level metric evaluating whether the predicted structure satisfies all the constraints. It can be seen that though RFold-E has compa- rable performance in the first three metrics with ours, many of its predicted structures are invalid. The optimization strategy of SPOT-RNA has incorporated no constraint that results in its low validity. Moreover, its strategy seems to not fit our model well, which may be caused by the simplicity of our proposed RFold model. Table 8. Ablation study on different optimization strategies (RNAStralign testing set). Method Precision Recall F1 Validity RFold 0.981 0.973 0.977 100.00% RFold-E 0.888 0.906 0.896 50.31% RFold-S 0.223 0.988 0.353 0.00% Seq2map Attention We also conduct an experiment to evaluate the proposed Seq2map attention. We replace the Seq2map attention with the hand-crafted features from UFold and the outer concatenation from SPOT-RNA, which are denoted as RFold-U and RFold-SS, respectively. In ad- dition to performance metrics, we also report the average inference time for each RNA sequence to evaluate the model complexity. We summarize the result in Table 9. It can be seen that RFold-U takes much more inference time than our RFold and RFold-SS due to the heavy computational cost when loading and learning from hand-crafted features. Moreover, it is surprising to find that RFold-SS has a little better performance than RFold-U, with the least inference time for its simple outer concatenation operation. However, neither RFold-U nor RFold-SS can provide informative rep- resentations like our proposed Seq2map attention. With comparable inference time with the simplest RFold-SS, our RFold outperforms baselines by large margins. Table 9. Ablation study on different pre-processing strategies (RNAStralign testing set). Method Precision Recall F1 Time RFold 0.981 0.973 0.977 0.0167 RFold-U 0.875 0.941 0.906 0.0507 RFold-SS 0.886 0.945 0.913 0.0158 Row-wise and Column-wise Componenets We con- ducted comprehensive ablation studies on the row-wise and column-wise components of our proposed model, RFold, by modifying the inference mechanism using pre-trained checkpoints. These studies were meticulously designed to isolate and understand the individual contributions of these components to our model\u2019s performance in RNA secondary structure prediction. The results, presented across three datasets\u2014RNAStralign (Table 10), ArchiveII (Table 11), and bpRNA-TS0 (Table 12)\u2014highlight two key findings: (i) Removing both the row-wise and column-wise compo- nents leads to a substantial drop in the model\u2019s performance, underscoring their pivotal role within our model\u2019s archi- tecture. This dramatic reduction in effectiveness clearly demonstrates that both components are integral to achieving high accuracy. The significant decline in performance when these components are omitted highlights their essential func- tion in capturing the complex dependencies within RNA sequences. (ii) The performance metrics when isolating either the row-wise or column-wise components are remark- ably similar across all datasets. This uniformity suggests that the training process, which incorporates row-wise and column-wise softmax functions, likely yields symmetric outputs. Consequently, this symmetry implies that each component contributes in an almost equal measure to the model\u2019s overall predictive capacity. Table 10. Ablation study on row-wise and column-wise compo- nents (RNAStralign testing set). Method Precision Recall F1 Validity RFold 0.981 0.973 0.977 100.00% RFold w/o C 0.972 0.975 0.973 75.99% RFold w/o R 0.972 0.975 0.973 75.99% RFold w/o R,C 0.016 0.031 0.995 0.00% 5.8. Visualization We visualize two examples predicted by RFold and UFold in Figure 6. The corresponding F1 scores are denoted at 8 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 11. Ablation study on row-wise and column-wise compo- nents (ArchiveII). Method Precision Recall F1 Validity RFold 0.938 0.910 0.921 100.00% RFold w/o C 0.919 0.914 0.914 49.14% RFold w/o R 0.919 0.914 0.914 49.14% RFold w/o R,C 0.013 0.997 0.025 0.00% Table 12. Ablation study on row-wise and column-wise compo- nents (bpRNA-TS0). Method Precision Recall F1 Validity RFold 0.693 0.635 0.644 100.00% RFold w/o C 0.652 0.651 0.637 12.97% RFold w/o R 0.652 0.651 0.637 12.97% RFold w/o R,C 0.021 0.995 0.040 0.00% the bottom right of each plot. The first row of secondary structures is a simple example of a nested structure. It can be seen that UFold may fail in such a case. The second row of secondary structures is much more difficult that contains over 300 bases of the non-nested structure. While UFold fails in such a complex case, RFold can predict the structure accurately. Due to the limited space, we provide more visualization comparisons in Appendix D.RFoldTrue UFold A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U UG U U C G G GG A C C A C C CG C U A A A U U A C A U G C U A C CG C A G C A G U G C U GA A A G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C GA A G U U CU A A G G C C U U C U G C U A C G A A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U A G U C G A A C CC C U C A G A G A U G A G G A U G G A A U C A A U G 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U UG U U C G G GG A C C A C C CG C U A A A U U A C A U G C U A C CG C A G C A G U G C U GA A A G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C GA A G U U CU A A G G C C U U C U G C U A C G A A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U A G U C G A A C CC C U C A G A G A U G A G G A U G G A A U C A A U G 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 A A C C A U U A A G G A A U A G A C C A A G C U C U A G G U G G U U G A G A A A C C C C U U U G U A U U A G U C C U G G A A A C A G G G C G A C A U U G U C A A A U U G U U C G GGG A C C A C C CG C U A A A U U A C A U G C U A C C G CA G C AGUGCU G A AA G G C C U G U G A G C A C U A G A G G U A A C G C C U C U A G G G A U G G U A A U A A C G C G U G U A U A G G G U A U A U C C G C A G C G A A G U U C U A A G G C C U U C U G C U A CGA A U C G C G U U C A C A G A C U A G A C G G C A A U G G G C U C C U U G C G G G G C U U A A G A U A U 1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 RFoldTrue UFold 1.000 0.995 0.558 0.823 Figure 6. Visualization of the true and predicted structures. 6. Conclusion In this study, we reformulate RNA secondary structure pre- diction as a K-Rook problem, thus transforming the predic- tion process into probabilistic matching. Subsequently, we introduce RFold, an efficient learning-based model, which utilizes a bidimensional optimization strategy to decom- pose the probabilistic matching into row-wise and column- wise components, simplifying the solving process while guaranteeing the validity of the output. Comprehensive experiments demonstrate that RFold achieves competitive performance with faster inference speed. The limitations of RFold primarily revolve around its strin- gent constraints. This strictness in constraints implies that RFold is cautious in predicting interactions, leading to higher precision but possibly at the cost of missing some true interactions. Though we have provided a naive so- lution in Appendix C, it needs further studies to obtain a better strategy that leads to more balanced precision-recall trade-offs and more comprehensive structural predictions. Acknowledgements This work was supported by National Science and Technol- ogy Major Project (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake Univer- sity and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake Univer- sity Industries of the Future Research Funding. Impact Statement RFold is the first learning-based method that guarantees the validity of predicted RNA secondary structures. Its capability to ensure accurate predictions. It can be a valuable tool for biologists to study the structure and function of RNA molecules. Additionally, RFold stands out for its speed, significantly surpassing previous methods, marking it as a promising avenue for future developments in this field. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Akiyama, M., Sato, K., and Sakakibara, Y. A max-margin training of rna secondary structure prediction integrated with the thermodynamic model. Journal of bioinformatics and computational biology, 16(06):1840025, 2018. Andronescu, M., Aguirre-Hernandez, R., Condon, A., and Hoos, H. H. Rnasoft: a suite of rna secondary struc- ture prediction and design software tools. Nucleic acids research, 31(13):3416\u20133422, 2003. Bellaousov, S., Reuter, J. S., Seetin, M. G., and Mathews, D. H. Rnastructure: web servers for rna secondary struc- ture prediction and analysis. Nucleic acids research, 41 (W1):W471\u2013W474, 2013. 9 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Bernhart, S. H., Hofacker, I. L., and Stadler, P. F. Local rna base pairing probabilities in large sequences. Bioinfor- matics, 22(5):614\u2013615, 2006. Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.- P., and Bach, F. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508\u20139519, 2020. Chen, X., Li, Y., Umarov, R., Gao, X., and Song, L. Rna secondary structure prediction by learning unrolled algo- rithms. In International Conference on Learning Repre- sentations, 2019. Cheong, H.-K., Hwang, E., Lee, C., Choi, B.-S., and Cheong, C. Rapid preparation of rna samples for nmr spectroscopy and x-ray crystallography. Nucleic acids research, 32(10):e84\u2013e84, 2004. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mo- hiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. Do, C. B., Woods, D. A., and Batzoglou, S. Contrafold: Rna secondary structure prediction without physics-based models. Bioinformatics, 22(14):e90\u2013e98, 2006. Elkies, N. and Stanley, R. P. Chess and mathematics. Recu- perado el, 11, 2011. Fallmann, J., Will, S., Engelhardt, J., Gr \u00a8uning, B., Backofen, R., and Stadler, P. F. Recent advances in rna folding. Journal of biotechnology, 261:97\u2013104, 2017. Fica, S. M. and Nagai, K. Cryo-electron microscopy snap- shots of the spliceosome: structural insights into a dy- namic ribonucleoprotein machine. Nature structural &amp; molecular biology, 24(10):791\u2013799, 2017. Franke, J., Runge, F., and Hutter, F. Probabilistic trans- former: Modelling ambiguities and distributions for rna folding and molecule design. Advances in Neural Infor- mation Processing Systems, 35:26856\u201326873, 2022. Franke, J. K., Runge, F., and Hutter, F. Scalable deep learning for rna secondary structure prediction. arXiv preprint arXiv:2307.10073, 2023. Fu, L., Cao, Y., Wu, J., Peng, Q., Nie, Q., and Xie, X. Ufold: fast and accurate rna secondary structure prediction with deep learning. Nucleic acids research, 50(3):e14\u2013e14, 2022. F \u00a8urtig, B., Richter, C., W \u00a8ohnert, J., and Schwalbe, H. Nmr spectroscopy of rna. ChemBioChem, 4(10):936\u2013962, 2003. Gardner, P. P. and Giegerich, R. A comprehensive compari- son of comparative rna structure prediction approaches. BMC bioinformatics, 5(1):1\u201318, 2004. Gardner, P. P., Daub, J., Tate, J. G., Nawrocki, E. P., Kolbe, D. L., Lindgreen, S., Wilkinson, A. C., Finn, R. D., Griffiths-Jones, S., Eddy, S. R., et al. Rfam: updates to the rna families database. Nucleic acids research, 37 (suppl 1):D136\u2013D140, 2009. Gorodkin, J., Stricklin, S. L., and Stormo, G. D. Discovering common stem\u2013loop motifs in unaligned rna sequences. Nucleic Acids Research, 29(10):2135\u20132144, 2001. Griffiths-Jones, S., Bateman, A., Marshall, M., Khanna, A., and Eddy, S. R. Rfam: an rna family database. Nucleic acids research, 31(1):439\u2013441, 2003. Gutell, R. R., Lee, J. C., and Cannone, J. J. The accuracy of ribosomal rna comparative structure models. Current opinion in structural biology, 12(3):301\u2013310, 2002. Hanson, J., Paliwal, K., Litfin, T., Yang, Y., and Zhou, Y. Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks. Bioinfor- matics, 34(23):4039\u20134045, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Hochsmann, M., Toller, T., Giegerich, R., and Kurtz, S. Local similarity in rna secondary structures. In Computa- tional Systems Bioinformatics. CSB2003. Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003, pp. 159\u2013168. IEEE, 2003. Hofacker, I. L., Bernhart, S. H., and Stadler, P. F. Alignment of rna base pairing probability matrices. Bioinformatics, 20(14):2222\u20132227, 2004. Hua, W., Dai, Z., Liu, H., and Le, Q. Transformer quality in linear time. In International Conference on Machine Learning, pp. 9099\u20139117. PMLR, 2022. Huang, L., Zhang, H., Deng, D., Zhao, K., Liu, K., Hen- drix, D. A., and Mathews, D. H. Linearfold: linear-time approximate rna folding by 5\u2019-to-3\u2019dynamic program- ming and beam search. Bioinformatics, 35(14):i295\u2013i304, 2019. Iorns, E., Lord, C. J., Turner, N., and Ashworth, A. Utilizing rna interference to enhance cancer drug discovery. Nature reviews Drug discovery, 6(7):556\u2013568, 2007. 10 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Jung, A. J., Lee, L. J., Gao, A. J., and Frey, B. J. Rtfold: Rna secondary structure prediction using deep learning with domain inductive bias. Kalvari, I., Nawrocki, E. P., Ontiveros-Palacios, N., Argasin- ska, J., Lamkiewicz, K., Marz, M., Griffiths-Jones, S., Toffano-Nioche, C., Gautheret, D., Weinberg, Z., et al. Rfam 14: expanded coverage of metagenomic, viral and microrna families. Nucleic Acids Research, 49(D1):D192\u2013 D200, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. Knudsen, B. and Hein, J. Pfold: Rna secondary structure prediction using stochastic context-free grammars. Nu- cleic acids research, 31(13):3423\u20133428, 2003. Lange, S. J., Maticzka, D., M \u00a8ohl, M., Gagnon, J. N., Brown, C. M., and Backofen, R. Global or local? predicting secondary structure and accessibility in mrnas. Nucleic acids research, 40(12):5215\u20135226, 2012. Lin, H., Huang, Y., Liu, M., Li, X. C., Ji, S., and Li, S. Z. Diffbp: Generative diffusion of 3d molecules for target protein binding. ArXiv, abs/2211.11214, 2022. URL https://api.semanticscholar. org/CorpusID:253734621. Lin, H., Huang, Y., Zhang, H., Wu, L., Li, S., Chen, Z., and Li, S. Z. Functional-group- based diffusion for pocket-specific molecule gen- eration and elaboration. ArXiv, abs/2306.13769, 2023. URL https://api.semanticscholar. org/CorpusID:259251644. Lorenz, R., Bernhart, S. H., H \u00a8oner zu Siederdissen, C., Tafer, H., Flamm, C., Stadler, P. F., and Hofacker, I. L. Viennarna package 2.0. Algorithms for molecular biology, 6(1):1\u201314, 2011. Lyngs\u00f8, R. B. and Pedersen, C. N. Rna pseudoknot predic- tion in energy-based models. Journal of computational biology, 7(3-4):409\u2013427, 2000. Mathews, D. H. and Turner, D. H. Dynalign: an algorithm for finding the secondary structure common to two rna sequences. Journal of molecular biology, 317(2):191\u2013 203, 2002. Mathews, D. H. and Turner, D. H. Prediction of rna sec- ondary structure by free energy minimization. Current opinion in structural biology, 16(3):270\u2013278, 2006. Nawrocki, E. P., Burge, S. W., Bateman, A., Daub, J., Eber- hardt, R. Y., Eddy, S. R., Floden, E. W., Gardner, P. P., Jones, T. A., Tate, J., et al. Rfam 12.0: updates to the rna families database. Nucleic acids research, 43(D1): D130\u2013D137, 2015. Nicholas, R. and Zuker, M. Unafold: Software for nucleic acid folding and hybridization. Bioinformatics, 453:3\u201331, 2008. Nussinov, R., Pieczenik, G., Griggs, J. R., and Kleitman, D. J. Algorithms for loop matchings. SIAM Journal on Applied mathematics, 35(1):68\u201382, 1978. Riordan, J. An introduction to combinatorial analysis. 2014. Rivas, E. The four ingredients of single-sequence rna sec- ondary structure prediction. a unifying perspective. RNA biology, 10(7):1185\u20131196, 2013. Ruan, J., Stormo, G. D., and Zhang, W. An iterated loop matching approach to the prediction of rna secondary structures with pseudoknots. Bioinformatics, 20(1):58\u2013 66, 2004. Sato, K., Akiyama, M., and Sakakibara, Y. Rna secondary structure prediction using deep learning with thermody- namic integration. Nature communications, 12(1):1\u20139, 2021. Seetin, M. G. and Mathews, D. H. Rna structure prediction: an overview of methods. Bacterial regulatory RNA, pp. 99\u2013122, 2012. Singh, J., Hanson, J., Paliwal, K., and Zhou, Y. Rna sec- ondary structure prediction using an ensemble of two- dimensional deep neural networks and transfer learning. Nature communications, 10(1):1\u201313, 2019. Singh, J., Paliwal, K., Zhang, T., Singh, J., Litfin, T., and Zhou, Y. Improved rna secondary structure and tertiary base-pairing prediction using evolutionary profile, mu- tational coupling and two-dimensional transfer learning. Bioinformatics, 37(17):2589\u20132600, 2021. Sloma, M. F. and Mathews, D. H. Exact calculation of loop formation probability identifies folding motifs in rna secondary structures. RNA, 22(12):1808\u20131818, 2016. So, D., Ma \u00b4nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Searching for efficient transformers for language modeling. Advances in Neural Information Processing Systems, 34:6010\u20136022, 2021. Steeg, E. W. Neural networks, adaptive optimization, and rna secondary structure prediction. Artificial intelligence and molecular biology, pp. 121\u2013160, 1993. Szikszai, M., Wise, M. J., Datta, A., Ward, M., and Mathews, D. Deep learning models for rna secondary structure prediction (probably) do not generalise across families. bioRxiv, 2022. 11 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Tan, C., Zhang, Y., Gao, Z., Hu, B., Li, S., Liu, Z., and Li, S. Z. Hierarchical data-efficient representation learning for tertiary structure-based rna design. In The Twelfth International Conference on Learning Representations, 2023. Tan, C., Gao, Z., Wu, L., Xia, J., Zheng, J., Yang, X., Liu, Y., Hu, B., and Li, S. Z. Cross-gate mlp with protein com- plex invariant embedding is a one-shot antibody designer. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 38, pp. 15222\u201315230, 2024. Tan, Z., Fu, Y., Sharma, G., and Mathews, D. H. Turbofold ii: Rna structural alignment and secondary structure pre- diction informed by multiple homologs. Nucleic acids research, 45(20):11570\u201311581, 2017. Touzet, H. and Perriquet, O. Carnac: folding families of related rnas. Nucleic acids research, 32(suppl 2):W142\u2013 W145, 2004. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Wang, L., Liu, Y., Zhong, X., Liu, H., Lu, C., Li, C., and Zhang, H. Dmfold: A novel method to predict rna sec- ondary structure with pseudoknots based on deep learning and improved base pair maximization principle. Frontiers in genetics, 10:143, 2019. Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J. Accu- rate de novo prediction of protein contact map by ultra- deep learning model. PLoS computational biology, 13(1): e1005324, 2017. Wang, X. and Tian, J. Dynamic programming for np-hard problems. Procedia Engineering, 15:3396\u20133400, 2011. Wayment-Steele, H. K., Kladwang, W., Strom, A. I., Lee, J., Treuille, A., Participants, E., and Das, R. Rna sec- ondary structure packages evaluated and improved by high-throughput experiments. BioRxiv, pp. 2020\u201305, 2021. Wu, L., Huang, Y., Tan, C., Gao, Z., Hu, B., Lin, H., Liu, Z., and Li, S. Z. Psc-cpi: Multi-scale protein sequence-structure contrasting for efficient and gener- alizable compound-protein interaction prediction. arXiv preprint arXiv:2402.08198, 2024a. Wu, L., Tian, Y., Huang, Y., Li, S., Lin, H., Chawla, N. V., and Li, S. Z. Mape-ppi: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. arXiv preprint arXiv:2402.14391, 2024b. Xu, X. and Chen, S.-J. Physics-based rna structure predic- tion. Biophysics reports, 1(1):2\u201313, 2015. Zakov, S., Goldberg, Y., Elhadad, M., and Ziv-Ukelson, M. Rich parameterization improves rna structure prediction. Journal of Computational Biology, 18(11):1525\u20131542, 2011. Zhang, H., Zhang, C., Li, Z., Li, C., Wei, X., Zhang, B., and Liu, Y. A new method of rna secondary structure prediction based on convolutional neural network and dynamic programming. Frontiers in genetics, 10:467, 2019. Zuker, M. Mfold web server for nucleic acid folding and hybridization prediction. Nucleic acids research, 31(13): 3406\u20133415, 2003. 12 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective A. Comparison of mainstream RNA secondary structure prediction methods We compare our proposed method RFold with several other leading RNA secondary structure prediction methods and summarize the results in Table 13. RFold satisfies all three constraints (a)-(c) for valid RNA secondary struc- tures, while the other methods do not fully meet some of the constraints. RFold utilizes a sequence-to-map attention mechanism to capture long-range dependencies, whereas SPOT-RNA simply concatenates pairwise sequence infor- mation and E2Efold/UFold uses hand-crafted features. In terms of prediction accuracy on the RNAStralign benchmark test set, RFold achieves the best F1 score of 0.977, outper- forming SPOT-RNA, E2Efold and UFold by a large margin. Regarding the average inference time, RFold is much more efficient and requires only 0.02 seconds to fold the RNAS- tralign test sequences. In summary, RFold demonstrates superior performance over previous methods for RNA sec- ondary structure prediction in both accuracy and speed. B. Experimental Details Datasets We use three benchmark datasets: (i) RNAS- tralign (Tan et al., 2017), one of the most comprehensive collections of RNA structures, is composed of 37,149 struc- tures from 8 RNA types; (ii) ArchiveII (Sloma &amp; Mathews, 2016), a widely used benchmark dataset in classical RNA folding methods, containing 3,975 RNA structures from 10 RNA types; (iii) bpRNA (Singh et al., 2019), is a large scale benchmark dataset, containing 102,318 structures from 2,588 RNA types. (iv) bpRNA-new (Sato et al., 2021), de- rived from Rfam 14.2 (Kalvari et al., 2021), containing sequences from 1500 new RNA families. Baselines We compare our proposed RFold with base- lines including energy-based folding methods such as Mfold (Zuker, 2003), RNAsoft (Andronescu et al., 2003), RNAfold (Lorenz et al., 2011), RNAstructure (Mathews &amp; Turner, 2006), CONTRAfold (Do et al., 2006), Con- textfold (Zakov et al., 2011), and LinearFold (Huang et al., 2019); learning-based folding methods such as SPOT- RNA (Singh et al., 2019), Externafold (Wayment-Steele et al., 2021), E2Efold (Chen et al., 2019), MXfold2 (Sato et al., 2021), and UFold (Fu et al., 2022). Metrics We evaluate the performance by precision, recall, and F1 score, which are defined as: Precision \u201c TP TP</code> FP , Recall \u201c TP TP <code>FN , F1 \u201c 2 Precision \u00a8 Recall Precision</code> Recall , (19) where TP, FP, and FN denote true positive, false positive and false negative, respectively. Implementation details Following the same experimental setting as (Fu et al., 2022), we train the model for 100 epochs with the Adam optimizer. The learning rate is 0.001, and the batch size is 1 for sequences with different lengths. C. Discussion on Abnormal Samples Although we have illustrated three hard constraints in 3.2, there exist some abnormal samples that do not satisfy these constraints in practice. We have analyzed the datasets used in this paper and found that there are some abnormal sam- ples in the testing set that do not meet these constraints. The ratio of valid samples in each dataset is summarized in the table below: As shown in Table 8, RFold forces the validity to be 100.00%, while other methods like E2Efold only achieve about 50.31%. RFold is more accurate than other methods in reflecting the real situation. Nevertheless, we provide a soft version of RFold to relax the strict constraints. A possible solution to relax the rigid pro- cedure is to add a checking mechanism before the Argmax function in the inference. Specifically, if the confidence given by the Softmax is low, we do not perform Argmax and assign more base pairs. It can be implemented as the following pseudo-code: 1 y_pred = row_col_softmax(y) 2 int_one = row_col_argmax(y_pred) 3 4 # get the confidence for each position 5 conf = y_pred * int_one 6 all_pos = conf &gt; 0.0 7 8 # select reliable position 9 conf_pos = conf &gt; thr1 10 11 # select unreliable position with the full row and column 12 uncf_pos = get_unreliable_pos(all_pos, conf_pos) 13 14 # assign \"1\" for the positions with the confidence higher than thr2 15 # note that thr2 &lt; thr1 16 y_pred[uncf_pos] = (y_pred[uncf_pos] &gt; thr2 ).float() 17 int_one[uncf_pos] = y_pred[uncf_pos] We conduct experiments to compare the soft-RFold and the original version of RFold in the RNAStralign dataset. The results are summarized in the Table 15. It can be seen that soft-RFold improves the recall metric by a small margin. The minor improvement may be because the number of abnormal samples is small. We then select those samples that do not obey the three constraints to further analyze the performance. The total number of such samples is 179. It can be seen that soft-RFold can deal with abnormal samples well. The improvement of the recall metric is more obvious. 13 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective Table 13. Comparison between RNA secondary structure prediction methods and RFold. Method SPOT-RNA E2Efold UFold RFold pre-processing pairwise concat pairwise concat hand-crafted seq2map attention optimization approach \u02c6 unrolled algorithm unrolled algorithm bi-dimensional optimization constraint (a) \u02c6 \u2713 \u2713 \u2713 constraint (b) \u02c6 \u2713 \u2713 \u2713 constraint (c) \u02c6 \u02c6 \u02c6 \u2713 F1 score 0.711 0.821 0.915 0.977 Inference time 77.80 s 0.40 s 0.16 s 0.02 s Table 14. The ratio of valid samples in the datasets. Dataset RNAStralign ArchiveII bpRNA Validity 93.05% 96.03% 96.51% Table 15. The results of soft-RFold and RFold on the RNAStralign. Method Precision Recall F1 RFold 0.981 0.973 0.977 soft-RFold 0.978 0.974 0.976 Table 16. The results of soft-RFold and RFold on the abnormal samples on the RNAStralign. Method Precision Recall F1 RFold 0.956 0.860 0.905 soft-RFold 0.949 0.889 0.918 D. Visualization 14 Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching PerspectiveRFoldTrue UFold Figure 7. Visualization of the true and predicted structures. 15</p>"},{"location":"reference/external_lit/RNA_papers/","title":"RNA Papers","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: [Aligning on your goal] You\u2019d like a systematic, side-by-side comparison of the three \u201clist of relevant RNA 3D prediction references\u201d (V1, V2, V3) and a concise indication of which references are most directly helpful for the Stanford RNA 3D Folding Kaggle. [emoji]: [Actionable response]</p> <p>Below is a structured comparison. Afterward, you\u2019ll see a short \u201cWhich Papers?\u201d section that highlights which references from all versions are most likely to help in this Kaggle competition.</p> <p>\u2e3b</p> <p>Side-by-Side Comparison of the Three Versions</p> <p>Aspect  Version 1   Version 2   Version 3 Overall Format &amp; Tone   - Longest list (10 references).  - Each reference is enumerated (1\u201310) with bullet points explaining \u201cWhy It\u2019s Useful.\u201d  - Ends with \u201cHow to Leverage These References\u201d and enumerates 5 strategic takeaways (combine physics+DL, secondary/tertiary, generative, transfer learning, model benchmarking).   - Shorter list (8 references + \u201cHonorable Mentions\u201d).  - Each reference is briefly summarized under \u201cWhy it is useful.\u201d  - Concludes with a bullet-point \u201cHow to Use in the Kaggle Competition.\u201d    - Similar to Version 2 in length (8 main references + short \u201cHonorable Mentions\u201d).  - More thematically organized: (1) Deep Learning\u2013Based 3D Methods, (2) Physics/Hybrid, (3) Motifs, (4) Benchmarking, (5) Honorable Mentions.  - Concludes with \u201cHow These Papers Can Help.\u201d Depth of Annotations    - Provides fairly thorough bullet points describing each article\u2019s main contribution (e.g., describing loop entropies, generative approaches, etc.).  - Mentions specific synergy with Kaggle scoring metric (TM-score) more explicitly (especially in references like RhoFold+, NuFold).   - Emphasizes each paper\u2019s high-level approach (deep learning vs. physics-based).  - Uses concise bullet points with less granular detail than Version 1.    - More concise bullet points than V1, but still systematically covers \u201cwhy it\u2019s useful.\u201d  - Groups references thematically, so you see at a glance how each fits (DL, physics, motif, etc.). Coverage of Physics-Based vs. DL    - Splits references into deep-learning (RhoFold+, NuFold) and physics-based (Vfold, free energy).  - Also calls out \u201chybrid approaches\u201d or new generative methods.  - Highlights the same key papers but in a smaller batch.  - Mentions physics-based \u201cVfold2D-MC,\u201d \u201cPhysics-based de novo\u2026\u201d and the deep learning ones (RhoFold+, NuFold, foundation model).  - Thoroughly categorizes references by approach: \u201cDeep Learning\u2013Based 3D\u201d vs. \u201cPhysics-based/hybrid.\u201d  - More explicit about motif-based references (RNA 3D Motif Atlas, noncanonical pairs). Mention of Benchmarks   - Specifically references CASP15, RNA-Puzzles, and highlights lessons from puzzle challenges.   - Includes \u201cRNA-Puzzles Round IV\u201d and \u201cAssessment of 3D RNA in CASP15\u201d under the same list, but more briefly.   - Dedicates a discrete \u201cBenchmarking and Assessment\u201d section, calling out \u201cCASP15\u201d and \u201cRNA-Puzzles.\u201d Extra Mentions &amp; Honorable Mentions - 10 references plus broad coverage from generative (RNADiffFold) to physics-based. - 8 references, then \u201cHonorable Mentions\u201d for RNADiffFold, kissing complexes, and TM-score GPU. - 8 references, \u201cHonorable Mentions,\u201d more explicit mention of motif detection resources (\u201cAutomated classification of RNA 3D motifs\u201d). Structure for Action    - Ends with a \u201cHow to Leverage\u201d breakdown into 5 action steps (combine physics+DL, better 2D\u21923D integration, generative sampling, foundation models, better benchmark). - Gives \u201cHow to Use These Papers\u201d bullet points with a short description (architectural ideas, physics insights, motif libraries, scoring/benchmarking, practical tips).    - Concludes with \u201cHow These Papers Can Help\u201d with a similar breakdown (end-to-end DL, physics-based, motif knowledge, benchmarking).</p> <p>\u2e3b</p> <p>Highlights and Differences     1.  Version 1 (V1)     \u2022   Most detailed in per-paper bullet points.     \u2022   Specifically calls out \u201cwhat best teams got right in RNA-Puzzles,\u201d synergy with CASP assessments, and clarifies how you might combine secondary structure predictions to 3D folds.     \u2022   Ends with a clear 5-step synergy plan (physics+DL, generative/ensemble sampling, etc.).     2.  Version 2 (V2)     \u2022   Shorter and a bit more streamlined.     \u2022   Mentions fewer references overall (eight references plus short honorable mentions).     \u2022   Emphasizes how each reference might factor into the Kaggle environment.     3.  Version 3 (V3)     \u2022   Organized thematically: separate categories for deep-learning, physics-based, motif-based, and assessment.     \u2022   Crisp bullet points on each reference\u2019s main selling point.     \u2022   Might be easiest to read if you prefer quickly scanning by approach (DL vs. physics vs. motifs vs. assessments).</p> <p>\u2e3b</p> <p>Which Papers Are Most Likely to Be Useful for the Kaggle Competition?</p> <p>All three versions converge on similar references, specifically these often come up as top picks for boosting RNA 3D structure predictions, especially with a TM-score\u2013type metric:     1.  Accurate RNA 3D structure prediction using a language model-based deep learning approach (RhoFold+)     \u2022   Exemplar of a deep learning pipeline that uses RNA language models plus data-scarcity strategies.     \u2022   Directly tested on RNA-Puzzles and CASP targets, with strong improvements in TM-score.     2.  NuFold: end-to-end approach for RNA tertiary structure prediction     \u2022   Emphasizes flexible backbone representation and direct sequence\u21923D predictions.     \u2022   Good for exploring advanced model architectures.     3.  Interpretable RNA Foundation Model from Unannotated Data     \u2022   Large-scale pretraining with massive unlabeled RNA sequences (self-supervision).     \u2022   Potentially valuable for feature extraction and for boosting accuracy on limited data.     4.  Physics-Based De Novo Prediction (Vfold family)     \u2022   Shows how to incorporate free-energy calculations, loop entropies, and all-atom refinement.     \u2022   If you want a \u201cphysics-aware\u201d or hybrid approach, these references help.     5.  Assessment of 3D RNA Structure Prediction in CASP15     \u2022   Summarizes best methods from the latest challenge and highlights open pitfalls.     \u2022   Explains how top methods integrated deep learning, 2D constraints, or specialized RNA knowledge.     6.  RNA-Puzzles Round IV     \u2022   Real-world 3D RNA challenges that highlight the importance of correct coaxial stacking and tertiary contacts.     \u2022   Informs common difficulties in modeling more complex RNAs.     7.  Automated classification of RNA 3D motifs (RNA 3D Motif Atlas)     \u2022   For motif-based or fragment-based pipeline steps.     \u2022   Potentially helpful if you want to incorporate known loops/junctions, especially for partial constraints or fragment assembly.     8.  RNADiffFold (discrete diffusion generative approach)     \u2022   Focuses on secondary structure but can be adapted for generating multiple 3D conformations.     \u2022   If your pipeline is exploring generative or ensemble sampling, this is relevant.</p> <p>\u2e3b</p> <p>Final Suggestion or Next Step</p> <p>If you want one single resource that highlights both modern deep learning and physics-based synergy, Version 3\u2019s \u201cthematic grouping\u201d is easier for scanning. If you want more detail on each paper\u2019s potential synergy with Kaggle steps (like how to incorporate scoring or build ensemble approaches), Version 1\u2019s \u201cHow to Leverage\u201d bullet list is more explicit. Either way, the references repeated in all versions (RhoFold+, NuFold, physics-based Vfold, CASP15 assessment, RNA-Puzzles) are the ones you\u2019ll likely find most directly beneficial for this Kaggle competition.</p> <p>Would you like more guidance on building a roadmap or pipeline that integrates multiple references and techniques?</p>"},{"location":"reference/residue_atom_bridging/audit_report/","title":"Audit Report: Residue vs. Atom Representation Mismatches (Stages B-D)","text":"<p>Date: 2025-04-10 Auditor: Roo</p>"},{"location":"reference/residue_atom_bridging/audit_report/#1-introduction","title":"1. Introduction","text":"<p>This report documents the findings of a code audit focused on identifying mismatches and ad-hoc fixes related to residue-level vs. atom-level tensor representations between Stages B, C, and D of the M2 RNA prediction pipeline. Stage B outputs are typically residue-level (<code>[N_residue, c_s]</code>), while Stage D often requires atom-level inputs (<code>[N_atom, c_s]</code> or <code>[N_atom, 3]</code>). The goal is to locate existing fixes (e.g., tensor repeats, slicing, warnings) to inform the design of a systematic bridging mechanism.</p>"},{"location":"reference/residue_atom_bridging/audit_report/#2-audit-findings","title":"2. Audit Findings","text":"<p>The audit focused on <code>rna_predict/pipeline/stageD/</code> and its subdirectories.</p>"},{"location":"reference/residue_atom_bridging/audit_report/#21-explicit-shape-validation-and-fixing","title":"2.1. Explicit Shape Validation and Fixing","text":"<ul> <li>Location: <code>rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py</code></li> <li>Function: <code>validate_and_fix_shapes(partial_coords, trunk_embeddings, input_features, ...)</code></li> <li>Description: This function explicitly checks the sequence dimensions of input <code>trunk_embeddings</code> tensors (specifically keys <code>s_trunk</code>, <code>s_inputs</code>, <code>sing</code>, <code>pair</code>) against the number of atoms (<code>num_atoms</code>) derived from the <code>partial_coords</code> tensor (<code>partial_coords.shape[1]</code>).</li> <li>Fix Mechanism:<ul> <li>Expansion: If an embedding's sequence dimension is smaller than <code>num_atoms</code>, <code>torch.Tensor.repeat()</code> is used to duplicate the tensor along that dimension until it matches or exceeds <code>num_atoms</code>. The result is then truncated to exactly <code>num_atoms</code>. (Lines 61-68 for single dim, 91-99 for pair dims).</li> <li>Truncation: If an embedding's sequence dimension is larger than <code>num_atoms</code>, simple slicing (<code>[:, :num_atoms]</code>) is used. (Lines 70-71 for single dim, 101-102 for pair dims).</li> </ul> </li> <li>Logging: Issues <code>warnings.warn</code> messages when adjustments are made:<ul> <li><code>\"Adjusting sequence length for {key} from {shape[1]} to {num_atoms}\"</code></li> <li><code>\"Adjusting sequence lengths for {key} from ({shape[1]}, {shape[2]}) to ({num_atoms}, {num_atoms})\"</code></li> </ul> </li> <li>Partial Coordinate Usage: <code>partial_coords</code> (assumed <code>[B, N_atom, 3]</code>) serves as the reference for the target atom dimension (<code>num_atoms</code>).</li> </ul>"},{"location":"reference/residue_atom_bridging/audit_report/#22-implicit-shape-fixing-via-monkey-patching","title":"2.2. Implicit Shape Fixing via Monkey-Patching","text":"<ul> <li>Location: <code>rna_predict/pipeline/stageD/tensor_fixes/__init__.py</code></li> <li>Function: <code>apply_tensor_fixes()</code> (called from <code>rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py::run_stageD_diffusion</code>)</li> <li>Description: This function applies several monkey-patches to core PyTorch operations to implicitly handle shape mismatches during runtime. Two relevant patches identified:<ol> <li><code>fix_tensor_add</code> (Patches <code>torch.Tensor.__add__</code>):<ul> <li>Fix Mechanism: Intercepts <code>RuntimeError</code> during tensor addition. Attempts to resolve mismatches by:<ul> <li>Handling different dimension counts (<code>_handle_dimension_count_mismatch</code>).</li> <li>Handling specific attention bias cases (<code>_handle_attention_bias_mismatch</code>).</li> <li>Attempting manual broadcasting (<code>_try_manual_broadcasting</code>).</li> <li>Uses <code>_expand_tensor_dimension</code> which employs <code>torch.Tensor.repeat_interleave()</code> for expansion or <code>torch.nn.functional.adaptive_avg_pool1d</code> for reduction along the mismatched dimension. This could affect residue-to-atom dimension mismatches if such tensors are added together.</li> </ul> </li> <li>Logging: None directly within the patch; relies on catching the <code>RuntimeError</code>.</li> </ul> </li> <li><code>fix_atom_attention_encoder</code> (Patches <code>torch.nn.Module.forward</code>):<ul> <li>Fix Mechanism: Specifically targets modules likely named <code>AtomAttentionEncoder</code>. If input tensors <code>r_l</code> and <code>s</code> have different sequence lengths (dimension 1), it truncates both to the minimum length using slicing (<code>[:, :min_len]</code>). This directly addresses sequence length mismatches, potentially between residue and atom representations if <code>r_l</code> and <code>s</code> represent these differently within this module type. (Lines 367-372).</li> <li>Logging: None.</li> </ul> </li> </ol> </li> </ul>"},{"location":"reference/residue_atom_bridging/audit_report/#3-partial-coordinate-usage-summary","title":"3. Partial Coordinate Usage Summary","text":"<ul> <li>Explicit: <code>partial_coords</code> are directly used in <code>validate_and_fix_shapes</code> to determine the target <code>num_atoms</code> dimension for explicit reshaping of <code>trunk_embeddings</code>. They are also assigned to <code>input_features['ref_pos']</code>.</li> <li>Implicit: Partial coordinates might be involved in operations affected by the monkey-patches in <code>tensor_fixes</code> (e.g., additions handled by <code>fix_tensor_add</code>), but this is indirect and depends on the specific model architecture and data flow within Stage D.</li> </ul>"},{"location":"reference/residue_atom_bridging/audit_report/#4-conclusion","title":"4. Conclusion","text":"<p>The audit identified two primary mechanisms for handling residue-atom representation mismatches: 1.  An explicit validation and fix function (<code>validate_and_fix_shapes</code>) primarily targeting <code>trunk_embeddings</code> based on <code>partial_coords</code>. 2.  Implicit fixes via monkey-patching core PyTorch operations (<code>tensor_fixes</code>), attempting to resolve errors during runtime, including dimension expansion/reduction and truncation in specific contexts.</p> <p>These ad-hoc fixes, particularly the tensor repetitions and warnings in <code>validate_and_fix_shapes</code>, confirm the need for a more systematic and robust bridging function to map residue-level representations (from Stage B/C) to the atom-level representations required by Stage D. The monkey-patching approach, while potentially functional, obscures the handling of these mismatches and makes debugging difficult.</p>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/","title":"Extended Guidelines for Residue-to-Atom Bridging Caveats","text":"<p>This document provides extended guidelines and discusses potential caveats related to the residue-to-atom bridging mechanism implemented in the pipeline. It addresses handling non-canonical residues, aligning partial coordinates, managing pair embeddings, and ensuring legacy code cleanup.</p>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#1-non-canonical-residue-handling","title":"1. Non-Canonical Residue Handling","text":""},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#explanation-pitfalls","title":"Explanation &amp; Pitfalls","text":"<ul> <li>Standard vs. Non-Standard: The current residue-to-atom mapping assumes canonical residues (A, U, G, C) with a fixed number of heavy atoms. Non-canonical residues (e.g., pseudouridine \u03a8, 5-methylcytosine m\u2075C) present challenges as they may have:<ul> <li>A different atom count than standard residues.</li> <li>Distinct structures or partial occupancy in experimental data, leading to missing coordinates.</li> </ul> </li> <li>Potential Mismatches: Encountering an unrecognized residue can cause the bridging function to:<ul> <li>Fail with a <code>KeyError</code> if the residue isn't found in the canonical dictionary.</li> <li>Use an inaccurate default atom count, leading to indexing errors or incorrect zero-padding.</li> </ul> </li> <li>Data Integration: Partial coordinates from Stage C or external sources might contain the correct atom set for modified residues. Failing to integrate this data can lead to shape mismatches or inaccurate embeddings.</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#recommendations","title":"Recommendations","text":"<ol> <li>Dynamic Mapping:<ul> <li>Implement logic (e.g., in <code>derive_residue_atom_map</code>) to check if a residue is recognized.</li> <li>If recognized: Use the standard canonical dictionary.</li> <li>If unrecognized: Consult a \"modification dictionary\" or parse partial coordinates/metadata specifying the actual atoms.</li> </ul> </li> <li>Metadata-Based Approach:<ul> <li>Allow users to provide metadata (e.g., CSV, JSON) listing non-canonical residue names, atom counts, and naming conventions.</li> <li>Use partial coordinates as a fallback if a residue is not found in any dictionary.</li> </ul> </li> <li>Error Handling:<ul> <li>Provide clear errors when a residue is missing from all references (e.g., \"Residue X is unrecognized. Provide partial coordinates or a custom atom map in config.\").</li> </ul> </li> <li>Testing:<ul> <li>Include test cases with non-canonical residues (or simplified mocks) to verify the bridging logic handles them correctly.</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#2-partial-coordinates-alignment","title":"2. Partial Coordinates Alignment","text":""},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#explanation-pitfalls_1","title":"Explanation &amp; Pitfalls","text":"<ul> <li>Alignment Challenge: Stage C often produces partial or full 3D coordinates (<code>[N_atom, 3]</code>) that need to align with the bridging function's expected atom count and order for the single-sequence embeddings (<code>[N_atom, c_s]</code>). Discrepancies can arise if:<ul> <li>Stage C coordinates skip atoms.</li> <li>Ring-closure logic adds or removes atoms, changing the final <code>N_atom</code>.</li> </ul> </li> <li>Missing Coordinates: Unresolved or omitted atoms in experimental data can reduce the practical <code>N_atom</code>, causing mismatches with the bridging function's theoretical count.</li> <li>Ring-Closure &amp; Edge Cases: Dynamic modifications like ring closure or sugar pucker corrections can alter atom counts or ordering. The bridging mechanism must remain consistent with the final atom representation.</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#recommendations_1","title":"Recommendations","text":"<ol> <li>Consistent Atom Ordering:<ul> <li>Define and enforce a canonical atom order within each residue (e.g., 5' to 3' backbone order, standard base ring order).</li> <li>Ensure Stage C outputs preserve or record this order for consistent indexing during bridging.</li> </ul> </li> <li>Fallback for Missing Atoms:<ul> <li>Establish a clear policy for handling atoms missing in partial coordinates: Are they excluded entirely, or represented with zero-embeddings (<code>[0, ..., 0]</code>)?</li> </ul> </li> <li>Validation Step:<ul> <li>Implement a consistency check post-Stage C processing (e.g., after ring closure) to verify that the final <code>N_atom</code> matches the bridging function's expectation. Fail fast with informative errors if they mismatch.</li> </ul> </li> <li>Documentation:<ul> <li>Clearly document the requirement for Stage C coordinate outputs to remain synchronized with the bridging approach, especially regarding potential changes in atom number or order.</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#3-pair-embeddings","title":"3. Pair Embeddings","text":""},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#explanation-pitfalls_2","title":"Explanation &amp; Pitfalls","text":"<ul> <li>Higher Dimensionality: Bridging pair embeddings (<code>[N, N, c_z]</code>) to the atom level (<code>[N_atom, N_atom, c_z]</code>) is significantly more complex than bridging single embeddings. Each residue pair corresponds to numerous potential atom-atom interactions.</li> <li>Memory Usage: A naive expansion from <code>[N, N, c_z]</code> to <code>[N_atom, N_atom, c_z]</code> can lead to substantial memory increases, especially if <code>N_atom</code> is much larger than <code>N</code>. (e.g., N=8 residues to N_atom=24 atoms implies a 9x increase in pair embedding size).</li> <li>Limited Stage D Requirements: Currently, Stage D might not require atom-level pair embeddings, making the bridging effort potentially unnecessary overhead. The existing plan defers this, suggesting Stage D works with single-atom embeddings or its own adjacency logic.</li> <li>Future Feature: If atom-level pair embeddings become necessary for Stage D later, a dedicated bridging strategy will be required, potentially involving mapping residue pairs to sub-blocks of atom pairs.</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#recommendations_2","title":"Recommendations","text":"<ol> <li>Confirm Stage D Requirements:<ul> <li>Verify whether Stage D absolutely needs atom-level pair embeddings or if single embeddings plus adjacency information suffice.</li> </ul> </li> <li>Explicitly Mark as \"Future Task\":<ul> <li>Clearly state in documentation that bridging pair embeddings (<code>[N, N, c_z]</code> -&gt; <code>[N_atom, N_atom, c_z]</code>) is a recognized potential future requirement but is currently out of scope to manage complexity.</li> </ul> </li> <li>Performance Considerations:<ul> <li>If implemented later, prioritize memory efficiency. Consider block-sparse representations or on-demand expansion instead of dense <code>[N_atom, N_atom, c_z]</code> tensors.</li> </ul> </li> <li>Potential Data Structures:<ul> <li>Explore data structures that reference residue-residue pairs and enumerate the corresponding sub-atom pairs, which might be more memory-efficient than a dense tensor.</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#4-legacy-code-cleanup","title":"4. Legacy Code Cleanup","text":""},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#explanation-pitfalls_3","title":"Explanation &amp; Pitfalls","text":"<ul> <li>Scattered Fixes: Previous patches addressing shape mismatches (e.g., in <code>validate_and_fix_shapes</code>, <code>tensor_fixes/</code>) might be scattered throughout the codebase, potentially undocumented or partially implemented.</li> <li>Risk of Redundant Behavior: Leftover expansion logic could conflict with the new bridging mechanism, causing double-application of expansions or incorrect shapes (e.g., logs showing \"Adjusting from 24 to 72\").</li> <li>Regression Risk: Unremoved legacy code could lead to regressions where the pipeline reverts to old expansion behaviors due to minor configuration changes or updates.</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#recommendations_3","title":"Recommendations","text":"<ol> <li>Global Search:<ul> <li>Perform a repository-wide search for keywords related to shape expansion and mismatch fixing (e.g., <code>expand</code>, <code>clamp</code>, <code>Adjusting sequence length</code>, <code>validate_and_fix_shapes</code>, <code>res-&gt;atom mismatch</code>).</li> </ul> </li> <li>Modular Removal:<ul> <li>Systematically remove or comment out code specifically intended to fix residue-to-atom mismatches. Test the pipeline after each removal to ensure the bridging function correctly handles the scenario.</li> </ul> </li> <li>Identify Unrelated Fixes:<ul> <li>Preserve shape or memory patches that address genuinely unrelated issues (e.g., 4D -&gt; 5D tensor reshaping for attention mechanisms) and do not interfere with residue-to-atom bridging.</li> </ul> </li> <li>Continuous Integration Checks:<ul> <li>Enhance CI tests to fail if legacy expansion logic is reintroduced or if logs indicating shape mismatches reappear. This helps prevent accidental reintroduction of problematic code.</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#additional-considerations","title":"Additional Considerations","text":""},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#implementation-examples","title":"Implementation Examples","text":"<ul> <li>Provide concise code snippets or references demonstrating the core bridging function logic.</li> <li>Include pseudo-code examples for handling partial coordinate mismatches or constructing a \"modification dictionary\" for non-canonical residues. (Example to be added based on final implementation)</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#testing-protocol","title":"Testing Protocol","text":"<ul> <li>Outline specific test strategies to verify:<ul> <li>Correct handling of partial coordinates by the bridging logic.</li> <li>Successful removal of legacy expansion code without introducing regressions.</li> <li>Graceful handling of non-canonical residues (if supported).</li> </ul> </li> <li>(Specific test cases/files to be referenced here)</li> </ul>"},{"location":"reference/residue_atom_bridging/bridging_caveats_guidelines/#faq-notes","title":"FAQ / Notes","text":"<ul> <li>Q: What if a residue has partial occupancy in PDB data?<ul> <li>A: The handling depends on the chosen policy for missing atoms (Section 2, Recommendation 2). If partial coordinates are used as input, atoms with zero occupancy might be treated as missing. The bridging function should either exclude them or represent them with zero-embeddings based on the defined policy.</li> </ul> </li> <li>(Add other relevant FAQs as they arise)</li> <li>Link to relevant design documents (e.g., <code>docs/pipeline/residue_atom_bridging/design_spec.md</code>) and automated test results/logs for further reference.</li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/","title":"Design Specification: Residue-to-Atom Bridging Function","text":"<p>Version: 1.0 Date: 2025-04-10 Author: Roo (Architect Mode)</p>"},{"location":"reference/residue_atom_bridging/design_spec/#1-introduction-goal","title":"1. Introduction &amp; Goal","text":"<p>This document outlines the design for a systematic bridging mechanism to convert residue-level tensor representations (e.g., single embeddings <code>s_emb</code> with shape <code>[N_residue, C_s]</code>) generated in earlier pipeline stages (like Stage B) to the atom-level representations (shape <code>[N_atom, C_s]</code>) often required by later stages (like Stage D).</p> <p>This replaces previous ad-hoc methods (like simple tensor repeats based on <code>partial_coords</code> shape or runtime monkey-patching) identified in <code>docs/pipeline/audit_residue_atom_mismatch.md</code> with a robust and explicit approach.</p> <p>The core components are: 1.  A helper function <code>derive_residue_atom_map</code> to determine the mapping between residue indices and their corresponding global atom indices. 2.  The main bridging function <code>residue_to_atoms</code> that uses this map to expand the residue embeddings.</p>"},{"location":"reference/residue_atom_bridging/design_spec/#2-target-file-location","title":"2. Target File &amp; Location","text":"<ul> <li>File: <code>rna_predict/utils/tensor_utils.py</code></li> <li>Rationale: Centralized utility location, accessible across pipeline stages, avoids duplication, and promotes reuse for potential future tensor manipulation utilities.</li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/#3-function-signatures-data-types","title":"3. Function Signatures &amp; Data Types","text":"<pre><code># In rna_predict/utils/tensor_utils.py\nimport torch\nfrom typing import List, Union, Optional, Dict\n\n# Define standard atom names (Example - requires verification/update with actual RNA atom names used in the project)\nSTANDARD_RNA_ATOMS = {\n    'A': ['P', 'O5\\'', 'C5\\'', 'C4\\'', 'O4\\'', 'C3\\'', 'O3\\'', 'C2\\'', 'C1\\'', 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2', 'N3', 'C4'], # 19 atoms\n    'U': ['P', 'O5\\'', 'C5\\'', 'C4\\'', 'O4\\'', 'C3\\'', 'O3\\'', 'C2\\'', 'O2\\'', 'C1\\'', 'N1', 'C2', 'O2', 'N3', 'C4', 'O4', 'C5', 'C6'], # 18 atoms\n    'G': ['P', 'O5\\'', 'C5\\'', 'C4\\'', 'O4\\'', 'C3\\'', 'O3\\'', 'C2\\'', 'C1\\'', 'N9', 'C8', 'N7', 'C5', 'C6', 'O6', 'N1', 'C2', 'N2', 'N3', 'C4'], # 20 atoms\n    'C': ['P', 'O5\\'', 'C5\\'', 'C4\\'', 'O4\\'', 'C3\\'', 'O3\\'', 'C2\\'', 'C1\\'', 'N1', 'C2', 'O2', 'N3', 'C4', 'N4', 'C5', 'C6'], # 17 atoms\n    # Add other standard residues if needed\n}\n\n# Type alias for clarity\nResidueAtomMap = List[List[int]] # List where index is residue_idx, value is list of global atom indices for that residue\n\ndef derive_residue_atom_map(\n    sequence: Union[str, List[str]],\n    partial_coords: Optional[torch.Tensor] = None, # Shape [B, N_atom, 3] or [N_atom, 3]\n    atom_metadata: Optional[Dict] = None, # e.g., {'atom_names': ['P', ...], 'residue_indices': [0, 0, ... 1, ...]}\n    atom_counts_map: Optional[Dict[str, int]] = None # Fallback: {'A': 19, 'U': 18, ...} derived from STANDARD_RNA_ATOMS\n) -&gt; ResidueAtomMap:\n    \"\"\"\n    Derives the mapping from residue index to a list of corresponding global atom indices.\n\n    This helper function determines how atoms are grouped by residue, which is essential\n    for the `residue_to_atoms` bridging function.\n\n    Args:\n        sequence (Union[str, List[str]]): The RNA sequence (e.g., \"AUCG\" or ['A', 'U', 'C', 'G']). Length must match N_residue.\n        partial_coords (Optional[torch.Tensor]): Atom coordinates, potentially used to infer N_atom. Shape [B, N_atom, 3] or [N_atom, 3].\n        atom_metadata (Optional[Dict]): Explicit metadata linking atoms to residues. Expected keys might include 'residue_indices' (list/tensor of length N_atom mapping each atom to its residue index).\n        atom_counts_map (Optional[Dict[str, int]]): A fallback map providing the number of atoms for each standard residue type (e.g., derived from STANDARD_RNA_ATOMS). Used when `partial_coords` and `atom_metadata` are insufficient.\n\n    Priority for Derivation:\n    1. Use `atom_metadata` if provided (most explicit and reliable).\n    2. Use `partial_coords` shape and assume contiguous block ordering based on `sequence` and standard atom counts if `atom_metadata` is missing. Requires validation that total inferred atoms match `partial_coords.shape[-2]`.\n    3. Use `atom_counts_map` (e.g., from STANDARD_RNA_ATOMS lengths) and `sequence` if coordinates and metadata are both missing. Assumes contiguous blocks.\n\n    Returns:\n        ResidueAtomMap: Map where index `i` contains the list of global atom indices for residue `i`.\n\n    Raises:\n        ValueError: If insufficient information is provided, inputs are inconsistent (e.g., sequence length mismatch, atom count mismatch), or assumptions (like contiguous order) are violated based on available data.\n        KeyError: If a residue in `sequence` is not found in `atom_counts_map` when operating in fallback mode (Method 3).\n    \"\"\"\n    # Implementation details deferred to Code mode.\n    # Key logic: Input validation, selecting derivation method based on priority,\n    # generating the list of lists, handling errors.\n    pass\n\ndef residue_to_atoms(\n    s_emb: torch.Tensor,\n    residue_atom_map: ResidueAtomMap,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Expands residue-level embeddings to atom-level embeddings using a precomputed map.\n\n    Assigns the embedding of residue `i` to all atoms belonging to residue `i` as defined\n    by the `residue_atom_map`.\n\n    Args:\n        s_emb (torch.Tensor): Residue embeddings. Shape [N_residue, C_s] or [B, N_residue, C_s].\n        residue_atom_map (ResidueAtomMap): A list where index `i` contains the list of\n                                           global atom indices corresponding to residue `i`.\n                                           The length of this list must equal N_residue.\n                                           The union of all inner lists must cover all atoms\n                                           from 0 to N_atom-1 exactly once and without overlaps.\n\n    Returns:\n        torch.Tensor: Atom-level embeddings. Shape [N_atom, C_s] or [B, N_atom, C_s].\n\n    Raises:\n        ValueError: If input shapes are inconsistent (e.g., `len(residue_atom_map)` != `s_emb.shape[-2]`)\n                    or `residue_atom_map` is invalid (e.g., gaps or overlaps in atom indices).\n    \"\"\"\n    # Implementation details deferred to Code mode.\n    # Key logic: Handle batched/unbatched input, calculate N_atom from map,\n    # initialize output tensor, use map to efficiently assign s_emb[i] to atom_embs[atom_indices],\n    # validate map integrity. Consider using torch.gather with an atom_to_residue_idx tensor for efficiency.\n    pass\n</code></pre>"},{"location":"reference/residue_atom_bridging/design_spec/#4-residue-to-atom-mapping-residue_atom_map","title":"4. Residue-to-Atom Mapping (<code>residue_atom_map</code>)","text":"<ul> <li>Role: This <code>List[List[int]]</code> structure is the critical link provided to the <code>residue_to_atoms</code> function. It explicitly defines the atom indices associated with each residue index.</li> <li>Generation: The <code>derive_residue_atom_map</code> helper function is responsible for creating this map based on available inputs (sequence, coords, metadata), prioritizing explicit metadata over inferred mappings.</li> <li>Standard Atoms: The <code>STANDARD_RNA_ATOMS</code> dictionary provides the default atom names and counts for A, U, G, C, used by <code>derive_residue_atom_map</code> primarily in its fallback mode or for validation. Note: The specific atom names in <code>STANDARD_RNA_ATOMS</code> must be verified against the project's conventions.</li> <li>Non-Canonical Handling: Handled within <code>derive_residue_atom_map</code>. By default, if operating in fallback mode (using <code>atom_counts_map</code>), encountering an unknown residue type in the <code>sequence</code> will raise a <code>KeyError</code> or <code>ValueError</code>, forcing explicit handling rather than silent failure. If derived from explicit <code>atom_metadata</code>, non-canonicals are handled if the metadata correctly describes them.</li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/#5-core-logic-residue_to_atoms","title":"5. Core Logic (<code>residue_to_atoms</code>)","text":"<ol> <li>Inputs: Takes <code>s_emb</code> (<code>[N_res, C_s]</code> or <code>[B, N_res, C_s]</code>) and the validated <code>residue_atom_map</code>.</li> <li>Calculate <code>N_atom</code>: Determine the total number of atoms by summing the lengths of the inner lists in <code>residue_atom_map</code> (or checking the max index + 1).</li> <li>Initialize Output: Create <code>atom_embs</code> tensor of shape <code>[N_atom, C_s]</code> or <code>[B, N_atom, C_s]</code> with the same dtype and device as <code>s_emb</code>.</li> <li>Expand/Assign: Iterate through <code>residue_atom_map</code>. For each residue <code>i</code> and its <code>atom_indices</code>:<ul> <li>Select the source embedding: <code>s_emb[i]</code> or <code>s_emb[:, i]</code>.</li> <li>Assign this embedding vector to the target atoms: <code>atom_embs[atom_indices]</code> or <code>atom_embs[:, atom_indices]</code>. (Implementation should aim for efficiency, e.g., using advanced indexing or potentially constructing an <code>atom_to_residue_idx</code> tensor and using <code>torch.gather</code>).</li> </ul> </li> <li>Return: Output the populated <code>atom_embs</code> tensor.</li> </ol>"},{"location":"reference/residue_atom_bridging/design_spec/#6-process-flow-diagram","title":"6. Process Flow Diagram","text":"<pre><code>graph TD\n    subgraph Input Preparation Phase\n        I1[Sequence Info (str/list)] --&gt; D(derive_residue_atom_map);\n        I2[Partial Coords [N_atom, 3] (Optional)] --&gt; D;\n        I3[Atom Metadata (Optional, e.g., residue_indices)] --&gt; D;\n        I4[Standard Atom Counts Map (Fallback)] --&gt; D;\n    end\n\n    subgraph Bridging Phase\n        E1[s_emb [N_res, C_s]] --&gt; R(residue_to_atoms);\n        D -- residue_atom_map [List[List[int]]] --&gt; R;\n        R -- atom_embs [N_atom, C_s] --&gt; O[Output Atom Embeddings];\n    end\n\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n    style R fill:#ccf,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"reference/residue_atom_bridging/design_spec/#7-edge-case-handling","title":"7. Edge Case Handling","text":"<ul> <li>Empty Inputs: Both functions should handle empty sequences/embeddings gracefully, returning empty maps/tensors of the correct dimensionality.</li> <li>Inconsistencies: <code>derive_residue_atom_map</code> must rigorously validate consistency between sequence length, <code>N_residue</code> inferred from inputs, and <code>N_atom</code> inferred from inputs. Raise <code>ValueError</code> on mismatch.</li> <li>Invalid Map: <code>residue_to_atoms</code> must validate the received <code>residue_atom_map</code> ensures all atoms 0 to <code>N_atom-1</code> are covered exactly once. Raise <code>ValueError</code> if gaps or overlaps exist. <code>len(residue_atom_map)</code> must match <code>s_emb.shape[-2]</code>.</li> <li>Non-Canonicals: Explicitly handled by <code>derive_residue_atom_map</code> raising an error in fallback mode if an unknown residue type is encountered.</li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/#8-logging","title":"8. Logging","text":"<ul> <li><code>derive_residue_atom_map</code>:<ul> <li>INFO: Log the method used for map generation (metadata, coords, fallback).</li> <li>WARN: Log any assumptions made (e.g., assuming contiguous atom order when using coords without explicit residue indices).</li> <li>ERROR: Log errors for inconsistent inputs or unknown residues in fallback mode.</li> </ul> </li> <li><code>residue_to_atoms</code>:<ul> <li>DEBUG: Log input and output tensor shapes.</li> <li>WARN: Log warnings for empty inputs being processed.</li> <li>ERROR: Log errors for invalid <code>residue_atom_map</code>.</li> </ul> </li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/#9-pair-embeddings-c_z","title":"9. Pair Embeddings (<code>c_z</code>)","text":"<ul> <li>Out of Scope: This design specification only covers the bridging of single embeddings (<code>s_emb</code>, shape <code>[N_res, C_s]</code>).</li> <li>Future Consideration: Bridging pair embeddings (<code>[N_res, N_res, C_z]</code> -&gt; <code>[N_atom, N_atom, C_z]</code>) is significantly more complex. It requires mapping every pair of residues to the corresponding block of atom-atom pairs and likely involves substantial memory overhead (<code>N_atom^2</code> vs <code>N_res^2</code>). This is deferred for future work if required by Stage D or other downstream modules.</li> </ul>"},{"location":"reference/residue_atom_bridging/design_spec/#10-implementation-notes","title":"10. Implementation Notes","text":"<ul> <li>Focus on efficiency within <code>residue_to_atoms</code>, potentially avoiding Python loops over residues/atoms if possible by using vectorized PyTorch operations like advanced indexing or <code>torch.gather</code>.</li> <li>Ensure comprehensive unit tests cover various input scenarios, edge cases, and map generation methods.</li> </ul>"},{"location":"reference/residue_atom_bridging/documentation_draft/","title":"Draft Documentation for Residue-Atom Bridging","text":""},{"location":"reference/residue_atom_bridging/documentation_draft/#draft-code-commentsdocstrings","title":"Draft Code Comments/Docstrings","text":"<pre><code>import torch\nfrom typing import Dict, List, Optional, Tuple, Union\n\n# Placeholder for actual atom definitions\nRNA_ATOM_MAP = {\n    'A': ['N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2', 'N3', 'C4', \"C1'\", \"C2'\", \"O2'\", \"C3'\", \"O3'\", \"C4'\", \"O4'\", \"C5'\", \"O5'\", 'P', 'OP1', 'OP2'],\n    'U': ['N1', 'C2', 'O2', 'N3', 'C4', 'O4', 'C5', 'C6', \"C1'\", \"C2'\", \"O2'\", \"C3'\", \"O3'\", \"C4'\", \"O4'\", \"C5'\", \"O5'\", 'P', 'OP1', 'OP2'],\n    'G': ['N9', 'C8', 'N7', 'C5', 'C6', 'O6', 'N1', 'C2', 'N2', 'N3', 'C4', \"C1'\", \"C2'\", \"O2'\", \"C3'\", \"O3'\", \"C4'\", \"O4'\", \"C5'\", \"O5'\", 'P', 'OP1', 'OP2'],\n    'C': ['N1', 'C2', 'O2', 'N3', 'C4', 'N4', 'C5', 'C6', \"C1'\", \"C2'\", \"O2'\", \"C3'\", \"O3'\", \"C4'\", \"O4'\", \"C5'\", \"O5'\", 'P', 'OP1', 'OP2'],\n    # Add other canonical/non-canonical mappings as needed\n}\n\nDEFAULT_ATOMS_PER_RESIDUE = max(len(v) for v in RNA_ATOM_MAP.values()) # Example default\n\ndef derive_residue_atom_map(\n    sequence: List[str],\n    partial_coords: Optional[torch.Tensor] = None,\n    atom_definitions: Dict[str, List[str]] = RNA_ATOM_MAP\n) -&gt; Tuple[Dict[int, List[int]], int, List[str]]:\n    \"\"\"\n    Derives a mapping from residue indices to corresponding atom indices.\n\n    This function constructs a map indicating which atoms belong to each residue\n    in the sequence. It primarily uses standard definitions but can potentially\n    be extended to use partial coordinates if available and necessary for\n    handling modifications or non-standard residues.\n\n    Args:\n        sequence (List[str]): List of residue identifiers (e.g., ['A', 'U', 'G']).\n        partial_coords (Optional[torch.Tensor]): Tensor of partial coordinates\n            (shape [N_atom_partial, 3]), potentially used for map derivation\n            in complex cases (currently placeholder). Defaults to None.\n        atom_definitions (Dict[str, List[str]]): A dictionary mapping residue\n            types to their constituent atom names. Defaults to RNA_ATOM_MAP.\n\n    Returns:\n        Tuple[Dict[int, List[int]], int, List[str]]:\n        - residue_to_atom_indices (Dict[int, List[int]]): Dictionary mapping\n          residue index (0 to N-1) to a list of global atom indices\n          (0 to N_atom-1) belonging to that residue.\n        - total_atoms (int): The total number of atoms inferred for the sequence.\n        - atom_names (List[str]): Flat list of atom names corresponding to the\n          global atom indices.\n    \"\"\"\n    residue_to_atom_indices = {}\n    atom_names = []\n    current_atom_index = 0\n    total_atoms = 0\n\n    # Basic implementation using standard definitions\n    # TODO: Enhance logic if partial_coords are needed for complex cases\n    if partial_coords is not None:\n        # Placeholder: Logic to use partial_coords if needed, e.g.,\n        # to infer atom counts or handle non-standard residues.\n        # This could involve checking coordinate presence/absence.\n        print(\"Warning: Partial coordinate usage in map derivation not fully implemented.\")\n\n    for i, res_type in enumerate(sequence):\n        if res_type in atom_definitions:\n            atoms = atom_definitions[res_type]\n            num_atoms_in_residue = len(atoms)\n            indices = list(range(current_atom_index, current_atom_index + num_atoms_in_residue))\n            residue_to_atom_indices[i] = indices\n            atom_names.extend(atoms)\n            current_atom_index += num_atoms_in_residue\n        else:\n            # Handle unknown residue types (e.g., use default, skip, error)\n            # Option: Use a default number of atoms or raise error\n            print(f\"Warning: Unknown residue type '{res_type}' at index {i}. Using default atom count (if defined) or skipping.\")\n            # Example fallback (needs refinement based on requirements)\n            # num_atoms_in_residue = DEFAULT_ATOMS_PER_RESIDUE\n            # indices = list(range(current_atom_index, current_atom_index + num_atoms_in_residue))\n            # residue_to_atom_indices[i] = indices\n            # atom_names.extend([f\"UNK_{j}\" for j in range(num_atoms_in_residue)])\n            # current_atom_index += num_atoms_in_residue\n            residue_to_atom_indices[i] = [] # Or handle differently\n\n    total_atoms = current_atom_index\n    print(f\"Derived residue-atom map for {len(sequence)} residues -&gt; {total_atoms} atoms.\")\n    return residue_to_atom_indices, total_atoms, atom_names\n\ndef residue_to_atoms(\n    s_emb: torch.Tensor,\n    sequence: List[str],\n    residue_atom_map: Optional[Dict[int, List[int]]] = None,\n    total_atoms: Optional[int] = None,\n    partial_coords: Optional[torch.Tensor] = None,\n    atom_definitions: Dict[str, List[str]] = RNA_ATOM_MAP\n) -&gt; torch.Tensor:\n    \"\"\"\n    Expands residue-level embeddings to atom-level embeddings.\n\n    Takes a tensor of residue embeddings (e.g., from Stage B) and expands it\n    to atom-level embeddings suitable for Stage D, using a residue-to-atom map.\n\n    Args:\n        s_emb (torch.Tensor): Residue-level embeddings tensor (shape [N, c_s]).\n        sequence (List[str]): List of residue identifiers corresponding to s_emb.\n        residue_atom_map (Optional[Dict[int, List[int]]]): Precomputed map from\n            residue index to list of global atom indices. If None, it will be\n            derived using `derive_residue_atom_map`. Defaults to None.\n        total_atoms (Optional[int]): Precomputed total number of atoms. If None,\n            it will be derived. Defaults to None.\n        partial_coords (Optional[torch.Tensor]): Tensor of partial coordinates\n            (shape [N_atom_partial, 3]), passed to `derive_residue_atom_map`\n            if the map needs to be derived. Defaults to None.\n        atom_definitions (Dict[str, List[str]]): Definitions used if deriving\n            the map. Defaults to RNA_ATOM_MAP.\n\n    Returns:\n        torch.Tensor: Atom-level embeddings tensor (shape [N_atom, c_s]).\n\n    Raises:\n        ValueError: If the residue dimension of s_emb doesn't match the sequence length.\n        ValueError: If the derived/provided map is inconsistent.\n    \"\"\"\n    N, c_s = s_emb.shape\n    if N != len(sequence):\n        raise ValueError(f\"Residue dimension mismatch: s_emb has {N} residues, sequence has {len(sequence)}.\")\n\n    if residue_atom_map is None or total_atoms is None:\n        print(\"Deriving residue-atom map internally...\")\n        residue_atom_map, total_atoms, _ = derive_residue_atom_map(\n            sequence, partial_coords, atom_definitions\n        )\n        if total_atoms == 0 and N &gt; 0:\n             raise ValueError(\"Failed to derive a valid residue-atom map; total atoms is zero.\")\n        elif total_atoms == 0 and N == 0:\n             print(\"Input sequence is empty, returning empty atom tensor.\")\n             return torch.empty((0, c_s), dtype=s_emb.dtype, device=s_emb.device)\n\n\n    # Create the target atom embedding tensor\n    atom_embs = torch.zeros((total_atoms, c_s), dtype=s_emb.dtype, device=s_emb.device)\n\n    # Expand embeddings using the map\n    all_mapped_indices = []\n    for res_idx, atom_indices in residue_atom_map.items():\n        if res_idx &lt; N: # Ensure residue index is valid for s_emb\n            if atom_indices: # Check if list is not empty\n                # Assign the residue embedding to all its corresponding atoms\n                atom_embs[atom_indices, :] = s_emb[res_idx, :]\n                all_mapped_indices.extend(atom_indices)\n        else:\n            print(f\"Warning: Residue index {res_idx} in map is out of bounds for s_emb (size {N}). Skipping.\")\n\n    # Verification (optional but recommended)\n    if len(all_mapped_indices) != total_atoms:\n         unmapped_indices = set(range(total_atoms)) - set(all_mapped_indices)\n         print(f\"Warning: Mismatch in mapped atoms. Expected {total_atoms}, mapped {len(all_mapped_indices)}. Unmapped indices: {unmapped_indices}\")\n         # Decide handling: error or proceed with potentially zero-filled embeddings for unmapped atoms\n\n    print(f\"Expanded residue embeddings [N={N}, c_s={c_s}] to atom embeddings [N_atom={total_atoms}, c_s={c_s}]\")\n    return atom_embs\n</code></pre>"},{"location":"reference/residue_atom_bridging/documentation_draft/#draft-readmemd-section","title":"Draft README.md Section","text":""},{"location":"reference/residue_atom_bridging/documentation_draft/#pipeline-residue-vs-atom-representation-bridging","title":"Pipeline: Residue vs. Atom Representation Bridging","text":"<p>Problem: The M2 pipeline involves stages operating at different levels of molecular representation. Specifically, Stage B (Pairformer + TorsionBERT) primarily outputs residue-level information (e.g., sequence embeddings <code>s_emb</code> with shape <code>[N, c_s]</code>, where <code>N</code> is the number of residues), while Stage D (Diffusion) requires atom-level inputs (e.g., atom embeddings <code>atom_embs</code> with shape <code>[N_atom, c_s]</code> or coordinates <code>[N_atom, 3]</code>, where <code>N_atom</code> is the total number of atoms). Previously, this mismatch was handled by ad-hoc fixes within Stage D or utility functions, leading to potential inconsistencies and reduced maintainability.</p> <p>Solution: Bridging Function To address this systematically, a dedicated bridging mechanism has been introduced between the residue-level outputs and atom-level inputs. This is implemented primarily via the <code>residue_to_atoms</code> utility function (likely located in <code>rna_predict/utils/tensor_utils.py</code> or similar). This function takes residue-level tensors (like <code>s_emb</code>) and expands them to the corresponding atom-level representation based on a defined mapping.</p> <p>Residue-to-Atom Map Derivation: The core of the bridging process relies on a <code>residue_atom_map</code>. This map defines which atoms constitute each residue in the sequence. The map is typically derived by the helper function <code>derive_residue_atom_map</code> using standard definitions for canonical RNA nucleotides (A, U, G, C) stored in a structure like <code>RNA_ATOM_MAP</code>. The derivation logic can potentially be extended to handle non-canonical residues or utilize partial coordinate information (e.g., from Stage C) if available and necessary, although the primary method relies on standard definitions for robustness.</p> <p>Data Flow: The typical data flow involving the bridge is: 1. Stage B produces residue-level outputs (e.g., <code>s_emb: [N, c_s]</code>). 2. Before Stage D, the <code>residue_to_atoms</code> function is called. 3. <code>residue_to_atoms</code> either uses a provided map or calls <code>derive_residue_atom_map</code> using the sequence information (and potentially partial coordinates). 4. The function expands <code>s_emb</code> to <code>atom_embs: [N_atom, c_s]</code>. 5. Stage D receives the correctly shaped <code>atom_embs</code> tensor.</p> <p>Interaction with Partial Coordinates (Stage C): Stage C might provide partial coordinates (e.g., backbone atoms). While the primary map derivation uses standard definitions, the <code>derive_residue_atom_map</code> function is designed to potentially accept these partial coordinates. This allows for future enhancements where partial coordinates could help resolve ambiguities, identify present atoms, or handle non-standard residues more accurately if needed. Currently, the standard definitions are the default mechanism.</p> <p>Conceptual Usage Example: The bridging function is integrated into the pipeline script (e.g., <code>run_full_pipeline.py</code> or the entry point of Stage D) after Stage B/C outputs are generated and before Stage D processing begins:</p> <pre><code># --- In run_full_pipeline.py or similar ---\n\n# Assume stage_b_output contains 's_emb' [N, c_s] and 'sequence' list\n# Assume stage_c_output might contain 'partial_coords' [N_atom_partial, 3] (optional)\n\nfrom rna_predict.utils.tensor_utils import residue_to_atoms # Adjust import path\n\nresidue_embeddings = stage_b_output['s_emb']\nsequence = stage_b_output['sequence']\npartial_coords = stage_c_output.get('partial_coords', None) # Get partial coords if available\n\n# Bridge residue embeddings to atom embeddings\natom_embeddings = residue_to_atoms(\n    s_emb=residue_embeddings,\n    sequence=sequence,\n    partial_coords=partial_coords # Pass partial coords if they should influence map derivation\n)\n\n# Prepare inputs for Stage D using atom_embeddings\nstage_d_input = {\n    # ... other inputs ...\n    'atom_embs': atom_embeddings,\n    'sequence': sequence, # Pass sequence along if needed by Stage D\n    # ... potentially pass atom_mask derived from map if needed ...\n}\n\n# Run Stage D\nstage_d_output = run_stage_d(stage_d_input)\n\n# --- End Example ---\n</code></pre> <p>This structured approach ensures that Stage D consistently receives atom-level data derived systematically from residue-level precursors, improving pipeline robustness and clarity.</p>"},{"location":"reference/residue_atom_bridging/implementation_notes/","title":"Implementation Notes: Residue-to-Atom Bridging","text":""},{"location":"reference/residue_atom_bridging/implementation_notes/#overview","title":"Overview","text":"<p>This document describes the implementation of the residue-to-atom bridging mechanism for Task 12: \"Unify Residue vs. Atom Representation in Stages B &amp; D\". The implementation follows the design specifications outlined in <code>design_spec.md</code> and the refactoring plan in <code>refactoring_plan.md</code>.</p>"},{"location":"reference/residue_atom_bridging/implementation_notes/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/residue_atom_bridging/implementation_notes/#1-new-utility-functions","title":"1. New Utility Functions","text":"<p>Two new utility functions have been implemented in <code>rna_predict/utils/tensor_utils.py</code>:</p> <ol> <li> <p><code>derive_residue_atom_map</code>: This function derives a mapping from residue indices to their corresponding atom indices. It can use sequence information, partial coordinates, and/or atom metadata to create this mapping.</p> </li> <li> <p><code>residue_to_atoms</code>: This function expands residue-level embeddings to atom-level embeddings using the mapping derived by <code>derive_residue_atom_map</code>.</p> </li> </ol> <p>These functions provide a systematic way to bridge the gap between residue-level representations (from Stage B) and atom-level representations (required by Stage D).</p>"},{"location":"reference/residue_atom_bridging/implementation_notes/#2-integration-points","title":"2. Integration Points","text":"<p>The bridging mechanism has been integrated into the pipeline at the following points:</p> <ol> <li> <p><code>run_stageD_unified.py</code>: The ad-hoc <code>validate_and_fix_shapes</code> function has been replaced with a new <code>bridge_residue_to_atom</code> function that uses the utility functions to systematically bridge residue-level embeddings to atom-level embeddings.</p> </li> <li> <p><code>run_stageD.py</code>: The call to <code>validate_and_fix_shapes</code> has been updated to use <code>bridge_residue_to_atom</code> instead.</p> </li> </ol>"},{"location":"reference/residue_atom_bridging/implementation_notes/#3-tensor-fixes-cleanup","title":"3. Tensor Fixes Cleanup","text":"<p>The <code>fix_atom_attention_encoder</code> function in <code>tensor_fixes/__init__.py</code> has been removed as it's now handled by the residue-to-atom bridging mechanism. The <code>apply_tensor_fixes</code> function has been updated to remove the call to this function.</p>"},{"location":"reference/residue_atom_bridging/implementation_notes/#testing","title":"Testing","text":"<p>A comprehensive test suite has been implemented in <code>tests/utils/test_tensor_utils.py</code> to verify the correctness of the new utility functions. The tests cover:</p> <ol> <li>Unit Tests for <code>derive_residue_atom_map</code>:</li> <li>Deriving the map from sequence only</li> <li>Deriving the map from sequence and partial coordinates</li> <li>Deriving the map from atom metadata</li> <li>Handling string sequences</li> <li>Handling empty sequences</li> <li> <p>Handling unknown residue types</p> </li> <li> <p>Unit Tests for <code>residue_to_atoms</code>:</p> </li> <li>Basic expansion of residue embeddings to atom embeddings</li> <li>Expansion with batched residue embeddings</li> <li>Handling empty inputs</li> <li>Handling invalid maps</li> <li> <p>Handling shape mismatches</p> </li> <li> <p>Integration Tests:</p> </li> <li>End-to-end tests from sequence to atom embeddings</li> <li>Tests with partial coordinates</li> </ol> <p>All tests are passing, indicating that the implementation is working as expected.</p>"},{"location":"reference/residue_atom_bridging/implementation_notes/#benefits-of-the-new-approach","title":"Benefits of the New Approach","text":"<p>The new residue-to-atom bridging mechanism offers several advantages over the previous ad-hoc approach:</p> <ol> <li> <p>Systematic and Explicit: The bridging process is now explicit and follows a systematic approach, making it easier to understand and maintain.</p> </li> <li> <p>Flexible: The mechanism can use different sources of information (sequence, partial coordinates, atom metadata) to derive the mapping, making it adaptable to different scenarios.</p> </li> <li> <p>Robust: The implementation includes comprehensive validation and error handling to ensure that the bridging process is robust and reliable.</p> </li> <li> <p>Testable: The utility functions are designed to be easily testable, allowing for comprehensive test coverage.</p> </li> <li> <p>Maintainable: The code is well-documented and follows a clear design, making it easier to maintain and extend in the future.</p> </li> </ol>"},{"location":"reference/residue_atom_bridging/implementation_notes/#limitations-and-future-work","title":"Limitations and Future Work","text":"<ol> <li> <p>Pair Embeddings: The current implementation only handles single embeddings (e.g., <code>s_trunk</code>, <code>s_inputs</code>). Bridging pair embeddings (e.g., <code>pair</code>) is more complex and is left for future work if needed.</p> </li> <li> <p>Non-Canonical Residues: The implementation includes basic handling for non-canonical residues, but more sophisticated handling may be needed in the future.</p> </li> <li> <p>Performance Optimization: The current implementation prioritizes correctness and clarity over performance. Future optimizations could include more efficient tensor operations for large-scale data.</p> </li> </ol>"},{"location":"reference/residue_atom_bridging/implementation_notes/#conclusion","title":"Conclusion","text":"<p>The implementation of the residue-to-atom bridging mechanism successfully addresses the requirements of Task 12. It provides a systematic and robust way to bridge the gap between residue-level and atom-level representations in the RNA prediction pipeline, replacing the previous ad-hoc approach with a well-designed and tested solution.</p>"},{"location":"reference/residue_atom_bridging/refactoring_plan/","title":"Refactoring Plan: Bridging Function Integration (Residue-to-Atom)","text":"<p>Version: 1.1 Date: 2025-04-10 Author: Roo (Architect Mode)</p> <p>Objective: Integrate the <code>residue_to_atoms</code> function (and its helper <code>derive_residue_atom_map</code>) into the M2 pipeline, replacing ad-hoc residue-to-atom expansion logic identified in <code>docs/pipeline/audit_residue_atom_mismatch.md</code>.</p> <p>References: *   Audit Report: <code>docs/pipeline/audit_residue_atom_mismatch.md</code> *   Bridging Function Design: <code>docs/pipeline/bridging_function_design.md</code> *   User Feedback (Provided 2025-04-10 ~17:11 PM CT)</p>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#1-integration-point","title":"1. Integration Point","text":"<ul> <li>Chosen Location: Within the main execution flow of Stage D, specifically in the script <code>rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py</code>.</li> <li>Timing: The bridging functions (<code>derive_residue_atom_map</code> and <code>residue_to_atoms</code>) should be called after receiving the data dictionary from the previous stage (containing residue-level embeddings like <code>trunk_embeddings</code>, sequence information, and potentially <code>partial_coords</code>) but before this data is passed into the core Stage D model components (e.g., the diffusion model itself or modules like <code>AtomAttentionEncoder</code>). Crucially, this should happen before the point where the now-obsolete <code>validate_and_fix_shapes</code> function was called.</li> <li>Rationale:<ul> <li>This point ensures that all necessary inputs for <code>derive_residue_atom_map</code> (sequence, potentially partial coords) and <code>residue_to_atoms</code> (residue embeddings) are available from the upstream stages.</li> <li>It centralizes the conversion logic, ensuring that all downstream components within Stage D receive consistently shaped atom-level embeddings.</li> <li>It directly replaces the location where the primary ad-hoc fix (<code>validate_and_fix_shapes</code>) was previously applied.</li> <li>Integrating earlier (e.g., <code>run_full_pipeline.py</code>) might be premature if Stage C outputs are strictly residue-level. Integrating later (within specific Stage D sub-modules) would lead to redundant calls and complex data passing.</li> </ul> </li> </ul>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#2-code-removal-modification","title":"2. Code Removal / Modification","text":"<p>The following code sections, identified in the audit, need modification or removal:</p> <ol> <li> <p>File: <code>rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py</code></p> <ul> <li>Action: Remove the entire <code>validate_and_fix_shapes</code> function.<ul> <li>Reasoning: Its logic (checking shapes and using <code>torch.Tensor.repeat()</code> or slicing based on <code>partial_coords</code>) is entirely superseded by the explicit <code>residue_to_atoms</code> bridging function.</li> </ul> </li> <li>Action: Remove any calls to <code>validate_and_fix_shapes</code>.</li> <li>Action: Remove associated <code>warnings.warn</code> calls related to shape adjustments within the old function.</li> </ul> </li> <li> <p>File: <code>rna_predict/pipeline/stageD/tensor_fixes/__init__.py</code></p> <ul> <li>Action: Remove the <code>fix_atom_attention_encoder</code> monkey-patch function and its application logic within <code>apply_tensor_fixes</code>.<ul> <li>Reasoning: Redundant due to guaranteed correct input shapes from the bridging function.</li> </ul> </li> <li>Action: Modify the <code>fix_tensor_add</code> monkey-patch.<ul> <li>Details: Remove any logic within <code>fix_tensor_add</code> (and its helpers like <code>_expand_tensor_dimension</code>) that specifically attempts to resolve dimension mismatches by repeating/expanding tensors in a way that mimics residue-to-atom conversion (e.g., expanding dimension N_res to N_atom based on size difference).</li> <li>Details: Retain the general-purpose shape mismatch handling logic within <code>fix_tensor_add</code> if it addresses other confirmed issues in the pipeline (e.g., dimension count differences, attention bias adjustments, general broadcasting attempts unrelated to residue/atom expansion). This requires careful analysis during implementation to isolate and preserve only the necessary non-residue/atom fixes.</li> <li>Reasoning: Avoid redundant residue/atom expansion logic while preserving fixes for other potential shape issues, as confirmed by user feedback.</li> </ul> </li> <li>Action: Retain the call to <code>apply_tensor_fixes()</code> from <code>run_stageD_unified.py</code> (assuming <code>fix_tensor_add</code> is partially retained). If implementation analysis reveals <code>fix_tensor_add</code> is entirely unnecessary after removing residue/atom logic, this call can be removed then.</li> </ul> </li> <li> <p>File: <code>rna_predict/utils/tensor_utils.py</code></p> <ul> <li>Action: Add the new functions <code>derive_residue_atom_map</code> and <code>residue_to_atoms</code> as specified in <code>docs/pipeline/bridging_function_design.md</code>.</li> <li>Action: Add the <code>STANDARD_RNA_ATOMS</code> dictionary (requires verification of atom names/counts against project standards during implementation).</li> <li>Action: Add necessary imports (<code>torch</code>, <code>typing</code>).</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#3-data-flow-configuration-changes","title":"3. Data Flow / Configuration Changes","text":"<ol> <li> <p>Integration Logic in <code>run_stageD_unified.py</code>:</p> <ul> <li>Retrieve necessary inputs from the data dictionary passed from the previous stage. Implementation Note: Verify exact keys during implementation (e.g., <code>'sequence'</code> vs <code>'rna_sequence'</code>, <code>'s_trunk'</code> vs <code>'s_embeddings'</code>, identify all relevant single-embedding keys needing conversion like <code>'s_inputs'</code>). Assumed keys:<ul> <li><code>'sequence'</code></li> <li><code>'partial_coords'</code> (Optional)</li> <li><code>'trunk_embeddings'</code> (or similar dict/tensor containing residue embeddings)</li> <li><code>'atom_metadata'</code> (Optional)</li> </ul> </li> <li>Step 1: Call <code>derive_residue_atom_map</code> once:     <pre><code># Example call signature (verify keys during implementation)\nresidue_atom_map = derive_residue_atom_map(\n    sequence=pipeline_data['sequence'],\n    partial_coords=pipeline_data.get('partial_coords'),\n    atom_metadata=pipeline_data.get('atom_metadata')\n    # atom_counts_map will be derived internally from STANDARD_RNA_ATOMS if needed\n)\n</code></pre></li> <li>Step 2: Iterate through the relevant residue-level embedding tensors (e.g., <code>pipeline_data['trunk_embeddings']['s_trunk']</code>, <code>pipeline_data['s_inputs']</code>, etc.). For each <code>s_emb_res</code>:     <pre><code># Example call signature\ns_emb_atom = residue_to_atoms(\n    s_emb=s_emb_res,\n    residue_atom_map=residue_atom_map\n)\n# Update the pipeline data dictionary with the atom-level tensor\n# (e.g., replace pipeline_data['trunk_embeddings']['s_trunk'] with s_emb_atom)\n</code></pre></li> <li>Step 3: Ensure the updated data dictionary (now containing atom-level embeddings) is passed to downstream Stage D components.</li> </ul> </li> <li> <p>Data Structures:</p> <ul> <li>The primary change is modifying the values within the existing data dictionary passed to Stage D (e.g., <code>pipeline_data</code> or <code>input_features</code>). Residue-level tensors (<code>[B, N_res, C_s]</code>) identified as needing conversion will be replaced by their atom-level counterparts (<code>[B, N_atom, C_s]</code>).</li> <li>No new persistent data structures are introduced, but the state of the data dictionary changes at the integration point.</li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>No changes to configuration files are anticipated based on the audit. The previous ad-hoc logic did not appear to be config-driven.</li> </ul> </li> <li> <p>Data Loaders:</p> <ul> <li>No changes anticipated for data loaders. They are assumed to provide the necessary base information (sequence, potentially coordinates, residue-level features) upon which the pipeline stages operate.</li> </ul> </li> </ol>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#4-partial-coordinates-handling","title":"4. Partial Coordinates Handling","text":"<ul> <li><code>partial_coords</code> (shape <code>[B, N_atom, 3]</code>), if provided by Stage C, will be passed as an optional input to <code>derive_residue_atom_map</code>.</li> <li><code>derive_residue_atom_map</code> can use the <code>N_atom</code> dimension from <code>partial_coords.shape[-2]</code> to validate or infer the residue-to-atom mapping (Method 2 in the design doc).</li> <li>The <code>residue_to_atoms</code> function transforms embeddings, it does not modify the <code>partial_coords</code> tensor itself.</li> <li>The target <code>N_atom</code> dimension for the output embeddings from <code>residue_to_atoms</code> will be consistent with the <code>N_atom</code> dimension derived by <code>derive_residue_atom_map</code> (which may have used <code>partial_coords</code> shape as input). This ensures consistency.</li> </ul>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#5-limitations","title":"5. Limitations","text":"<ul> <li>This plan and the underlying bridging function design only address single embeddings (e.g., <code>s_trunk</code>, <code>s_inputs</code> with shape <code>[N_res, C_s]</code>).</li> <li>Pair embeddings (e.g., <code>pair</code> features with shape <code>[N_res, N_res, C_z]</code>) are out of scope for this refactoring task. Converting these to atom-level (<code>[N_atom, N_atom, C_z]</code>) requires a separate, more complex design and implementation effort if needed by Stage D.</li> </ul>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#6-implementation-considerations","title":"6. Implementation Considerations","text":"<ul> <li>Non-Canonicals: Ensure <code>derive_residue_atom_map</code> raises appropriate errors if unknown residue types are encountered without explicit metadata.</li> <li>Device Placement: Ensure tensors created or manipulated by the bridging functions are placed on the correct device (<code>.to(device)</code>), especially in distributed/multi-GPU scenarios.</li> <li>Testing: Test thoroughly with real PDB data inputs to confirm shapes are correct and no mismatch warnings/errors occur in Stage D.</li> </ul>"},{"location":"reference/residue_atom_bridging/refactoring_plan/#7-diagram-integration-point","title":"7. Diagram: Integration Point","text":"<p>```mermaid graph TD     subgraph Pipeline Flow         StageB_C[Stage B/C Output] --&gt; DataDictRes{Data Dictionary (Residue Lvl)};         DataDictRes -- Sequence, Embeddings, Coords? --&gt; StageD_Entry(run_stageD_unified.py);</p> <pre><code>    subgraph StageD_Entry [run_stageD_unified.py]\n        direction LR\n        Start --&gt; Call_DeriveMap[Call derive_residue_atom_map];\n        Call_DeriveMap -- residue_atom_map --&gt; Call_Bridge[Loop: Call residue_to_atoms for each s_emb];\n        Call_Bridge -- atom_level_embeddings --&gt; UpdateDict[Update Data Dictionary];\n        UpdateDict --&gt; DataDictAtom{Data Dictionary (Atom Lvl)};\n        DataDictAtom --&gt; StageD_Model[Stage D Core Model];\n    end\n\n    StageD_Model --&gt; StageD_Output[Stage D Output];\n\n    subgraph Obsolete/Modified Code\n       Removed_Validate[validate_and_fix_shapes (Removed)]\n       Modified_MonkeyPatch[tensor_fixes (Modified: fix_tensor_add)]\n       Removed_MonkeyPatch2[tensor_fixes (Removed: fix_atom_attention_encoder)]\n    end\nend\n\nstyle Call_DeriveMap fill:#f9f,stroke:#333,stroke-width:2px\nstyle Call_Bridge fill:#ccf,stroke:#333,stroke-width:2px\nstyle Removed_Validate fill:#ddd,stroke:#666,stroke-width:1px,stroke-dasharray: 5 5\nstyle Modified_MonkeyPatch fill:#ffe0b3,stroke:#666,stroke-width:1px,stroke-dasharray: 2 2 /* Orange dashed for modified */\nstyle Removed_MonkeyPatch2 fill:#ddd,stroke:#666,stroke-width:1px,stroke-dasharray: 5 5\n</code></pre>"},{"location":"reference/torsion_calculations/Standard_Bond_Lengths_and_Angles_in_RNA_Nucleotides/","title":"Bond Lengths & Angles","text":"<p>Standard Bond Lengths and Angles in RNA Nucleotides</p> <p>Figure: Structure of an RNA nucleotide (guanosine monophosphate) with standard atom numbering. Ribose carbons are numbered 1\u2032 through 5\u2032 (red labels). The nitrogenous base (here, guanine) attaches at the 1\u2032 carbon, and the phosphate attaches at the 5\u2032 carbon, linking to the 3\u2032 carbon of the next nucleotide (5\u2032\u21923\u2032 direction).</p> <p>Overview: RNA nucleotides have well-defined covalent geometry with characteristic bond lengths and bond angles. These geometry parameters have been determined from high-resolution crystallography and are often treated as standard (ideal) values in structural models \ufffc. Table 1 and Table 2 summarize typical bond lengths and bond angles for the sugar-phosphate backbone of RNA, based on analyses of crystal structures \ufffc \ufffc. Table values are mean lengths/angles with minor variations (standard deviations in parentheses) observed across many high-quality structures. These serve as reference points; real structures may show small deviations due to conformation or environment. All values are for ribose (RNA) nucleotides; where relevant, the analogous DNA values are noted for comparison (RNA's 2\u2032-OH subtly affects some geometry \ufffc).</p> <p>Backbone (Sugar\u2013Phosphate) Geometry</p> <p>Bond Lengths: The ribose sugar ring and the phosphate linkage exhibit consistent bond lengths (in \u00e5ngstr\u00f6ms). Table 1 lists the typical lengths for bonds in the sugar (C5\u2032\u2013C4\u2032\u2013C3\u2032\u2013C2\u2032\u2013C1\u2032\u2013O4\u2032 ring and exocyclic attachments) and the connecting phosphate group. Notably, single bonds between carbon atoms in the sugar (C\u2013C) are about 1.52\u20131.53 \u00c5, the C\u2013O bonds in the ring are shorter (~1.41\u20131.45 \u00c5), and the glycosidic bond (N\u2013C1\u2032, linking the base) is ~1.47 \u00c5 \ufffc. Bonds involving the phosphate are slightly longer: the P\u2013O bonds to the backbone oxygens are ~1.59\u20131.61 \u00c5, and the P\u2013O bonds to the non-bridging oxygens are ~1.48 \u00c5 \ufffc.</p> <p>Table 1: Typical Bond Lengths in RNA Backbone (mean \u00b1 standard deviation, in \u00c5) \ufffc \ufffc</p> <p>Bond    Length (\u00c5)    Description C1\u2032\u2013C2\u2032 (sugar)    1.528 \u00b1 0.010    Sugar ring C\u2013C bond (ribose) C2\u2032\u2013C3\u2032 (sugar)    1.525 \u00b1 0.011    Sugar ring C\u2013C bond C3\u2032\u2013C4\u2032 (sugar)    1.524 \u00b1 0.011    Sugar ring C\u2013C bond C4\u2032\u2013O4\u2032 (sugar)    1.453 \u00b1 0.012    Sugar ring C\u2013O bond O4\u2032\u2013C1\u2032 (sugar)    1.414 \u00b1 0.012    Sugar ring O\u2013C bond C5\u2032\u2013C4\u2032 (sugar exocyc)    1.510 \u00b1 0.013    Exocyclic C\u2013C bond to 5\u2032 carbon C3\u2032\u2013O3\u2032 (sugar exocyc)    1.423 \u00b1 0.014    Exocyclic C\u2013O bond at 3\u2032 (to phosphate) C2\u2032\u2013O2\u2032 (sugar 2\u2032-OH)    1.413 \u00b1 0.013    2\u2032-hydroxyl C\u2013O bond (absent in DNA) N\u2013C1\u2032 (glycosidic)    1.47 \u00b1 0.02    N-glycosidic bond (base to sugar C1\u2032) P\u2013O5\u2032 (phosphate)    1.593 \u00b1 0.010    Phosphate to 5\u2032-oxygen (bridging) P\u2013O3\u2032 (phosphate)    1.607 \u00b1 0.012    Phosphate to 3\u2032-oxygen (bridging) P\u2013O (non-bridging)    1.485 \u00b1 0.017    Phosphate to free oxygens (average) O5\u2032\u2013C5\u2032 (sugar-phosphate)    1.440 \u00b1 0.016    5\u2032-oxygen to 5\u2032-carbon (sugar\u2013phosphate) O3\u2032\u2013C3\u2032 (sugar-phosphate)    1.433 \u00b1 0.019    3\u2032-oxygen to 3\u2032-carbon (sugar\u2013phosphate)</p> <p>Sources: Data compiled from crystallographic surveys of nucleic acid geometry \ufffc \ufffc. For comparison, deoxyribose (DNA) sugars have very similar bond lengths (e.g. C1\u2032\u2013C2\u2032 ~1.521 \u00c5 in DNA) \ufffc. The glycosidic bond (N1\u2013C1\u2032 in pyrimidines, N9\u2013C1\u2032 in purines) is consistently ~1.47 \u00c5 in both RNA and DNA \ufffc. Phosphate P\u2013O bond lengths are the largest in the backbone (~1.59\u20131.61 \u00c5 for the bridging bonds) due to the tetrahedral phosphate geometry \ufffc.</p> <p>Bond Angles: Bond angles in the RNA backbone cluster near the tetrahedral ideal (~109\u2013115\u00b0 for sp\u00b3 centers) with some variation depending on ring geometry and substituents. Table 2 lists typical valence angles at key atoms in the sugar ring and at the phosphate linkage. Within the five-membered ribose ring, angles at carbon atoms are around 100\u2013106\u00b0, and angles involving the ring oxygen O4\u2032 are a bit larger (~109\u2013110\u00b0) \ufffc. The exocyclic angles where the ring attaches to substituents (like C4\u2032\u2013C5\u2032\u2013O5\u2032 and C3\u2032\u2013O3\u2032\u2013P) are around 109\u2013112\u00b0, close to tetrahedral. Around the phosphate, the O\u2013P\u2013O angles are ~104\u2013111\u00b0, and the angle between the two non-bridging oxygens at P is ~120\u00b0 \ufffc (reflecting the trigonal bipyramidal resonance structure of phosphate). Notably, the C1\u2032\u2013N\u2013C (sugar\u2013base\u2013base) angle at the glycosidic nitrogen is ~118\u2013121\u00b0, reflecting the planar geometry of the base attachment \ufffc.</p> <p>Table 2: Typical Bond Angles in RNA Backbone (mean \u00b1 standard deviation, in degrees) \ufffc \ufffc</p> <p>Angle (Atom triplet)    Angle (\u00b0)    Description C1\u2032\u2013C2\u2032\u2013C3\u2032 (ring)    101.5 \u00b1 0.9    Sugar ring angle at C2\u2032 C2\u2032\u2013C3\u2032\u2013C4\u2032 (ring)    102.7 \u00b1 1.0    Sugar ring angle at C3\u2032 C3\u2032\u2013C4\u2032\u2013O4\u2032 (ring)    105.5 \u00b1 1.4    Sugar ring angle at C4\u2032 C4\u2032\u2013O4\u2032\u2013C1\u2032 (ring)    109.6 \u00b1 0.9    Sugar ring angle at O4\u2032 O4\u2032\u2013C1\u2032\u2013C2\u2032 (ring)    106.4 \u00b1 1.4    Sugar ring angle at C1\u2032 C2\u2032\u2013C1\u2032\u2013N (glycosidic)    113.4 \u00b1 1.6    Base-to-sugar angle at C1\u2032 (pyrimidine) O4\u2032\u2013C1\u2032\u2013N (glycosidic)    108.2 \u00b1 1.0    Base-to-sugar angle at C1\u2032 (purine) C5\u2032\u2013C4\u2032\u2013C3\u2032 (ring junction)    115.5 \u00b1 1.5    Angle at C4\u2032 (C5\u2032\u2013C4\u2032\u2013C3\u2032) C5\u2032\u2013C4\u2032\u2013O4\u2032    109.2 \u00b1 1.4    Angle at C4\u2032 (C5\u2032\u2013C4\u2032\u2013O4\u2032) (O5\u2032)\u2013C5\u2032\u2013C4\u2032    111.6 \u00b1 1.7    Angle at C5\u2032 (O5\u2032\u2013C5\u2032\u2013C4\u2032) C3\u2032\u2013O3\u2032\u2013P (backbone)    119.7 \u00b1 1.2    Angle at O3\u2032 (sugar\u2013phosphate link) P\u2013O5\u2032\u2013C5\u2032 (backbone)    120.9 \u00b1 1.6    Angle at O5\u2032 (phosphate\u2013sugar link) O5\u2032\u2013P\u2013O (non-bridging)    110.7 \u00b1 1.2 / 105.7 \u00b1 0.9    Angles at P between 5\u2032-O and non-bridging O's (two values for larger/smaller angle) O3\u2032\u2013P\u2013O (non-bridging)    110.5 \u00b1 1.1 / 105.2 \u00b1 2.2    Angles at P between 3\u2032-O and non-bridging O's O1P\u2013P\u2013O2P (non-bridging)    119.6 \u00b1 1.5    Angle between the two non-bridging O's at P</p> <p>Sources: Angles derived from surveys of RNA crystal structures \ufffc \ufffc. The sugar ring is slightly distorted from a perfect pentagon: internal C\u2013C\u2013C angles (~101\u2013103\u00b0) are smaller than the ideal tetrahedral angle due to ring strain, while angles involving oxygen are larger (~106\u2013110\u00b0). The glycosidic bond angles at C1\u2032 (listed for purine and pyrimidine cases) are around 108\u2013113\u00b0 \ufffc. At the phosphate, the O\u2013P\u2013O angles reflect a tetrahedral geometry: one wide angle (~120\u00b0) between the two non-bridging oxygens, and two pairs of smaller angles (~110\u00b0 and ~105\u00b0) between each bridging oxygen and the non-bridging oxygens \ufffc.</p> <p>Nucleobase Internal Geometry</p> <p>Each RNA base (adenine, guanine, cytosine, uracil) is a planar aromatic heterocycle with standard bond lengths and angles that are relatively uniform across different environments. In general, C\u2013N and C\u2013C bonds within the base rings are around 1.33\u20131.38 \u00c5 in length, reflecting partial double-bond character of the aromatic system \ufffc \ufffc. For example, in uracil (a pyrimidine), N1\u2013C2 \u2248 1.38 \u00c5 and C5\u2013C6 \u2248 1.34 \u00c5 \ufffc; in adenine (a purine), N1\u2013C2 \u2248 1.34 \u00c5 and C5\u2013C6 \u2248 1.41 \u00c5 \ufffc. Double bonds to exocyclic oxygens (carbonyl groups) are shorter, about 1.23 \u00c5 (e.g. C2=O2 in uracil is 1.219 \u00c5 \ufffc). Exocyclic N\u2013H groups (like the N6 amine in adenine or N4 in cytosine) have N\u2013C bonds ~1.33\u20131.34 \u00c5 \ufffc, similar to a single\u2013double bond resonance. The bond angles within base rings are close to 120\u00b0 (typical of trigonal planar sp\u00b2 centers in an aromatic ring). For instance, angles in the six-membered rings of uracil range from about 114.6\u00b0 to 127.0\u00b0 \ufffc, and in adenine from about 110.6\u00b0 to 129.3\u00b0 \ufffc (the wider angles occur at positions where the ring fusion or exocyclic substituents impose strain). The angle at the glycosidic nitrogen (where the base attaches to the sugar) is also around 118\u2013121\u00b0 \ufffc, consistent with the base being planar while bound to C1\u2032. Overall, the bases maintain an almost planar geometry with slight distortions; these internal bond lengths and angles are very stable, varying only by a few hundredths of an \u00c5 or a few degrees between different RNA structures \ufffc \ufffc.</p> <p>Variation with Environment: Covalent bond lengths and angles in RNA are remarkably consistent, but small variations occur due to environmental and conformational factors:     \u2022   Sugar Pucker and Conformation: The ribose can adopt different puckers (C3\u2032-endo in A-form RNA vs C2\u2032-endo in DNA). This affects some bond angles subtly. For example, the C1\u2032\u2013C2\u2032\u2013C3\u2032 angle is ~101.5\u00b0 in RNA's typical C3\u2032-endo pucker, compared to ~102.7\u00b0 in the C2\u2032-endo form \ufffc. Other angles and lengths show minor shifts with pucker, but remain within a few degrees or 0.01 \u00c5. The presence of the 2\u2032-OH in RNA also slightly influences geometry (compare RNA vs DNA values in Tables above), but the differences in bond lengths are on the order of 0.005\u20130.01 \u00c5 \ufffc (e.g., the C4\u2032\u2013O4\u2032 bond is 1.453 \u00c5 in ribose vs 1.446 \u00c5 in deoxyribose).     \u2022   Base Pairing and Hydrogen Bonding: When bases form Watson\u2013Crick pairs or other hydrogen bonds, there are small but discernible shifts in their bond lengths and angles due to electronic effects of pairing. A recent high-precision study found that bond geometries in paired bases differ only slightly from those in isolated bases \ufffc. For instance, the C=O bond in guanine might lengthen by ~0.003 \u00c5 when hydrogen-bonded, and ring angles adjust by 1\u20132\u00b0 at most \u2013 changes that are statistically significant but very small \ufffc. Overall, base pairing does not drastically distort covalent geometry, a fact exploited in refinement restraints for crystal structures.     \u2022   Solvent and Ionic Environment: In crystal structures (often at cryogenic temperature), the measured bond lengths are effectively static averages. In solution at room temperature (or in molecular dynamics simulations), covalent bonds undergo thermal vibrations. These vibrations are small \u2013 covalent bonds in MD simulations typically fluctuate with an RMS deviation of ~0.01\u20130.02 \u00c5 around their equilibrium length, and bond angles wiggle by a few degrees. Strong ionic interactions or unusual solvation can impose slight stresses (for example, a tightly bound metal ion or a protein contact might compress a phosphate angle or stretch a bond marginally), but such effects are usually within experimental error. Force-field based simulations often keep bond lengths very close to reference values via stiff potentials or constraints, so the average geometry in a well-parametrized MD simulation remains in line with the crystallographic standard \ufffc.     \u2022   Chemical Modifications and Protonation: If nucleotides are chemically modified (e.g., methylated) or protonated (as can happen in acid conditions or specific enzyme active sites), their bond geometry can change slightly. Protonation of a base can alter bond orders and lengths (e.g., protonated adenine has an N1\u2013C2 bond ~0.02 \u00c5 longer than in neutral adenine) \ufffc. Nonetheless, even in these cases, the covalent geometry changes are minor adjustments rather than large deformations.</p> <p>In summary, the standard geometry of RNA nucleotides \u2013 as summarized in the tables \u2013 is a reliable reference across a wide range of conditions. Real-world structures may deviate by small amounts, but significant departures (beyond ~0.03 \u00c5 for bond lengths or ~3\u20135\u00b0 for angles) are usually indicators of either experimental error or unusual strain. Structure validation programs like MolProbity flag such outliers by comparing to these standard values \ufffc.</p> <p>Software Tools for RNA Geometry Analysis and Modeling</p> <p>Researchers have developed numerous tools to analyze, visualize, and model RNA structures with respect to their bond lengths and angles. Below is a selection of commonly used software and resources (with an emphasis on Python-based tools) relevant to RNA geometry:     \u2022   Barnaba (Python Library) \u2013 Barnaba is a dedicated Python library for analysis of nucleic acid structures and molecular dynamics trajectories \ufffc. It provides convenient functions to calculate geometric parameters such as interatomic distances, bond angles, backbone torsion angles, sugar pucker, and even detect base pairing interactions \ufffc. Barnaba is well-suited for processing MD simulation outputs or PDB structures, allowing users to extract time series of bond lengths or angles and compare them to standard values. It also includes plotting routines and can back-calculate experimental data for validation \ufffc \ufffc. Barnaba is available open-source (GitHub) and is a go-to tool for scientists studying RNA dynamics.     \u2022   MDAnalysis and MDTraj (Python Libraries) \u2013 MDAnalysis \ufffc and MDTraj are libraries that allow one to read molecular dynamics trajectory files (or static PDB/mmCIF structures) and compute geometric quantities. With these libraries, a user can easily select atoms and calculate distances, bond angles, and dihedral angles. MDAnalysis, for example, has a module specialized for nucleic acids that can directly compute all backbone torsions (\u03b1, \u03b2, \u2026 \u03b6) and the \u03c7 (chi) glycosidic angle \ufffc, which indirectly involves the bond geometry. While these tools do not by themselves enforce geometry, they are excellent for analyzing whether an RNA structure's bonds and angles stay near standard values during a simulation. They can also interface with popular simulation packages (Amber, GROMACS, OpenMM) to analyze results in Python.     \u2022   3DNA/DSSR (Nucleic Acid Analysis Suite) \u2013 3DNA is a longstanding software package (written in C/C++) for the analysis and rebuilding of nucleic acid structures. Its newer component DSSR (Discrete Structural States of RNA) focuses on RNA specifically. 3DNA can calculate a comprehensive set of parameters: not only backbone torsions and sugar puckers, but also base-pair geometric parameters. While 3DNA isn't a Python library, it is commonly used in the field for validating structures. It provides ideal values and deviations for bonds and angles and can be scripted or called from Python. For instance, one can use DSSR to quickly identify any bond geometry outliers in an RNA PDB file, or to rebuild an RNA structure with idealized geometry for simulation input \ufffc. Integrations like DSSR-PyMOL and DSSR-Jmol also allow interactive visualization of RNA geometry in those programs.     \u2022   Visualization Tools (PyMOL, ChimeraX, VMD) \u2013 High-quality molecular graphics programs are essential for inspecting RNA geometry. PyMOL (which has a Python API) can be used to measure distances and angles between selected atoms in an RNA structure. Scripting in PyMOL's Python interface makes it possible to automate the measurement of all bond lengths in a structure and compare them to reference values. UCSF ChimeraX (and its predecessor Chimera) provide commands to report bond lengths/angles and can be scripted in Python. They are handy for visualizing where in a 3D structure any unusual geometry might occur (e.g., highlighting a bond angle outlier). VMD (Visual Molecular Dynamics) is another tool (with Tcl and Python interfaces) widely used to visualize MD simulations of RNA; one can use it to track a particular bond length over time, for example. These visualization tools often work in tandem with analysis libraries by providing an intuitive view of the numeric data.     \u2022   Structure Building and Refinement Tools \u2013 Software that models RNA structures usually incorporates standard geometry as a foundation. AmberTools (particularly the tleap and nucleic acid builder (NAB) components) can build ideal A-form or custom RNA helices. They use an internal library of standard bond lengths/angles (derived from crystallographic data) to construct initial coordinates. OpenMM (a Python API for molecular simulations) can be used to energy-minimize or simulate RNA; with a proper force field (e.g., Amber RNA force field), it will maintain bonds near their equilibrium lengths (which correspond to the standard values in Table 1). PHENIX and Coot are crystallography programs (not Python-focused) that allow one to refine RNA structures \u2013 they apply geometric restraints so that bond lengths and angles stay close to ideal unless the experimental data strongly suggest otherwise \ufffc. These tools often use a restraint library (e.g., the PDB's Chemical Component Dictionary for each nucleotide) which contains the target bond length/angle for every bond in an A, U, G, or C nucleotide.     \u2022   Rosetta and RNA Structure Prediction \u2013 Rosetta is a computational modeling suite that includes methods for RNA 3D structure prediction (e.g., FARFAR2 in Rosetta) \ufffc \ufffc. Rosetta's all-atom refinement treats bond lengths and angles with scoring penalties if they deviate from ideal values, ensuring that any predicted RNA structure has realistic geometry. Rosetta's Python binding (PyRosetta) enables users to script custom protocols. While Rosetta is primarily about predicting tertiary structure (the arrangement of helices and loops), it inherently maintains standard covalent geometry during its Monte Carlo fragment assembly and energy minimization steps. Thus, researchers using Rosetta for RNA modeling can trust that the output structures will obey the standard bond lengths/angles unless intentionally altered.     \u2022   Databases and Reference Data Sets: A number of databases provide reference data for RNA structures and geometry. The Protein Data Bank (PDB) is the primary repository for RNA-containing 3D structures; the PDB's Chemical Component Dictionary (CCD) entry for each nucleotide (e.g., AMP, UMP, GMP, CMP for RNA monomers) lists the ideal bond lengths and angles used in structure refinement. The Nucleic Acid Database (NDB) is a specialized collection focusing on nucleic acid structures and often provides curated geometric data. The RNA 3D Hub (Bowling Green State University) compiles RNA structures and even classifies motifs; it includes an RNA Knowledge Base where one can download lists of high-resolution structures and derive geometry statistics. For example, the NAKB (Nucleic Acid Knowledge Base) we cited in tables is part of this hub, offering downloadable data on nucleotide geometry and base pair parameters. Additionally, the Cambridge Structural Database (CSD) contains small-molecule crystal structures of nucleobases and nucleosides; these high-precision structures are used to update ideal geometry (as in the study by Gilski et al., 2019) \ufffc. Researchers can query CSD for, say, \"adenosine\" to retrieve experimentally measured bond lengths in isolated nucleosides for comparison.     \u2022   MolProbity and Validation Tools: MolProbity is a web server/tool for all-atom structure validation (often used for proteins, DNA/RNA). It checks bond lengths and angles in RNA and flags any outliers beyond a certain cutoff \ufffc. While MolProbity is not a modeling tool, it is invaluable for analyzing geometry: after building or refining an RNA structure, one can run MolProbity (or the PDB's built-in validation reports) to get a list of bond length/angle deviations. This helps ensure the model adheres to standard geometry. The output highlights, for example, if a phosphate bond angle is significantly compressed, guiding the researcher to correct it. MolProbity's underlying data (Chen et al., 2010) includes target RNA geometry values consistent with those given in our tables.</p> <p>In practice, researchers often use a combination of these tools: for instance, one might simulate an RNA with OpenMM, analyze the trajectory with MDAnalysis or Barnaba to extract bond length distributions, visualize the structure in VMD or PyMOL to see any anomalies, and consult reference data from NDB or CCD to compare observed values against ideal ones. The availability of Python APIs for many of these tasks makes it feasible to set up automated pipelines\u2014for example, scanning a set of RNA PDB files to tabulate all bond lengths and angles and check them against standard references.</p> <p>References: The bond length and angle data in this report were drawn from crystallographic analyses by Gelbin et al. and Clowney et al. (1996) \ufffc and updated values from Parkinson et al. (1996) \ufffc, as compiled in the Nucleic Acid Knowledge Base. These have been recently cross-validated by quantum calculations and ultra-high-resolution structures \ufffc. The software tools mentioned are documented in their respective publications and manuals (e.g., Barnaba by Bottaro et al., 2019 \ufffc, MDAnalysis by Gowers et al., 2016, 3DNA by Lu &amp; Olson, 2003, and Rosetta FARFAR by Das et al., 2010). Together, these resources enable accurate modeling and analysis of RNA geometry across experimental and computational contexts.</p>"},{"location":"reference/torsion_calculations/Torsion_Angles_3D_Coordinates_RNA_Structure/","title":"3D Coordinates","text":"<p>Torsion Angles and 3D Coordinates in RNA Structure</p> <p>Theoretical Background</p> <p>Definition and Importance of Torsion Angles: In the context of RNA, a torsion angle (or dihedral angle) is the angle between two intersecting planes defined by four sequential atoms in the RNA backbone or sugar\u2013base linkage \ufffc. These angles describe the rotation around bonds in the nucleotide chain and are a fundamental way to represent RNA conformation. Unlike Cartesian coordinates (which give absolute atom positions), torsional coordinates capture the molecule\u2019s internal geometry and are invariant under translation or rotation of the whole molecule \ufffc. This makes torsion angles extremely useful for comparing structures and understanding flexibility, since they directly encode the backbone\u2019s rotational degrees of freedom. Each nucleotide can be seen as a link with several rotatable bonds; collectively, the torsion angles of all these bonds determine how an RNA strand folds in three-dimensional space \ufffc. Because RNA folding involves rotations about these bonds, torsion angles play a pivotal role in RNA structural formation and stability. They influence how bases stack and pair, and certain angle values are necessary for forming stable helices or loops. In fact, the energetic landscape governing RNA folding is largely shaped by bonded interactions (including planar and torsional angles) along with non-bonded interactions \ufffc. Small changes in a torsion angle can significantly alter the 3D structure, meaning that torsional configurations often correlate with specific motifs or stable conformational states of the RNA.</p> <p>Relationship to RNA Folding and Stability: RNA molecules fold into specific 3D structures by adopting particular sets of torsion angles that minimize free energy and satisfy functional constraints. Certain combinations of backbone angles are associated with well-known secondary or tertiary structure motifs. For example, the standard A-form double helix corresponds to a characteristic set of backbone torsions (with the sugar in C3\u2032-endo pucker and typical values for angles like \u03b2, \u03b3, and \u03b5) that support tight base stacking \ufffc. Departures from these preferred ranges can introduce strain or require compensation by other interactions, affecting stability. Indeed, analysis of high-resolution RNA structures has shown that RNA backbone torsions cluster into a limited number of rotameric conformations (analogous to protein Ramachandran regions). A seminal survey identified about 46 discrete backbone conformers that RNAs commonly adopt; tellingly, most of these conformers correspond to specific structural roles or motifs in known RNA structures \ufffc. For instance, one such conformer (\u201c1a\u201d) represents the canonical A-form helix geometry, while another (\u201c5z\u201d) corresponds to the start of an S-turn motif \ufffc. This demonstrates that particular torsion angle patterns are integral to forming recurring RNA folds. Moreover, certain torsional arrangements are often associated with functional sites \u2013 for example, the angles in a receptor loop or a tRNA anticodon may differ from those in a helix, enabling unique interactions. Studies have explicitly noted that specific torsion angles (or unusual angle flips) can be markers of functional RNA motifs such as protein or ligand binding sites \ufffc. Thus, mapping the torsional profile of an RNA helps predict and rationalize its folding and stability: if all backbone angles lie in favorable ranges, the structure is likely stable, whereas out-of-range torsions might signal strained or flexible regions. In summary, torsion angles are not only descriptive of RNA conformation but also predictive of folding behavior, with certain angle sets required for stable, biologically active structures.</p> <p>Conventions for Defining RNA Torsion Angles: By convention, the RNA backbone is described by six main-chain torsion angles per residue (labeled \u03b1 through \u03b6) plus one torsion for the glycosidic bond (\u03c7). The standard IUPAC nomenclature defines these angles based on the sequence of atoms in the phosphodiester backbone \ufffc. The six backbone angles correspond to rotations about the bonds: P\u2013O5\u2032 (\u03b1), O5\u2032\u2013C5\u2032 (\u03b2), C5\u2032\u2013C4\u2032 (\u03b3), C4\u2032\u2013C3\u2032 (\u03b4), C3\u2032\u2013O3\u2032 (\u03b5), and O3\u2032\u2013P (next nucleotide, \u03b6) \ufffc. For example, \u03b1 is the torsion around the O3\u2032(i\u20131)\u2013P bond linking one nucleotide to the next, and \u03b4 is the torsion around the C5\u2032\u2013C4\u2032 bond within the sugar-phosphate backbone \ufffc. In addition, the \u03c7 torsion describes the rotation about the N-glycosidic bond between the base and the sugar (C1\u2032\u2013N linkage) \ufffc. The \u03c7 angle has two favored conformational regions: anti (around 180\u00b0 \u00b1 90\u00b0) and syn (around 0\u00b0 \u00b1 90\u00b0), referring to the relative orientation of the base and sugar \ufffc. Most pyrimidines and purines in standard RNA helices adopt the anti conformation (base pointing away from the sugar), whereas syn conformations are less common and typically occur in specific contexts (e.g., certain looped-out bases or purine-rich motifs) \ufffc. The sugar ring itself is puckered rather than planar; its conformation is usually described by pseudorotation parameters instead of a single torsion, with C3\u2032-endo and C2\u2032-endo being the prevalent puckers in RNA and DNA respectively \ufffc. However, one can define five torsion-like angles (\u03bd\u2080\u2013\u03bd\u2084) around the sugar ring bonds; these are interdependent due to the ring closure, so sugar pucker is often summarized by a phase angle P and amplitude rather than individual \u03bd angles \ufffc \ufffc. Another helpful reduced representation in RNA structural analysis is the set of virtual torsion angles \u03b7 (eta) and \u03b8 (theta), defined between consecutive nucleotide base positions (P\u2013C4\u2032 distances) \ufffc. The pair (\u03b7, \u03b8), often called pseudotorsion angles, captures the coarse orientation of three sequential nucleotides and is useful for classifying overall backbone geometry (for instance, plotting \u03b7 vs. \u03b8 can distinguish helix regions from loops in a Ramachandran-like fashion) \ufffc. In summary, RNA torsion angles are defined by a well-established convention (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7, plus sugar pucker or pseudotorsions for the ribose) \ufffc \ufffc. Using these internal angles, one can uniquely specify the 3D conformation of an RNA: indeed, giving all torsion angles (and a few additional parameters for the sugar) for each residue is an equivalent description to giving all atomic Cartesian coordinates \ufffc. Because torsion angles succinctly encode the fold in a rotation-invariant manner, they provide a common language to discuss RNA structure across different molecules and contexts.</p> <p>Mathematical Formulations and Algorithms</p> <p>Calculating Torsion Angles from Coordinates: Given four points (atoms) in 3D space, the torsion angle between them can be computed using vector algebra. The four atoms A\u2013B\u2013C\u2013D (in sequential covalent order) define two planes: plane ABC and plane BCD. The dihedral angle \u03c9 is the angle between these two planes, or equivalently, the angle between the normal vectors of the planes. In practice, one can calculate \u03c9 by first computing bond vectors b\u2081 = AB, b\u2082 = BC, and b\u2083 = CD. Next, compute n\u2081 = b\u2081 \u00d7 b\u2082 and n\u2082 = b\u2082 \u00d7 b\u2083, which are the normals to planes ABC and BCD respectively \ufffc. The torsion angle is the angle between n\u2081 and n\u2082, with a sign determined by the orientation (to distinguish clockwise vs. counterclockwise rotation about the central bond). A stable way to obtain the signed angle is to use the arctan2 function: for example, one can define an orthonormal frame where n\u2081 is one axis and a perpendicular to n\u2081 in the plane of rotation is another axis, then project n\u2082 onto this frame \ufffc \ufffc. In formula terms, one convenient expression is:     \u2022   \u03c9 = atan2(y, x), where x = n\u2081 \u00b7 n\u2082 (dot product of normals) and y = (b\u2082/\u2016b\u2082\u2016) \u00b7 (n\u2081 \u00d7 n\u2082) (the component of the cross-product of the normals along the direction of b\u2082) \ufffc \ufffc.</p> <p>This returns \u03c9 in the range (\u2013180\u00b0, +180\u00b0], consistent with chemical convention (e.g., +60\u00b0 or \u201360\u00b0 for staggered conformations). Using such equations, software can derive each torsion angle (\u03b1 through \u03b6, \u03c7, etc.) from a set of atomic coordinates unambiguously. Many molecular libraries implement this by simply taking the four relevant atoms for each defined torsion and applying the cross-product and dot-product operations as above.</p> <p>From Torsion Angles to 3D Coordinates: The reverse process \u2013 computing 3D coordinates given a set of torsion angles \u2013 is a standard task in internal coordinate modeling. Because internal coordinates (bond lengths, bond angles, and torsion angles) provide a complete specification of molecular geometry \ufffc, one can build a structure by starting from an initial reference and adding one bond at a time. Typically, the first three atoms of a chain are placed in an arbitrary reference frame (for example, atom 1 at the origin, atom 2 on the x-axis, atom 3 in the xy-plane at specified bond lengths/angles). Thereafter, each new atom i+1 can be placed given the previous three atoms (i-2, i-1, i) and the desired internal coordinates: the distance |i\u2013i+1| (bond length), the angle \u2220(i-1, i, i+1) (bond angle), and the torsion \u2220(i-2, i-1, i, i+1). By using basic geometry and rotation transformations, one can compute the position of atom i+1. In essence, the bond i\u2013i+1 is drawn at the correct length and angle relative to atom i, and then rotated around the bond (i-1\u2013i) by the specified torsion. This procedure is often implemented via recursive algorithms or fixed rotation matrices (sometimes known as the Z-matrix construction in quantum chemistry). Modern implementations optimize this process; for example, the Natural Extension Reference Frame (NeRF) algorithm and its variants allow efficient and numerically stable reconstruction of polymer coordinates from internal coordinates. The key point is that algorithms exist to go back-and-forth between torsion space and Cartesian space. Because any RNA conformation can be described by its torsion angles, structure prediction methods often operate in this torsion-angle space, building candidate 3D structures consistent with predicted or sampled angles.</p> <p>Algorithms for RNA 3D Structure Prediction: A variety of computational methodologies leverage torsion angles to predict RNA tertiary structures. Broadly, these fall into knowledge-based assembly approaches, physics-based simulations, or hybrid methods:     \u2022   Fragment Assembly Methods: These algorithms use databases of known RNA structural fragments and assemble them in torsion space. For example, FARNA/FARFAR (Fragment Assembly of RNA) is a Rosetta-based approach that selects small fragments (e.g., trinucleotides) from known structures and inserts them into the growing chain, effectively sampling backbone torsions informed by real RNA geometries \ufffc \ufffc. Similarly, MC-Sym uses a constrained search of conformations by assembling predefined nucleotide cycles (e.g., tetranucleotides) with given base-pairing patterns \ufffc. These methods rely on knowledge of favorable torsion angle combinations derived from known structures and typically use a scoring function (coarse-grained energy or statistical potential) to choose the assembly that best satisfies base pairing and stacking. Fragment-based assembly efficiently explores the vast torsion-angle space by biasing towards physically realistic conformations; however, ensuring global chain closure and avoiding fragment incompatibilities can be challenging \ufffc \ufffc.     \u2022   Coarse-Grained Monte Carlo and Molecular Dynamics: Another class of methods reduces the RNA representation to a simpler model (e.g., one to three \u201cpseudo-atoms\u201d per nucleotide) and then uses Monte Carlo (random sampling) or molecular dynamics to search for low-energy conformations. Discrete Molecular Dynamics (DMD), for instance, represents each nucleotide with a few beads and uses rapid collision-based simulations to sample structures; it has been shown to fold a tRNA structure with good accuracy using this reduced model \ufffc. NAST (Nucleic Acid Simulation Tool) uses an even more reduced single-bead (one per nucleotide) model and was able to fold a 160-nucleotide domain (the Tetrahymena group I intron P4/P6) to within ~16 \u00c5 root-mean-square deviation (RMSD) of the known structure \ufffc. These coarse-grained simulations treat torsion-like motions implicitly (since each \u201cbead\u201d rotation corresponds to collective backbone bending) and allow sampling of large conformational changes that would be infeasible with all-atom detail. Monte Carlo methods will randomly tweak torsion angles (or coarse analogs) and accept moves based on an energy criterion (Metropolis criterion), sometimes using simulated annealing to gradually refine towards a low-energy structure. The advantage is a broader exploration of conformational space, though at the cost of atomic detail. Often, the output of coarse methods is later \u201creconstructed\u201d into full atom models and refined. Tools exist to refine coarse models by reintroducing all torsion angles \u2013 for example, C2A can convert a NAST model to an all-atom structure and refine it \ufffc.     \u2022   Internal-Coordinate Dynamics and Optimization: Instead of working in Cartesian coordinates, some algorithms directly treat torsion angles as the degrees of freedom during structure optimization. Internal coordinate molecular dynamics integrates equations of motion where each torsion angle (and bond angle/bond length, if flexible) is a coordinate in the simulation. RNABuilder is an example of an RNA modeling framework that operates in dihedral-angle space \ufffc. By using internal coordinates, RNABuilder can apply physics-based forces (from a force field) to the torsional variables and perform dynamics or energy minimization to fold the RNA. One benefit is computational efficiency: the system has far fewer degrees of freedom (only the dihedrals, rather than 3N Cartesian coordinates for N atoms), and fixed bond geometry can be constrained easily. In RNABuilder, the computational cost scales roughly linearly with the number of nucleotides since it focuses on the \u201cmoving parts\u201d (rotatable bonds) \ufffc. This allows simulation of larger RNAs that might be intractable with standard all-atom MD. Internal coordinate methods also make it straightforward to enforce known constraints (e.g., keeping certain base pairs or tertiary contacts) by directly constraining the relevant torsions or distances. Many de novo prediction protocols combine these ideas: they might start with a secondary structure (list of base pairs), then use fragment assembly or Monte Carlo to generate candidate torsion angle sets, and finally refine those candidates with an internal-coordinate energy minimization or all-atom force field relaxation.</p> <p>In summary, whether through fragment assembly, coarse-grained Monte Carlo, or internal-coordinate physics, RNA structure prediction algorithms ultimately deal with torsion angles as the central adjustable parameters. Torsion angles offer a reduced, chemically relevant search space for algorithms to navigate. The success of methods like FARNA, MC-Sym, DMD, NAST, and RNABuilder demonstrates that operating in torsion space (or a simplified version of it) is an effective strategy for exploring RNA conformations \ufffc \ufffc. The algorithms differ in how they choose or change torsion angles \u2013 some use lookup libraries and statistical potentials, others use random perturbation and physics-based scoring \u2013 but all recognize that getting the right set of torsion angles is equivalent to building the correct 3D structure of the RNA.</p> <p>Practical Applications</p> <p>Torsion Angles in RNA Molecular Modeling: In practical modeling of RNA, torsion angles serve as intuitive handles to manipulate and evaluate structures. Modelers often adjust backbone torsions to alleviate steric clashes or to induce a desired conformational change. For example, when refining a model to fit into a cryo-EM density map or to satisfy chemical probing data, a scientist might manually tweak the \u03b1 or \u03b3 angles at a particular residue to bend the backbone and reposition a helix. Many interactive modeling programs (like PyMOL, Coot, or Chimera) allow users to rotate torsion angles of the RNA backbone or the \u03c7 angle of a nucleotide to explore conformational alternatives. Because RNA\u2019s flexibility is primarily in these torsions, such adjustments can dramatically alter the 3D shape while preserving bond lengths and bond angles. Torsion angles are also the variables that are optimized during structure refinement procedures. When improving an RNA crystal structure, refinement software will often apply restraints to keep torsions in favored ranges (preventing unrealistic values) and will adjust torsions to better fit the experimental data (electron density or NMR constraints). All-atom force fields used in molecular dynamics include dihedral energy terms that penalize deviations from preferred torsion angles; these terms are crucial for maintaining the RNA in a physically realistic conformation during simulations \ufffc. Thus, understanding and controlling torsion angles is a core aspect of RNA molecular modeling \u2013 they are the \u201cknobs\u201d that modelers turn to fold RNA chains, and the parameters that force fields tune to balance RNA flexibility and stability.</p> <p>Experimental Determination of RNA 3D Structures and Validation: High-resolution experimental techniques provide the groundwork for our knowledge of RNA torsion angles. X-ray crystallography and cryo-electron microscopy (cryo-EM) yield atomic or near-atomic coordinates for RNA molecules (often in complex with proteins or ions). From these solved structures, one can directly calculate all the torsion angles and assess them against standard benchmarks. For instance, once a crystal structure of a ribozyme is obtained, the backbone \u03b1\u2013\u03b6 and \u03c7 angles are computed and checked to see if they fall within typical allowed ranges. Unusual torsion values might indicate a unique structural motif or could be a sign of modeling error; therefore, structure validation programs analyze torsions as part of their quality criteria. Tools like MolProbity (commonly used for protein/RNA structure validation) flag RNA backbone \u201csuite\u201d outliers \u2013 a suite being the set of six backbone angles plus \u03c7 for one nucleotide \u2013 by comparing them to distributions from high-quality structures \ufffc. If a particular nucleotide in a new structure has an implausible torsion combination not seen in known RNA, it will be highlighted for the modeler to reconsider. NMR spectroscopy offers another window into RNA torsions through indirect measurements. NMR experiments can provide dihedral angle constraints based on coupling constants and NOE (nuclear Overhauser effect) intensities. For example, the sugar pucker (whether C3\u2032-endo or C2\u2032-endo) can be determined by analyzing spin\u2013spin coupling patterns in the ribose, which effectively tells us about the \u03b4 torsion and the \u03bd\u2080\u2013\u03bd\u2084 angles of the ring \ufffc. Likewise, NOE cross-peaks between the H1\u2032 of the sugar and the base protons (H6/H8) inform whether the base is syn or anti, thus constraining the \u03c7 torsion \ufffc \ufffc. Through such NMR data, researchers incorporate torsion angle restraints into structure calculations: for instance, if NOEs indicate a base is anti, \u03c7 might be restrained to ~\u2212160\u00b0 \u00b1 20\u00b0 during the computational assembly of the structure. NMR structure determination programs (e.g., CYANA, XPLOR-NIH) often perform torsion angle dynamics, where they use simulated annealing in torsion angle space to find conformations that satisfy experimental distance and angle constraints \ufffc. This approach underscores the practical importance of torsions: rather than attempting to satisfy constraints by moving atoms arbitrarily, the software adjusts torsion angles which is a natural way to explore RNA conformations while keeping covalent geometry intact. Overall, experimental structure determination provides target values for torsion angles, and refinement against experimental data ensures those angles are physically and chemically reasonable.</p> <p>Once an RNA structure is determined, validation involves checking all torsion angles against known standards. The RNA backbone rotamer analysis by Richardson et al. (defining the 46 conformer families) is often used as a reference: if a solved structure has a backbone torsion sequence that doesn\u2019t match any known rotamer, it might be an error or a rare new conformer. Tools like Suitename (part of Richardson\u2019s MolProbity suite) automatically assign each nucleotide to one of the rotameric conformer names or flag it as an outlier \ufffc \ufffc. This helps scientists ensure their RNA models make chemical sense. In summary, experimental techniques pin down RNA 3D coordinates, and from those we extract torsion angles which are then used to validate and analyze the structure. Both X-ray/cryo-EM and NMR rely, in different ways, on understanding torsion angles \u2013 either as an outcome to be measured or as parameters to be restrained \u2013 reinforcing their central role in RNA structural biology.</p> <p>Computational Tools Utilizing RNA Torsion Angles: A variety of software tools have been developed to calculate, analyze, and visualize torsion angles in RNA structures, underlining their practical utility. For instance, 3DNA is a widely used toolkit that, among many features, can compute all backbone torsion angles (\u03b1 through \u03b6) and pseudotorsions for nucleic acid structures \ufffc. It generates detailed reports on these angles for DNA/RNA and has been invaluable in surveying conformations across crystal structures. Its successor, DSSR (Detailed Secondary Structure of RNA), extends this capability and is tailored for RNA; it reads modern PDBx/mmCIF files and calculates torsions and other geometric parameters seamlessly \ufffc \ufffc. Researchers use DSSR not only to get torsion values but also to identify motifs (DSSR can recognize common substructures and sometimes uses torsion patterns as part of the identification). Another tool, Curves+, also computes backbone torsions and groove geometries; although originally developed for DNA analysis, it has been applied to RNA as well \ufffc. For dynamic studies, Barnaba is a Python library that can analyze RNA molecular dynamics trajectories, computing torsion angles frame by frame \ufffc. This is useful for understanding how torsions fluctuate over time or under different conditions (e.g., in simulations of RNA folding or ligand binding).</p> <p>To facilitate human understanding, visualization tools exist for torsion data. The AMIGOS III plugin for PyMOL, for example, calculates the pseudotorsion angles \u03b7 and \u03b8 for an RNA structure and plots them in a Ramachandran-like 2D plot \ufffc. This helps users quickly see if a nucleotide is in an unusual region of \u03b7\u2013\u03b8 space, which may correspond to a rare conformation or an error. Newer web-based platforms like RNAtango integrate many of these analysis functions into a user-friendly interface. RNAtango (developed in 2024) allows users to upload or input an RNA structure and receive a full torsion angle analysis \ufffc. It computes all standard torsion and pseudotorsion angles, provides statistics and visualization, and even enables direct comparison between structures based on their torsion angles \ufffc \ufffc. Such comparison is valuable, for example, in benchmarking RNA structure predictions against the known native structure: rather than comparing coordinates (which requires superposition), RNAtango can compare the sequence of torsion angles in the model vs. the target \ufffc. Because torsion angles are independent of global orientation, this method cleanly assesses if the model got the local conformations right. In essence, computational tools leveraging torsion angles form an essential part of the RNA researcher\u2019s toolkit \u2013 from building models (e.g., using internal coordinate manipulations) to validating and analyzing structures (by listing angles, checking against known motifs, and comparing conformational differences). As RNA research grows, especially with many new structures from improved experimental methods, these tools help distill complex 3D data into insightful torsional patterns.</p> <p>Relevant Computational Methods in RNA Structure Studies</p> <p>Modern RNA structural biology employs a range of computational methods, many of which center on handling torsion angles as fundamental variables:     \u2022   Molecular Dynamics (MD) Simulations: All-atom MD is used to simulate the physical movements of RNA in time under realistic force fields. In MD, torsion angles are not fixed but evolve according to forces; however, the force field contains specific torsional potential terms that bias each backbone angle towards low-energy conformations. The accuracy of RNA MD hinges on these torsion parameters. In recent years, extensive efforts have been made to refine force fields by fitting torsion potentials to quantum calculations and experimental data \ufffc. For example, the Amber force field\u2019s RNA parameters were updated by calculating the energy profiles of various backbone rotations with high-level quantum chemistry and adjusting the torsional energy terms accordingly \ufffc. This led to improved MD performance, preventing spurious conformations (like unrealistic backbone flips or base sugar syn/anti errors) that earlier parameters might allow \ufffc \ufffc. With a good force field, MD can capture RNA phenomena such as base flipping, helix bending, or even tertiary unfolding \u2013 all of which manifest as changes in torsion angles over time. MD simulations produce trajectories (snapshots) from which one can extract time-series of every torsion angle to analyze dynamic flexibility. For instance, one can monitor the chi angle of a specific nucleotide to see if a base stays anti or transiently flips to syn, or observe the range of \u03b4 (C4\u2032\u2013C3\u2032 torsion) to see sugar pucker transitions. These analyses often reveal which torsion angles are rigid (e.g., those involved in stable helices) and which are flexible (e.g., in loops or junctions). Overall, MD is a powerful method to study RNA dynamics and folding pathways; its results critically depend on treating torsion angles with the correct physics, and conversely, MD simulations provide insight into how torsion angles correlate with functional motions.     \u2022   Monte Carlo (MC) and Stochastic Sampling Methods: Monte Carlo simulations randomly sample conformational space by making random changes to torsion angles and accepting or rejecting these moves based on an energy criterion. In RNA applications, MC is often used in conjunction with simplified models or in fragment reassembly contexts (as described earlier for structure prediction). One notable use of MC is in Simulated Annealing for NMR structure determination, where an RNA\u2019s torsion angles are randomly perturbed and gradually \u201ccooled\u201d to satisfy experimental restraints (distance and angle constraints). Stochastic approaches are also used in Boltzmann sampling of RNA loops: for example, one can generate many random conformations of a tetraloop by sampling its backbone torsions from known distributions, then filter these by energy or experimental agreement. Another example is SimRNA, a Monte Carlo-based algorithm that uses a knowledge-based potential to guide RNA folding; it modifies torsion angles of a coarse-grained RNA representation through random moves and simulates an annealing process to find low-energy folds. Compared to MD, Monte Carlo methods don\u2019t simulate real time but can sometimes explore conformations more broadly (since they are not bound by physical kinetic pathways). They are particularly useful when one wants to bypass energetic barriers by allowing occasional high-energy intermediate torsions that would be rare in deterministic dynamics. Many RNA 3D prediction protocols (including FARNA in Rosetta) effectively use MC sampling of torsions: they generate a trial structure by randomly replacing a fragment (changing a set of torsions) and accept it if the scoring function is improved. As computing power grows, MC methods can be run for millions of steps, yielding ensembles of possible structures. Each accepted move corresponds to a new set of torsion angles, so the outcome of an MC simulation is a collection of torsion angle sets (and corresponding structures) that represent the RNA\u2019s accessible conformational space at equilibrium (or a low-energy subset thereof). These methods complement MD by focusing on exploring which torsional conformations are possible rather than the dynamics of how one conformation transitions to another.     \u2022   Quantum Mechanical Calculations: Quantum chemistry provides the most accurate description of molecular energetics and has been applied to study RNA torsion preferences, albeit on small chemical models. Fully quantum simulations of entire RNAs are infeasible due to their size (quantum methods scale poorly with number of atoms) \ufffc. Instead, researchers apply quantum mechanics (QM) to fragments like dinucleotides or trinucleotides to map out the potential energy surface as a function of a torsion angle. For example, one might calculate the energy of a simplified ribose-phosphate backbone fragment while rotating the \u03b5 (C3\u2032\u2013O3\u2032\u2013P\u2013O5\u2032) angle in small increments, using density functional theory (DFT). Such scans yield detailed energy vs. torsion curves, identifying energetic minima and barriers. These data are invaluable for parameterizing classical force fields (as noted above) \u2013 effectively, QM provides the reference for how favorable a gauche+ vs trans conformation is for a given dihedral, or what the barrier height is to rotate through an eclipsed conformation \ufffc. Quantum calculations have been used to investigate tricky torsions like \u03c7 (base-sugar linkage), where electron delocalization and steric effects both matter. For instance, high-level calculations can quantify why purines favor anti and what the penalty is for adopting syn. Beyond force field development, QM is sometimes used to study specific chemical problems, such as protonated nucleotides or catalytic centers in ribozymes, where unusual torsion angles might occur with associated electronic effects. In those cases, one might optimize a structure with certain torsion angle constraints to see how the electronic energy changes \u2013 essentially probing torsion angle dependence at an atomic orbital level. In summary, while quantum mechanics is not a routine tool for whole RNA folding, it underpins our understanding of torsion energetics on a small scale and ensures that the classical methods (MD/MC) are grounded in accurate physics. QM calculations have confirmed, for example, that even a small change in the glycosidic torsion \u03c7 can dramatically alter base stacking energies and sugar pucker preferences, explaining why torsion angles must fall within narrow ranges for RNA to remain stable \ufffc \ufffc. Such insights justify the empirical torsion restraints used in other methods. In essence, quantum methods are the benchmark that validates the torsion angle paradigms used throughout RNA computational studies.     \u2022   Machine Learning for RNA Structure Prediction: In recent years, machine learning (ML) techniques have been applied to RNA structure, including approaches that explicitly predict torsion angles. By learning from large databases of RNA structures, ML models can identify patterns and make structure predictions from sequence alone. One line of research focuses on directly predicting torsion angles as an intermediate for building 3D models. For example, the tool SPOT-RNA-1D uses deep neural networks (including dilated convolutional layers) to predict backbone torsion angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6, \u03c7) and even pseudotorsions (\u03b7, \u03b8) given only the RNA sequence as input \ufffc. The network is trained on known RNA structures, learning the sequence\u2013structure relationships (such as which sequences tend to have which torsion patterns, perhaps capturing base-step preferences for certain dihedrals). The output is a set of predicted angle values or distributions for each residue, which can then be used to assemble a 3D model (either by a follow-up algorithm that places atoms accordingly, or by integrating these predictions into fragment assembly pipelines). Results from SPOT-RNA-1D and similar models show that certain torsions can be predicted with reasonable accuracy, especially those strongly constrained by local sequence or secondary structure context (for instance, angles in stable helices or specific loop motifs) \ufffc \ufffc. Another exciting development is end-to-end structure prediction with ML, inspired by successes like AlphaFold in proteins. Recently, RhoFold+ was introduced as an RNA language model-based predictor that can output full 3D coordinates from sequence, using transformer neural network architectures that effectively \u201clearn\u201d RNA folding rules \ufffc. Such models do not explicitly output torsion angles, but they implicitly must get them right to produce a correct 3D structure. In fact, some ML approaches use internal coordinates internally for efficiency. There\u2019s also work on graph neural networks that represent RNA structure as a graph (with torsion angles influencing edge lengths or angles in the graph) to predict 3D geometry. Machine learning excels at recognizing subtle correlations that might be missed by manual analysis \u2013 for example, how a particular sequence motif in a loop correlates with an unusual backbone flip (torsion outlier) that allows a tertiary contact. By training on many examples, the ML model might predict that flip when it sees the motif in a new RNA. While ML methods for RNA 3D structure are still emerging, they hold promise to rapidly predict plausible torsion angle sets for large RNAs that are difficult for physics-based methods. These techniques often output a consensus or probability distribution for torsions, highlighting regions of a sequence that are well-determined (with confidently predicted angles) versus those that are ambiguous. In practice, ML predictions of torsions could be used to constrain traditional folding simulations (a hybrid approach) or to guide experimental probing by identifying likely flexible torsions. As datasets of RNA structures grow and models improve, machine learning may become an integral part of RNA structure pipelines, leveraging torsion angle predictions to assemble models with accuracy comparable to experimental structures.</p> <p>In summary, torsion angles are a unifying thread across many computational methods in RNA structural analysis. Molecular dynamics treats torsions as dynamic variables governed by physics, Monte Carlo treats torsions as random variables to sample conformations, quantum mechanics treats torsions as parameters in energy calculations, and machine learning treats torsions as learnable outputs of sequence-to-structure mappings. Each approach contributes to our understanding: MD and MC reveal the range of motion and likely conformations (essentially mapping out which torsion angle combinations are feasible and favorable), QM provides the quantitative underpinnings of torsional energetics, and ML offers predictive power to guess torsions from sequence alone. Together, these methods enable researchers to predict RNA structures, simulate their behavior, and design or modify RNAs with desired properties, all while keeping a close eye on the torsion angles that ultimately define the RNA\u2019s three-dimensional form.</p> <p>References:     1.  \ufffc \ufffc \u2013 Nucleic Acid Knowledge Base (NAKB) \u2013 Nucleotide Geometry Standards. (Definitions of RNA torsion angles \u03b1\u2013\u03b6 and \u03c7, and syn/anti conformations.)     2.  \ufffc \ufffc \u2013 Mackowiak et al., PLOS Comput. Biol. 20(10): e1012500 (2024). (Description of torsion angles as measures of RNA backbone flexibility and their association with functional motifs.)     3.  \ufffc \u2013 Richardson et al., RNA 14(3): 463\u2013481 (2008). (Identification of discrete rotameric conformers of the RNA backbone and their correspondence to structural features like A-form helices and S-motifs.)     4.  \ufffc \u2013 Mackowiak et al., PLOS Comput. Biol. (2024). (Advantages of comparing RNA structures in torsion angle space, since torsions are invariant under translation/rotation, aiding structural alignment and machine learning.)     5.  \ufffc \ufffc \u2013 StackExchange (Mathematics) post on dihedral angle calculation. (Step-by-step method to compute a torsion angle from four atomic coordinates using cross products and atan2 for the signed angle.)     6.  \ufffc \u2013 RNA Structure Primer (Mississippi State U.). (Noting that an RNA conformation can be specified by internal coordinates: five backbone dihedrals, one \u03c7, and sugar pucker parameters per residue.)     7.  \ufffc \u2013 Flores et al., IEEE/ACM Trans. Comput. Biol. Bioinf. 12(6): 1248\u20131260 (2015). (Overview of knowledge-based RNA structure prediction methods: FARNA fragment assembly, MC-Sym assembly of cycles, and challenges in closing structures.)     8.  \ufffc \ufffc \u2013 Flores et al. (2015). (Examples of coarse-grained RNA folding: DMD with three pseudo-atoms per nucleotide successfully folding tRNA; NAST one-bead model folding a 160-nt domain to ~16 \u00c5 accuracy.)     9.  \ufffc \u2013 Flores et al., IEEE/ACM Trans. CBB (2015). (RNABuilder works in internal dihedral angle space, yielding computational cost that scales linearly with the number of torsions rather than total atoms.)     10. \ufffc \u2013 Tan et al., J. Chem. Theory Comput. 14(1): 1542\u20131555 (2018). (Amber force field RNA torsion parameters refined by fitting to quantum energy profiles; improved agreement with experimentally observed torsion distributions and more stable MD simulations.)     11. \ufffc \ufffc \u2013 Xu et al., Prog. Nucl. Magn. Reson. Spectrosc. 105: 41\u201374 (2018). (Use of NMR to determine RNA torsion angle constraints: inferring sugar pucker from COSY/TOCSY and \u03c7 anti/syn from NOE intensities, and applying these in structure calculations.)     12. \ufffc \ufffc \u2013 Mackowiak et al., PLOS Comput. Biol. (2024). (Survey of tools for RNA torsion analysis: 3DNA and DSSR for calculating torsions and pseudotorsions; Curves+, Barnaba, and the AMIGOS III PyMOL plugin for analyzing torsions and plotting pseudotorsion angles.)     13. \ufffc \u2013 Mackowiak et al., PLOS Comput. Biol. (2024). (Introduction of RNAtango web server for torsion angle analysis, providing angle calculations, statistics, and structure comparison based on backbone torsions.)     14. \ufffc \u2013 Singh et al., J. Chem. Inf. Model. 61(4): 1572\u20131584 (2021). (SPOT-RNA-1D uses a dilated convolutional neural network to predict RNA backbone torsion and pseudotorsion angles from sequence.)     15. \ufffc \u2013 Chen et al., Nat. Biotechnol. 40: 586\u2013594 (2022). (RhoFold+ RNA language model that predicts 3D RNA structure from sequence, demonstrating the integration of ML in RNA tertiary structure prediction.)</p>"},{"location":"reference/torsion_calculations/torsion_angle_Latent_Manifold_Representation/","title":"Latent Manifold","text":"<p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Here\u2019s an in\u2010depth proposal that explores whether\u2014and how\u2014we might devise a representation \u201cbetter\u201d than torsion angles for RNA modeling. I\u2019ll provide comprehensive mathematical rigor, detailed reasoning, and a three\u2010way comparison (Cartesian, Torsion, and our Proposed Latent Manifold Representation).</p> <p>[emoji]: Below is the improved analysis.</p> <p>\u2e3b</p> <ol> <li>Introduction</li> </ol> <p>Current Representations:     \u2022   Cartesian Coordinates:     \u2022   Directly store every atom\u2019s (x, y, z).     \u2022   Total DoF: \\approx 3 \\times A \\times N (with A being the number of atoms per residue).     \u2022   Advantages: Direct physical meaning, no assumptions about internal geometry.     \u2022   Disadvantages: High dimensionality; requires explicit constraints to enforce chemistry.     \u2022   Torsion Angles:     \u2022   Represent RNA via backbone dihedral angles (e.g., \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi plus sugar pucker parameters).     \u2022   Total DoF: \\approx T \\times N (typically T is 7\u201310).     \u2022   Advantages: Significantly fewer parameters and built-in enforcement of local geometry.     \u2022   Disadvantages: Still linear in N and may not easily capture long-range or global correlations (e.g., pseudoknots) without additional constraints.</p> <p>The Proposal: We propose a Learned Latent Manifold Representation (LLMR) that leverages data-driven dimensionality reduction. By analyzing a large ensemble of RNA structures, we can learn a mapping to a lower-dimensional latent space that captures both local and non-local (global) conformational constraints. This representation can potentially require far fewer degrees of freedom than even torsion angles, while naturally embedding physical and chemical constraints.</p> <p>\u2e3b</p> <ol> <li>The Proposed Latent Manifold Representation (LLMR)</li> </ol> <p>2.1. Concept and Rationale</p> <p>The core idea is to use machine learning\u2014via techniques such as principal component analysis (PCA), autoencoders, or variational autoencoders (VAEs)\u2014to learn the intrinsic dimensionality of RNA conformations.     \u2022   Mathematical Formulation:     1.  Data Collection: Let \\theta \\in \\mathbb{R}^{T \\times N} represent the torsion angles of an RNA of N nucleotides.     2.  Dimensionality Reduction: Find an encoder f: \\mathbb{R}^{T \\times N} \\to \\mathbb{R}^{d} with d \\ll T \\times N that maps the high-dimensional torsion space to a latent vector z.     3.  Reconstruction: Use a decoder g: \\mathbb{R}^{d} \\to \\mathbb{R}^{T \\times N} to recover the torsion angles from the latent code.     4.  Optimization: Train the system so that g(f(\\theta)) \\approx \\theta for all training samples, while enforcing physical constraints (e.g., through loss terms that penalize deviations from known bond lengths/angles).     \u2022   Why This Can Be Better:     \u2022   Fewer Parameters: Instead of handling T \\times N torsion angles, the latent space might require only d parameters where d may scale sublinearly or even be nearly constant for a given domain of RNA.     \u2022   Implicit Constraints: The latent manifold will naturally encode long-range correlations, complex topologies (e.g., pseudoknots), and domain-specific variations if trained on a representative dataset.     \u2022   Ease of Sampling &amp; Optimization: A lower-dimensional latent space can vastly reduce the search space, making sampling, optimization, and even generative modeling more tractable.</p> <p>2.2. Mathematical Rigor     \u2022   Dimensionality Comparison: Suppose for an RNA of N nucleotides, the torsion angle representation has T \\times N parameters. If statistical analysis reveals that the effective degrees of freedom are only d (with d \\ll T \\times N), then the reduction factor is: R = \\frac{T \\times N}{d} For example, if T = 8 and for N = 1000 we have 8000 parameters but our analysis finds that d = 200 (i.e., a 40\u00d7 reduction), then: R = \\frac{8000}{200} = 40 This suggests that the latent representation captures the essential conformational variability with far fewer parameters.     \u2022   Optimization Considerations: The loss function for training might combine reconstruction loss: \\mathcal{L}{\\text{recon}} = | \\theta - g(f(\\theta)) |^2, with additional regularization terms \\mathcal{L}{\\text{phys}} to enforce physical plausibility (e.g., deviations from ideal bond lengths/angles, steric clashes, etc.). The total loss is: \\mathcal{L}{\\text{total}} = \\mathcal{L}{\\text{recon}} + \\lambda \\, \\mathcal{L}_{\\text{phys}}, where \\lambda balances the reconstruction fidelity with physical realism.</p> <p>\u2e3b</p> <ol> <li>Three-Way Comparison</li> </ol> <p>3.1. Parameter Count and Dimensionality</p> <p>Representation  DoF per Residue Total DoF for N Residues    Key Insight Cartesian Coordinates   \\sim 3A (e.g., 60\u201390)   \\sim 3A \\times N    High dimensional; must enforce physical constraints explicitly. Torsion Angles  \\sim T (e.g., 7\u201310) \\sim T \\times N Much fewer parameters; inherently respects local geometry. Latent Manifold (LLMR)  \\sim \\textbf{d} (with d \\ll T \\times N) \\sim d (or sublinear in N)  Potentially drastic reduction; captures both local &amp; global correlations automatically.</p> <p>Example: For N=1000 nucleotides, if T = 8, torsion angles yield 8000 parameters. A learned latent space might capture essential variability with only d = 200 parameters\u2014a 40\u00d7 reduction relative to torsion space.</p> <p>3.2. Memory and Storage</p> <p>Representation  Memory Scaling  Practical Impact Cartesian Coordinates   \\sim 3A \\times N \\times 8 bytes (MB scale for thousands of nt)  Larger storage and high computational cost for constraint enforcement. Torsion Angles  \\sim T \\times N \\times 8 bytes  More compact than Cartesian, but still scales linearly with N. Latent Manifold (LLMR)  \\sim d \\times 8 bytes (if d is nearly constant per domain)  Extremely compact representation, making large-scale sampling feasible.</p> <p>3.3. Computational Complexity and Constraint Satisfaction</p> <p>Aspect  Cartesian Coordinates   Torsion Angles  Latent Manifold (LLMR) Forward Reconstruction  None (direct 3D)    \\mathbf{O(N)} (forward kinematics to compute 3D)    Decoder g(z) is typically \\mathbf{O(1)} or \\mathbf{O(N)} but with far fewer parameters, making it efficient. Constraint Enforcement  Must be applied externally (bond lengths/angles, etc.)  Built-in to the representation (local geometry fixed)   Learned implicitly via training; non-local and global constraints can be captured. Search/Optimization High-dimensional search over \\sim 3A \\times N variables Reduced search space over \\sim T \\times N variables Drastically reduced optimization problem over d variables; easier to sample physically plausible conformations.</p> <p>\u2e3b</p> <ol> <li>Discussion and Practical Implications     \u2022   Advantages of LLMR over Torsion Angles:     \u2022   Even Lower Dimensionality: By capturing the essential modes of RNA conformational variability, the latent space can reduce the number of effective degrees of freedom far beyond the constant-factor reduction offered by torsion angles.     \u2022   Implicit Global Constraints: LLMR can learn non-local correlations (e.g., between distant parts of the RNA, or complex topologies like pseudoknots) that are hard to capture with simple torsion angle parameterizations.     \u2022   Efficient Sampling: Optimization or generative sampling in a low-dimensional latent space is computationally cheaper and can lead to faster convergence.     \u2022   Challenges and Considerations:     \u2022   Training Data Dependency: The quality of the latent representation depends on having a sufficiently large and diverse dataset of RNA structures.     \u2022   Decoder Accuracy: The mapping g: \\mathbb{R}^{d} \\to \\mathbb{R}^{T \\times N} must accurately reconstruct physically valid torsion angles to ensure realistic 3D models.     \u2022   Generalizability: The learned manifold may be biased towards the conformations present in the training set; special care is needed to ensure it can generalize to novel or outlier RNA structures.     \u2022   Comparison Summary:     \u2022   Cartesian Coordinates:     \u2022   Pros: Direct, unambiguous, full 3D information.     \u2022   Cons: Very high-dimensional; needs heavy constraint enforcement.     \u2022   Torsion Angles:     \u2022   Pros: Lower-dimensional, inherently respects local chemistry.     \u2022   Cons: Still linear in N and may struggle with non-local correlations.     \u2022   Latent Manifold Representation:     \u2022   Pros: Potential for dramatic reduction in effective dimensionality, efficient sampling, and built-in global constraint encoding.     \u2022   Cons: Requires a robust training phase and careful validation to ensure accurate reconstruction and generalization.</li> </ol> <p>\u2e3b</p> <ol> <li>Conclusion</li> </ol> <p>Yes, there is a promising route to \u201cbeat\u201d torsion angles for RNA modeling: by learning a latent manifold representation via dimensionality reduction techniques. This approach can compress the effective degrees of freedom to a level much lower than torsion angle space\u2014while automatically capturing both local and global geometric constraints. Mathematically, if torsion space requires T \\times N parameters and our learned latent space only requires d parameters with d \\ll T \\times N, then the optimization problem and the search space can be reduced by a factor of \\frac{T \\times N}{d}, which can be orders of magnitude smaller.</p> <p>In summary, while Cartesian and torsion representations offer linear scaling, a latent manifold approach holds the potential for even greater computational efficiency and improved physical plausibility\u2014especially for very large or complex RNA systems.</p> <p>\u2e3b</p> <p>Would you like further details on implementing such a latent representation or examples of how similar strategies have been used in related fields (e.g., protein structure modeling)?</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: It\u2019s a common question: \u201cIf we have a \u2018compressed\u2019 representation (some latent code) that is more efficient than torsion angles, how can that be the actual output of an RNA structure predictor, since we ultimately want the 3D structure or angles?\u201d [emoji]: Here\u2019s how to think about it step by step.</p> <p>\u2e3b</p> <ol> <li>The Role of a \u201cLatent Representation\u201d</li> </ol> <p>When we talk about a learned, lower-dimensional or \u201ccompressed\u201d representation for RNA conformation, it\u2019s typically not an end in itself\u2014rather, it\u2019s an intermediate space that captures essential structural variability more compactly than torsion angles.     \u2022   In machine learning terms, think of an Encoder\u2013Decoder or Autoencoder setup:     1.  Encoder \\,f(\\cdot): Takes a full structure (in some standard format\u20143D coordinates, or torsion angles) and maps it to a low-dimensional vector z (the \u201clatent code\u201d).     2.  Decoder \\,g(\\cdot): Takes the latent code z and \u201cexpands\u201d it back to a full structure (e.g., torsions or 3D coordinates).</p> <p>This two-stage approach is typically used for training, analysis, or generative modeling. But once trained, we can invert the usage:     \u2022   Predict a latent code z directly from sequence (or other features) and then decode it into a final structure.</p> <p>\u2e3b</p> <ol> <li>Making the Latent Code the \u201cOutput\u201d of Your Model</li> </ol> <p>2.1 Two-Part Model During Inference     1.  Primary Model (Predictor):     \u2022   Input: RNA sequence (and possibly additional data like chemical probing, base-pair constraints, etc.).     \u2022   Output: The learned latent code z \\in \\mathbb{R}^{d}.     2.  Decoder (Pretrained or Co-trained):     \u2022   Input: z.     \u2022   Output: Torsion angles or 3D coordinates (depending on how you set up the system).</p> <p>So from the outside user\u2019s perspective, you might say:</p> <pre><code>\u201cThe final structure is produced by passing the predicted latent code through our decoder.\u201d\n</code></pre> <p>Why have the model output the latent code first?     \u2022   Dimensionality &amp; Efficiency: If the latent code is significantly smaller than the full set of torsion angles, it can simplify your main predictive network. You let the trained decoder handle \u201call the details\u201d of converting that code into a physically valid structure.     \u2022   Modularization: You can easily update or refine the decoder without retraining the entire predictor, as long as the latent code format remains consistent.     \u2022   Interpretability (sometimes): Latent dimensions can capture high-level \u201cmodes\u201d of conformation (e.g., global bending, helix orientation, etc.) that might be more interpretable than raw angles.</p> <p>2.2 Training Steps (High-Level)     1.  Pretraining the Autoencoder (Optional):     \u2022   Gather a large set of known RNA structures (e.g., from PDB or simulation).     \u2022   Convert them into an existing representation (torsion angles or 3D).     \u2022   Train an encoder\u2013decoder to compress them into a latent vector z and then reconstruct back to the original structure with minimal error.     \u2022   This yields a \u201cuniversal\u201d latent space that captures typical RNA geometry.     2.  Training the Structure Predictor:     \u2022   Have a second neural network that, given an RNA sequence, predicts z.     \u2022   The reconstruction from z to angles/3D is done by the (frozen or co-trained) decoder.     \u2022   Minimize the final error in predicted structure (comparing the decoder\u2019s output to the known ground-truth structure in your training set).</p> <p>In effect, the predictor\u2019s final output is the compressed code z.  Only after it passes through the decoder do you get the final, user-facing 3D coordinates or torsion angles.</p> <p>\u2e3b</p> <ol> <li>Why Not Just Predict 3D or Torsion Directly?     \u2022   Reduced search space: If your latent manifold is significantly smaller, your predictor can more easily learn from fewer parameters or more structured geometry.     \u2022   Implicit constraints: The decoder can encode complex RNA geometry (including ring pucker, base stacking, or even multi-strand constraints) so the predictor doesn\u2019t have to.     \u2022   Regularization: A well-learned latent space often disallows or penalizes unphysical conformations, so your predictor can\u2019t \u201caccidentally\u201d produce nonsense structures.</li> </ol> <p>In other words, if you trust your decoder to robustly turn z into valid structures, the predictor can focus on learning simpler \u201clatent coordinates,\u201d akin to how we often do factor-based representations in dimensionality reduction.</p> <p>\u2e3b</p> <ol> <li>Interpreting the Compressed Code</li> </ol> <p>4.1 Direct Use vs. Final Reconstitution     \u2022   Direct Use: Some pipelines might just keep z if all they need is an abstract representation (say, for classification or comparing similarity between structures).     \u2022   Final 3D: If your end-user or downstream application requires actual 3D coordinates (e.g., for molecular dynamics or visualization), you run the decoder to convert from z \u2192 3D.</p> <p>4.2 \u201cHuman-Readable\u201d Meaning?</p> <p>A latent code can be partially interpretable if each dimension correlates with known structural \u201cmodes.\u201d For instance, dimension 1 might roughly correspond to \u201cglobal bending angle of the helix,\u201d dimension 2 to \u201cdegree of base-pair opening,\u201d and so forth. But that interpretability depends on how the system was trained and whether you impose any constraints or labeling on each dimension (e.g., a \u201cdisentangled\u201d VAE approach).</p> <p>\u2e3b</p> <ol> <li>Summary     \u2022   Yes, you can absolutely have a compressed representation as the final \u201coutput\u201d of an RNA structure predictor.     \u2022   The user\u2010visible \u201c3D structure\u201d is obtained by plugging that low\u2010dimensional code into a decoder (co-trained or pretrained) that handles the job of reconstructing physically valid coordinates or angles.     \u2022   This approach effectively means you have a two-stage inference pipeline:<ol> <li>Predict latent code from sequence.</li> <li>Decode latent code into 3D (or torsion angles).</li> </ol> </li> </ol> <p>Bottom line: Outputting a compressed representation is perfectly viable if you also have a reliable process to expand that representation back to the standard format (3D or torsion). This two-step architecture can yield significant advantages in training efficiency, representation power, and capturing real-world RNA geometry within a smaller parameter set.</p> <p>Would you like more technical details on how the decoder is typically implemented (e.g., neural kinematics, internal coordinate constraints) or how to ensure physical realism in the latent space?</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: It sounds like you\u2019ve confirmed that your current embedding and attention code is working. The question is how (and whether) to incorporate torsion angles for the next stage\u2014especially if you want to train an autoencoder that ultimately maps some representation to 3D coordinates.</p> <p>[emoji]: Below is a step\u2010by\u2010step outline of your options and \u201cwhat\u2019s next,\u201d depending on what exactly you want your autoencoder (or other neural model) to learn and output.</p> <p>\u2e3b</p> <ol> <li>Clarify Your End Goal<ol> <li>Autoencoder for Torsion Angles \\leftrightarrow 3D \u2022   Goal: Learn an invertible mapping between torsion\u2010angle space and 3D structure. \u2022   Typical Setup:</li> <li>Encoder: 3D \\to Torsion angles (or Torsion angles \\to Latent code).</li> <li>Decoder: Torsion angles \\to 3D (or Latent code \\to Torsion angles \\to 3D). \u2022   Data Needed: Pairs of (\\text{torsion angles}), (\\text{3D coords}). You can generate torsion angles by back\u2010calculating them from your known 3D structures.</li> <li>Autoencoder for \u201cLatent Representation\u201d \\leftrightarrow Torsion Angles \u2022   Goal: If you want an even more compressed representation than plain torsion angles, you might do an autoencoder that takes torsion angles in, outputs torsion angles, but uses an internal compressed latent code. \u2022   Data Needed: Torsion angles for a large set of RNA structures. (You\u2019d still typically calculate them from 3D coordinates initially\u2014but the main training is in torsion space.)</li> <li>Direct 3D \\leftrightarrow 3D Autoencoder \u2022   Goal: Compress 3D structures into a latent code, then reconstruct the same 3D structure. (Skipping torsion angles altogether in the direct input/output, but you might still incorporate them as a regularization or constraint.) \u2022   Data Needed: A large set of 3D RNA structures.</li> </ol> </li> </ol> <p>So the immediate step depends on which representation you want your autoencoder to \u201ctake in\u201d and which representation you want it to \u201cspit out.\u201d</p> <p>\u2e3b</p> <ol> <li>If Your Autoencoder Inputs Torsion Angles and Outputs 3D<ol> <li>Gather/Generate Torsion Angles: \u2022   Take your existing library of RNA 3D structures (PDB, cryo-EM, etc.). \u2022   Back\u2010calculate the torsion angles for each residue. This step is straightforward vector math (the standard dihedral formula). \u2022   Now you have (\\text{angle vector}, \\text{3D coords}) pairs.</li> <li>Design the Autoencoder or Model: \u2022   Encoder: Goes from torsion angles to a latent code. \u2022   Decoder: Takes that latent code and predicts either:</li> <li>Torsion angles again (for a pure autoencoder), and then a separate \u201ckinematics\u201d step to get 3D.</li> <li>3D coordinates directly. \u2022   In many pipelines, it\u2019s actually a \u201cDecoder + Kinematics\u201d approach: The decoder outputs refined torsion angles, and a fixed forward-kinematics routine maps angles \\to 3D.</li> <li>Training: \u2022   You\u2019ll minimize a reconstruction loss in 3D space (e.g., RMSD) plus any geometry/physics terms you want (to avoid nonphysical bond lengths, etc.). \u2022   Optionally, you can also include a direct torsion-angle reconstruction loss if your decoder explicitly outputs angles.</li> <li>Inference: \u2022   Input: Torsion angles (perhaps from partial predictions or from an RNA sequence\u2013based predictor). \u2022   Output: 3D coords, via \\text{Decoder}(z) \\rightarrow \\text{Torsion}\\rightarrow \\text{3D}.</li> </ol> </li> </ol> <p>Answer to your question: \u201cDo we now need torsion angles so we can train the autoencoder to learn the mapping to 3D?\u201d     \u2022   Yes, if your pipeline\u2019s input to the autoencoder is angles and the output is 3D (or angles that get converted to 3D), you do need to precompute those angles from your reference dataset.</p> <p>\u2e3b</p> <ol> <li>Potential Workflows</li> </ol> <p>Workflow A: Direct Torsion \\leftrightarrow 3D Autoencoder     1.  Dataset Prep:     \u2022   For each RNA structure, store \\bigl(\\mathbf{\\theta}, \\mathbf{X}\\bigr), where \\mathbf{\\theta} = torsion angles, \\mathbf{X} = 3D coordinates.     2.  Model:     \u2022   Encoder: \\mathbf{\\theta} \\to z.     \u2022   Decoder: z \\to \\mathbf{\\theta}^.     \u2022   Then a known \u201cforward-kinematics\u201d function K(\\cdot) maps \\mathbf{\\theta}^ \\to \\mathbf{X}^.     \u2022   Loss: |\\mathbf{X}^ - \\mathbf{X}| plus possible torsion/physics terms.</p> <p>Workflow B: Torsion \\leftrightarrow Torsion Autoencoder, Then Torsion \\to 3D     1.  Model: Train a standard autoencoder in torsion space only (i.e., no direct 3D reconstruction).     2.  Kinematics: After the autoencoder\u2019s decoder yields \\mathbf{\\theta}^, you have a separate function K that does \\mathbf{\\theta}^ \\to \\mathbf{X}^.     3.  Loss: Might be purely |\\mathbf{\\theta} - \\mathbf{\\theta}^|. However, you might add a 3D alignment loss in parallel if you want the angles to reflect physical correctness.</p> <p>Workflow C: 3D \\leftrightarrow 3D Autoencoder (Skipping Torsions)     \u2022   You can do a direct 3D\u20103D autoencoder, but then you typically rely on a physically informed loss or constraints to ensure bond lengths, angles, ring closures, etc., remain realistic.</p> <p>\u2e3b</p> <ol> <li>Why Bother With Torsion Angles for the Autoencoder?     \u2022   Lower Dimensional: Torsion angles drastically cut down the parameter space compared to raw 3D coordinates.     \u2022   Implicit Local Constraints: Angles automatically fix bond lengths and angles, meaning you won\u2019t have to teach your model that those are \u201cinviolable.\u201d     \u2022   Aligns With Existing Tools: Many RNA modeling pipelines operate in angle space (Rosetta, for instance).</li> </ol> <p>Hence, it\u2019s very common to do:</p> <pre><code>Autoencoder: Angles \\to latent code \\to angles \\to 3D.\n</code></pre> <p>\u2e3b</p> <ol> <li>Putting It All Together     \u2022   Yes, if your plan is to have an autoencoder that learns a mapping from angles to 3D (or angles \\to angles \\to 3D), you\u2019ll need a set of torsion angles for training.     \u2022   The straightforward next step is:<ol> <li>Take your reference 3D structures (the same ones you\u2019re presumably using to verify embeddings).</li> <li>Compute torsion angles for each residue.</li> <li>Pair those angles with the original 3D coordinates in your training set.</li> <li>Train your chosen autoencoder approach to minimize reconstruction error.</li> </ol> </li> </ol> <p>In other words, you do want the torsion angles if your autoencoder\u2019s fundamental input (or output) is \u201cRNA torsion angles,\u201d because that\u2019s how it will learn the correct mapping to physically valid 3D conformations.</p> <p>\u2e3b</p> <p>Final Advice / Next Step     1.  Implement Torsion Computation: Write (or grab a library) to parse each residue\u2019s \\alpha, \\beta, \\gamma, \\dots, \\chi angles from your 3D references.     2.  Assemble the Dataset: (\\mathbf{\\theta}_i, \\mathbf{X}i) pairs for i=1 to N{\\text{structures}}.     3.  Define the Autoencoder: Decide if you want to decode directly to 3D or decode back to angles.     4.  Train: Minimizing a 3D RMSD or angle-level reconstruction error (plus any physical constraints).</p> <p>If your local attention and embeddings are working, you now incorporate this angle\u20133D stage as the \u201cgeometry module\u201d or \u201cautoencoder module.\u201d</p> <p>\u2e3b</p> <p>Would you like more technical detail on how to structure the autoencoder\u2019s forward/backward pass, or suggestions on specific libraries for dihedral calculation?</p>"},{"location":"reference/torsion_calculations/torsion_angles_intro/","title":"Introduction","text":"<p>Computational and Theoretical Methods for Calculating RNA Torsion Angles</p> <p>Basic Introduction to RNA Torsion Angles</p> <p>RNA torsion angles (also called dihedral angles) describe the rotation around bonds in the RNA backbone and nucleoside structure. In essence, if we imagine the RNA backbone as a flexible chain, each link can rotate about its connecting bonds \u2013 torsion angles measure these rotations \ufffc. Unlike simple Cartesian coordinates that give atom positions, torsion angles capture the rotational relationships between bonded atoms, reflecting the backbone\u2019s flexibility and the molecule\u2019s conformational freedom \ufffc. RNA has multiple torsional degrees of freedom per nucleotide: six backbone angles (\u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6) along the sugar-phosphate backbone, plus the \u03c7 angle for the glycosidic bond that connects the ribose sugar to the base \ufffc. Figure 1 below illustrates these angles on an RNA nucleotide. Each torsion angle corresponds to rotation about a specific bond (e.g. \u03b1 around the P\u2013O5\u2032 bond, \u03b2 around O5\u2032\u2013C5\u2032, and so on, as listed in standard nomenclature \ufffc).</p> <p>Figure 1: Diagram of an RNA nucleotide (unit i) showing the backbone torsion angles \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6 (orange curved arrows) and the glycosidic torsion \u03c7. The six backbone angles occur around consecutive bonds from the previous nucleotide\u2019s O3\u2032 through the phosphate and sugar ring to the next phosphate. \u03c7 is the rotation of the base around the C1\u2032\u2013N glycosidic bond. These angles jointly determine the RNA\u2019s 3D conformation. \ufffc \ufffc</p> <p>Understanding torsion angles is crucial because they define the RNA\u2019s 3D shape and thus its biological function. Certain angle combinations correspond to well-known structural motifs or helical forms (analogous to protein Ramachandran angles). For example, RNA typically adopts an A-form helix characterized by specific ranges of backbone angles, and unusual angle values can signal kinks or binding-induced conformational changes. Specific torsion angle patterns are often associated with functional RNA motifs such as enzyme active sites or binding pockets \ufffc. Thus, analyzing torsion angles can help identify and predict functional sites in an RNA structure \ufffc. In the context of RNA folding, the complex energy landscape is strongly influenced by bonded rotations \u2013 torsional and pseudo-torsional angles play a pivotal role in how the RNA folds and bends \ufffc. Because of this, many computational RNA structure prediction methods incorporate torsion angle information (or even directly use torsion angles as variables) to model RNA 3D conformations \ufffc. In fact, knowing the torsional angles for each residue can be so informative that recent machine-learning approaches aim to predict backbone torsions from sequence alone to aid RNA 3D structure prediction \ufffc.</p> <p>In addition to the standard torsions, RNA scientists often use pseudo-torsion angles as a simplified descriptor of RNA backbone shape. Pseudo-torsion angles are defined not by a single covalent bond rotation, but by four atoms that form a \u201cvirtual\u201d dihedral. A common pseudo-torsional pair is \u03b7 (eta) and \u03b8 (theta), defined by the positions of atoms along the RNA backbone (for instance, using the phosphorus (P) and C4\u2032 atoms of consecutive residues) \ufffc. These \u03b7/\u03b8 angles provide a coarse-grained description of the RNA chain\u2019s overall fold, analogous to how protein \u03c6/\u03c8 plots summarize backbone conformation \ufffc. While pseudo-torsions sacrifice atomic detail, they allow one to visualize and compare RNA conformations in a 2D plot (\u03b7 vs. \u03b8) and have been effective for classifying RNA loop structures and folding trajectories \ufffc. Another important concept is the sugar pucker: the ribose ring in RNA is not planar and can pucker in different ways (C3\u2032-endo, C2\u2032-endo, etc.). Rather than tracking five separate ring torsions (\u03bd\u2080\u2013\u03bd\u2084), chemists use a pseudorotation phase angle P that uniquely describes the ribose pucker conformation \ufffc. P ranges 0\u2013360\u00b0 and corresponds to distinct sugar pucker geometries (e.g. P \u2248 18\u00b0 for C3\u2032-endo, P \u2248 162\u00b0 for C2\u2032-endo), effectively compressing five highly correlated torsions into one parameter \ufffc. Together, the backbone torsions, glycosidic \u03c7, and sugar pucker (or pseudotorsions) provide a complete internal coordinate description of an RNA molecule\u2019s structure. This internal angle representation is very powerful for analyzing RNA 3D structures, comparing conformations, and even guiding the search in RNA folding simulations \ufffc \ufffc.</p> <p>Step-by-Step Algorithm for Calculating Torsion Angles</p> <p>Calculating a torsion angle from an RNA structure involves a straightforward geometric procedure using the coordinates of four atoms. The torsion angle (dihedral) defined by four points A\u2013B\u2013C\u2013D (where the angle is around the bond between B\u2013C) is the angle between the plane formed by atoms A-B-C and the plane formed by B-C-D. Below is a step-by-step guide to computing a torsion angle from atomic coordinates:     1.  Identify the four defining atoms: Determine which four atoms define the torsion angle of interest. For example, to calculate a backbone angle like \u03b1 or \u03b2, you would pick the four atoms as defined by standard nomenclature (e.g. for \u03b1: O3\u2032(i\u22121)\u2013P(i)\u2013O5\u2032(i)\u2013C5\u2032(i); for \u03b2: P(i)\u2013O5\u2032(i)\u2013C5\u2032(i)\u2013C4\u2032(i); etc.) \ufffc. Ensure the atoms are ordered consecutively along the chain (A\u2013B\u2013C\u2013D).     2.  Retrieve atomic coordinates: Let the coordinates of atoms A, B, C, D be \\(\\mathbf{r}_A, \\mathbf{r}_B, \\mathbf{r}_C, \\mathbf{r}_D\\) in Cartesian space (x, y, z from the PDB or model). Compute the bond vectors between them:     \u2022   \\(\\mathbf{b}_1\\) = B \u2192 C: \\(\\mathbf{b}_1 = \\mathbf{r}_B - \\mathbf{r}_A\\) (vector from A to B)     \u2022   \\(\\mathbf{b}_2\\) = C \u2192 D: \\(\\mathbf{b}_2 = \\mathbf{r}_C - \\mathbf{r}_B\\)     \u2022   \\(\\mathbf{b}_3\\) = (for completeness) D \u2192 (next): \\(\\mathbf{b}_3 = \\mathbf{r}_D - \\mathbf{r}_C\\) Actually, for the dihedral at bond B\u2013C, we use vectors on each side of that bond: so we will use \\(\\mathbf{b}_1 = \\mathbf{r}_A - \\mathbf{r}_B\\), \\(\\mathbf{b}_2 = \\mathbf{r}_C - \\mathbf{r}_B\\), and \\(\\mathbf{b}_3 = \\mathbf{r}_D - \\mathbf{r}_C\\). (This means \\(\\mathbf{b}_1\\) and \\(\\mathbf{b}_2\\) span the first plane A-B-C, and \\(\\mathbf{b}_2\\) and \\(\\mathbf{b}_3\\) span the second plane B-C-D.)     3.  Compute the plane normals: Calculate two normal vectors, one for each of the two planes:     \u2022   \\(\\mathbf{n}_1\\): normal to plane A-B-C = \\(\\mathbf{b}_1 \\times \\mathbf{b}_2\\) (cross product of \\(\\mathbf{b}_1\\) and \\(\\mathbf{b}_2\\))     \u2022   \\(\\mathbf{n}_2\\): normal to plane B-C-D = \\(\\mathbf{b}_2 \\times \\mathbf{b}_3\\) These normals are perpendicular to the respective planes. (If the three atoms are perfectly collinear, the cross product will be zero \u2013 in a well-formed RNA structure this won\u2019t occur for torsion angles because the backbone isn\u2019t straight.)     4.  Normalize and define reference axis: It\u2019s often useful to normalize these normal vectors and also define a unit vector along the middle bond:     \u2022   \\(\\hat{\\mathbf{n}}_1 = \\mathbf{n}_1 / ||\\mathbf{n}_1||\\), \\(\\hat{\\mathbf{n}}_2 = \\mathbf{n}_2 / ||\\mathbf{n}_2||\\) (unit normals).     \u2022   \\(\\hat{\\mathbf{u}} = \\mathbf{b}_2 / ||\\mathbf{b}_2||\\) (unit vector along the B\u2013C bond, the rotation axis). Now \\(\\hat{\\mathbf{n}}_1\\) and \\(\\hat{\\mathbf{n}}_2\\) lie in planes perpendicular to B\u2013C, so the dihedral angle between the planes is the angle between these normal vectors.     5.  Calculate the angle magnitude: Compute the angle between \\(\\hat{\\mathbf{n}}_1\\) and \\(\\hat{\\mathbf{n}}_2\\). A convenient formula is via the dot product: \\(\\cos \\phi = \\hat{\\mathbf{n}}_1 \\cdot \\hat{\\mathbf{n}}2\\). So, \\(\\phi{\\text{unsigned}} = \\arccos(\\hat{\\mathbf{n}}_1 \\cdot \\hat{\\mathbf{n}}_2)\\) gives the magnitude of the torsion angle (between 0\u00b0 and 180\u00b0). However, this alone doesn\u2019t tell us if the rotation from one plane to the other is clockwise or counterclockwise. At this stage, we have the absolute angle; for example, we might get \\(\\phi = 60\u00b0\\) or \\(120\u00b0\\) etc., but we need to determine if it\u2019s +60\u00b0 or \u201360\u00b0.     6.  Determine the sign (orientation): To get the signed torsion angle (in the range \u2013180\u00b0 to +180\u00b0), we need to know whether \\(\\hat{\\mathbf{n}}_2\\) is \u201cahead\u201d of \\(\\hat{\\mathbf{n}}_1\\) in a right-handed or left-handed sense around the B\u2013C axis. One robust method is to use the vector cross product and the reference axis \\(\\hat{\\mathbf{u}}\\). Compute a value \\(s = (\\hat{\\mathbf{n}}_1 \\times \\hat{\\mathbf{n}}_2) \\cdot \\hat{\\mathbf{u}}\\). This essentially projects the cross-product of the normals onto the direction of the bond axis. If \\(s\\) is positive, one convention is to assign a positive sign to \\(\\phi\\); if \\(s\\) is negative, assign a negative sign \ufffc. Another equivalent approach is to use the two-argument arctan function: define \\(x = \\hat{\\mathbf{n}}_1 \\cdot \\hat{\\mathbf{n}}_2\\) and \\(y = (\\hat{\\mathbf{n}}_1 \\times \\hat{\\mathbf{n}}_2) \\cdot \\hat{\\mathbf{u}}\\), then \\(\\phi = \\operatorname{atan2}(y, x)\\) \ufffc. The atan2 formulation gives the correct signed angle directly, avoiding ambiguities and numerical issues that can occur with just \\(\\cos^{-1}\\) \ufffc. Using atan2 is recommended for stability, since \\(\\cos^{-1}\\) alone loses information about sign and can be unstable near 0\u00b0 or 180\u00b0 \ufffc.     7.  Output the torsion angle: The result is \\(\\phi\\), typically reported in degrees (e.g., \u03c6 = \u201360\u00b0 meaning that the second plane is rotated \u201360\u00b0 relative to the first when looking along the bond from B to C). By convention, RNA torsion angles are often reported in the range 0\u00b0 to 360\u00b0 (sometimes 0 to \u00b1180\u00b0). Tools might output, for example, \u03c7 = 280\u00b0 instead of \u201380\u00b0; these are equivalent representations (differing by 360\u00b0). It\u2019s important to be consistent with conventions (most nucleic acid literature uses 0\u00b0 to 360\u00b0 for \u03c7 syn vs anti, and \u2013180\u00b0 to 180\u00b0 for backbone angles, but it can vary).</p> <p>Using the above method, any torsion angle can be calculated from atomic coordinates. This procedure is implemented in virtually every computational chemistry or molecular modeling package \ufffc. It\u2019s worth noting that correct identification of the four atoms and maintaining the order (A\u2013B\u2013C\u2013D) is critical \u2013 reversing the order will invert the sign of the angle. The algorithm described ensures a clear geometric interpretation: it finds the angle between two planes and uses the right-hand rule (or a defined convention) to get the direction of rotation \ufffc. For RNA specifically, one would apply this calculation to each defined torsion in the backbone and base linkage. For instance, to get all backbone angles for a nucleotide i, you would gather: O3\u2032(i\u22121), P(i), O5\u2032(i), C5\u2032(i) for \u03b1; then P(i), O5\u2032(i), C5\u2032(i), C4\u2032(i) for \u03b2; and so on through \u03b6 \ufffc, plus the \u03c7 angle using O4\u2032(i), C1\u2032(i), N (base), and appropriate base atom (N1 for pyrimidine or N9 for purine) \ufffc. Repeating this for each nucleotide yields the full set of torsion angles describing the RNA\u2019s conformation.</p> <p>Computational Methods and Tools for Torsion Angle Calculation</p> <p>A variety of computational tools are available to calculate torsion angles in RNA structures. These range from general molecular visualization software to specialized nucleic acid analysis programs and libraries. Below we explore some of the commonly used methods:     \u2022   Interactive Molecular Visualization Software: PyMOL and UCSF Chimera/ChimeraX are popular programs that allow users to visualize 3D structures and interactively measure geometric parameters. In PyMOL, one can measure a torsion (dihedral) angle by selecting four atoms and using the get_dihedral command, or simply by Ctrl-right-clicking on a bond to read out the dihedral angle \ufffc. PyMOL also allows manual adjustment of torsions (e.g., using set_dihedral atom1, atom2, atom3, atom4, angle) which can be useful for modeling how changes in angles affect structure \ufffc. Chimera provides a similar feature: its Angles/Torsions tool lets you select four atoms and it will report the torsion angle in a table \ufffc. These tools are very handy for examining individual angles or small numbers of angles in a structure. They often provide a visual confirmation \u2013 for example, highlighting the chosen torsion bond and showing the numerical value \u2013 which aids in teaching or quick analysis. However, for systematically computing all torsion angles in a large RNA or across many structures, other tools are more efficient.     \u2022   Dedicated Nucleic Acid Analysis Software: There are specialized programs designed to analyze nucleic acid geometries, which can automatically calculate all backbone torsion angles of RNA:     \u2022   3DNA: 3DNA is an open-source toolkit widely used for DNA/RNA structural analysis. It can calculate the six backbone torsions \u03b1 through \u03b6 and the \u03c7 angle for each nucleotide, along with sugar pucker parameters \ufffc. 3DNA reads PDB files and outputs a report of torsion angles. A limitation noted in recent years is that older versions of 3DNA did not support the newer PDBx/mmCIF file format \ufffc, meaning it might require converting mmCIF to PDB.     \u2022   DSSR (Discrete Structural Signature of RNA): DSSR is effectively the successor to 3DNA by the same author, providing a streamlined command-line tool for RNA (and DNA) structure annotation. DSSR will compute torsions and pseudotorsions and is updated to handle PDBx/mmCIF smoothly \ufffc. It\u2019s highly automated and can integrate with visualization tools; however, DSSR is distributed as a closed-source (commercial) program in some versions \ufffc.     \u2022   Curves+: Curves+ is another program that was historically used to analyze helical parameters and torsions for nucleic acids. It generates detailed backbone conformational analyses. Unfortunately, as of some reports, the official Curves+ download became unavailable (website issues) \ufffc, making it hard to obtain.     \u2022   Barnaba: Barnaba is a Python library specifically for analyzing RNA structures and trajectories. It can calculate torsion angles (and other metrics like base stacking, hydrogen bonds) from either single structures or molecular dynamics (MD) trajectories \ufffc. Barnaba is handy for researchers who prefer scripting and want to integrate torsion calculations into Python workflows (e.g., batch-processing many PDB files or analyzing simulation data). One caveat is that Barnaba, like 3DNA, may not natively support mmCIF input, so PDB format might be needed \ufffc.     \u2022   RNAtango: RNAtango is a newer web server introduced in 2024 that specializes in computing and comparing RNA torsion and pseudo-torsion angles \ufffc. It provides a user-friendly interface where one can upload an RNA 3D model, and it returns all the torsion angles, plus analysis like whether the angles match typical values or how two structures differ in torsion space \ufffc. RNAtango even produces graphs (like \u201ctorsion profiles\u201d along the sequence and cluster analyses in angle space) and supports multiple structure comparisons \ufffc. It essentially brings torsion angle analysis to non-programmers via a web interface.     \u2022   AMIGOS III: AMIGOS III is a PyMOL plugin that specifically calculates RNA/DNA pseudo-torsion angles \u03b7 and \u03b8 and can plot them in a Ramachandran-like plot \ufffc. This is useful for quick visualization of the pseudo-torsion distribution of a structure or detecting outliers in loops, etc.     \u2022   Molecular Dynamics (MD) Simulation Tools: In an MD simulation, an RNA molecule\u2019s torsion angles will fluctuate over time. Analyzing these torsions is key to understanding RNA flexibility and transitions (e.g., sugar pucker flips or base flips). MD packages and analysis tools provide ways to calculate torsion angles from trajectory data:     \u2022   In Amber (a popular MD software for biomolecules), the post-processing tool cpptraj can directly calculate torsion angles for specified atom groups (for instance, one can script cpptraj to output all \u03b1, \u03b2, \u2026, \u03b6 angles over time for an RNA). Amber also has predefined masks for nucleic acid backbone torsions.     \u2022   In GROMACS, one can use the module gmx angle with the -type dihedral option to compute dihedral angles over a trajectory (requiring an index file that defines the four atoms for each angle of interest).     \u2022   MDAnalysis (Python library): As an example, MDAnalysis has a nuclinfo.torsions() function that returns the backbone torsions and \u03c7 for each residue in a structure or trajectory frame \ufffc \ufffc. This can be used to generate time series of each torsion angle. Using such libraries, one can programmatically analyze thousands of frames, compute average angles, track which torsions are stable vs. which undergo rotations, etc.     \u2022   VMD (Visual Molecular Dynamics): VMD provides an interactive way to measure torsions in a single frame (similar to PyMOL) but also Tcl scripting to loop over trajectory frames and calculate dihedrals. Many MD practitioners use VMD scripts to identify when an angle transitions from one conformation to another (for example, tracking a \u03c7 angle to see base flipping). MD-related computations can produce large datasets of angles (angle vs. time). These are often processed with statistical tools to compute distributions or free energy profiles as a function of a torsion (for example, computing a 1D potential of mean force for a certain torsion by Boltzmann-inverting the angle distribution). In essence, MD simulations rely on the same geometry calculations but applied repeatedly for each frame. Modern tools optimize this by using vectorized math and can handle tens of thousands of angle calculations efficiently.     \u2022   Custom Scripting and Libraries: For full flexibility, researchers sometimes write their own scripts (in Python, MATLAB, etc.) to calculate torsion angles, especially if they want to integrate the calculation into a larger analysis pipeline. The algorithm described earlier can be coded easily using vector algebra libraries (NumPy for Python, for example). In fact, the algorithm is so standard that many textbooks and online resources provide pseudocode or examples \ufffc. As a case in point, the 3DNA website provides a step-by-step MATLAB/Octave script for dihedral calculation \ufffc \ufffc. By writing custom code, one can also calculate non-standard torsions (for instance, an angle between any four chosen atoms to study a particular conformational parameter). Nonetheless, for routine backbone torsions in RNA, it\u2019s usually preferable to use validated tools like those above to avoid mistakes in atom ordering or definitions.</p> <p>Each of these computational methods has its niche. Visualization programs (PyMOL, Chimera) are great for quick checks or interactive work, while command-line tools and libraries (3DNA, DSSR, Barnaba, MDAnalysis) excel at high-throughput or large-scale analyses. Web servers like RNAtango make torsion analysis accessible without programming, which is useful for educational purposes or quick exploration of a new structure. All these tools ultimately use the same geometric formulas under the hood, so they should in principle yield the same angle values (within rounding differences). The choice often comes down to convenience, the scale of the task, and integration with other analyses.</p> <p>Theoretical Approaches and Models for Torsion Angles</p> <p>Beyond direct computation from coordinates, there are theoretical frameworks and models that provide deeper insight into RNA torsion angles and even predict them under various circumstances. These approaches treat torsion angles not just as outputs to calculate, but as fundamental parameters in understanding and modeling RNA structure. Key theoretical aspects include:     \u2022   Internal Coordinates and Torsion Angle Dynamics: One theoretical approach in modeling biomolecules is to use internal coordinates (bond lengths, bond angles, and torsions) instead of Cartesian coordinates. In such models, torsion angles serve as the primary degrees of freedom during conformational search. For example, some RNA 3D structure prediction algorithms perform Monte Carlo or molecular dynamics in torsion angle space \u2013 they keep bond lengths and bond angles fixed, and randomly tweak torsions to explore new conformations. This is analogous to methods used in protein folding (\u03c6/\u03c8 angle sampling). By operating in torsion space, the search avoids generating distorted structures with unrealistic bond lengths. However, it requires having reasonable initial values and ranges for each torsion. Force fields play a role here: they include torsion angle potential terms (periodic trigonometric functions that assign an energy to a given torsion value) to guide the molecule toward known favorable conformations. For instance, a torsion potential might have minima corresponding to gauche+/gauche\u2013/trans orientations. Theoretical studies of these potentials \u2013 sometimes by quantum chemical calculations on small model compounds (like dimethyl phosphate for backbone or a nucleoside for \u03c7) \u2013 help determine the preferred torsion angle values and barriers. These are then incorporated into MD force fields or Monte Carlo acceptance criteria.     \u2022   Mathematical Models of Angle Correlations: RNA has many torsion angles per residue, and they are not all independent \u2013 physical constraints and stereochemistry couple certain angles. Theoretical work has been done to understand these correlations. A prime example is the concept of RNA rotamers or conformers. Just as protein side chains have rotamer libraries, RNA backbones also exhibit a finite set of recurrent conformations. An influential analysis by Murray et al. (2003) showed that if one takes high-quality RNA crystal structures and looks at the distributions of backbone angles, distinct clusters emerge \ufffc. In that work, they introduced the idea of RNA suite conformers: instead of treating one nucleotide\u2019s backbone as \u03b1\u2013\u03b6, they considered a \u201csuite\u201d from one base to the next base (essentially P(i) to P(i+1), involving the seven angles \u03b1, \u03b2, \u03b3, \u03b4, \u03b5, \u03b6 of one nucleotide plus the \u03b1 of the next, or equivalently \u03b4\u2081 through \u03b4\u2082 in their terms) \ufffc. They found about 42 discrete backbone conformers that RNA generally adopts \ufffc. This is a theoretical framework for thinking about RNA: rather than continuous ranges, each torsion angle tends to fall into one of a set of allowed combinations (rotamers) that can be named and catalogued. Such rotamer libraries (expanded in subsequent work by Richardson et al. 2008 to include all seven angles) provide a basis for validating structures (does a given RNA backbone have a valid combination or an unusual one?) and for building models (one can restrain a modeling algorithm to these rotameric states to reduce the search space). In practice, these libraries are part of RNA structural knowledge bases and help in tasks like conformational scoring or structure prediction.     \u2022   Pseudo-torsional Simplification: Theoretical models sometimes simplify RNA representation by focusing on pseudo-torsion angles \u03b7 and \u03b8 (or variants like \u03b7\u2032/\u03b8\u2032 for different atom choices). This idea, originally introduced by Olson (1980) and later advocated by Pyle and colleagues \ufffc, treats the RNA backbone as a series of virtual bonds connecting, say, the P atom of one nucleotide to the C4\u2032 of the next. The dihedral angle formed by four atoms P(i\u22121)\u2013P(i)\u2013P(i+1)\u2013P(i+2) (or a similar scheme) can capture large-scale bending of the backbone without dealing with every single torsion. The pseudotorsional space (\u03b7, \u03b8) plot has proven useful for classifying RNA loop structures and has even been used in machine learning to compare predicted vs. known folds \ufffc. Theoretically, if one can predict the (\u03b7, \u03b8) sequence for an RNA, one has a low-resolution path of the backbone, which can guide more detailed modeling. This coarse-grained model is grounded in the observation that \u03b7/\u03b8 are strongly correlated with the overall fold (for example, certain loop types occupy distinct regions in \u03b7\u2013\u03b8 space). Thus, theoretical frameworks often consider multi-scale representations: use pseudo-angles for global shape and real torsions for local detail.     \u2022   Sugar Pucker and Pseudorotation Theory: The ribose sugar pucker is another aspect with a solid theoretical underpinning. The Altona &amp; Sundaralingam pseudorotation parameters (P and amplitude) provide a mathematical model of the five-membered ring conformation. The formula for the pseudorotation angle P (not reproduced fully here) combines the five ring torsions (\u03bd\u2080 through \u03bd\u2084) with fixed phase offsets (36\u00b0 increments) \ufffc. The result is a single angle P that cycles through 0\u2013360\u00b0, with P = 0\u00b0 defined as one specific twist conformation (C2\u2032-exo/C3\u2032-endo) and P increasing corresponding to moving the pucker through C3\u2032-endo, then C3\u2032-exo, C2\u2032-endo, C2\u2032-exo, etc. Every 180\u00b0 apart in P yields enantiomeric puckers (opposite sides of the plane) \ufffc. This theoretical construction is extremely useful in practice: rather than dealing with five correlated dihedrals in the sugar (which are hard to interpret individually), researchers use P to quickly describe the sugar (e.g., \u201cthe sugar pucker is P = 18\u00b0, close to C3\u2032-endo\u201d). The amplitude (tau_m) tells how far from planar the puckering is. The theoretical underpinnings here ensure that these parameters are rigorously defined; software calculating sugar pucker will often use these formulas internally.     \u2022   Analytical Scoring Functions and ML Models: Another theoretical angle is using mathematical or machine-learning models to predict or evaluate torsion angles. For example, knowledge-based scoring functions for RNA 3D models sometimes include terms for torsion angles \u2013 essentially a statistical potential that gives a score based on how favorable a particular combination of angles is (derived from known structures) \ufffc. One such method, 3dRNAscore, explicitly combines distance-based and torsion-based energy terms to evaluate RNA models \ufffc. On the machine learning front, as mentioned earlier, new models like RNA-TorsionBERT treat the sequence of an RNA as input and predict a sequence of torsion angle values as output \ufffc \ufffc. These predictions can be used to assemble a rough 3D structure or to assess a predicted structure by comparing predicted vs actual angles. Such approaches essentially create a direct mapping from sequence to the internal geometry (bypassing full 3D prediction), which underscores how torsion angles serve as a bridge between sequence and structure in theoretical models.</p> <p>In summary, theoretical approaches to torsion angles encompass using them as parameters in modeling and simulation, analyzing their statistical distributions to find underlying principles (like rotameric states), and simplifying or transforming them (pseudo-angles, phase angles) to gain conceptual insight. These frameworks complement direct computation: while computation gives the value of angles for a given structure, theoretical approaches tell us what those values mean, which ones are allowed or common, and how we might predict or manipulate them.</p> <p>Comparison of Different Approaches</p> <p>There are several ways to approach torsion angles \u2013 from direct computation to theoretical modeling \u2013 and each has its advantages and limitations. Here we compare these approaches in terms of accuracy, efficiency, and practical application:     \u2022   Direct Coordinate Calculation vs. Theoretical Prediction: Computing a torsion angle from known 3D coordinates (as done by PyMOL, 3DNA, etc.) is a straightforward geometric task that is extremely accurate (within numerical precision) \u2013 essentially, it\u2019s exact given the input coordinates. On the other hand, predicting torsion angles ab initio (for example, from sequence or by sampling) can be less accurate if the prediction method is imperfect. For instance, a physics-based simulation might sample a slightly non-ideal angle due to force field limitations, or a machine learning model might predict \u03c7 = 200\u00b0 when the true structure has \u03c7 = 250\u00b0. That said, once a structure is determined (experimentally or by modeling), all computational tools should report the same torsion values (any differences are usually &lt;1\u00b0 due to rounding). So, in terms of measuring angles on a known structure, all methods are equally accurate mathematically. The real comparison comes in predictive power: theoretical approaches like rotamer libraries or ML prediction offer insights or initial guesses for angles, while direct computation simply reads them off a structure. In practice, these complement each other \u2013 e.g., one might use a rotamer library to validate if the angles computed from a structure make sense (fall into a known rotamer cluster or not).     \u2022   All-Angle Detail vs. Pseudo-Angle Simplicity: Using the full set of torsion angles (\u03b1\u2013\u03b6, \u03c7, \u03bd\u2080\u2013\u03bd\u2084) gives a complete and detailed description of RNA conformation. This level of detail is necessary to analyze specific interactions (like if a base is syn or anti, which \u03c7 tells, or if the backbone has an unusual \u03b3 torsion indicative of a particular motif). However, this richness comes with complexity \u2013 comparing two RNA structures by all their torsion angles can be unwieldy (dozens of values to compare per nucleotide). Pseudo-torsion approaches (\u03b7, \u03b8) simplify this by reducing the dimensionality. They allow quick, coarse comparisons: for example, overlaying two folding pathways in \u03b7\u2013\u03b8 space or clustering RNA loops by their pseudo-angle values. The trade-off is that pseudo-angles can\u2019t distinguish some differences \u2013 two structures with the same (\u03b7, \u03b8) might differ in \u03c7 or other fine details. In comparative analysis, one often uses both: first use pseudo-torsions or an angle subset to get a broad alignment, then examine full torsions for fine differences. Notably, RNAtango and similar tools emphasize that torsion-based comparison is a powerful complement to Cartesian coordinate RMSD methods \ufffc. Angles provide a rotationally invariant, sequence-order-based way to compare conformations. They reduce the complexity (no need to worry about superposition in space as for RMSD) \ufffc, but they also can miss subtle shifts that coordinates would catch (hence a recommendation to use both angle-based and coordinate-based comparisons for a comprehensive analysis \ufffc).     \u2022   Software Tool Differences: We can compare the computational tools themselves:     \u2022   Ease of use: PyMOL/Chimera are user-friendly for single structures or a handful of measurements \u2013 a scientist can point and click to get angles. Tools like 3DNA or DSSR require command-line use and parsing text output, which might be less intuitive but are easily scripted for batch jobs. Web servers (RNAtango) score high on ease for non-technical users, at the cost of requiring internet access and possibly limits on structure size.     \u2022   Output and analysis features: General tools just give angle values. Specialized RNA tools often provide additional analysis \u2013 e.g., RNAtango not only gives angles but also computes differences between models, highlights which torsions deviate from typical ranges, and even outputs \u201cRamachandran-like\u201d plots for RNA \ufffc \ufffc. 3DNA/DSSR can give sugar pucker classification (C3\u2032-endo vs C2\u2032-endo) and identify unusual backbone conformers. Barnaba can integrate angle calculation with other analyses like stacking energies. So, for an in-depth RNA study, these domain-specific tools offer more context.     \u2022   Performance: If one needs to calculate torsions for hundreds of structures or thousands of MD frames, efficiency matters. Compiled programs (3DNA, DSSR) are typically very fast in processing PDB files. Python libraries (Barnaba, MDAnalysis) are convenient but may be slower per structure (though often fast enough, and they can leverage vectorization in C under the hood). PyMOL and Chimera, while great visually, are not designed for high-throughput automation (though PyMOL can be scripted, it\u2019s slower for large batches). In a head-to-head, a tool like DSSR can parse and output torsions for a typical RNA in milliseconds, whereas doing the same by manually clicking in PyMOL would take minutes. Thus, for large datasets (like scanning an entire database of RNA PDBs for certain torsion patterns), one would rely on command-line or library approaches.     \u2022   Accuracy and consistency: As mentioned, all tools should theoretically agree on the angle values. However, small implementation details can differ. For example, one tool might report \u03c7 in the range [0,360) and another in [\u2013180,180). Or a tool might define the 0\u00b0 reference differently for pseudo-torsions. For instance, Olson\u2019s original \u03b7/\u03b8 vs. the definition used by some software might differ by a constant offset; it\u2019s important to confirm the convention used. Also, rounding to one decimal place vs. more could matter for identifying borderline cases. By and large, these differences are minor, but they matter if one is combining data from different sources \u2013 a practical tip is to use one consistent tool or clearly convert conventions when comparing.     \u2022   Accuracy of Theoretical Models: The rotamer libraries and pseudo-torsion models are derived from analyzing many high-resolution structures \ufffc. They are statistically robust and have been validated by their predictive power (e.g., the existence of rotamer clusters was later confirmed by newly solved structures falling into those categories). However, they are inherently a simplification \u2013 reality is continuous and occasionally an RNA will have an intermediate torsion value that doesn\u2019t cleanly fit a rotamer bin (especially if the structure is at lower resolution or under strain). The 42 conformers from Murray et al. (2003) cover most cases, but not absolutely all. So these libraries serve as a guide rather than a strict rule. In comparison, direct computational methods will flag that torsion as simply 123\u00b0 \u2013 whether that\u2019s \u201callowed\u201d or not is up to interpretation via the theoretical framework. In essence, the theoretical frameworks provide an accuracy of expectation: they tell you if an observed angle is likely or unusual. Tools like MolProbity (for proteins) have been extended to RNA to give \u201cbackbone quality\u201d metrics based on these rotamer preferences \u2013 a form of comparison between observed angles and theoretical distributions. So, comparing approaches, the question is not which gives a better number (they all measure the same geometry), but which gives more insight. The theoretical models shine in providing context (clustering, expected ranges, correlations), whereas the computational direct methods excel at raw precision and enumeration.     \u2022   Use in Practice: Often, an RNA researcher will use multiple approaches in tandem. For example, consider someone studying a newly modeled RNA structure: they might use DSSR to list all torsion angles (computational extraction), then consult the Richardson rotamer library to label each backbone as conformer A, B, etc. (theoretical classification), then use that to explain why a certain loop is strained or to compare with known motifs. If doing an MD simulation, they might compute torsions at every frame (computational) and then perform a principal component analysis in torsion space to identify dominant motions (a data-driven theoretical analysis technique). Thus, the approaches are complementary. The direct calculation is fundamental \u2013 it feeds data into all higher-level analyses. The theoretical frameworks interpret that data.</p> <p>In summary, no single approach is \u201cbetter\u201d overall; each serves a purpose. Computational tools ensure we can obtain torsion angles accurately and efficiently from structures or trajectories. Theoretical models and frameworks ensure we understand those angles in context \u2013 what ranges are physically meaningful, how they interplay, and how we might predict or enforce them. Modern RNA structural studies leverage both: raw calculations for measurement, and theoretical constructs for making sense of the numbers.</p> <p>Advanced Technical Details and Considerations</p> <p>Calculating and working with torsion angles in RNA structures, especially at scale or with high precision requirements, involves several technical considerations. Here we delve into some advanced details, including precision, challenges, and optimizations:     \u2022   Numerical Precision and Stability: When calculating torsion angles, numerical stability is important. As noted earlier, using functions like atan2 for the final angle computation helps avoid numerical instabilities near 0\u00b0 and 180\u00b0 \ufffc. Floating-point precision can lead to tiny errors \u2013 e.g., a dot product might yield 1.0002 or 0.9998 due to rounding, which an arccos would treat as an invalid value ( &gt;1) or give an angle slightly over 180\u00b0. Robust implementations typically clamp the dot product into [\u20131, 1] before acos, or better, use atan2 which inherently stays in range. Additionally, when \\(\\mathbf{n}_1\\) and \\(\\mathbf{n}_2\\) are almost parallel or antiparallel (angle near 0 or 180), the cross product magnitude goes to zero, which can make the sign determination noisy (small numerical errors might flip the sign). A technique to handle this is to set a tolerance: if \\(||\\mathbf{n}_1 \\times \\mathbf{n}_2||\\) is below a threshold, treat the angle as 0 or 180 exactly and the sign as indeterminate (or determined by another small perturbation like looking at a tiny rotation). Most of the time, RNA structures won\u2019t hit these pathological cases, but automated pipelines include such guards to avoid crashes or nonsense output.     \u2022   Periodic Boundary Handling: Torsion angles are inherently periodic (360\u00b0 wraparound). This means special care is needed when averaging angles or comparing differences. For example, an angle of 5\u00b0 and an angle of 355\u00b0 are essentially only 10\u00b0 apart despite a naive subtraction giving 350\u00b0. Advanced analysis of torsions uses modular arithmetic to compute differences. RNAtango, for instance, explicitly uses a formula to ensure the minimum angular difference is considered, accounting for the periodic nature \ufffc. If one is computing an average torsion from multiple models or MD frames, using a simple arithmetic mean can be very wrong if angles straddle the 0/360 boundary. Instead, one uses circular mean formulas or vector averaging (convert each angle to a unit vector on the circle, average the vectors, then convert back to an angle). This falls under circular statistics \u2013 a technical but important detail for correct analysis. Many tools will internally do this when, say, reporting an \u201caverage structure\u201d torsion or comparing two structures\u2019 angles.     \u2022   Atom Naming and Reference Standards: In RNA PDB files, atoms must be correctly identified for torsion calculations. A technical challenge arises with modified nucleotides or missing atoms. For example, if a 2\u2032-OCH3 modification is present, the backbone is the same but the atom names might differ slightly, or if a structure is missing the O3\u2032 on the last residue (common for a 3\u2032 end in some structures), then the terminal \u03b1 of the next residue can\u2019t be computed. Tools like DSSR have to gracefully handle these cases (e.g., skipping angles that involve missing atoms, or recognizing modified residue atom names via a dictionary). Ensuring compatibility with file formats (PDB vs mmCIF) is another technical aspect \u2013 mmCIF uses different naming conventions and numbering, which older software might not parse. The development of new tools (like RNAtango) often stems from updating these technicalities \u2013 e.g., supporting mmCIF, handling non-canonical residues, etc., which earlier tools lacked \ufffc. When writing custom code, one must pay attention to chain breaks and sequence indexing to pick the correct four atoms for each torsion.     \u2022   Optimization for Large Data Sets: If analyzing thousands of RNA structures or a long MD trajectory, performance optimizations become important. One approach is vectorization: instead of computing each torsion in a loop, use linear algebra operations on matrices of coordinates. For instance, in Python with NumPy, one can calculate cross products and dot products on array batches, exploiting fast C implementations. MDAnalysis and similar libraries do this internally, allowing them to compute hundreds of torsions per frame quickly. Parallelization is another strategy \u2013 distribute different molecules or trajectory segments across CPU cores. Some advanced uses might even employ GPU acceleration for dihedral calculations, although that\u2019s more common in the context of MD simulations themselves (force calculations) rather than post-analysis. Still, if one were developing a custom analysis for a very large dataset (like scanning every RNA in the PDB for a certain torsion pattern), using efficient data structures and possibly compiled code (C/C++ or Cython) for the inner loop could be worthwhile.     \u2022   Precision in Output and Rounding: For publication-quality analysis, the precision of reported angles can matter. Typically, one might report torsions to the nearest degree or tenth of a degree. But when comparing, say, two models, the difference might be small (a few degrees). Some tools allow adjusting the number of decimal places in output \ufffc. It\u2019s a minor detail, but if you\u2019re looking at a torsion difference of 2.5\u00b0, rounding to whole numbers could obscure it. Conversely, too many decimals can imply false precision given experimental uncertainty (crystal structures might only be accurate to a few degrees for torsions). The key is consistency and appropriate precision.     \u2022   Edge Cases \u2013 Unusual Geometries: Occasionally, RNA structures (especially if low-resolution or modeled) might have abnormal geometries that challenge torsion calculations. For instance, if two atoms that should be distinct are very close, or if there\u2019s a formatting error causing atom order issues. Good software includes validation steps: e.g., ensuring the four atoms chosen are all distinct and bonded in series (A\u2013B\u2013C\u2013D should be a connected path). There have been cases in the PDB of mis-modeled RNA that result in bizarre torsion outputs (like an angle near 0 which should never happen due to sterics). Tools may flag such outliers. For example, a backbone angle of 0\u00b0 or 180\u00b0 might trigger a warning as it could indicate a cis sugar-phosphate linkage or a modeling error. Advanced analysis could then involve checking if such values are real or artifacts.     \u2022   Conventions and Reference Frames: A subtle technical point is the definition of the torsion\u2019s sign convention. The standard IUPAC convention for nucleic acids defines angles in a specific way (usually using a right-hand rule with the bond direction B\u2013C as reference). If one uses a left-hand rule or defines the atom order differently, the sign inverts \ufffc. Most tools adhere to the same convention (often the IUPAC 1982 standard \ufffc), but if you ever integrate data from two sources, ensure they didn\u2019t use opposite sign conventions. The magnitude (0\u2013180) is consistent, but what is labeled \u201c+\u201d vs \u201c\u2013\u201d could differ. This is less an issue for backbone angles which people often treat modulo 360 (e.g., saying \u03b3 = 60\u00b0 vs \u03b3 = \u2013300\u00b0 is usually not distinguished), but for \u03c7, syn vs anti is conventionally defined by ranges (anti ~ 180\u00b190\u00b0, syn ~ 0\u00b190\u00b0) \ufffc. Thus, it\u2019s important that syn/anti classification matches the convention used when calculating \u03c7. In practice, just be aware of the definitions your software uses (most documentation will state the atom order for each torsion; e.g., \u03c7 for purines is O4\u2032\u2013C1\u2032\u2013N9\u2013C4 in one direction \ufffc).     \u2022   Future and Emerging Techniques: On the horizon, as machine learning approaches become more integrated, we might see tools that directly annotate an RNA structure with \u201ctorsion outliers\u201d or suggest corrections in torsion space. Already, some RNA refinement protocols (particularly in crystallography and cryo-EM model building) use torsion restraints \u2013 these are constraints that keep backbone angles near ideal values during refinement. The algorithms need to compute deviations and apply forces in torsion space, which is another layer of complexity (differentiating the dihedral angle with respect to atomic coordinates for geometry optimization). This merges computational geometry with optimization techniques. The theoretical understanding of allowed torsions (rotamers) becomes practically useful here: one can bias the refinement toward the nearest rotamer state to get a more reliable structure.</p> <p>In conclusion, while calculating a single torsion angle is straightforward, performing torsion angle analysis at scale or with high rigor involves careful attention to technical details. By using well-established algorithms (cross products, atan2), respecting angular periodicity, and leveraging robust software, one can achieve precise and meaningful torsion angle calculations. These advanced considerations ensure that torsion angles remain a powerful and reliable tool for RNA structural analysis, from basic research to high-end applications in modeling and refinement.</p> <p>References: The content above integrates information from various sources, including algorithm descriptions and examples \ufffc \ufffc, educational resources on nucleotide geometry \ufffc \ufffc, and recent research on RNA torsion analysis and tools \ufffc \ufffc. Key references include Mackowiak et al. (2024) on the RNAtango tool for torsion angle analysis \ufffc \ufffc, the IUPAC nucleotide geometry standards (1982) \ufffc \ufffc, and foundational work by Murray et al. (2003) identifying rotameric states in RNA backbones \ufffc \ufffc. These, along with additional cited materials throughout the text, provide a thorough basis for understanding both the practical computations and theoretical context of RNA torsion angles.</p> <p>When we refer to a \u201cchain-of-atoms\u201d or \u201ctorsion\u2010angle\u201d (sometimes also called an internal\u2010coordinate) representation, we are treating the molecule essentially as a linked chain of atoms in 3D, rather than embedding all atoms into (x,y,z) Cartesian coordinates directly. For RNA, this typically means one of the following:     1.  Use backbone torsion angles\u2014i.e., dihedrals like \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, sugar pucker angles, plus side and base torsions\u2014to specify each residue\u2019s 3D conformation.     2.  Use \u201cchain-of-atoms\u201d coordinates\u2014directly store each atom\u2019s position, but respect the connectivity along the backbone (and possibly the sugar ring) to keep the sense of a 1D chain in 3D.</p> <p>Below is a deeper explanation of how such a representation works in the context of RNA structure prediction and deep learning, including pros and cons and how it might align with the Kaggle Stanford RNA 3D Folding challenge.</p> <p>\u2e3b</p> <ol> <li>Motivation and Overview</li> </ol> <p>For RNA and proteins alike, the polymer chain has a known sequence of monomers (nucleotides or amino acids). Each residue\u2019s 3D geometry is strongly governed by local dihedral angles and a handful of constraints (bond lengths, planarity of certain groups, etc.). In a chain-of-atoms model, we make that linear connectivity explicit:     \u2022   We index each residue 1, 2, 3, \\ldots, N.     \u2022   Each residue has a set of atoms (for RNA, typically the phosphate group P, sugar ring atoms C_1{\\prime}, C_2{\\prime}, C_3{\\prime}, O_4{\\prime},\\ldots, and the base ring(s) A, U, G, C with attached ring atoms).     \u2022   If we use torsion angles, we can represent each residue\u2019s local 3D structure by dihedral angles along the backbone and sugar ring, along with some constraints for planarity or sugar puckering.</p> <p>Comparisons to other geometry representations:     \u2022   Voxels or grids are typically too coarse for highly precise atomic data.     \u2022   Point clouds or meshes can represent the overall molecular surface, but do not easily encode the backbone connectivity or local dihedral constraints.     \u2022   Distance or orientation-based models (like AlphaFold\u2019s pairwise distances + angles) do capture local geometry, but they might be less direct for eventually producing 3D coordinates without a final relaxation.</p> <p>Hence, for atomic-level RNA modeling, a chain-of-atoms or torsion-angle parameterization is very direct and domain-specific.</p> <p>\u2e3b</p> <ol> <li>What This Representation Looks Like in Practice</li> </ol> <p>2.1 Per-Residue Internal Coordinates</p> <p>In RNA, each residue has a known chemical structure (phosphate\u2013sugar\u2013base). We can track:     \u2022   Bond lengths: P!-!O, O!-!C, etc.     \u2022   Bond angles: e.g., the angle at the sugar ring, or between phosphate\u2013ribose\u2013base.     \u2022   Torsion (dihedral) angles: \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, plus sugar pucker angles, plus the \\chi angle controlling base orientation relative to the sugar, etc.</p> <p>In a purely torsion-based approach, the bond lengths and angles are often fixed or constrained to near-ideal values, and we let the dihedrals vary to represent the backbone conformation. A small additional set of parameters can encode side/base geometry if needed.</p> <p>If we use a direct chain-of-atoms approach, we might store each atom in a local coordinate system that depends on the previous residue\u2019s configuration, ensuring we only have to learn or refine the local transformations from one residue to the next.</p> <p>2.2 Forward Kinematics for 3D Reconstruction</p> <p>Once the torsion angles are known for each residue, we can reconstruct the entire 3D structure by \u201cforward kinematics\u201d: starting from a reference orientation for residue 1, rotate the next residue\u2019s sub-block by the correct set of dihedral angles, place the next residue, and so forth. (Similarly for side chains or base orientation in RNA.)</p> <p>2.3 Where Deep Learning Fits In</p> <p>Modern deep neural networks (like AlphaFold or potential RNA analogues) often:     \u2022   Predict distances, torsions, or orientation distributions between residues.     \u2022   Convert these predicted internal coordinates (or pairwise constraints) into a full 3D conformation, typically by gradient-based or iterative geometry refinement.     \u2022   Optionally incorporate physically-based constraints (e.g., no large bond length changes, rings must remain planar or nearly so, etc.).</p> <p>Thus, the \u201cchain-of-atoms\u201d viewpoint is fully domain-aligned for molecular modeling.</p> <p>\u2e3b</p> <ol> <li>Pros and Cons in the Context of RNA</li> </ol> <p>3.1 Advantages     1.  Domain alignment. RNA is fundamentally a single chain with branching only at base attachments, so a chain-of-atoms representation respects that biology from the outset.     2.  Small number of degrees of freedom. Instead of learning \\sim 3N Cartesian coordinates, we might only learn \\sim 7N dihedral angles (plus ring constraints, etc.). This smaller parameter space can be easier to search or to embed into neural architectures, especially if we incorporate prior knowledge (e.g., typical sugar pucker angles).     3.  Direct \u201cbuildable\u201d structure. Once angles are predicted, it is straightforward to reconstruct an all-atom structure with minimal extra steps, if the rest of the geometry is constrained to known bond lengths.     4.  Common in protein structure prediction. By analogy with alpha carbons and backbone angles in proteins, many RNA modeling pipelines have established ways to handle chain-of-atoms data, refine local geometry, or run molecular dynamics.</p> <p>3.2 Disadvantages     1.  Must encode many local constraints. If any bond lengths or ring constraints are variable, we must incorporate them carefully. The more physics or domain constraints, the more specialized the pipeline.     2.  Potentially complicated. The sugar ring in RNA is not quite as simple as the protein backbone. It has a flexible pucker, so any chain-of-atoms approach must consider extra angles or constraints for the ring\u2019s conformation.     3.  Requires careful loss functions. We typically want local angles to remain in physically realistic ranges and to penalize collisions or large bond distortions. This might require carefully engineered prior potentials or physics-based energy terms.     4.  Topology is fixed. A chain-of-atoms approach is not designed to handle big topological rearrangements or multi-strand interactions (like multi-strand complexes or pseudoknots) unless we add extra cross-link constraints.</p> <p>Given that RNA is a single chain but can fold in complicated ways, the chain-of-atoms representation is well-suited for many tasks. On the Kaggle challenge, if we want to produce final atomic coordinates, we can do so from a learned set of local dihedrals. We just need to ensure that the network architecture and the final coordinate reconstruction stage incorporate the relevant domain constraints (like sugar pucker states, base planarity, etc.).</p> <p>\u2e3b</p> <ol> <li>Practical Implications for Kaggle\u2019s Stanford RNA 3D Folding<ol> <li>Model Input. If we are training from sequence alone, we\u2019d have to learn all local torsion angles as a function of sequence context. Additional data like known base pairs or secondary structures can impose constraints on angles.</li> <li>Model Output. Instead of outputting 3D coordinates (Cartesian), we can directly output the torsion angles or a small set of dihedrals that define each residue\u2019s geometry. A final geometric transformation step can produce 3D coordinates for scoring.</li> <li>Loss/Scoring. The official Kaggle metric is 3D alignment (TM-score). We can either: \u2022   Convert the predicted chain-of-atoms angles to coordinates, align to the ground truth, and compute TM-score. \u2022   Or we can define a differentiable approximation to the coordinate reconstruction as part of the pipeline.</li> <li>Incorporating Priors. Biological constraints can be explicitly baked in: e.g., typical sugar pucker angles, base planarity, or heavy-atom bond lengths. This might reduce search space and help generalization, but requires more domain engineering.</li> <li>Scalability. For longer RNAs, the number of torsion angles grows linearly with length, so in principle this is more tractable than a naive (x, y, z) approach. However, for very large RNAs or multi-chain complexes, one must integrate more advanced constraints or multi-chain transformations.</li> </ol> </li> </ol> <p>In short, for a chain-based RNA structure predictor, the network might:     \u2022   Take the primary sequence (and possibly 2D constraints).     \u2022   Predict local dihedral distributions per residue or per short window.     \u2022   Use a forward-kinematics step to build up coordinates of the entire chain.     \u2022   Refine (or physically relax) those coordinates using a standard geometry pipeline.</p> <p>\u2e3b</p> <ol> <li>Summary</li> </ol> <p>The chain-of-atoms/torsion-angle approach is a Lagrangian representation that aligns closely with how RNA actually folds: it is a single connected polymer with local dihedral angles driving large-scale structure. This representation is widely used in biomolecular modeling, especially for proteins (e.g. in AlphaFold\u2019s angle/distance pipeline). For RNA, it similarly allows a direct handle on each residue\u2019s geometry, straightforward reconstruction of 3D coordinates, and the possibility of encoding advanced domain constraints on angles and ring geometry. The tradeoff is that this representation is more specialized\u2014one must encode or learn the local chemical geometry constraints\u2014yet, for tasks like RNA 3D structure prediction, that often proves advantageous.</p> <p>Below is a \u201cbest of both worlds\u201d version (let\u2019s call it V5) that integrates the strongest points from the previous four answers (V1\u2013V4). It provides:     1.  Quantitative comparisons (clear numeric examples, a ratio table).     2.  Detailed discussion on domain-specific constraints and large-scale modeling.     3.  Clarity around memory usage (avoiding unit slips).     4.  Insights for multi-chain or complex topologies in RNA.</p> <p>We aim for rational verbosity: a thorough yet structured treatment of 3D Cartesian vs. Torsion-Angle (Internal) representations for RNA.</p> <p>\u2e3b</p> <ol> <li>Preliminaries &amp; Key Assumptions</li> </ol> <p>When modeling RNA with length N nucleotides, each residue has a certain number of heavy atoms\u2014let\u2019s assume:     \u2022   A \\approx 30 atoms/residue (a typical all-heavy-atom RNA count, not including explicit hydrogens).     \u2022   Therefore, total atoms \\approx 30N.</p> <p>Each 3D atom in a Cartesian model has (x, y, z) coordinates, i.e. 3A numbers per residue if unconstrained. By contrast, torsion-angle (internal) modeling tries to lock bond lengths and bond angles to standard values, leaving only dihedral angles free, e.g.:     \u2022   6 backbone angles (\\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta)     \u2022   1\u20132 angles for sugar pucker     \u2022   1 angle for glycosidic bond (\\chi)</p> <p>Hence, the exact count of torsions can vary (~7\u201310 per residue). For clarity, let\u2019s pick T = 8 angles/residue as a typical midpoint.</p> <p>\u2e3b</p> <ol> <li>Degrees of Freedom &amp; Parameter Counts</li> </ol> <p>2.1 Cartesian Representation     \u2022   Na\u00efve DOF Count: \\text{Cartesian DOF} \\approx 3A \\times N. With A = 30, that is 3 \\times 30 = 90 DOF per residue.     \u2022   For N Residues: \\approx 90N \\text{ degrees of freedom.}</p> <pre><code>(Aside: You can subtract 6 \u201cglobal rigid body\u201d DOF for the entire molecule, but for large N this is negligible in the scaling.)\n</code></pre> <p>2.2 Torsion-Angle (Internal) Representation     \u2022   Per-Residue Torsions: Let T = 8 to include backbone angles + sugar pucker + base orientation.     \u2022   For N Residues: \\text{Torsion DOF} \\approx T \\times N = 8N.</p> <p>Thus, both representations scale linearly with N, but the constant factor differs drastically: 90N vs. 8N.</p> <p>\u2e3b</p> <ol> <li>Sample DOF Table</li> </ol> <p>Below is a numeric illustration for various RNA lengths N. We assume:     \u2022   A=30 atoms/res.     \u2022   T=8 torsion angles/res.</p> <p>RNA Length  Total Atoms Cartesian DOF   Torsion DOF Ratio (\\frac{\\text{Cartesian}}{\\text{Torsion}}) 50  30 \\times 50 = 1500 90 \\times 50 = 4500 8 \\times 50 = 400   \\frac{4500}{400} = 11.25 100 30 \\times 100 = 3000    90 \\times 100 = 9000    8 \\times 100 = 800  \\frac{9000}{800} = 11.25 200 30 \\times 200 = 6000    90 \\times 200 = 18,000  8 \\times 200 = 1600 \\frac{18{,}000}{1600} \\approx 11.25 500 30 \\times 500 = 15{,}000    90 \\times 500 = 45{,}000    8 \\times 500 = 4000 \\frac{45{,}000}{4000} = 11.25 1000    30 \\times 1000=30{,}000 90 \\times 1000=90{,}000 8 \\times 1000=8000  \\frac{90{,}000}{8000}\\approx 11.25</p> <pre><code>\u2022   Key point: The ratio \\approx 11.25 remains constant across lengths, reinforcing that both scale linearly but with different constants.\n</code></pre> <p>\u2e3b</p> <ol> <li>Memory and Computational Complexity</li> </ol> <p>4.1 Memory Footprint     \u2022   Cartesian: Storing 90N floating-point numbers (assuming double precision, 8 bytes each) \u2192 720N bytes.     \u2022   For N=1000, that\u2019s 720{,}000 bytes = 0.72 MB (just for coordinate storage, ignoring overhead).     \u2022   Torsion: Storing 8N floating-point angles \u2192 64N bytes in double precision.     \u2022   For N=1000, 64kB of just torsion parameters.</p> <pre><code>In realistic software, there is additional overhead (indices, connectivity info, partial charges, etc.), but the baseline is clearly smaller for torsion-based storage.\n</code></pre> <p>4.2 Computational Complexity     1.  Forward Kinematics (Reconstructing 3D from Torsions):     \u2022   Torsion Model: O(N). You iteratively apply transformations residue by residue.     \u2022   Cartesian: Already in 3D, no kinematics step needed.     2.  Energy Evaluations (e.g., non-bonded interactions):     \u2022   In a na\u00efve approach, both have O(N^2) if you check every pair of atoms.     \u2022   With cutoffs or more advanced algorithms (spatial grids, neighbor lists), you can reduce the effective complexity.     \u2022   Torsion-based methods can sometimes exploit chain connectivity for local moves or advanced sampling more effectively, but you still end up evaluating many pairwise interactions.     3.  Constraint Enforcement:     \u2022   Cartesian: Must explicitly handle covalent geometry constraints (bond lengths, angles, ring closure). This can be done via energy penalty terms or constraint solvers, which often adds overhead.     \u2022   Torsion: Already bakes in standard bond lengths &amp; angles; ring closure is simplified to sugar pucker parameters. Fewer physically invalid states are possible, which can reduce wasted compute.</p> <p>\u2e3b</p> <ol> <li>Physical Realism &amp; Domain Constraints<ol> <li>Sugar Pucker and Base Planarity: \u2022   Torsion-angle models can incorporate sugar pucker angles directly, ensuring physically realistic ring conformations. In Cartesian space, ring closure constraints must be enforced separately.</li> <li>Multi-Strand or Branched RNAs: \u2022   Cartesian: Each chain can be placed arbitrarily in 3D; multi-chain systems are quite straightforward to represent. \u2022   Torsion: Typically handles a single \u201clinear\u201d chain of torsions well, but separate chains or branch points require additional transformations/constraints.</li> <li>Global vs. Local Moves: \u2022   Cartesian: You can translate/rotate entire fragments easily (global moves) but might break bond geometry if you do random coordinate perturbations. \u2022   Torsion: You can do local dihedral changes without breaking the chain; large rigid-body reorientations across multiple domains need either specialized transformations or a hierarchical framework.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Large RNA &amp; Multi-Chain Complexes     \u2022   Large RNAs (e.g., rRNA with thousands of nucleotides):     \u2022   Cartesian can become unwieldy; you have \\sim 90N DOF. If N=5{,}000, that\u2019s 450k free parameters to optimize or sample.     \u2022   Torsion is \\sim 8N; for 5{,}000 that\u2019s 40k parameters, still big, but ~1/10 the size in principle.     \u2022   Multi-Chain or Complex Topology:     \u2022   Torsion-based modeling typically handles each strand\u2019s backbone angles but must unify them with inter-chain base-pairing or bridging constraints.     \u2022   Cartesian-based approaches can place multiple chains in one coordinate frame straightforwardly, but again, more DOF, so constraints/penalties must maintain correct geometry between them.</li> </ol> <p>\u2e3b</p> <ol> <li>Summary Comparison Table</li> </ol> <p>Here\u2019s a concise side-by-side highlighting both scaling and practical considerations:</p> <p>Aspect  Cartesian (3D)  Torsion (Internal) Main DOF    3 \\times A \\times N \\approx 90N T \\times N \\approx 8N Scaling Linear in N, high constant factor   Linear in N, smaller constant factor Bond Geometry   Must be enforced via energy/constraints Often built-in (fixed bond lengths &amp; angles) Sugar Pucker    Complex constraints for ring closure    Modeled directly in 1\u20132 angles per residue 3D Reconstruction   Instant (already stored as coords)  Forward kinematics O(N) Memory Footprint    \\sim 90N floats \u2192 higher memory usage   \\sim 8N floats \u2192 more compact Multi-Chain Handling    Straightforward in 3D, but many DOF Per-chain torsions + extra inter-chain constraints Large RNA Feasibility   Potentially very large DOF (90N)    Fewer DOF (8N), more tractable for big systems Physical Validity   Many random coords are physically invalid   Angles more naturally yield valid structures Preferred Use   Direct structure refinement, easy multi-chain   Efficient for single-chain, reduced DOF, built-in constraints</p> <p>\u2e3b</p> <ol> <li>Concluding Remarks<ol> <li>Linear but Different Constants: Both approaches are O(N) in raw parameter count. Torsion angles, however, sharply reduce the constant factor (~11\u00d7 fewer DOF with the assumptions used), significantly easing the parameter-explosion problem.</li> <li>Domain-Aware Constraints: Torsion angle modeling automatically encodes covalent geometry, ring constraints, and base planarity. In Cartesian space, one must explicitly add or enforce these constraints, inflating complexity and risking physically invalid conformations without careful checks.</li> <li>Large-Scale RNA / Multi-Strand: For huge RNAs or complexes, torsion-based methods remain more compact but require specialized treatments for multi-chain, branching, or domain-level motions. Cartesian-based methods might be simpler conceptually (just 3D coords for all chains) but come at a large DOF cost.</li> <li>Practical Usage: \u2022   Torsion is popular in many RNA/protein folding frameworks (e.g., Rosetta, parts of AlphaFold pipeline, etc.) due to the built-in geometric constraints and lower dimensionality. \u2022   Cartesian is often used in direct structure refinement (MD simulations) or in contexts where we want fine local distortions or easier multi-chain manipulations.</li> </ol> </li> </ol> <p>In sum, torsion angle (chain-of-atoms) modeling provides a more compact parameterization that scales linearly at a much lower constant (~8\u201310 angles vs. up to 90 coordinates per residue). This leads to less memory usage, simpler constraint handling, and often more efficient exploration of physically valid conformations\u2014especially critical as RNA lengths grow into hundreds or thousands of nucleotides.</p> <p>\u2e3b</p> <p>Final Note</p> <p>This improved version (V5) weaves together explicit numerical ratios (from V1 and V3), a broader discussion of large complex constraints (from V2), and a clear breakdown of memory + computational aspects (from V3/V4). It aims to be both quantitatively illustrative and domain-aware, reflecting why torsion-angle modeling is so frequently favored for RNA 3D structure prediction\u2014particularly at scale.</p> <p>\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Let\u2019s create an improved answer that merges the strongest features from the previous versions and addresses any concerns (like memory-unit slips or multi-chain complexities). [emoji]: Below is a consolidated explanation, complete with numeric examples, a thorough comparison table, and discussions of constraints, large-scale modeling, and practical implications\u2014while correcting earlier issues such as the MB vs. GB confusion.</p> <p>\u2e3b</p> <p>Improved Answer: Scaling Properties of 3D Coordinates vs. Torsion Angle Modeling for RNA</p> <ol> <li>Conceptual Overview</li> </ol> <p>Two primary ways to represent an RNA molecule\u2019s 3D conformation are:     1.  Cartesian Coordinates (Full 3D)     \u2022   Each atom has (x,y,z) coordinates.     \u2022   Straightforward to visualize (direct spatial data), but typically must enforce covalent geometry constraints (bonds, angles, ring closures) via external methods.     2.  Torsion Angles (Internal Coordinates)     \u2022   Each residue is described by dihedral angles (e.g., \\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\chi, sugar pucker).     \u2022   Far fewer parameters, automatically embedding local geometry (bond lengths/angles remain fixed or near-fixed).</p> <p>Both approaches scale linearly with the number of nucleotides (N) in terms of their parameter counts, but with different constant factors. In practice, torsion-angle models usually have about 6\u201310\u00d7 fewer degrees of freedom (DoF) than Cartesian coordinates, making them especially advantageous for large RNAs.</p> <p>\u2e3b</p> <ol> <li>Quantitative Example: Degrees of Freedom</li> </ol> <p>We merge the numeric clarity of earlier versions to illustrate how many parameters each approach might use. Suppose:     \u2022   A = average number of heavy atoms per nucleotide (\\approx 20 to \\approx 30, depending on the representation).     \u2022   T = number of torsion angles per nucleotide (\\approx 7 to \\approx 10, including backbone dihedrals and sugar pucker/base angles).</p> <p>Then, for N nucleotides:     1.  Cartesian (Full 3D): \\approx 3 \\times A \\times N     2.  Torsion Angles: \\approx T \\times N</p> <p>Example Calculation:     \u2022   If A=20 and T=7, we get:     \u2022   Cartesian \\approx 60N parameters     \u2022   Torsion \\approx 7N parameters     \u2022   Ratio: \\frac{60}{7} \\approx 8.57.</p> <p>\u2e3b</p> <ol> <li>Illustrative Table of Parameter Counts and Memory Usage</li> </ol> <p>Below is a sample table assuming ~30 atoms per residue (i.e., 90 coordinates/residue) and 8 torsion angles/residue:</p> <p>RNA Length (N)  Cartesian DOF (3 \\times 30 \\times N)    Torsion DOF (8 \\times N)    Ratio   Approx. Memory (Cartesian) Approx. Memory (Torsion) 50  4,500   400 11.25   ~0.034 MB   ~0.003 MB 100 9,000   800 11.25   ~0.068 MB   ~0.006 MB 500 45,000  4,000   11.25   ~0.34 MB    ~0.03 MB 1,000   90,000  8,000   11.25   ~0.68 MB    ~0.06 MB 10,000  900,000 80,000  11.25   ~6.8 MB ~0.64 MB</p> <pre><code>*Memory assumption: 8 bytes (double precision) \u00d7 (# parameters). 1 MB \\approx 1\\!{,}048{,}576 bytes.\n</code></pre> <p>Note: In previous versions, some references to \u201cGB\u201d for 1,000-nt models were likely a numerical slip. Actual storage for a few hundred thousand parameters is in the MB range, not GB.</p> <p>\u2e3b</p> <ol> <li>Computational Complexity<ol> <li>Forward or Reverse Kinematics \u2022   Cartesian: Already in (x,y,z) form, no further \u201ckinematics\u201d needed, but one must impose constraints to keep bond lengths/angles physically correct. \u2022   Torsion: Requires an \\mathbf{O(N)} forward-kinematics pass to convert angles into 3D coordinates. In practice, this step is relatively cheap compared to the gain in having fewer overall parameters.</li> <li>Constraint Satisfaction \u2022   Cartesian: Typically requires explicit potential terms or constraint algorithms to maintain bond lengths/angles. Otherwise, a naive model might produce many unphysical structures. \u2022   Torsion: Automatically enforces local covalent geometry. The search space is narrower, focusing on dihedral angles and sugar pucker states.</li> <li>Non-Bonded Interactions \u2022   Regardless of the representation, computing pairwise interactions (van der Waals, electrostatics) can scale \\mathbf{O(N^2)} in the worst case (for all-atom calculations). Torsion angles, however, reduce the risk of exploring unphysical coordinate sets, potentially speeding up sampling/optimization.</li> <li>Gradient-Based Optimization \u2022   Both require gradient calculations, but torsion angles can involve more chain-rule complexity. With an efficient implementation, both can achieve \\mathbf{O(N)} scaling in gradients.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Physical Realism &amp; Domain Knowledge     \u2022   Cartesian Coordinates:     \u2022   Very direct representation; minimal assumptions about geometry. \u2013 Without carefully enforcing constraints, a large fraction of \u201crandom\u201d solutions will be unphysical.     \u2022   Torsion Angles:     \u2022   Incorporate known bond lengths and angles; sampling remains closer to physically relevant conformations. \u2013 If exotic conformations are required (rare ring distortions, unusual backbone angles), one must explicitly allow them\u2014though this is still simpler than adding constraints in Cartesian space.</li> </ol> <p>\u2e3b</p> <ol> <li>Large-Scale and Multi-Chain RNA Modeling     \u2022   For long RNAs (thousands of nucleotides), the smaller constant factor in torsion angles becomes crucial for feasible computations.     \u2022   Multi-strand complexes: Both representations can handle multiple chains, but:     \u2022   Cartesian: Straightforward to store separate coordinate sets, but the total DoF explodes (simply add 3\u00d7atoms for each chain).     \u2022   Torsion: Each chain has fewer local DoF, yet special constraints must manage inter-chain base-pairing, pseudoknots, etc. Rigid-body transformations or domain-level \u201cblocks\u201d can help.</li> </ol> <p>In practice, hybrid approaches that combine torsion angles for each chain (to handle local geometry) plus domain-level rigid-body transformations or constraints can provide a robust solution for large or complex systems.</p> <p>\u2e3b</p> <ol> <li>Comparative Summary Table</li> </ol> <p>Dimension   Cartesian Coordinates   Torsion Angles  Key Takeaway Parameter Count (per residue)   \\sim 60\\text{\u2013}90 (if 20\u201330 heavy atoms)    \\sim 7\\text{\u2013}10 (backbone + sugar pucker + base)   Both linear in N, but torsion\u2019s constant factor is less Memory Footprint    Higher, scales with 3\u00d7atoms\u00d7N   Lower, scales with angles\u00d7N Drastically less memory needed for large N Constraint Handling Must enforce bond lengths, ring closure, etc. externally    Many covalent constraints automatically satisfied   Torsion approach yields fewer \u201cgarbage\u201d structures Reconstruction  Already in 3D   \\mathbf{O(N)} forward kinematics    Torsion is a 1D\u21923D expansion, but typically cheap Sampling Complexity Vast search space; naive random coords likely unphysical    Smaller DoF; random angles more likely valid    Torsion drastically simplifies physically valid search Multi-Chain / Domain    Easy conceptually, but DoF sum quickly grows    Fewer DoF per chain, but more complex cross-chain constraints   Hybrid or hierarchical methods often used</p> <p>\u2e3b</p> <ol> <li>Conclusion<ol> <li>Linear Scaling but Different Constants \u2022   Both Cartesian and torsion representations have parameter counts \\mathbf{O(N)} in the RNA length. However, torsion angles typically have a much lower constant factor (often 6\u201310\u00d7 fewer parameters).</li> <li>Why Torsion Angles Are Advantageous \u2022   Automatically incorporate local covalent geometry. \u2022   Significantly reduce the size of the conformational search space. \u2022   Simpler to sample physically reasonable conformations, critical for large RNAs.</li> <li>Practical Considerations \u2022   Cartesian might be useful if unusual distortions are common or if direct coordinate manipulations are needed. \u2022   Torsion angles excel at standard single-stranded or moderately branched RNAs, and can be extended to multi-chain systems with extra constraints or hierarchical methods. \u2022   For huge complexes (thousands of nucleotides, multiple strands), combining torsion angles for local geometry plus domain-based rigid-body transformations is often the best compromise.</li> <li>Bottom Line \u2022   For typical RNA folding tasks, torsion-angle modeling is far more efficient, especially as chain length grows. The differences in parameter counts\u2014and in the overhead of enforcing basic geometry\u2014often make torsion-based methods the only tractable approach at scale.</li> </ol> </li> </ol> <p>\u2e3b</p> <p>Would you like more detail on any specific aspect (e.g., typical torsion angle sets, sugar pucker parameterization, or how to handle multi-strand RNA in torsion space)?</p>"},{"location":"testing/cosmic-ray/","title":"Cosmic ray","text":"<p>Directory Structure:</p> <p>\u2514\u2500\u2500 ./     \u251c\u2500\u2500 docs     \u2502   \u2514\u2500\u2500 source     \u2502       \u251c\u2500\u2500 how-tos     \u2502       \u2502   \u251c\u2500\u2500 distributor.rst     \u2502       \u2502   \u251c\u2500\u2500 filters.rst     \u2502       \u2502   \u251c\u2500\u2500 implementation.rst     \u2502       \u2502   \u251c\u2500\u2500 index.rst     \u2502       \u2502   \u2514\u2500\u2500 operators.rst     \u2502       \u251c\u2500\u2500 reference     \u2502       \u2502   \u251c\u2500\u2500 api     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.ast.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.commands.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.distribution.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.operators.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.tools.filters.rst     \u2502       \u2502   \u2502   \u251c\u2500\u2500 cosmic_ray.tools.rst     \u2502       \u2502   \u2502   \u2514\u2500\u2500 modules.rst     \u2502       \u2502   \u251c\u2500\u2500 badge.rst     \u2502       \u2502   \u251c\u2500\u2500 commands.rst     \u2502       \u2502   \u251c\u2500\u2500 continuous_integration.rst     \u2502       \u2502   \u251c\u2500\u2500 index.rst     \u2502       \u2502   \u2514\u2500\u2500 tests.rst     \u2502       \u251c\u2500\u2500 tutorials     \u2502       \u2502   \u251c\u2500\u2500 distributed     \u2502       \u2502   \u2502   \u2514\u2500\u2500 index.rst     \u2502       \u2502   \u2514\u2500\u2500 intro     \u2502       \u2502       \u2514\u2500\u2500 index.rst     \u2502       \u251c\u2500\u2500 concepts.rst     \u2502       \u251c\u2500\u2500 index.rst     \u2502       \u2514\u2500\u2500 theory.rst     \u251c\u2500\u2500 tests     \u2502   \u2514\u2500\u2500 resources     \u2502       \u2514\u2500\u2500 fast_tests     \u2502           \u2514\u2500\u2500 README.md     \u251c\u2500\u2500 CONTRIBUTING.rst     \u2514\u2500\u2500 README.rst</p>"},{"location":"testing/cosmic-ray/#file-docssourcehow-tosdistributorrst","title":"File: /docs/source/how-tos/distributor.rst","text":"<p>============ Distributors ============</p> <p>TODO: Explain how to create a distributor.</p>"},{"location":"testing/cosmic-ray/#file-docssourcehow-tosfiltersrst","title":"File: /docs/source/how-tos/filters.rst","text":"<p>======= Filters =======</p> <p>The <code>cosmic-ray init</code> commands scans a module for all possible mutations, but we don't always want to execute all of these. For example, we may know that some of these mutations will result in equivalent mutants, so we need a way to prevent these mutations from actually being run.</p> <p>To account for this, Cosmic Ray includes a number of filters. Filters are nothing more than programs - generally small ones - that modify a session in some way, often by marking certains mutations as \"skipped\", thereby preventing them from running. The name \"filter\" is actually a bit misleading since these programs could modify a session in ways other than simply skipping some mutations. In practice, though, the need to skip certain tests is by far the most common use of these programs.</p>"},{"location":"testing/cosmic-ray/#using-filters","title":"Using filters","text":"<p>Generally speaking, filters will be run immediately after running <code>cosmic-ray init</code>. It's up to you to decide which to run, and often they will be run along with <code>init</code> in a batch script or CI configuration.</p> <p>For example, if you wanted to apply the <code>cr-filter-pragma</code> filter to your session, you could do something like this:</p> <p>.. code-block:: bash</p> <p>cosmic-ray init cr.conf session.sqlite   cr-filter-pragma session.sqlite</p> <p>The <code>init</code> would first create a session where all mutation would be run, and then the <code>cr-filter-pragma</code> call would mark as skipped all mutations which are on a line with the pragma comment.</p>"},{"location":"testing/cosmic-ray/#filters-included-with-cosmic-ray","title":"Filters included with Cosmic Ray","text":"<p>Cosmic Ray comes with a number of filters. Remember, though, that they are nothing more than simple programs that modify a session in some way; it should be straightforward to write your own filters should the need arise.</p>"},{"location":"testing/cosmic-ray/#cr-filter-operators","title":"cr-filter-operators","text":"<p><code>cr-filter-operators</code> allows you to filter out operators according to their names. You provide the filter with a set of regular expressions, and any Cosmic Ray operator who's name matches a one of these expressions will be skipped entirely.</p> <p>The configuration is provided through a TOML file such as a standard Cosmic Ray configuration. The expressions must be in a list at the key \"cosmic-ray.filters.operators-filter.exclude-operators\". Here's an example:</p> <p>.. code-block:: toml</p> <p>[cosmic-ray.filters.operators-filter]   exclude-operators = [     \"core/ReplaceComparisonOperator_Is(Not)?(Not)?(Eq|[LG]tE?)\",     \"core/ReplaceComparisonOperator(Not)?(Eq|[LG]tE?)_Is(Not)?\",     \"core/ReplaceComparisonOperator_LtE_Eq\",     \"core/ReplaceComparisonOperator_Lt_NotEq\",   ]</p> <p>The first regular expression here is skipping the following operators:</p> <ul> <li>core/ReplaceComparisonOperator_Is_Eq</li> <li>core/ReplaceComparisonOperator_Is_Lt</li> <li>core/ReplaceComparisonOperator_Is_LtE</li> <li>core/ReplaceComparisonOperator_Is_Gt</li> <li>core/ReplaceComparisonOperator_Is_GtE</li> <li>core/ReplaceComparisonOperator_Is_NotEq</li> <li>core/ReplaceComparisonOperator_Is_NotLt</li> <li>core/ReplaceComparisonOperator_Is_NotLtE</li> <li>core/ReplaceComparisonOperator_Is_NotGt</li> <li>core/ReplaceComparisonOperator_Is_NotGtE</li> <li>core/ReplaceComparisonOperator_IsNot_Eq</li> <li>core/ReplaceComparisonOperator_IsNot_Lt</li> <li>core/ReplaceComparisonOperator_IsNot_LtE</li> <li>core/ReplaceComparisonOperator_IsNot_Gt</li> <li>core/ReplaceComparisonOperator_IsNot_GtE</li> <li>core/ReplaceComparisonOperator_IsNot_NotEq</li> <li>core/ReplaceComparisonOperator_IsNot_NotLt</li> <li>core/ReplaceComparisonOperator_IsNot_NotLtE</li> <li>core/ReplaceComparisonOperator_IsNot_NotGt</li> <li>core/ReplaceComparisonOperator_IsNot_NotGtE</li> </ul> <p>While all of the entries in <code>operators-filter.exclude-operators</code> are treated as regular expressions, you don't need to us \"fancy\" regular expression features in them. As in the last two entries in the example above, you can do matching against an exact string; these are still regular expressions, albeit simple ones.</p> <p>For a list of all operators in your Cosmic Ray installation, run <code>cosmic-ray operators</code>.</p>"},{"location":"testing/cosmic-ray/#cr-filter-pragma","title":"cr-filter-pragma","text":"<p>The <code>cr-filter-pragma</code> filter looks for lines in your source code containing the comment \"# pragma: no mutate\". Any mutation in a session that would mutate such a line is skipped.</p>"},{"location":"testing/cosmic-ray/#cr-filter-git","title":"cr-filter-git","text":"<p>The <code>cr-filter-git</code> filter looks for edited or new lines from the given git branch. Any mutation in a session that would mutate other lines is skipped.</p> <p>By default the <code>master</code> branch is used, but you could define another one like this:</p> <p>.. code-block:: toml</p> <p>[cosmic-ray.filters.git-filter]   branch = \"rolling\"</p>"},{"location":"testing/cosmic-ray/#external-filters","title":"External filters","text":"<p>Other filters are defined in separate projects.</p>"},{"location":"testing/cosmic-ray/#cosmic-ray-spor-filter","title":"cosmic-ray-spor-filter","text":"<p>The <code>cosmic-ray-spor-filter</code> filter modifies a session by skipping mutations which are indicated in a <code>spor &lt;https://github.com/abingham/spor&gt;</code>_ anchored metadata repository. In short, <code>spor</code> provides a way to associated arbitrary metadata with ranges of code, and this metadata is stored outside of the code. As your code changes, <code>spor</code> has algorithms to update the metadata (and its association with the code) automatically.</p> <p>Get more details at <code>the project page &lt;https://github.com/abingham/cosmic-ray-spor-filter&gt;</code>_.</p>"},{"location":"testing/cosmic-ray/#file-docssourcehow-tosimplementationrst","title":"File: /docs/source/how-tos/implementation.rst","text":""},{"location":"testing/cosmic-ray/#implementation","title":"Implementation","text":"<p>Cosmic Ray works by parsing the module under test (MUT) and its submodules into abstract syntax trees using <code>parso &lt;https://github.com/davidhalter/parso&gt;</code>_. It walks the parse trees produced by parso, allowing mutation operators to modify or delete them. These modified parse trees are then turned back into code which is written to disk for use in a test run.</p> <p>For each individual mutation, Cosmic Ray applies a mutation to the code on disk. It then uses user-supplied test commands to run tests against mutated code.</p> <p>In effect, the mutation testing algorithm is something like this:</p> <p>.. code:: python</p> <pre><code>for mod in modules_under_test:\n    for op in mutation_operators:\n        for site in mutation_sites(op, mod):\n            mutant_ast = mutate_ast(op, mod, site)\n            write_to_disk(mutant_ast)\n\n            try:\n                if discover_and_run_tests():\n                    print('Oh no! The mutant survived!')\n                else:\n                    print('The mutant was killed.')\n            except Exception:\n                print('The mutant was incompetent.')\n</code></pre> <p>Obviously this can result in a lot of tests, and it can take some time if your test suite is large and/or slow.</p>"},{"location":"testing/cosmic-ray/#file-docssourcehow-tosindexrst","title":"File: /docs/source/how-tos/index.rst","text":"<p>======= How-tos =======</p> <p>.. toctree::    :maxdepth: 1</p> <p>filters    distributor    implementation    operators</p>"},{"location":"testing/cosmic-ray/#file-docssourcehow-tosoperatorsrst","title":"File: /docs/source/how-tos/operators.rst","text":""},{"location":"testing/cosmic-ray/#mutation-operators","title":"Mutation Operators","text":"<p>In Cosmic Ray we use mutation operators to implement the various forms of mutation that we support. For each specific kind of mutation \u2013 constant replacement, break/continue swaps, and so forth \u2013 there is an operator class that knows how to create that mutation from un-mutated code.</p>"},{"location":"testing/cosmic-ray/#implementation-details","title":"Implementation details","text":"<p>Cosmic Ray relies on <code>parso &lt;https://github.com/davidhalter/parso&gt;</code>_ to parse Python code into trees. Cosmic Ray operators work directly on this tree, and the results of modifying this tree are written to disk for each mutation.</p> <p>Each operator is ultimately a subclass of <code>cosmic_ray.operators.operator.Operator</code>. We pass operators to various parse-tree visitors that let the operator view and modify the tree. When an operator reports that it can potentially modify a part of the tree, Cosmic Ray notes this and, later, asks the operator to actually perform this mutation.</p>"},{"location":"testing/cosmic-ray/#implementing-an-operator","title":"Implementing an operator","text":"<p>To implement a new operator you need to create a subclass of <code>cosmic_ray.operators.operator.Operator</code>. The first method an operator must implement is <code>Operator.mutation_positions()</code> which tells Cosmic Ray how the operator could mutate a particular parse-tree node.</p> <p>Second, an operator subclass must implement <code>Operator.mutate()</code> which actually mutates a parse-tree node.</p> <p>Finally, an operator must implement the class method <code>Operator.examples()</code>. This provides a set of before and after code snippets showing how the operator works. These examples are used in the test suite and potentially for documenation purposes. An operator can choose to provide no examples simply by returning an empty iterable from <code>examples</code>, though we may decide to check for an absence of examples in the future. In any case, it's good form to provide examples.</p> <p>In both cases, the operator implementation works directly with the <code>parso</code> parse tree objects.</p>"},{"location":"testing/cosmic-ray/#operator-provider-plugins","title":"Operator provider plugins","text":"<p>Cosmic Ray is designed to be extended with arbitrary operators provided by users. It dynamically discovers operators at runtime using the <code>stevedore</code> plugin system which relies on the <code>setuptools</code> <code>entry_points</code> concept.</p> <p>Rather than having individual plugins for each operator, Cosmic Ray lets users specify operator provider plugins. An operator provider can supply any number of operators to Cosmic Ray. At a high level, Cosmic Ray finds all of the operators available to it by iterating over the operator provider plugins, and for each of those iterating over the operators that it exposes.</p> <p>The operator provider API is very simple:</p> <p>.. code-block:: python</p> <pre><code>class OperatorProvider:\n    def __iter__(self):\n        \"The sequence of operator names that this provider supplies\"\n        pass\n\n    def __getitem__(self, name):\n        \"Get an operator class by name.\"\n        pass\n</code></pre> <p>In other words, a provider must have a (locally) unique name for each operator it provides, it must provide an iterator over those names, and it must allow Cosmic Ray to look up operator classes by name.</p> <p>To make a new operator provider available to Cosmic Ray you need to create a <code>cosmic_ray.operator_providers</code> entry point; this is generally done in <code>setup.py</code>. We'll show an example of how to do this later.</p> <p>Operator naming ~~~~~~~~~~~~~~~</p> <p>All operators in Cosmic Ray have a unique name for any given session. The name of an operator is based on two elements:</p> <ol> <li>The name of the <code>operator_provider</code> entry point (i.e. as specified in    <code>setup.py</code>)</li> <li>The name that the provider associates with the operator.</li> </ol> <p>The full name of an operator is simply the provider's name and the operator's name joined with \"/\". For example, if the provider's name was \"widget_corp\" and the operator's name was \"add_whitespace\", the full name of the operator would be \"widget_corp/add_whitespace\".</p>"},{"location":"testing/cosmic-ray/#a-full-example-numberreplacer","title":"A full example: <code>NumberReplacer</code>","text":"<p>One of the operators bundled with Cosmic Ray is implemented with the clas <code>cosmic_ray.operators.number_replacer.NumberReplacer</code>. This operator looks for <code>Num</code> nodes (number literals in source code) and replaces them with new <code>Num</code> nodes that have a different numeric value. To demonstrate how to create a mutation operator and provider, we'll step through how to create that operator in a new package called <code>example</code>.</p> <p>Creating the operator class ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The initial layout for our package is like this:</p> <p>.. code-block:: text</p> <pre><code>setup.py\nexample/\n  __init__.py\n</code></pre> <p><code>__init__.py</code> is empty and <code>setup.py</code> has very minimal content:</p> <p>.. code-block:: python</p> <pre><code>from setuptools import setup\n\nsetup(\n    name='example',\n    version='0.1.0',\n)\n</code></pre> <p>The first thing we need to do is create a new Python source file to hold our new operator. Create a file named <code>number_replacer.py</code> in the <code>example</code> directory. It has the following contents:</p> <p>.. code-block:: python</p> <pre><code>from cosmic_ray.operators.operator import Operator\nimport parso\n\nclass NumberReplacer(Operator):\n    \"\"\"An operator that modifies numeric constants.\"\"\"\n\n    def mutation_positions(self, node):\n        if isinstance(node, parso.python.tree.Number):\n            yield (node.start_pos, node.end_pos)\n\n    def mutate(self, node, index):\n        \"\"\"Modify the numeric value on `node`.\"\"\"\n\n        assert isinstance(node, parso.python.tree.Number)\n\n        val = eval(node.value) + 1\n        return parso.python.tree.Number(' ' + str(val), node.start_pos)\n</code></pre> <p>Let's step through this line-by-line. We first import <code>Operator</code> because we need to inherit from it:</p> <p>.. code-block:: python</p> <pre><code>from cosmic_ray.operators.operator import Operator\n</code></pre> <p>We then import <code>parso</code> because we need to use it to create mutated nodes:</p> <p>.. code-block:: python</p> <pre><code>import parso\n</code></pre> <p>We define our new operator by creating a subclass of <code>Operator</code> called <code>NumberReplacer</code>:</p> <p>.. code-block:: python</p> <pre><code>class NumberReplacer(Operator):\n</code></pre> <p>The <code>mutate_positions</code> method is called whenever Cosmic Ray needs to know if an operator can mutate a particular node. We implement ours to report a single mutation at each \"number\":</p> <p>.. code-block:: python</p> <pre><code>def mutation_positions(self, node):\n    if isinstance(node, parso.python.tree.Number):\n        yield (node.start_pos, node.end_pos)\n</code></pre> <p>Finally we implement <code>Operator.mutate()</code> which is called to actually perform the mutation. <code>mutate()</code> should return one of:</p> <ul> <li><code>None</code> if the <code>node</code> argument should be removed from the tree, or</li> <li>a new <code>parso</code> node to replace the original one</li> </ul> <p>In this case, we simply create a new <code>Number</code> node with a new value and return it:</p> <p>.. code-block:: python</p> <pre><code>def mutate(self, node, index):\n    \"\"\"Modify the numeric value on `node`.\"\"\"\n\n    assert isinstance(node, parso.python.tree.Number)\n\n    val = eval(node.value) + 1\n    return parso.python.tree.Number(' ' + str(val), node.start_pos)\n</code></pre> <p>That's all there is to it. This mutation operator is now ready to be applied to any code you want to test.</p> <p>However, before it can really be used, you need to make it available as a plugin.</p> <p>Creating the provider ~~~~~~~~~~~~~~~~~~~~~</p> <p>In order to expose our operator to Cosmic Ray we need to create an operator provider plugin. In the case of a single operator like ours, the provider implementation is very simple. We'll put the implementation in <code>example/provider.py</code>:</p> <p>.. code-block:: python</p> <pre><code># example/provider.py\n\nfrom .number_replacer import NumberReplacer\n\nclass Provider:\n    _operators = {'number-replacer': NumberReplacer}\n\n    def __iter__(self):\n        return iter(Provider._operators)\n\n    def __getitem__(self, name):\n        return Provider._operators[name]\n</code></pre> <p>Creating the plugin ~~~~~~~~~~~~~~~~~~~</p> <p>In order to make your operator available to Cosmic Ray as a plugin, you need to define a new <code>cosmic_ray.operator_providers</code> entry point. This is generally done through <code>setup.py</code>, which is what we'll do here.</p> <p>Modify <code>setup.py</code> with a new <code>entry_points</code> argument to <code>setup()</code>:</p> <p>.. code-block:: python</p> <pre><code>setup(\n    . . .\n    entry_points={\n        'cosmic_ray.operator_providers': [\n            'example = example.provider:Provider'\n        ]\n    })\n</code></pre> <p>Now when Cosmic Ray queries the <code>cosmic_ray.operator_providers</code> entry point it will see your provider - and hence your operator - along with all of the others.</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_rayastrst","title":"File: /docs/source/reference/api/cosmic_ray.ast.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_rayast-package","title":"cosmic_ray.ast package","text":""},{"location":"testing/cosmic-ray/#submodules","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_rayastast_query-module","title":"cosmic_ray.ast.ast_query module","text":"<p>.. automodule:: cosmic_ray.ast.ast_query    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.ast    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_raycommandsrst","title":"File: /docs/source/reference/api/cosmic_ray.commands.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_raycommands-package","title":"cosmic_ray.commands package","text":""},{"location":"testing/cosmic-ray/#submodules_1","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_raycommandsexecute-module","title":"cosmic_ray.commands.execute module","text":"<p>.. automodule:: cosmic_ray.commands.execute    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raycommandsinit-module","title":"cosmic_ray.commands.init module","text":"<p>.. automodule:: cosmic_ray.commands.init    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raycommandsnew_config-module","title":"cosmic_ray.commands.new_config module","text":"<p>.. automodule:: cosmic_ray.commands.new_config    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_1","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.commands    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_raydistributionrst","title":"File: /docs/source/reference/api/cosmic_ray.distribution.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_raydistribution-package","title":"cosmic_ray.distribution package","text":""},{"location":"testing/cosmic-ray/#submodules_2","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_raydistributiondistributor-module","title":"cosmic_ray.distribution.distributor module","text":"<p>.. automodule:: cosmic_ray.distribution.distributor    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raydistributionhttp-module","title":"cosmic_ray.distribution.http module","text":"<p>.. automodule:: cosmic_ray.distribution.http    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raydistributionlocal-module","title":"cosmic_ray.distribution.local module","text":"<p>.. automodule:: cosmic_ray.distribution.local    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_2","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.distribution    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_rayoperatorsrst","title":"File: /docs/source/reference/api/cosmic_ray.operators.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_rayoperators-package","title":"cosmic_ray.operators package","text":""},{"location":"testing/cosmic-ray/#submodules_3","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsbinary_operator_replacement-module","title":"cosmic_ray.operators.binary_operator_replacement module","text":"<p>.. automodule:: cosmic_ray.operators.binary_operator_replacement    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsboolean_replacer-module","title":"cosmic_ray.operators.boolean_replacer module","text":"<p>.. automodule:: cosmic_ray.operators.boolean_replacer    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsbreak_continue-module","title":"cosmic_ray.operators.break_continue module","text":"<p>.. automodule:: cosmic_ray.operators.break_continue    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorscomparison_operator_replacement-module","title":"cosmic_ray.operators.comparison_operator_replacement module","text":"<p>.. automodule:: cosmic_ray.operators.comparison_operator_replacement    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsexception_replacer-module","title":"cosmic_ray.operators.exception_replacer module","text":"<p>.. automodule:: cosmic_ray.operators.exception_replacer    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorskeyword_replacer-module","title":"cosmic_ray.operators.keyword_replacer module","text":"<p>.. automodule:: cosmic_ray.operators.keyword_replacer    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsno_op-module","title":"cosmic_ray.operators.no_op module","text":"<p>.. automodule:: cosmic_ray.operators.no_op    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsnumber_replacer-module","title":"cosmic_ray.operators.number_replacer module","text":"<p>.. automodule:: cosmic_ray.operators.number_replacer    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsoperator-module","title":"cosmic_ray.operators.operator module","text":"<p>.. automodule:: cosmic_ray.operators.operator    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsprovider-module","title":"cosmic_ray.operators.provider module","text":"<p>.. automodule:: cosmic_ray.operators.provider    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsremove_decorator-module","title":"cosmic_ray.operators.remove_decorator module","text":"<p>.. automodule:: cosmic_ray.operators.remove_decorator    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsunary_operator_replacement-module","title":"cosmic_ray.operators.unary_operator_replacement module","text":"<p>.. automodule:: cosmic_ray.operators.unary_operator_replacement    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorsutil-module","title":"cosmic_ray.operators.util module","text":"<p>.. automodule:: cosmic_ray.operators.util    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayoperatorszero_iteration_for_loop-module","title":"cosmic_ray.operators.zero_iteration_for_loop module","text":"<p>.. automodule:: cosmic_ray.operators.zero_iteration_for_loop    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_3","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.operators    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_rayrst","title":"File: /docs/source/reference/api/cosmic_ray.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_ray-package","title":"cosmic_ray package","text":""},{"location":"testing/cosmic-ray/#subpackages","title":"Subpackages","text":"<p>.. toctree::</p> <p>cosmic_ray.ast    cosmic_ray.commands    cosmic_ray.distribution    cosmic_ray.operators    cosmic_ray.tools</p>"},{"location":"testing/cosmic-ray/#submodules_4","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_raycli-module","title":"cosmic_ray.cli module","text":"<p>.. automodule:: cosmic_ray.cli    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayconfig-module","title":"cosmic_ray.config module","text":"<p>.. automodule:: cosmic_ray.config    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayexceptions-module","title":"cosmic_ray.exceptions module","text":"<p>.. automodule:: cosmic_ray.exceptions    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raymodules-module","title":"cosmic_ray.modules module","text":"<p>.. automodule:: cosmic_ray.modules    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raymutating-module","title":"cosmic_ray.mutating module","text":"<p>.. automodule:: cosmic_ray.mutating    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayplugins-module","title":"cosmic_ray.plugins module","text":"<p>.. automodule:: cosmic_ray.plugins    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayprogress-module","title":"cosmic_ray.progress module","text":"<p>.. automodule:: cosmic_ray.progress    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytesting-module","title":"cosmic_ray.testing module","text":"<p>.. automodule:: cosmic_ray.testing    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytiming-module","title":"cosmic_ray.timing module","text":"<p>.. automodule:: cosmic_ray.timing    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_rayversion-module","title":"cosmic_ray.version module","text":"<p>.. automodule:: cosmic_ray.version    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raywork_db-module","title":"cosmic_ray.work_db module","text":"<p>.. automodule:: cosmic_ray.work_db    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raywork_item-module","title":"cosmic_ray.work_item module","text":"<p>.. automodule:: cosmic_ray.work_item    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_4","title":"Module contents","text":"<p>.. automodule:: cosmic_ray    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_raytoolsfiltersrst","title":"File: /docs/source/reference/api/cosmic_ray.tools.filters.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_raytoolsfilters-package","title":"cosmic_ray.tools.filters package","text":""},{"location":"testing/cosmic-ray/#submodules_5","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_raytoolsfiltersfilter_app-module","title":"cosmic_ray.tools.filters.filter_app module","text":"<p>.. automodule:: cosmic_ray.tools.filters.filter_app    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolsfiltersgit-module","title":"cosmic_ray.tools.filters.git module","text":"<p>.. automodule:: cosmic_ray.tools.filters.git    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolsfiltersoperators_filter-module","title":"cosmic_ray.tools.filters.operators_filter module","text":"<p>.. automodule:: cosmic_ray.tools.filters.operators_filter    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolsfilterspragma_no_mutate-module","title":"cosmic_ray.tools.filters.pragma_no_mutate module","text":"<p>.. automodule:: cosmic_ray.tools.filters.pragma_no_mutate    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_5","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.tools.filters    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapicosmic_raytoolsrst","title":"File: /docs/source/reference/api/cosmic_ray.tools.rst","text":""},{"location":"testing/cosmic-ray/#cosmic_raytools-package","title":"cosmic_ray.tools package","text":""},{"location":"testing/cosmic-ray/#subpackages_1","title":"Subpackages","text":"<p>.. toctree::</p> <p>cosmic_ray.tools.filters</p>"},{"location":"testing/cosmic-ray/#submodules_6","title":"Submodules","text":""},{"location":"testing/cosmic-ray/#cosmic_raytoolsbadge-module","title":"cosmic_ray.tools.badge module","text":"<p>.. automodule:: cosmic_ray.tools.badge    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolshtml-module","title":"cosmic_ray.tools.html module","text":"<p>.. automodule:: cosmic_ray.tools.html    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolshttp_workers-module","title":"cosmic_ray.tools.http_workers module","text":"<p>.. automodule:: cosmic_ray.tools.http_workers    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolsreport-module","title":"cosmic_ray.tools.report module","text":"<p>.. automodule:: cosmic_ray.tools.report    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolssurvival_rate-module","title":"cosmic_ray.tools.survival_rate module","text":"<p>.. automodule:: cosmic_ray.tools.survival_rate    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#cosmic_raytoolsxml-module","title":"cosmic_ray.tools.xml module","text":"<p>.. automodule:: cosmic_ray.tools.xml    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#module-contents_6","title":"Module contents","text":"<p>.. automodule:: cosmic_ray.tools    :members:    :undoc-members:    :show-inheritance:</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceapimodulesrst","title":"File: /docs/source/reference/api/modules.rst","text":""},{"location":"testing/cosmic-ray/#cosmic-ray-api","title":"Cosmic Ray API","text":"<p>.. toctree::    :maxdepth: 4</p> <p>cosmic_ray</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferencebadgerst","title":"File: /docs/source/reference/badge.rst","text":"<p>===== Badge =====</p> <p>Utility to generate badge useful to decorate your preferred Continuous Integration system (github, gitlab, ...). The badge indicate the percentage of failing migrations.</p> <p>This utility is based on <code>anybadge &lt;https://github.com/jongracecox/anybadge&gt;</code>__.</p>"},{"location":"testing/cosmic-ray/#command","title":"Command","text":"<p>::</p> <p>cr-badge [--config ]"},{"location":"testing/cosmic-ray/#configuration","title":"Configuration","text":"<p>::</p> <p>[cosmic-ray.badge]  label = \"mutation\"  format = \"%.2f %%\"</p> <p>[cosmic-ray.badge.thresholds]  50  = 'red'  70  = 'orange'  100 = 'yellow'  101 = 'green'</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferencecommandsrst","title":"File: /docs/source/reference/commands.rst","text":""},{"location":"testing/cosmic-ray/#commands","title":"Commands","text":"<p>TODO: This is pretty wildly out of date! Perhaps we can use value-add to do this.</p>"},{"location":"testing/cosmic-ray/#details-of-common-commands","title":"Details of Common Commands","text":"<p>Most Cosmic Ray commands use a verb-options pattern, similar to how git does things.</p> <p>Possible verbs are:</p> <ul> <li><code>exec &lt;#exec&gt;</code>__</li> <li>help</li> <li><code>init &lt;#init&gt;</code>__</li> <li>load</li> <li>new-config</li> <li>operators</li> <li><code>dump &lt;#dump&gt;</code>__</li> <li>run</li> <li>worker</li> <li>apply</li> <li>baseline</li> </ul> <p>Detailed information on each command can be found by running <code>cosmic-ray help &lt;command&gt;</code> in the terminal.</p> <p>Cosmic Ray also installs a few other separate commands for producing various kinds of reports. These commands are:</p> <ul> <li>cr-report: provides a report on the status of a session</li> <li>cr-rate: prints the survival rate of a session</li> <li>cr-html: prints an HTML report on a session</li> </ul> <p>Verbosity: Getting more Feedback when Running ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The base command, <code>cosmic-ray</code>, has a single option: <code>--verbose</code>. The <code>--verbose</code> option changes the internal logging level from <code>WARN</code> to <code>INFO</code> and thus prints more information to the terminal.</p> <p>When used with <code>init</code>, <code>--verbose</code> will list how long it took to create the mutation list and will also list which modules were found:</p> <p>.. code:: shell</p> <pre><code>(.venv-pyerf) ~/PyErf$ cosmic-ray --verbose init --baseline=2 test_session pyerf -- pyerf/tests\nINFO:root:timeout = 0.259958 seconds\nINFO:root:Modules discovered: ['pyerf.tests', 'pyerf.tests.test_pyerf', 'pyerf.pyerf', 'pyerf', 'pyerf.__about__']\n(.venv-pyerf) C:\\dev\\PyErf&gt;cosmic-ray --verbose init --baseline=2 test_session pyerf --exclude-modules=.*tests.* -- pyerf/tests\nINFO:root:timeout = 0.239948 seconds\nINFO:root:Modules discovered: ['pyerf.pyerf', 'pyerf', 'pyerf.__about__']\n</code></pre> <p>When used with <code>exec</code>, <code>--verbose</code> displays which mutation is currently being tested:</p> <p>.. code:: shell</p> <pre><code>(.venv-pyerf) ~/PyErf$ cosmic-ray --verbose exec test_session\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '0', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '1', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '2', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '3', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '4', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '5', 'unittest', '--', 'pyerf/tests']\nINFO:cosmic_ray.tasks.worker:executing: ['cosmic-ray', 'worker', 'pyerf.pyerf', 'number_replacer', '6', 'unittest', '--', 'pyerf/tests']\n</code></pre> <p>The <code>--verbose</code> option does not add any additional information to the <code>dump</code> verb.</p> <p>Command: init ~~~~~~~~~~~~~</p> <p>The <code>init</code> verb creates a list of mutations to apply to the source code. It has the following optional arguments:</p> <ul> <li><code>--no-local-import</code>: Allow importing module from the current    directory.</li> </ul> <p>The <code>init</code> verb use following entries from the configuration file:</p> <ul> <li><code>[cosmic-ray] excluded-modules = []</code>: Exclude modules matching those glob   patterns from mutation. Use <code>glob.glob</code> syntax.</li> </ul> <p>Sample for django projects:</p> <p>::</p> <p>excluded-modules = [\"/tests/\", \"/migrations/\"]</p> <p>As mentioned in :ref:<code>here &lt;note_separation_test_code&gt;</code>, test directory can be handled via the <code>excluded-modules</code> option.</p> <p>The list of files that will be mutate effectively can be show by running <code>cosmic-ray init</code> with INFO debug level:</p> <p>::</p> <p>cosmic-ray init -v INFO</p> <p>Command: exec ~~~~~~~~~~~~~</p> <p>The <code>exec</code> command is what actually runs the mutation testing.</p> <p>Command: dump ~~~~~~~~~~~~~</p> <p>The <code>dump</code> command writes a detailed JSON representation of a session to stdout.</p> <p>.. code:: shell</p> <pre><code>$ cosmic-ray dump test_session\n{\"data\": [\"&lt;TestReport 'test_project/tests/test_adam.py::Tests::test_bool_if' when='call' outcome='failed'&gt;\"], \"test_outcome\": \"killed\", \"worker_outcome\": \"normal\", \"diff\": [\"--- mutation diff ---\", \"--- a/Users/sixtynorth/projects/sixty-north/cosmic-ray/test_project/adam.py\", \"+++ b/Users/sixtynorth/projects/sixty-north/cosmic-ray/test_project/adam.py\", \"@@ -20,7 +20,7 @@\", \"     return (not object())\", \" \", \" def bool_if():\", \"-    if object():\", \"+    if (not object()):\", \"         return True\", \"     raise Exception('bool_if() failed')\", \" \"], \"module\": \"adam\", \"operator\": \"cosmic_ray.operators.boolean_replacer.AddNot\", \"occurrence\": 0, \"line_number\": 32, \"command_line\": [\"cosmic-ray\", \"worker\", \"adam\", \"add_not\", \"0\", \"pytest\", \"--\", \"-x\", \"tests\"], \"job_id\": \"c2bb71e6203d44f6af42a7ee35cb5df9\"}\n. . .\n</code></pre> <p><code>dump</code> is designed to allow users to develop their own reports. To do this, you need a program which reads a series of JSON structures from stdin.</p>"},{"location":"testing/cosmic-ray/#concurrency","title":"Concurrency","text":"<p>Note that most Cosmic Ray commands can be safely executed while <code>exec</code> is running. One exception is <code>init</code> since that will rewrite the work manifest.</p> <p>For example, you can run <code>cr-report</code> on a session while that session is being executed. This will tell you what progress has been made.</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferencecontinuous_integrationrst","title":"File: /docs/source/reference/continuous_integration.rst","text":"<p>========================  Continuous Integration ========================</p> <p>Cosmic Ray has a continuous integration system based on <code>Travis &lt;https://travis-ci.org&gt;</code>__. Whenever we push new changes to our github repository, travis runs a set of tests. These :doc:<code>tests &lt;tests&gt;</code> include low-level unit tests, end-to-end integration tests, static analysis (e.g. linting), and testing documentation builds. Generally speaking, these tests are run on all versions of Python which we support.</p>"},{"location":"testing/cosmic-ray/#automated-release-deployment","title":"Automated release deployment","text":"<p>Cosmic Ray also has an automated release deployment scheme. Whenever you push changes to <code>the release branch &lt;https://github.com/sixty-north/cosmic-ray/tree/release&gt;</code>__, travis attempts to make a new release. This process involves determining the release version by reading <code>cosmic_ray/version.py</code>, creating and uploading PyPI distributions, and creating new release tags in git.</p>"},{"location":"testing/cosmic-ray/#releasing-a-new-version","title":"Releasing a new version","text":"<p>As described above, the release process for Cosmic Ray is largely automatic. In order to do a new release, you simply need to:</p> <ol> <li>Bump the version with <code>bumpversion</code>.</li> <li>Push it to <code>master</code> on github.</li> <li>Push the changes to the <code>release</code> branch on github.</li> </ol> <p>Once the push is made to <code>release</code>, the automated release system will take over.</p> <p>Note that only the Python 3.6 travis build will attempt to make a release deployment. So to see the progress of your release, check the output for that build.</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferenceindexrst","title":"File: /docs/source/reference/index.rst","text":"<p>========= Reference =========</p> <p>.. toctree::    :maxdepth: 1</p> <p>api/modules    commands    tests    continuous_integration    badge</p>"},{"location":"testing/cosmic-ray/#file-docssourcereferencetestsrst","title":"File: /docs/source/reference/tests.rst","text":""},{"location":"testing/cosmic-ray/#tests","title":"Tests","text":"<p>Cosmic Ray has a number of test suites to help ensure that it works. To install the necessary dependencies for testing, run:</p> <p>::</p> <pre><code>pip install -e .[dev,test]\n</code></pre>"},{"location":"testing/cosmic-ray/#pytest-suite","title":"<code>pytest</code> suite","text":"<p>The first suite is a <code>pytest &lt;http://pytest.org/&gt;</code>__ test suite that validates some if its internals. You can run that like this:</p> <p>::</p> <pre><code>pytest tests/test_suite\n</code></pre>"},{"location":"testing/cosmic-ray/#the-adam-tests","title":"The \"adam\" tests","text":"<p>There is also a set of tests which verify the various mutation operators. These tests comprise a specially prepared body of code, <code>adam.py</code>, and a full-coverage test-suite. The idea here is that Cosmic Ray should be 100% lethal against the mutants of <code>adam.py</code> or there's a problem.</p> <p>We have \"adam\" configurations for each of the test-runner/execution-engine combinations. For example, the configuration which uses <code>unittest</code> and the <code>local</code> execution engine is in <code>test_project/cosmic-ray.unittest.local.conf</code>.</p> <p>To run an \"adam\" test, first switch to the <code>test_project</code> directory:</p> <p>::</p> <pre><code>cd tests/example_project\n</code></pre> <p>Then initialize a new session using one of the configurations. Here's an example using the <code>pytest</code>/<code>local</code> configuration:</p> <p>::</p> <pre><code>cosmic-ray init cosmic-ray.pytest.local.conf pytest-local.sqlite\n</code></pre> <p>(Note that if you were going to use the <code>celery4</code> engine instead, you need to make sure that celery workers were running.)</p> <p>Execute the session like this:</p> <p>::</p> <pre><code>cosmic-ray exec pytest-local.sqlite\n</code></pre> <p>Finally, view the results of this test with <code>dump</code> and <code>cr-report</code>:</p> <p>::</p> <pre><code>cr-report pytest-local.sqlite\n</code></pre> <p>You should see a 0% survival rate at the end of the report.</p>"},{"location":"testing/cosmic-ray/#the-full-test-suite","title":"The full test suite","text":"<p>While the \"adam\" tests verify the various mutation operators in Cosmic Ray, the full test suite comprises a few more tests for other behaviors and functionality. To run all of these tests, it's often simplest to use tox. Just run::</p> <pre><code>$ tox\n</code></pre> <p>at the root of the project.</p>"},{"location":"testing/cosmic-ray/#file-docssourcetutorialsdistributedindexrst","title":"File: /docs/source/tutorials/distributed/index.rst","text":"<p>================================================== Tutorial: Distributed, concurrent mutation testing ==================================================</p> <p>One of the main practical challenges to mutation testing is that it can take a long time. Even on moderately sized projects, you might need millions of individual mutations and test runs. This can be prohibitive to run on a single system.</p> <p>One way to cope with these long runtimes is to parallelize the mutation and testing procedures. Fortunately, mutation testing is <code>embarassingly parallel in nature &lt;https://en.wikipedia.org/wiki/Embarrassingly_parallel&gt;</code>__, so we can apply some relatively simple techniques to get really nice scaling up of the work. To support parallel execution of mutation testing runs, Cosmic Ray has the notion of distributors which can control where and how tests are run. Different distributors can run tests in different contexts: in parallel on a single machine, by distributing them across a message bus, or perhaps by spawning test runs on cloud systems.</p>"},{"location":"testing/cosmic-ray/#the-http-distributor","title":"The HTTP distributor","text":"<p>Cosmic Ray includes :class:<code>cosmic_ray.distributors.http.HttpDistributor</code>, a distributor which allows you to send mutation-and-test requests to workers running locally or remotely. You can run as many of these workers as you  want, thereby making it possible to run as many mutations in parallel as you want. </p> <p>Each worker is a small HTTP server, listening for requests from the <code>exec</code> command to perform a mutation and test. Each worker handlers only one mutation request at a time. Critically, each worker has its own copy of the code under test, meaning that it can make mutations to that copy of the code without interfering with other workers.</p> <p>You need to make sure that workers are running prior to running the <code>exec</code> command. <code>exec</code> doesn't have any support for starting workers. The major configuration involved with the HTTP distributor is telling <code>exec</code> where there workers are listening.</p>"},{"location":"testing/cosmic-ray/#a-sample-project","title":"A sample project","text":"<p>To demonstrate <code>HttpDistributor</code> we'll need a sample module and test suite. We'll use a very simple set of code, as we did in :ref:<code>the basic tutorial &lt;basic tutorial&gt;</code>.</p> <p>Create a new directory to hold this code. We'll refer to this directory as <code>ROOT</code>.</p> <p>Create the file <code>ROOT/mod.py</code> with these contents:</p> <p>.. literalinclude:: mod.1.py</p> <p>Then create <code>ROOT/test_mod.py</code> with these contents:</p> <p>.. literalinclude:: test_mod.1.py</p> <p>Finally, we'll create a configuration, <code>ROOT/config.toml</code>:</p> <p>.. literalinclude:: config.1.toml     :linenos:</p> <p>This config is similar to others that we've looked at, with the major difference that it specifies the use of the 'http' distributor rather than 'local'. On line 8 we set \"cosmic-ray.distributor.name\" to \"http\". </p> <p>Then on line 11 we set the \"cosmic-ray.distributor.http.worker-urls\" setting to a list containing a URL. This is the address at which a worker will be listening for mutation requests. This configuration only specifies a single worker, but we can put as many workers here as we want.</p>"},{"location":"testing/cosmic-ray/#starting-a-worker","title":"Starting a worker","text":"<p>Before Cosmic Ray can send requests to a worker, we need to start it. From the <code>ROOT</code> directory, start a worker using the <code>http-worker</code> command:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ncosmic-ray --verbosity INFO http-worker --port 9876\n</code></pre> <p>The <code>--verbosity INFO</code> argument configures the worker's logging to show more messages than normal. The <code>--port 9876</code> argument instructs it to listen for requests on port 9876, the same port we specified in the 'worker-urls' list in our configuration. The worker will tell you that it's waiting to process requests on port 9876:</p> <p>.. code-block:: bash</p> <pre><code>======== Running on http://0.0.0.0:9876 ========\n(Press CTRL+C to quit)\n</code></pre> <p>Note that your worker must be running in the same directory as you would normally run the tests from. In this case, we're expecting the tests to be run in <code>$ROOT</code>, so make sure your worker is running in that directory. Generally speaking, the worker doesn't do much more than mutate the code on disk and run the test command you've specified in your config.</p>"},{"location":"testing/cosmic-ray/#running-the-tests","title":"Running the tests","text":"<p>We need to leave the worker running in its own terminal, so for these next steps you'll need to start a new terminal.</p> <p>First we need to initialize a new Cosmic Ray session:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ncosmic-ray init config.toml session.sqlite\n</code></pre> <p>Once the session is created, we can execute the tests:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray exec config.toml session.sqlite\n</code></pre> <p>This should execute very quickly. The most important thing to note is that our worker process is where the mutation and testing actually occurred. If you switch back to the terminal hosting your worker, you should see that it  produced output something like this:</p> <p>.. code-block:: bash</p> <pre><code>[05/16/21 11:31:10] INFO     INFO:cosmic_ray.mutating:Applying mutation: path=mod.py,                                mutating.py:111\n                             op=&lt;cosmic_ray.operators.number_replacer.NumberReplacer object at 0x10d2b9550&gt;,                        \n                             occurrence=1                                                                                           \n                    INFO     INFO:cosmic_ray.testing:Running test (timeout=10.0): python -m unittest test_mod.py       testing.py:36\n                    INFO     INFO:aiohttp.access:::1 [16/May/2021:09:31:10 +0000] \"POST / HTTP/1.1\" 200 899 \"-\"       web_log.py:206\n                             \"Python/3.7 aiohttp/3.7.4.post0\"                                                                       \n                    INFO     INFO:cosmic_ray.mutating:Applying mutation: path=mod.py,                                mutating.py:111\n                             op=&lt;cosmic_ray.operators.number_replacer.NumberReplacer object at 0x10d4cdf60&gt;,                        \n                             occurrence=0                                                                                           \n                    INFO     INFO:cosmic_ray.testing:Running test (timeout=10.0): python -m unittest test_mod.py       testing.py:36\n[05/16/21 11:31:11] INFO     INFO:aiohttp.access:::1 [16/May/2021:09:31:10 +0000] \"POST / HTTP/1.1\" 200 899 \"-\"       web_log.py:206\n                             \"Python/3.7 aiohttp/3.7.4.post0\"\n</code></pre> <p>Congratulations! You've just performed your first distributed mutation testing with Cosmic Ray. There are other details you need to consider when scaling beyond a single worker, but this small example covers the most important elements: setting up the configuration and starting a worker.</p> <p>At this point you can kill the worker you started earlier.</p>"},{"location":"testing/cosmic-ray/#concurrent-execution-with-multiple-workers","title":"Concurrent execution with multiple workers","text":"<p>In the previous example we only ran a single worker process, so from a concurrency point of view this was no different from  using the 'local' distributor. Before we can run multiple workers, though, we need to consider what resources each worker  requires. Ultimately, each worker needs two things:</p> <ul> <li>An HTTP endpoint</li> <li>A copy of the code under test that it can modify</li> </ul> <p>In this example we'll create the unique endpoints by giving each worker its own port. In principle, though, workers may be running on entirely different machines on a network.</p>"},{"location":"testing/cosmic-ray/#distinct-copies-of-the-code","title":"Distinct copies of the code","text":"<p>As mentioned earlier, Cosmic Ray mutation works by actually modifying the code on disk. As such, multiple workers can't share a single copy of the code; their mutations would interfere with one another. So we need to make sure each worker has a copy of the code under test.</p> <p>For this example, we'll manually copy the files around:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\nmkdir worker1\ncp mod.py worker1\ncp test_mod.py worker1\nmkdir worker2\ncp mod.py worker2\ncp test_mod.py worker2\n</code></pre> <p>Now the directories <code>worker1</code> and <code>worker2</code> contain separate copies of the code under test.</p>"},{"location":"testing/cosmic-ray/#starting-the-workers","title":"Starting the workers","text":"<p>Now we can start the workers. Remember that each will run in its own terminal. In one terminal, start the first worker:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT/worker1\ncosmic-ray --verbosity INFO http-worker --port 9876\n</code></pre> <p>Then in another terminal start a second worker:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT/worker2\ncosmic-ray --verbosity INFO http-worker --port 9877\n</code></pre> <p>Note that the workers are using different ports.</p>"},{"location":"testing/cosmic-ray/#update-the-configuration","title":"Update the configuration","text":"<p>To tell Cosmic Ray to use both of these workers, we need to update our configuration. Edit <code>config.toml</code> to specify both workers URLs:</p> <p>.. literalinclude:: config.2.toml     :linenos:     :emphasize-lines: 11</p> <p>On line 11 we now list the endpoints for both workers.</p>"},{"location":"testing/cosmic-ray/#running-the-tests_1","title":"Running the tests","text":"<p>We're now ready to run the tests. Go back to <code>ROOT</code> and re-initialize your session:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ncosmic-ray init config.toml session.sqlite\n</code></pre> <p>Finally, we can execute the tests:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray exec config.toml session.sqlite\n</code></pre> <p>If you run <code>cr-report</code> you should see that two tests were run and that there were no survivors:</p> <p>.. code-block:: bash</p> <pre><code>$ cr-report session.sqlite\ne4e56a71a059466f861d62c987988efe mod.py core/NumberReplacer 0\nworker outcome: normal, test outcome: killed\n7820da3f68cd40a7b60d69506e87c4aa mod.py core/NumberReplacer 1\nworker outcome: normal, test outcome: killed\ntotal jobs: 2\ncomplete: 2 (100.00%)\nsurviving mutants: 0 (0.00%)\n</code></pre> <p>Likewise, if you look at the terminals for your two workers, you should see that they each received a request to perform a mutation test.</p> <p>That's really all there is to distributed mutation testing with <code>HttpDistributor</code>. You simply start as many workers as you need, specifying their endpoints in your configuration. </p> <p>.. important::</p> <pre><code>At this point you should kill the workers you started.\n</code></pre>"},{"location":"testing/cosmic-ray/#cr-http-workers-a-tool-for-starting-workers","title":"cr-http-workers: A tool for starting workers","text":"<p>It's extremely common for the code under test (and the tests themselves) to be in a git repository. As such, a simple way to create the isolated copies of the code that each worker requires is to clone this git repository. Once the mutation testing is done, these clones can be deleted.</p> <p>To simplify this process Cosmic Ray provides <code>cr-http-workers</code>. This program reads your configuration to determine how many workers to start, and you provide it with a git repository to clone. For each 'worker-url' in your configuration it will clone the git repository and start a worker in that clone. You can then run <code>exec</code> to distribute work to those workers. Once the testing is over, you can kill <code>cr-http-workers</code> and it will clean up the workers and their clones.</p>"},{"location":"testing/cosmic-ray/#preparing-the-git-repository","title":"Preparing the git repository","text":"<p>To use <code>cr-http-workers</code> we first need a git repository, so we'll create one from our existing code. </p> <p>.. note::</p> <pre><code>You should first delete the ``worker1`` and ``worker2`` directories if they still exist. This isn't critical, but it\nmight be confusing to leave them around.\n</code></pre> <p>Here's how to initialize the git repository:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ngit init\ngit add mod.py \ngit add test_mod.py\ngit commit -a -m \"initialized repo\"\n</code></pre>"},{"location":"testing/cosmic-ray/#running-the-workers","title":"Running the workers","text":"<p>Once the git repo is initialized, we can start the workers:</p> <p>.. code-block:: bash</p> <pre><code>cr-http-workers config.toml .\n</code></pre> <p>This tell <code>cr-http-workers</code> to read <code>config.toml</code> to determine the worker endpoints. The second argument, \".\", tells it to clone the git repository in the current directory. In practice this repo URL will often be hosted elsewhere (e.g. github), but for our purposes we'll just work with the local repo.</p> <p>This will start both workers processes, and the output from those workers will be shown in the output from <code>cr-http-workers</code>.</p>"},{"location":"testing/cosmic-ray/#running-the-tests_2","title":"Running the tests","text":"<p>Once the workers are running, running the tests just involves the standard <code>init</code> and <code>exec</code> commands:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ncosmic-ray init config.toml session.sqlite\ncosmic-ray exec config.toml session.sqlite\n</code></pre> <p>Remember that you'll need to run this in another terminal.</p> <p>Once the tests complete you can kill the <code>cr-http-workers</code> process. There's not much more to it than that!</p>"},{"location":"testing/cosmic-ray/#limitations","title":"Limitations","text":"<p>The main limitation of <code>cr-http-workers</code> is that it can only start workers on your local machine. If you want to run workers on other machines, you'll need to use some other mechanism. But very often, being able to run multiple workers on a single machine is a huge gain for mutation testing. Mutation testing time will scale down linearly with the number of workers you run, so running 4 workers on your system will - within certain limits - let you run your mutation testing 4 times faster.</p>"},{"location":"testing/cosmic-ray/#alternatives-to-httpdistributor","title":"Alternatives to HttpDistributor","text":"<p>If <code>HttpDistributor</code> doesn't meet your needs, Cosmic Ray allows you to write your own distributor and use it as a plugin. You might want to write a distributor plugin using <code>Celery &lt;https://docs.celeryproject.org/en/stable/getting-started/introduction.html&gt;</code>_, for example, to take advantage of its sophisticated message bus.</p>"},{"location":"testing/cosmic-ray/#file-docssourcetutorialsintroindexrst","title":"File: /docs/source/tutorials/intro/index.rst","text":"<p>.. _basic tutorial:</p> <p>==================== Tutorial: The basics ====================</p> <p>This tutorial will walk you through the steps needed to install, configure, and run Cosmic Ray. </p>"},{"location":"testing/cosmic-ray/#installation","title":"Installation","text":"<p>First you'll need to install Cosmic Ray. The simplest (and generally best) way to do this is with <code>pip</code>:</p> <p>.. code-block:: bash</p> <pre><code>pip install cosmic-ray\n</code></pre> <p>You'll generally want to do this in a virtual environment, but it's not required.</p>"},{"location":"testing/cosmic-ray/#installation-from-source","title":"Installation from source","text":"<p>If you need to install Cosmic Ray from source, first change to the directorying containing <code>setup.py</code>. Then run::</p> <pre><code>pip install .\n</code></pre> <p>Or, if you want to <code>install from source in \"editable\" mode &lt;https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-e&gt;</code>_, you can use the <code>`-e</code> flag::</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"testing/cosmic-ray/#source-module-and-tests","title":"Source module and tests","text":"<p>Mutation testing works by making small mutations to the code under test (CUT) and then running a test suite over the mutated code. For this tutorial, then, we'll need to create our CUT and a test suite for it.</p> <p>You should create a new directory which will contain the CUT, the tests, and eventually the Cosmic Ray data. For the rest of this tutorial we'll refer to this new directory as <code>ROOT</code> (or <code>$ROOT</code> if we're showing shell code). </p> <p>Now create the file <code>ROOT/mod.py</code> with these contents:</p> <p>.. literalinclude:: mod.1.py</p> <p>This file contains your code under test, i.e. the code that Cosmic Ray will mutate. It's clearly very simple, and it has very few opportunities for mutation, but it's sufficient for this tutorial. In fact, having simple code like this will make it easier to see what Cosmic Ray is doing without getting bogged down by scale.</p> <p>Next create the file <code>ROOT/test_mod.py</code> with these contents:</p> <p>.. literalinclude:: test_mod.1.py</p> <p>This contains the test suite for <code>mod.py</code>. Cosmic Ray will not mutate this code. Rather, it will run this test suite for every mutation that it creates.</p> <p>Before moving on, let's make sure that the test suite works correctly:</p> <p>.. code-block:: bash</p> <pre><code>python -m unittest test_mod.py\n</code></pre> <p>This should show that all tests pass:</p> <p>.. code-block:: bash</p> <pre><code>.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n</code></pre> <p>If you see one test passing like this, then you're ready to continue!</p>"},{"location":"testing/cosmic-ray/#creating-a-configuration","title":"Creating a configuration","text":"<p>Before you do run any mutation tests, you need to create a configuration. A configuration is a <code>TOML &lt;https://toml.io/&gt;</code>_ file that specifies the modules you want to mutate, the test scripts to use, and so forth. A configuration is used to create a session, something we'll look at in the next section.</p>"},{"location":"testing/cosmic-ray/#the-new-config-command","title":"The <code>new-config</code> command","text":"<p>You can create a configuration by hand if you want. In fact, you'll generally need to edit them by hand to get the exact configuration you need. But you can create an initial configuration using the <code>new-config</code> command. This will ask you a series of questions and construct a new configuration based on your answers.</p> <p>To create your config for this tutorial, do this:</p> <p>.. code-block:: bash</p> <pre><code>cd $ROOT\ncosmic-ray new-config tutorial.toml\n</code></pre> <p>This will ask you a series of questions. Answer them like this:</p> <p>.. code-block:: text</p> <pre><code>[?] Top-level module path: mod.py\n[?] Test execution timeout (seconds): 10\n[?] Test command: python -m unittest test_mod.py\n-- MENU: Distributor --\n  (0) http\n  (1) local\n[?] Enter menu selection: 1\n</code></pre> <p>This will create the file <code>tutorial.toml</code> with these contents:</p> <p>.. literalinclude:: tutorial.toml.1     :linenos:     :language: toml</p> <p>Configuration contents ~~~~~~~~~~~~~~~~~~~~~~</p> <p>Let's examine the contents of this file before moving on. On line 1 we define the 'cosmic-ray' key in the TOML structure; this key will contain all Cosmic Ray configuration information.</p> <p>On line 2 we set the 'module-path' key to the string \"mod.py\":</p> <p>.. literalinclude:: tutorial.toml.1     :lines: 2     :language: toml</p> <p>This tells Cosmic Ray that we're going to be mutating the module in the file <code>mod.py</code>. Every Cosmic Ray configuration refers to a single top-level module that will be mutated, and in this case we're telling Cosmic Ray to mutate the <code>mod</code> module, contained in the file <code>mod.py</code>.</p> <p>.. note::</p> <pre><code>The 'module-path' is a *path* to a file or directory, not the name of the module of package. If it's a file then\nCosmic Ray will treat it as a single module, but if it's a directory then Cosmic Ray will treat it as a package.\n\nWhen working on a package, Cosmic Ray will apply mutations to all submodules in the package.\n\nAdditionally, the 'module-path' can be a list of directories or files: `module-path = [\"file1.py\", \"some_directory\"]`\n</code></pre> <p>Line 3 tells Cosmic Ray the maximium amount of time to let a test run before it's considered a failure:</p> <p>.. literalinclude:: tutorial.toml.1     :lines: 3     :language: toml</p> <p>In this case, we're telling Cosmic Ray to kill a test if it runs longer than 10 seconds. This timeout is important because some mutations can cause the tests to go into an infinite loop. Without timeout we'd never exit the test! It's important to  set this timeout such that it's long enough for all legitimate tests.</p> <p>Next, line 4 tells Cosmic Ray which modules to exclude from mutation:</p> <p>.. literalinclude:: tutorial.toml.1     :lines: 4     :language: toml</p> <p>In this case we're not excluding any, but there may be times when you need to skip certain modules, e.g. because  you know that you don't have sufficient tests for them at the moment. This parameter expects glob-patterns, so to exclude files that end with <code>_test.py</code> recursively for example, you would add <code>\"**/*_test.py\"</code>.</p> <p>Line 5 is one of the most critical lines in the configuration. This tells Cosmic Ray how to run your test suite:</p> <p>.. literalinclude:: tutorial.toml.1     :lines: 5     :language: toml</p> <p>In this case, our test suite uses the standard <code>unittest testing framework &lt;https://docs.python.org/3/library/unittest.html&gt;</code>_, and the tests are in the file <code>test_mod.py</code>.</p> <p>The last two lines tell Cosmic Ray which \"distributor\" to use:</p> <p>.. literalinclude:: tutorial.toml.1     :lines: 7-8     :language: toml</p> <p>A distributor controls how mutation jobs are assigned to one or more workers so that they can (potentially) run in parallel. In this case we're using the default 'local' distributor which only runs one mutation at a time. There are other, more sophisticated distributors which we discuss elsewhere.</p>"},{"location":"testing/cosmic-ray/#create-a-session-and-baseline","title":"Create a session and baseline","text":"<p>Cosmic Ray uses a notion of sessions to encompass a full mutation testing suite. Since mutation testing runs can take a long time, and since you might need to stop and start them, sessions store data about the progress of a run.</p> <p>.. note::</p> <pre><code>Most Cosmic Ray commands allow you to increase their \"verbosity\" via the command line. This will make them print out\nmore information about what they're doing.\n\nTry adding \"--verbosity INFO\" to the command you run if you more details about\nwhat's going on!\n</code></pre>"},{"location":"testing/cosmic-ray/#initializing-a-session","title":"Initializing a session","text":"<p>The first step in a full testing run, then, is to initialize a session:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray init tutorial.toml tutorial.sqlite\n</code></pre> <p>.. note::</p> <pre><code>This command prepares all the mutations that will later be applied to code.\nAs such, its execution time is proportional to the amount of code and\nthe code complexitly. You can expect about 15-30s per 1kloc.\n</code></pre> <p>This will create a database file called <code>tutorial.sqlite</code>. There is a record in the database for each mutation that Cosmic Ray will perform, and Cosmic Ray will associate testing results with these records as it executes.</p> <p>When does <code>init</code> need to be run? ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p><code>init</code> completely rewrites the session file you tell it to use, so you should not re-run <code>init</code> on a session that contains any results that you want to keep. At the same time, if you change your configuration in a way that alters which tests are run and which mutations are made, then you should re-initialize your session.</p> <p>Generally speaking, if you change the 'module-path', 'timeout', 'excluded-modules', or 'test-command' parts of your configuration, or if you change any of the filters you use, then you need to re-initialize your session and start over. Any of these changes can affect the operations that the subsequent <code>exec</code> command will run.</p> <p>Similarly, you need to create a new session with <code>init</code> whenever your code-under-test or your tests themselves change. This is necessary because changes to the CUT will affect which mutations are made and changes to the tests affect which tests are run.</p>"},{"location":"testing/cosmic-ray/#baselining","title":"Baselining","text":"<p>Before running the full mutation suite, it's important to make sure that the test suite passes in the absence of any mutations. If the test suite does not pass in the absence of mutations, then the results of the mutation testing are essentially useless.</p> <p>You can use the <code>baseline</code> command to check that the test suite passes on unmutated code:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray --verbosity=INFO baseline tutorial.toml\n</code></pre> <p>This should report that the tests pass, something like this:</p> <p>.. code-block:: text</p> <pre><code>[07/23/21 10:00:20] INFO     INFO:root:Reading config from 'tutorial.toml'                                                                                                                                            config.py:103\n                    INFO     INFO:cosmic_ray.commands.execute:Beginning execution                                                                                                                                     execute.py:45\n                    INFO     INFO:cosmic_ray.testing:Running test (timeout=10.0): python -m unittest test_mod.py                                                                                                      testing.py:36\n                    INFO     INFO:cosmic_ray.commands.execute:Job baseline complete                                                                                                                                   execute.py:43\n                    INFO     INFO:cosmic_ray.commands.execute:Execution finished                                                                                                                                      execute.py:53\n                    INFO     INFO:root:Baseline passed. Execution with no mutation works fine.\n</code></pre> <p>If this command succeeds, then you're ready to start mutating code and testing it!</p>"},{"location":"testing/cosmic-ray/#examining-the-session-with-cr-report","title":"Examining the session with cr-report","text":"<p>Our session file, <code>tutorial.sqlite</code>, is essentially a list of mutations that Cosmic Ray will perform on the code under test. We haven't actually tested any mutants, so none of our mutations have testing results yet. With that in mind, let's examine the contents of our session with the <code>cr-report</code> program:</p> <p>.. code-block:: bash</p> <pre><code>cr-report tutorial.sqlite --show-pending\n</code></pre> <p>This will produce output like this (though note that the test IDs will be different):</p> <p>.. code-block:: text</p> <pre><code>[job-id] f168ef23dff24b75846a730858fe0111\nmod.py core/NumberReplacer 0\n[job-id] 929a563b613242b48dae0f2de74ad2af\nmod.py core/NumberReplacer 1\ntotal jobs: 2\nno jobs completed\n</code></pre> <p>This is telling us that Cosmic Ray detected two mutations that it can make to our code, both using the mutation operator \"core/NumberReplacer\". Without going into details, this means that Cosmic Ray has found one or more numeric literals in our code, and it plans to make two mutations to those numbers. We can see in our code that there is only one numeric literal, the value returned from <code>func()</code> on line 2:</p> <p>.. literalinclude:: mod.1.py     :linenos:     :emphasize-lines: 2</p> <p>So Cosmic Ray is going to mutate that number in two ways, running the test suite each time. </p> <p>The <code>cr-report</code> tool is useful for examining sessions, and it's main purpose is to give you summary reports after an entire session has been executed, which we'll do in the next step.</p>"},{"location":"testing/cosmic-ray/#execution","title":"Execution","text":"<p>Now that you've initialized and baselined your session, it's time to start making mutants and testing them. We do this with the <code>exec</code> command. <code>exec</code> looks in your session file, <code>tutorial.sqlite</code>, for any mutations which were detected in the <code>init</code> phase that don't yet have results. For each of these, it performs the specified mutation and runs the test suite.</p> <p>As we saw, we only have two mutations to make, and our test suite is very small. As a result the <code>exec</code> command will run quite quickly:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray exec tutorial.toml tutorial.sqlite\n</code></pre> <p>This should produce no output. </p> <p>.. note::</p> <pre><code>The module and test suite for this tutorial are \"toys\" by design. As such, they run very quickly. Most real-world\nmodules and test suites are much more substantial and require much longer to run. For example, if a test suite takes\n10 seconds to run and Cosmic Ray finds 1000 mutations, a full ``exec`` will take 10 x 1000 = 10,000 seconds, or\nabout 2.7 hours.\n</code></pre>"},{"location":"testing/cosmic-ray/#committing-before-exec","title":"Committing before <code>exec</code>","text":"<p>If you're using revision control with your code (you are, right?!), you should consider committing your changes before running <code>exec</code>. While it's not strictly necessary to do this in simple cases, it's often important to commit if you're using tools like <code>cr-http-workers</code> that rely on fetching code from a repository. </p> <p>Also, while Cosmic Ray is designed to be robust in the face of exceptions and crashes, there is always the possibility that Cosmic Ray won't correctly undo a mutation. Remember, it makes mutations directly on disk, so if a mutation is not correctly undone, and if you haven't committed your changes prior to testing, you run the risk of introducing a mutation into you code accidentally.</p>"},{"location":"testing/cosmic-ray/#reporting-the-results","title":"Reporting the results","text":"<p>Assuming it ran correctly, we can now use <code>cr-report</code> to see the updated state of our session:</p> <p>.. code-block:: bash</p> <pre><code>cr-report tutorial.sqlite --show-pending\n</code></pre> <p>This time we see that both mutations were made, tests were run for each, and both were \"killed\":</p> <p>.. code-block:: text</p> <pre><code>[job-id] f168ef23dff24b75846a730858fe0111\nmod.py core/NumberReplacer 0\nworker outcome: normal, test outcome: killed\n[job-id] 929a563b613242b48dae0f2de74ad2af\nmod.py core/NumberReplacer 1\nworker outcome: normal, test outcome: killed\ntotal jobs: 2\ncomplete: 2 (100.00%)\nsurviving mutants: 0 (0.00%)\n</code></pre> <p>.. tip::</p> <pre><code>You don't have to wait for ``exec`` to complete to generate a report. If you have a long-running session and want to\nsee your progress, you can execute ``cr-report`` while ``cosmic-ray exec`` is running to view the progress the\nlatter is making.\n</code></pre>"},{"location":"testing/cosmic-ray/#html-reports","title":"HTML reports","text":"<p>You can also generate a handy HTML report with <code>cr-html</code>:</p> <p>::</p> <pre><code>cr-html tutorial.sqlite &gt; report.html\n</code></pre> <p>You can then open <code>report.html</code> in your browser to see the details. One nice feature of these HTML reports is that they show the actual mutation that was used.</p>"},{"location":"testing/cosmic-ray/#file-docssourceconceptsrst","title":"File: /docs/source/concepts.rst","text":"<p>==========  Concepts ==========</p> <p>Cosmic Ray comprises a number of important and potentially confusing concepts. In this section we'll look at each of these concepts, explaining their role in Cosmic Ray and how they relate to other concepts. We'll also use this section to establish the terminology that we'll use throughout the rest of the documentation.</p>"},{"location":"testing/cosmic-ray/#operators","title":"Operators","text":"<p>An operator in Cosmic Ray is a class that represents a specific type of mutation. The first role of an operator is to identify points in the code where a specific mutation can be applied. The second role of an operator is to actually perform the mutation when requested.</p> <p>An example of an operator is :mod:<code>cosmic_ray.operators.break_continue</code>. As its name implies, this operator mutates code by replacing <code>break</code> with <code>continue</code>. During the initialization of a session, this operator identifies all of the locations in the code where this mutation can be applied. Then, during execution of a session, it actually mutates the code by replacing <code>break</code> nodes with <code>continue</code> nodes.</p> <p>Operators are exposed to Cosmic Ray via plugins, and users can choose to extend the available operator set by providing their own operators. Operators are implemented as subclasses of :class:<code>cosmic_ray.operators.operator.Operator</code>.</p>"},{"location":"testing/cosmic-ray/#distributors","title":"Distributors","text":"<p>Distributors determine the context in which tests are executed. The primary examples of distributors are :class:<code>cosmic_ray.distribution.local.LocalDistributor</code> and :class:<code>cosmic_ray.distribution.http.HttpDistributor</code>. The local distributor tests on the local machine, modifying an existing copy of the code in-place, running each test serially with no concurrency.</p> <p>The http distributor distributes tests to remote workers via HTTP. There can be any number of workers, and they can run the tests in parallel. Because of this concurrency, each HTTP worker will generally have its own copy of the code under test.</p> <p>Distributors have broad control over how they execute tests. During the execution phase they are given a sequence of pending mutations to execute, and it's their job to execute the tests in the appropriate context and return a result. Cosmic Ray doesn't impose any real constraints on how distributors accomplish this.</p> <p>Distributors can require arbitrarily complex infrastructure and configuration. For example, the HTTP distributor requires you to start the workers prior to starting execution, and it requires that you provide each worker with its own  copy of the code under test.</p> <p>Distributors are implemented as plugins to Cosmic Ray. They are dynamically discovered, and users can create their own distributors. Cosmic Ray includes two execution engines plugins, local and http.</p>"},{"location":"testing/cosmic-ray/#configurations","title":"Configurations","text":"<p>A configuration is a TOML file that describes the work that Cosmic Ray will do. For example, it tells Cosmic Ray which modules to mutate, how to run tests, which tests to run, and so forth. You need to create a config before doing any real work with Cosmic Ray.</p> <p>You can create a skeleton config by running <code>cosmic-ray new-config &lt;config file&gt;</code>. This will ask you a series of questions and create a config from the answers. Note that this config will generally be incomplete and require you to edit it for completeness.</p> <p>In many Cosmic Ray examples we'll use the name \"config.toml\" for configurations. You are not required to use this name, however. You can use any file name you want for your configurations.</p> <p>.. important::</p> <pre><code>The full set of configuration options are not currently well documented. Each plugin can, in principle and often in\npractice, use their own specialized configuration options. We need to work on making the documentation of these\noptions automatic and part of the plugin API. For detail on configuration options, the best place to check is\ncurrently in the ``tests/example_project`` directory.\n</code></pre>"},{"location":"testing/cosmic-ray/#sessions","title":"Sessions","text":"<p>Cosmic Ray has a notion of sessions which encompass an entire mutation testing run. Essentially, a session is a database which records the work that needs to be done for a run. Then as results are available from workers that do the actual testing, the database is updated with results. By having a database like this, Cosmic Ray can safely stop in the middle of a (potentially very long) session and be restarted. Since the session knows which work is already completed, it can continue where it left off.</p> <p>Sessions also allow for arbitrary post-facto analysis and report generation.</p>"},{"location":"testing/cosmic-ray/#initializing-sessions","title":"Initializing sessions","text":"<p>Before you can do mutation testing with Cosmic Ray, you need to first initialize a session. You can do this using the <code>init</code> command. With this command you tell Cosmic Ray a) the name of the session, b) which module(s) you wish to mutate and c) the location of the test suite. For example, to mutate the package <code>allele</code>, using the <code>unittest</code> to run the tests in <code>allele_tests</code>, and using the <code>local</code> execution engine, you could first need to create a configuration like this:</p> <p>.. code-block:: ini</p> <pre><code>[cosmic-ray]\nmodule-path = \"allele\"\ntimeout = 10\nexcluded-modules = []\ntest-command = python -m unittest allele_tests\ndistributor.name = \"local\"\n</code></pre> <p>You would run <code>cosmic-ray init</code> like this:</p> <p>::</p> <pre><code>cosmic-ray init config.toml session.sqlite\n</code></pre> <p>You'll notice that this creates a new file called <code>allele_session.sqlite</code>. This is the database for your session.</p> <p>.. _test_suite:</p>"},{"location":"testing/cosmic-ray/#test-suite","title":"Test suite","text":"<p>To be able to kill the mutants Cosmic Ray uses your test cases. But the mutants are not considered \"more dead\" when more test cases fail. Given that a single failing test case is sufficient to kill a mutant, it's a good idea to configure the test runner to exit as soon as a failing test case is found.</p> <p>For <code>pytest</code> and <code>nose</code> that can be achieved with the <code>-x</code> option.</p> <p>.. _note_separation_test_code:</p> <p>.. admonition:: An important note on separating tests and production code</p> <pre><code>Cosmic Ray has a relatively simple view of how to mutate modules. Fundamentally, it will attempt to mutate any and all\ncode in a module. This means that if you have test code in the same module as your code under test, Cosmic Ray will\nhappily mutate the test code along with the production code. This is probably not what you want.\n\nThe best way to avoid this problem is to keep your test code in separate modules from your production code. This way you\ncan tell Cosmic Ray precisely what to mutate.\n\nIdeally, your test code will be in a different package from your production code. This way you can tell Cosmic Ray to\nmutate an entire package without needing to filter anything out. However, if your test code is in the same package as\nyour production code (a common configuration), you can use the ``excluded-modules`` setting in your configuration to\nprevent mutation of your tests.\n\nGiven the choice, though, we recommend keeping your tests outside of the package for your code under test.\n</code></pre>"},{"location":"testing/cosmic-ray/#executing-tests","title":"Executing tests","text":"<p>Once a session has been initialized, you can start executing tests by using the <code>exec</code> command. This command needs the config and the session you provided to <code>init</code>:</p> <p>.. code-block:: bash</p> <pre><code>cosmic-ray exec config.toml session.sqlite\n</code></pre> <p>Normally this won't produce any output unless there are errors.</p>"},{"location":"testing/cosmic-ray/#viewing-the-results","title":"Viewing the results","text":"<p>Once your tests have completed, you can view the results using the <code>cr-report</code> command:</p> <p>.. code-block:: bash</p> <pre><code>cr-report test_session.sqlite\n</code></pre> <p>This will give you detailed information about what work was done, followed by a summary of the entire session.</p>"},{"location":"testing/cosmic-ray/#test-commands","title":"Test commands","text":"<p>The <code>test-command</code> field of a configuration tells Cosmic Ray how to run tests. Cosmic Ray runs this command from whatever directory you run the <code>exec</code> command (or, in the case of remote execution, in whatever directory the remote command handler is running).</p>"},{"location":"testing/cosmic-ray/#timeouts","title":"Timeouts","text":"<p>One difficulty mutation testing tools have to face is how to deal with mutations that result in infinite loops (or other pathological runtime effects). Cosmic Ray takes the simple approach of using a timeout to determine when to kill a test and consider it incompetent. That is, if a test of a mutant takes longer than the timeout, the test is killed, and the mutant is marked incompetent.</p> <p>You specify a test time through the <code>timeout</code> configuration key. This key specifies an absolute number of seconds that a test will be allowed to run. After the timeout is up, the test is killed. For example, to specify that tests should timeout after 10 seconds, use:</p> <p>.. code-block:: ini</p> <p># config.toml    [cosmic-ray]    timeout = 10</p>"},{"location":"testing/cosmic-ray/#file-docssourceindexrst","title":"File: /docs/source/index.rst","text":"<p>.. Cosmic Ray documentation documentation master file, created by    sphinx-quickstart on Fri Oct 27 12:29:41 2017.    You can adapt this file completely to your liking, but it should at least    contain the root <code>toctree</code> directive.</p>"},{"location":"testing/cosmic-ray/#cosmic-ray-mutation-testing-for-python","title":"Cosmic Ray: mutation testing for Python","text":"<p>\"Four human beings -- changed by space-born cosmic rays into something more than merely human.\"</p> <p>-- The Fantastic Four</p> <p>Cosmic Ray is a mutation testing tool for Python 3. It makes small changes to your production source code, running your test suite for each change. If a test suite passes on mutated code, then you have a mismatch between your tests and your functionality. </p> <p>Like coverage analysis, mutation testing helps ensure that you're testing all of your code. But while coverage only tells you if a line of code is executed, mutation testing will determine if your tests actually check the behavior of your code. This adds tremendous value to your test suite by helping it fulfill its primary role: making sure your code does what you expect it to do!</p> <p>Cosmic Ray has been successfully used on a wide variety of projects ranging from assemblers to oil exploration software.</p>"},{"location":"testing/cosmic-ray/#contents","title":"Contents","text":"<p>.. toctree::    :maxdepth: 1</p> <p>theory    tutorials/intro/index    tutorials/distributed/index    concepts    how-tos/index    reference/index</p>"},{"location":"testing/cosmic-ray/#indices-and-tables","title":"Indices and tables","text":"<ul> <li>:ref:<code>genindex</code></li> <li>:ref:<code>modindex</code></li> <li>:ref:<code>search</code></li> </ul>"},{"location":"testing/cosmic-ray/#file-docssourcetheoryrst","title":"File: /docs/source/theory.rst","text":""},{"location":"testing/cosmic-ray/#theory","title":"Theory","text":"<p>Mutation testing is conceptually simple and elegant. You make certain kinds of controlled changes (mutations) to your code under test [1]_, and then you run your test suite over this mutated code. If your test suite fails, then we say that your tests \"killed\" (i.e. detected) the mutant. If the changes cause your code to simply crash, then we say the mutant is \"incompetent\". If your test suite passes, however, we say that the mutant has \"survived\".</p> <p>Needless to say, we want to kill all of the mutants.</p> <p>The goal of mutation testing is to verify that your test suite is actually testing all of the parts of your code that it needs to, and that it is doing so in a meaningful way. If a mutant survives your test suite, this is an indication that your test suite is not adequately checking the code that was changed. This means that either a) you need more or better tests or b) you've got code which you don't need.</p> <p>You can read more about mutation testing at <code>the repository of all human knowledge &lt;https://en.wikipedia.org/wiki/Mutation_testing&gt;</code>. Lionel Brian has a <code>nice set of slides &lt;http://www.uio.no/studier/emner/matnat/ifi/INF4290/v10/undervisningsmateriale/INF4290-Mutest.pdf&gt;</code> introducing mutation testing as well.</p> <p>.. [1] By \"code under test\", we mean the code that your test suite is testing. Mutation testing is trying        to ensure that your unaltered test suite can detect explicitly incorrect behavior in your code.</p>"},{"location":"testing/cosmic-ray/#file-testsresourcesfast_testsreadmemd","title":"File: /tests/resources/fast_tests/README.md","text":"<p>This is intended to be a very fast test suite. On some platform (e.g. Windows) we've found that test suites like this can run faster than the resolution of the filesystem timestamps. This leads to problems where Python doesn't regenerate pycs files when necessary, leading to incorrect mutation testing results. </p> <p>We've modified CR to work around these problems, and this test (as driven by the pytest suite) will hopefully detect regressions in our workaround.</p>"},{"location":"testing/cosmic-ray/#file-contributingrst","title":"File: /CONTRIBUTING.rst","text":"<p>================= How to contribute =================</p> <p>Third-party patches are welcomed for improving Cosmic Ray. There is plenty of work to be done on bug fixes, documentation, new features, and improved tooling.</p> <p>Although we want to keep it as easy as possible to contribute changes that get things working in your environment, there are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things.</p>"},{"location":"testing/cosmic-ray/#getting-started","title":"Getting Started","text":"<p>The easiest way to help is by submitting issues reporting defects or requesting additional features.</p> <ul> <li> <p>Make sure you have a <code>GitHub account &lt;https://github.com/signup/free&gt;</code>_</p> </li> <li> <p>Submit an issue, assuming one does not already exist.</p> </li> <li> <p>Clearly describe the issue including steps to reproduce when it is a bug.</p> </li> <li> <p>If appropriate, include a Cosmic Ray config file and, if possible, some way for us to get access to     the code you're working with.</p> </li> <li> <p>Make sure you mention the earliest version that you know has the issue.</p> </li> <li> <p>Fork the repository on GitHub</p> </li> </ul>"},{"location":"testing/cosmic-ray/#making-changes","title":"Making Changes","text":"<ul> <li>You must own the copyright to the patch you're submitting, and be in a   position to transfer the copyright to Sixty North by agreeing to the either   the |ICLA|   (for private individuals) or the |ECLA|   (for corporations or other organisations).</li> <li>Make small commits in logical units.</li> <li>Ensure your code is in the spirit of <code>PEP 8 &lt;https://www.python.org/dev/peps/pep-0008/&gt;</code>_,   although we accept that much of what is in PEP 8 are guidelines   rather than rules, so we value readability over strict compliance.</li> <li>Check for unnecessary whitespace with <code>git diff --check</code> before committing.</li> <li> <p>Make sure your commit messages are in the proper format::</p> <p>Issue #1234 - Make the example in CONTRIBUTING imperative and concrete</p> <p>Without this patch applied the example commit message in the CONTRIBUTING document is not a concrete example.  This is a problem because the contributor is left to imagine what the commit message should look like based on a description rather than an example.  This patch fixes the problem by making the example concrete and imperative.</p> <p>The first line is a real life imperative statement with an issue number from our issue tracker.  The body describes the behavior without the patch, why this is a problem, and how the patch fixes the problem when applied.</p> </li> <li> <p>Make sure you have added the necessary tests for your changes.</p> </li> <li>Run all the tests to assure nothing else was accidentally broken.</li> </ul>"},{"location":"testing/cosmic-ray/#making-trivial-changes","title":"Making Trivial Changes","text":""},{"location":"testing/cosmic-ray/#documentation","title":"Documentation","text":"<p>For changes of a trivial nature to comments and documentation, it is not always necessary to create a new issue. In this case, it is appropriate to start the first line of a commit with 'Doc -' instead of an issue number::</p> <pre><code>Doc - Add documentation commit example to CONTRIBUTING\n\nThere is no example for contributing a documentation commit\nto the Cosmic Ray repository. This is a problem because the contributor\nis left to assume how a commit of this nature may appear.\n\nThe first line is a real life imperative statement with 'Doc -' in\nplace of what would have been the ticket number in a\nnon-documentation related commit. The body describes the nature of\nthe new documentation or comments added.\n</code></pre>"},{"location":"testing/cosmic-ray/#submitting-changes","title":"Submitting Changes","text":"<ul> <li>Agree to the |ICLA| or the |ECLA|   by attaching a copy of the current CLA to an email (so we know which   version you're agreeing to). The body of the message should contain   the text \"I, , [representing ] have read the   attached CLA and agree to its terms.\"  Send the email to austin@sixty-north.com <li>Push your changes to a topic branch in your fork of the repository.</li> <li>Submit a pull request to the repository in the sixty-north organization.</li>"},{"location":"testing/cosmic-ray/#additional-resources","title":"Additional Resources","text":"<ul> <li>|ICLA|</li> <li>|ECLA|</li> <li><code>PEP 8 &lt;https://www.python.org/dev/peps/pep-0008/&gt;</code>_</li> <li><code>General GitHub documentation &lt;http://help.github.com/&gt;</code>_</li> <li><code>GitHub pull request documentation &lt;http://help.github.com/send-pull-requests/&gt;</code>_</li> </ul> <p>.. |ICLA| replace:: <code>Individual Contributors License Agreement &lt;https://github.com/sixty-north/cosmic-ray/raw/master/docs/source/legal/cosmic-ray-individual-cla.pdf&gt;</code> .. |ECLA| replace:: <code>Entity Contributor License Agreement &lt;https://github.com/sixty-north/cosmic-ray/raw/master/docs/source/legal/cosmic-ray-entity-cla.pdf&gt;</code></p>"},{"location":"testing/cosmic-ray/#file-readmerst","title":"File: /README.rst","text":"<p>|Python version| |Python version windows| |Build Status| |Documentation|</p>"},{"location":"testing/cosmic-ray/#cosmic-ray-mutation-testing-for-python_1","title":"Cosmic Ray: mutation testing for Python","text":"<p>\"Four human beings -- changed by space-born cosmic rays into something more than merely human.\"</p> <p>-- The Fantastic Four</p> <p>Cosmic Ray is a mutation testing tool for Python 3.</p> <p>It makes small changes to your source code, running your test suite for each one. Here's how the mutations look:</p> <p>.. image:: docs/source/cr-in-action.gif</p> <p>|full_documentation|_</p>"},{"location":"testing/cosmic-ray/#contributing","title":"Contributing","text":"<p>The easiest way to contribute is to use Cosmic Ray and submit reports for defects or any other issues you come across. Please see CONTRIBUTING.rst for more details.</p> <p>.. |Python version| image:: https://img.shields.io/badge/Python_version-3.9+-blue.svg    :target: https://www.python.org/ .. |Python version windows| image:: https://img.shields.io/badge/Python_version_(windows)-3.9+-blue.svg    :target: https://www.python.org/ .. |Build Status| image:: https://github.com/sixty-north/cosmic-ray/actions/workflows/python-package.yml/badge.svg    :target: https://github.com/sixty-north/cosmic-ray/actions/workflows/python-package.yml .. |Code Health| image:: https://landscape.io/github/sixty-north/cosmic-ray/master/landscape.svg?style=flat    :target: https://landscape.io/github/sixty-north/cosmic-ray/master .. |Code Coverage| image:: https://codecov.io/gh/sixty-north/cosmic-ray/branch/master/graph/badge.svg    :target: https://codecov.io/gh/Vimjas/covimerage/branch/master .. |Documentation| image:: https://readthedocs.org/projects/cosmic-ray/badge/?version=latest    :target: http://cosmic-ray.readthedocs.org/en/latest/ .. |full_documentation| replace:: Read the full documentation at readthedocs. .. _full_documentation: http://cosmic-ray.readthedocs.org/en/latest/</p>"},{"location":"testing/mutatest/","title":"Mutatest","text":"<p>Directory Structure:</p> <p>\u2514\u2500\u2500 ./     \u251c\u2500\u2500 .github     \u2502   \u2514\u2500\u2500 pull_request_template.md     \u251c\u2500\u2500 docs     \u2502   \u251c\u2500\u2500 api_tutorial     \u2502   \u2502   \u2514\u2500\u2500 api_tutorial.rst     \u2502   \u251c\u2500\u2500 changelog.rst     \u2502   \u251c\u2500\u2500 commandline.rst     \u2502   \u251c\u2500\u2500 contributing.rst     \u2502   \u251c\u2500\u2500 index.rst     \u2502   \u251c\u2500\u2500 install.rst     \u2502   \u251c\u2500\u2500 license.rst     \u2502   \u251c\u2500\u2500 modules.rst     \u2502   \u2514\u2500\u2500 mutants.rst     \u251c\u2500\u2500 AUTHORS.rst     \u251c\u2500\u2500 CHANGELOG.rst     \u251c\u2500\u2500 CONTRIBUTING.rst     \u2514\u2500\u2500 README.rst</p>"},{"location":"testing/mutatest/#file-githubpull_request_templatemd","title":"File: /.github/pull_request_template.md","text":"<p>PR Checklist:</p> <ul> <li>[ ] Description of the change is included.</li> <li>[ ] Ensure all automated CI checks pass (though ask for help if needed).</li> </ul>"},{"location":"testing/mutatest/#file-docsapi_tutorialapi_tutorialrst","title":"File: /docs/api_tutorial/api_tutorial.rst","text":"<p>.. _API Tutorial:</p>"},{"location":"testing/mutatest/#api-tutorial","title":"API Tutorial","text":"<p>This is a walkthrough of using the <code>mutatest</code> API. These are the same method calls used by the CLI and provide additional flexibility for customization. The code and notebook to generate this tutorial is located under the <code>docs/api_tutorial</code> folder on GitHub.</p> <p>.. code:: ipython3</p> <pre><code># Imports used throughout the tutorial\n\nimport ast\n\nfrom pathlib import Path\n\nfrom mutatest import run\nfrom mutatest import transformers\nfrom mutatest.api import Genome, GenomeGroup, MutationException\nfrom mutatest.filters import CoverageFilter, CategoryCodeFilter\n</code></pre>"},{"location":"testing/mutatest/#tutorial-setup","title":"Tutorial setup","text":"<p>The <code>example/</code> folder has two Python files, <code>a.py</code> and <code>b.py</code>, with a <code>test_ab.py</code> file that would be automatically detected by <code>pytest</code>.</p> <p>.. code:: ipython3</p> <pre><code># This folder and included .py files are in docs/api_tutoral/\n\nsrc_loc = Path(\"example\")\n</code></pre> <p>.. code:: ipython3</p> <pre><code>print(*[i for i in src_loc.iterdir()\n        if i.is_file()], sep=\"\\n\")\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/a.py\nexample/test_ab.py\nexample/b.py\n</code></pre> <p><code>a.py</code> holds two functions: one to add five to an input value, another to return <code>True</code> if the first input value is greater than the second input value. The add operation is represented in the AST as <code>ast.Add</code>, a <code>BinOp</code> operation type, and the greater-than operation is represented by <code>ast.Gt</code>, a <code>CompareOp</code> operation type. If the source code is executed the expected value is to print <code>10</code>.</p> <p>.. code:: ipython3</p> <pre><code>def open_print(fn):\n    \"\"\"Open a print file contents.\"\"\"\n    with open(fn) as f:\n        print(f.read())\n\n# Contents of a.py example source file\nopen_print(src_loc / \"a.py\")\n</code></pre> <p>.. parsed-literal::</p> <pre><code>\"\"\"Example A.\n\"\"\"\n\n\ndef add_five(a):\n    return a + 5\n\n\ndef greater_than(a, b):\n    return a &gt; b\n\n\nprint(add_five(5))\n</code></pre> <p><code>b.py</code> has a single function that returns whether or not the first input <code>is</code> the second input. <code>is</code> is represented by <code>ast.Is</code> and is a <code>CompareIs</code> operation type. The expected value if this source code is executed is <code>True</code>.</p> <p>.. code:: ipython3</p> <pre><code># Contents of b.py example source file\n\nopen_print(src_loc / \"b.py\")\n</code></pre> <p>.. parsed-literal::</p> <pre><code>\"\"\"Example B.\n\"\"\"\n\n\ndef is_match(a, b):\n    return a is b\n\n\nprint(is_match(1, 1))\n</code></pre> <p><code>test_ab.py</code> is the test script for both <code>a.py</code> and <code>b.py</code>. The <code>test_add_five</code> function is intentionally broken to demonstrate later mutations. It will pass if the value is greater than 10 in the test using 6 as an input value, and fail otherwise.</p> <p>.. code:: ipython3</p> <pre><code># Contents of test_ab.py example test file\n\nopen_print(src_loc / \"test_ab.py\")\n</code></pre> <p>.. parsed-literal::</p> <pre><code>from a import add_five\nfrom b import is_match\n\n\ndef test_add_five():\n    assert add_five(6) &gt; 10\n\n\ndef test_is_match():\n    assert is_match(\"one\", \"one\")\n</code></pre>"},{"location":"testing/mutatest/#run-a-clean-trial-and-generate-coverage","title":"Run a clean trial and generate coverage","text":"<p>We can use <code>run</code> to perform a \u201cclean trial\u201d of our test commands based on the source location. This will generate a <code>.coverage</code> file that will be used by the <code>Genome</code>. A <code>.coverage</code> file is not required. This run method is useful for doing clean trials before and after mutation trials as a way to reset the <code>__pycache__</code>.</p> <p>.. code:: ipython3</p> <pre><code># The return value of clean_trial is the time to run\n# this is used in reporting from the CLI\n\nrun.clean_trial(\n    src_loc, test_cmds=[\"pytest\", \"--cov=example\"]\n)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>datetime.timedelta(microseconds=411150)\n</code></pre> <p>.. code:: ipython3</p> <pre><code>Path(\".coverage\").exists()\n</code></pre> <p>.. parsed-literal::</p> <pre><code>True\n</code></pre>"},{"location":"testing/mutatest/#genome-basics","title":"Genome Basics","text":"<p><code>Genomes</code> are the basic representation of a source code file in <code>mutatest</code>. They can be initialized by passing in the path to a specific file, or initialized without any arguments and have the source file added later. The basic properties include the Abstract Syntax Tree (AST), the source file, the coverage file, and any category codes for filtering.</p> <p>.. code:: ipython3</p> <pre><code># Initialize with the source file location\n# By default, the \".coverage\" file is set\n# for the coverage_file property\n\ngenome = Genome(src_loc / \"a.py\")\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.source_file\n</code></pre> <p>.. parsed-literal::</p> <pre><code>PosixPath('example/a.py')\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.coverage_file\n</code></pre> <p>.. parsed-literal::</p> <pre><code>PosixPath('.coverage')\n</code></pre> <p>.. code:: ipython3</p> <pre><code># By default, no filter codes are set\n# These are categories of mutations to filter\n\ngenome.filter_codes\n</code></pre> <p>.. parsed-literal::</p> <pre><code>set()\n</code></pre> <p>Finding mutation targets ~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The <code>Genome</code> has two additional properties related to finding mutation locations: <code>targets</code> and <code>covered_targets</code>. These are sets of <code>LocIndex</code> objects (defined in <code>transformers</code>) that represent locations in the AST that can be mutated. Covered targets are those that have lines covered by the set <code>coverage_file</code> property.</p> <p>.. code:: ipython3</p> <pre><code>genome.targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16),\n LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.covered_targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.targets - genome.covered_targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)}\n</code></pre> <p>Accessing the AST ~~~~~~~~~~~~~~~~~</p> <p>The <code>ast</code> property is the AST of the source file. You can access the properties directly. This is used to generate the targets and covered targets. The AST parser is defined in <code>transformers</code> but is encapsulted in the <code>Genome</code>.</p> <p>.. code:: ipython3</p> <pre><code>genome.ast\n</code></pre> <p>.. parsed-literal::</p> <pre><code>&lt;_ast.Module at 0x7f68a4014bb0&gt;\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.ast.body\n</code></pre> <p>.. parsed-literal::</p> <pre><code>[&lt;_ast.Expr at 0x7f68a4014ca0&gt;,\n &lt;_ast.FunctionDef at 0x7f68a4014ac0&gt;,\n &lt;_ast.FunctionDef at 0x7f68a4014eb0&gt;,\n &lt;_ast.Expr at 0x7f68a402c040&gt;]\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.ast.body[1].__dict__\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{'name': 'add_five',\n 'args': &lt;_ast.arguments at 0x7f68a4014d30&gt;,\n 'body': [&lt;_ast.Return at 0x7f68a4014dc0&gt;],\n 'decorator_list': [],\n 'returns': None,\n 'type_comment': None,\n 'lineno': 5,\n 'col_offset': 0,\n 'end_lineno': 6,\n 'end_col_offset': 16}\n</code></pre> <p>Filtering mutation targets ~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>You can set filters on a <code>Genome</code> for specific types of targets. For example, setting <code>bn</code> for <code>BinOp</code> will filter both targets and covered targets to only <code>BinOp</code> class operations.</p> <p>.. code:: ipython3</p> <pre><code># All available categories are listed\n# in transformers.CATEGORIES\n\nprint(*[f\"Category:{k}, Code: {v}\"\n        for k,v in transformers.CATEGORIES.items()],\n      sep=\"\\n\")\n</code></pre> <p>.. parsed-literal::</p> <pre><code>Category:AugAssign, Code: aa\nCategory:BinOp, Code: bn\nCategory:BinOpBC, Code: bc\nCategory:BinOpBS, Code: bs\nCategory:BoolOp, Code: bl\nCategory:Compare, Code: cp\nCategory:CompareIn, Code: cn\nCategory:CompareIs, Code: cs\nCategory:If, Code: if\nCategory:Index, Code: ix\nCategory:NameConstant, Code: nc\nCategory:SliceUS, Code: su\n</code></pre> <p>.. code:: ipython3</p> <pre><code># If you attempt to set an invalid code a ValueError is raised\n# and the valid codes are listed in the message\n\ntry:\n    genome.filter_codes = (\"asdf\",)\n\nexcept ValueError as e:\n    print(e)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>Invalid category codes: {'asdf'}.\nValid codes: {'AugAssign': 'aa', 'BinOp': 'bn', 'BinOpBC': 'bc', 'BinOpBS': 'bs', 'BoolOp': 'bl', 'Compare': 'cp', 'CompareIn': 'cn', 'CompareIs': 'cs', 'If': 'if', 'Index': 'ix', 'NameConstant': 'nc', 'SliceUS': 'su'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Set the filter using an iterable of the two-letter codes\n\ngenome.filter_codes = (\"bn\",)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Targets and covered targets will only show the filtered value\n\ngenome.targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.covered_targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Reset the filter_codes to an empty set\ngenome.filter_codes = set()\n</code></pre> <p>.. code:: ipython3</p> <pre><code># All target classes are now listed again\n\ngenome.targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16),\n LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)}\n</code></pre> <p>Using custom filters ~~~~~~~~~~~~~~~~~~~~</p> <p>If you need more flexibility, the <code>filters</code> define the two classes of filter used by <code>Genome</code>: the <code>CoverageFilter</code> and the <code>CategoryCodeFilter</code>. These are encapsultated by <code>Genome</code> and <code>GenomeGroup</code> already but can be accessed directly.</p> <p>Coverage Filter ^^^^^^^^^^^^^^^</p> <p>.. code:: ipython3</p> <pre><code>cov_filter = CoverageFilter(coverage_file=Path(\".coverage\"))\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Use the filter method to filter targets based on\n# a given source file.\n\ncov_filter.filter(\n    genome.targets, genome.source_file\n)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># You can invert the filtering as well\n\ncov_filter.filter(\n    genome.targets, genome.source_file,\n    invert=True\n)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)}\n</code></pre> <p>Category Code Filter ^^^^^^^^^^^^^^^^^^^^</p> <p>.. code:: ipython3</p> <pre><code># Instantiate using a set of codes\n# or add them later\n\ncatcode_filter = CategoryCodeFilter(codes=(\"bn\",))\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Valid codes provide all potential values\n\ncatcode_filter.valid_codes\n</code></pre> <p>.. parsed-literal::</p> <pre><code>dict_values(['aa', 'bn', 'bc', 'bs', 'bl', 'cp', 'cn', 'cs', 'if', 'ix', 'nc', 'su'])\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Valid categories are also available\n\ncatcode_filter.valid_categories\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{'AugAssign': 'aa',\n 'BinOp': 'bn',\n 'BinOpBC': 'bc',\n 'BinOpBS': 'bs',\n 'BoolOp': 'bl',\n 'Compare': 'cp',\n 'CompareIn': 'cn',\n 'CompareIs': 'cs',\n 'If': 'if',\n 'Index': 'ix',\n 'NameConstant': 'nc',\n 'SliceUS': 'su'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># add more codes\n\ncatcode_filter.add_code(\"aa\")\ncatcode_filter.codes\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{'aa', 'bn'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># see all validation mutations\n# based on the set codes\n\ncatcode_filter.valid_mutations\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{_ast.Add,\n _ast.Div,\n _ast.FloorDiv,\n _ast.Mod,\n _ast.Mult,\n _ast.Pow,\n _ast.Sub,\n 'AugAssign_Add',\n 'AugAssign_Div',\n 'AugAssign_Mult',\n 'AugAssign_Sub'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># discard codes\n\ncatcode_filter.discard_code(\"aa\")\ncatcode_filter.codes\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{'bn'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>catcode_filter.valid_mutations\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{_ast.Add, _ast.Div, _ast.FloorDiv, _ast.Mod, _ast.Mult, _ast.Pow, _ast.Sub}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Filter a set of targets based on codes\n\ncatcode_filter.filter(genome.targets)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Optionally, invert the filter\n\ncatcode_filter.filter(\n    genome.targets, invert=True\n)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)}\n</code></pre> <p>Changing the source file in a Genome ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>If you change the source file property of the <code>Genome</code> all core properties except the coverage file and filters are reset - this includes targets, covered targets, and AST.</p> <p>.. code:: ipython3</p> <pre><code>genome.source_file = src_loc / \"b.py\"\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;, end_lineno=6, end_col_offset=17)}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>genome.covered_targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)}\n</code></pre>"},{"location":"testing/mutatest/#creating-mutations","title":"Creating Mutations","text":"<p>Mutations are applied to specific <code>LocIndex</code> targets in a <code>Genome</code>. You must speicfy a valid operation e.g., \u201cadd\u201d can be mutated to \u201cdivide\u201d or \u201csubtract\u201d, but not \u201cis\u201d. The <code>Genome</code> itself is not modified, a returned <code>Mutant</code> object holds the information required to create a mutated version of the <code>__pycache__</code> for that source file. For this example, we\u2019ll change <code>a.py</code> to use a multiplication operation instead of an addition operation for the <code>add_five</code> function. The original expected result of the code was <code>10</code> from <code>5 + 5</code> if executed, with the mutation it will be <code>25</code> since the mutation creates <code>5 * 5</code>.</p> <p>.. code:: ipython3</p> <pre><code># Set the Genome back to example a\n# filter to only the BinOp targets\n\ngenome.source_file = src_loc / \"a.py\"\ngenome.filter_codes = (\"bn\",)\n\n# there is only one Binop target\n\nmutation_target = list(genome.targets)[0]\nmutation_target\n</code></pre> <p>.. parsed-literal::</p> <pre><code>LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># The mutate() method applies a mutation operation\n# and returns a mutant\n\nmutant = genome.mutate(mutation_target, ast.Mult)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># applying an invalid mutation\n# raises a MutationException\n\ntry:\n    genome.mutate(mutation_target, ast.IsNot)\n\nexcept MutationException as e:\n    print(e)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>&lt;class '_ast.IsNot'&gt; is not a member of mutation category bn.\nValid mutations for bn: {&lt;class '_ast.Mult'&gt;, &lt;class '_ast.Sub'&gt;, &lt;class '_ast.Add'&gt;, &lt;class '_ast.Pow'&gt;, &lt;class '_ast.FloorDiv'&gt;, &lt;class '_ast.Mod'&gt;, &lt;class '_ast.Div'&gt;}.\n</code></pre> <p>.. code:: ipython3</p> <pre><code># mutants have all of the properties\n# needed to write mutated __pycache__\n\nmutant\n</code></pre> <p>.. parsed-literal::</p> <pre><code>Mutant(mutant_code=&lt;code object &lt;module&gt; at 0x7f68a4040b30, file \"example/a.py\", line 1&gt;, src_file=PosixPath('example/a.py'), cfile=PosixPath('example/__pycache__/a.cpython-38.pyc'), loader=&lt;_frozen_importlib_external.SourceFileLoader object at 0x7f689cfbd310&gt;, source_stats={'mtime': 1571346690.5703905, 'size': 118}, mode=33188, src_idx=LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16), mutation=&lt;class '_ast.Mult'&gt;)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># You can directly execute the mutant_code\n# This result is with the mutated target being\n# applied as Mult instead of Add in a.py\n\nexec(mutant.mutant_code)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>25\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Mutants have a write_cache() method to apply\n# the change to __pycache__\n\nmutant.write_cache()\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Alternatively, use run to do a single trial\n# and return the result\n\nmutant_trial_result = run.create_mutation_run_trial(\n    genome, mutation_target, ast.Mult, [\"pytest\"], max_runtime=5\n)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># In this case the mutation would survive\n# The test passes if the value is\n# greater than 10.\n\nmutant_trial_result.status\n</code></pre> <p>.. parsed-literal::</p> <pre><code>'SURVIVED'\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Using a different operation, such as Div\n# will be a detected mutation\n# since the test will fail.\n\nmutant_trial_result = run.create_mutation_run_trial(\n    genome, mutation_target, ast.Div, [\"pytest\"], max_runtime=5\n)\n\nmutant_trial_result.status\n</code></pre> <p>.. parsed-literal::</p> <pre><code>'DETECTED'\n</code></pre>"},{"location":"testing/mutatest/#genomegroups","title":"GenomeGroups","text":"<p>The <code>GenomeGroup</code> is a way to interact with multiple <code>Genomes</code>. You can create a <code>GenomeGroup</code> from a folder of files, add new <code>Genomes</code>, and access shared properties across the <code>Genomes</code>. It is a <code>MutableMapping</code> and behaves accordingly, though it only accepts <code>Path</code> keys and <code>Genome</code> values. You can use the <code>GenomeGroup</code> to assign common filters, common coverage files, and to get all targets across an entire collection of <code>Genomes</code>.</p> <p>.. code:: ipython3</p> <pre><code>ggrp = GenomeGroup(src_loc)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># key-value pairs in the GenomeGroup are\n# the path to the source file\n# and the Genome object for that file\n\nfor k,v in ggrp.items():\n    print(k, v)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/a.py &lt;mutatest.api.Genome object at 0x7f689cfc8c10&gt;\nexample/b.py &lt;mutatest.api.Genome object at 0x7f689cfc8f70&gt;\n</code></pre> <p>.. code:: ipython3</p> <pre><code># targets, and covered_targets produce\n# GenomeGroupTarget objects that have\n# attributes for the source path and\n# LocIdx for the target\n\nfor t in ggrp.targets:\n    print(\n        t.source_path, t.loc_idx\n    )\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/b.py LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;, end_lineno=6, end_col_offset=17)\nexample/a.py LocIndex(ast_class='Compare', lineno=10, col_offset=11, op_type=&lt;class '_ast.Gt'&gt;, end_lineno=10, end_col_offset=16)\nexample/a.py LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># You can set a filter or\n# coverage file for the entire set\n# of genomes\n\nggrp.set_coverage = Path(\".coverage\")\n\nfor t in ggrp.covered_targets:\n    print(\n        t.source_path, t.loc_idx\n    )\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/b.py LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;, end_lineno=6, end_col_offset=17)\nexample/a.py LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;, end_lineno=6, end_col_offset=16)\n</code></pre> <p>.. code:: ipython3</p> <pre><code># Setting filter codes on all Genomes\n# in the group\n\nggrp.set_filter((\"cs\",))\nggrp.targets\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{GenomeGroupTarget(source_path=PosixPath('example/b.py'), loc_idx=LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;, end_lineno=6, end_col_offset=17))}\n</code></pre> <p>.. code:: ipython3</p> <pre><code>for k, v in ggrp.items():\n    print(k, v.filter_codes)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/a.py {'cs'}\nexample/b.py {'cs'}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># MutableMapping operations are\n# available as well\n\nggrp.values()\n</code></pre> <p>.. parsed-literal::</p> <pre><code>dict_values([&lt;mutatest.api.Genome object at 0x7f689cfc8c10&gt;, &lt;mutatest.api.Genome object at 0x7f689cfc8f70&gt;])\n</code></pre> <p>.. code:: ipython3</p> <pre><code>ggrp.keys()\n</code></pre> <p>.. parsed-literal::</p> <pre><code>dict_keys([PosixPath('example/a.py'), PosixPath('example/b.py')])\n</code></pre> <p>.. code:: ipython3</p> <pre><code># pop a Genome out of the Group\n\ngenome_a = ggrp.pop(Path(\"example/a.py\"))\nggrp\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{PosixPath('example/b.py'): &lt;mutatest.api.Genome object at 0x7f689cfc8f70&gt;}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># add a Genome to the group\n\nggrp.add_genome(genome_a)\nggrp\n</code></pre> <p>.. parsed-literal::</p> <pre><code>{PosixPath('example/b.py'): &lt;mutatest.api.Genome object at 0x7f689cfc8f70&gt;, PosixPath('example/a.py'): &lt;mutatest.api.Genome object at 0x7f689cfc8c10&gt;}\n</code></pre> <p>.. code:: ipython3</p> <pre><code># the add_folder options provides\n# more flexibility e.g., to include\n# the test_ files.\n\nggrp_with_tests = GenomeGroup()\nggrp_with_tests.add_folder(\n    src_loc, ignore_test_files=False\n)\n\nfor k, v in ggrp_with_tests.items():\n    print(k, v)\n</code></pre> <p>.. parsed-literal::</p> <pre><code>example/a.py &lt;mutatest.api.Genome object at 0x7f68a4044700&gt;\nexample/test_ab.py &lt;mutatest.api.Genome object at 0x7f689cfd7340&gt;\nexample/b.py &lt;mutatest.api.Genome object at 0x7f689cfd74f0&gt;\n</code></pre>"},{"location":"testing/mutatest/#file-docschangelogrst","title":"File: /docs/changelog.rst","text":"<p>.. _Change log:</p> <p>.. include:: ../CHANGELOG.rst</p>"},{"location":"testing/mutatest/#file-docscommandlinerst","title":"File: /docs/commandline.rst","text":"<p>.. _Command Line Controls:</p>"},{"location":"testing/mutatest/#command-line-controls","title":"Command Line Controls","text":""},{"location":"testing/mutatest/#specifying-source-files-and-test-commands","title":"Specifying source files and test commands","text":"<p>If you have a Python package in a directory with an associated <code>tests/</code> folder (or internal <code>test_</code> prefixed files, see the examples below) that are auto-detected with <code>pytest</code>, then you can run <code>mutatest</code> without any arguments.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest\n</code></pre> <p>It will detect the package, and run <code>pytest</code> by default. If you want to run with special arguments, such as to exclude a custom marker, you can pass in the <code>--testcmds</code> argument with the desired string.</p> <p>Here is the command to run <code>pytest</code> and exclude tests marked with <code>pytest.mark.slow</code>.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --testcmds \"pytest -m 'not slow'\"\n\n# using shorthand arguments\n$ mutatest -t \"pytest -m 'not slow'\"\n</code></pre> <p>You can use this syntax if you want to specify a single module in your package to run and test.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --src mypackage/run.py --testcmds \"pytest tests/test_run.py\"\n\n# using shorthand arguments\n$ mutatest -s mypackage/run.py -t \"pytest tests/test_run.py\"\n</code></pre> <p>There is an option to exclude files from the source set. Exclude files using the <code>--exclude</code> argument and pointing to the file. Multiple <code>--exclude</code> statements may be used to exclude multiple files. The default behavior is that no files are excluded.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --exclude mypackage/__init__.py --exclude mypackage/_devtools.py\n\n# using shorthand arguments\n$ mutatest -e mypackage/__init__.py -e mypackage/_devtools.py\n</code></pre> <p>These commands can all be combined in different ways to target your sample space for mutations.</p>"},{"location":"testing/mutatest/#coverage-filtering","title":"Coverage filtering","text":"<p>Any command combination that generates a <code>.coverage</code> file will use that as a restriction mechanism for the sample space to only select mutation locations that are covered. For example, running:</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --testcmds \"pytest --cov=mypackage tests/test_run.py\"\n\n# using shorthand arguments\n$ mutatest -t \"pytest --cov=mypackage tests/test_run.py\"\n</code></pre> <p>would generate the <code>.coverage</code> file based on <code>tests/test_run.py</code>. Therefore, even though the entire package is seen only the lines covered by <code>tests/test_run.py</code> will be mutated during the trials. If you specified a source with <code>-s</code> only the covered lines in that source file would become valid targets for mutation. Excluded files with <code>-e</code> are still skipped. You can override this behavior with the <code>--nocov</code> flag on the command line.</p> <p>If you have a <code>pytest.ini</code> file that includes the <code>--cov</code> command the default behavior of <code>mutatest</code> will generate the coverage file. You will see a message in the CLI output at the beginning of the trials if coverage is ignored.</p> <p>.. code-block:: bash</p> <pre><code># note the smaller sample based on the coverage\n\n$ mutatest -n 4 -t \"pytest --cov=mypackage\"\n\n... prior output...\n\n... Total sample space size: 287\n... Selecting 4 locations from 287 potentials.\n... Starting individual mutation trials!\n\n... continued output...\n\n\n# even with coverage specified the --nocov flag is used\n# sample size is larger, and the note on ignoring is present\n\n$ mutatest -n 4 -t \"pytest --cov=mypackage\" --nocov\n\n... prior output...\n\n... Ignoring coverage file for sample space creation.\n... Total sample space size: 311\n... Selecting 4 locations from 311 potentials.\n... Starting individual mutation trials!\n\n... continued output...\n</code></pre> <p>.. versionadded:: 2.1.0     Support for <code>coverage</code> version 4.x and 5.x.</p>"},{"location":"testing/mutatest/#auto-detected-package-structures","title":"Auto-detected package structures","text":"<p>The following package structures would be auto-detected if you ran <code>mutatest</code> from the same directory holding <code>examplepkg/</code>. You can always point to a specific directory using the <code>--source</code> argument. These are outlined in the <code>Pytest Test Layout</code>_ documentation.</p> <p>Example with internal tests ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>.. code-block:: bash</p> <pre><code>.\n\u2514\u2500\u2500 examplepkg\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 run.py\n    \u2514\u2500\u2500 test_run.py\n</code></pre> <p>Example with external tests ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>.. code-block:: bash</p> <pre><code>.\n\u251c\u2500\u2500 examplepkg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 run.py\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 test_run.py\n</code></pre>"},{"location":"testing/mutatest/#selecting-a-running-mode","title":"Selecting a running mode","text":"<p><code>mutatest</code> has different running modes to make trials faster. The running modes determine what will happen after a mutation trial. For example, you can choose to stop further mutations at a location as soon as a survivor is detected. The different running mode choices are:</p> <p>Run modes:     - f: full mode, run all possible combinations (slowest but most thorough).     - s: break on first SURVIVOR per mutated location e.g. if there is a single surviving mutation       at a location move to the next location without further testing.       This is the default mode.     - d: break on the first DETECTION per mutated location e.g. if there is a detected mutation on       at a location move to the next one.     - sd: break on the first SURVIVOR or DETECTION (fastest, and least thorough).</p> <p>The API for <code>mutatest.controller.run_mutation_trials</code> offers finer control over the run method beyond the CLI.</p> <p>A good practice when first starting is to set the mode to <code>sd</code> which will stop if a mutation survives or is detected, effectively running a single mutation per candidate location. This is the fastest running mode and can give you a sense of investigation areas quickly.</p> <p>.. code-block::</p> <pre><code>$ mutatest --mode sd\n\n# using shorthand arguments\n$ mutatest -m sd\n</code></pre>"},{"location":"testing/mutatest/#controlling-randomization-behavior-and-trial-number","title":"Controlling randomization behavior and trial number","text":"<p><code>mutatest</code> uses random sampling of all source candidate locations and of potential mutations to substitute at a location. You can set a random seed for repeatable trials using the <code>--rseed</code> argument. The <code>--nlocations</code> argument controls the size of the sample of locations to mutate. If it exceeds the number of candidate locations then the full set of candidate locations is used.</p> <p>.. code-block::</p> <pre><code>$ mutatest --nlocations 5 --rseed 314\n\n# using shorthand arguments\n$ mutatest -n 5 -r 314\n</code></pre>"},{"location":"testing/mutatest/#selecting-categories-of-mutations","title":"Selecting categories of mutations","text":"<p><code>mutatest</code> categorizes families of mutations with two-letter category codes (available in the help output and in the mutants section below). You can use these category codes in the <code>--only</code> and <code>--skip</code> arguments to opt-in or opt-out of types of mutations for your trials. This impacts the pool of potential locations to draw from for the sample, but the number of mutations specified in <code>--nlocations</code> still determines the final sample size. You will see the categories used in the output during the trial. Categories are space delimited as an input list on the CLI.</p> <p>.. code-block::</p> <pre><code># selects only the categories \"aa\" (AugAssign), \"bn\" (BinOp), and \"ix\" (Index) mutations\n$ mutatest --only aa bn ix\n\n... prior output...\n\n... Category restriction, chosen categories: ['aa', 'bn', 'ix']\n... Setting random.seed to: None\n... Total sample space size: 311\n... Selecting 10 locations from 311 potentials.\n... Starting individual mutation trials!\n\n... continued output...\n\n# using shorthand\n$ mutatest -y aa bn ix\n\n# using the skip list instead, selects all categories except \"aa\", \"bn\", and \"ix\"\n$ mutatest --skip aa bn ix\n\n# with shorthand\n$ mutatest -k aa bn ix\n</code></pre>"},{"location":"testing/mutatest/#setting-the-output-location","title":"Setting the output location","text":"<p>By default, <code>mutatest</code> will only create CLI output to <code>stdout</code>. You can set path location using the <code>--output</code> argument for a written RST report of the mutation trial results.</p> <p>.. code-block::</p> <pre><code>$ mutatest --output path/to/my_custom_file.rst\n\n# using shorthand arguments\n$ mutatest -o path/to/my_custom_file.rst\n</code></pre> <p>The output report will include the arguments used to generate it along with the total runtimes. The SURVIVORS section of the output report is the one you should pay attention to. These are the mutations that were undetected by your test suite. The report includes file names, line numbers, column numbers, original operation, and mutation for ease of diagnostic investigation.</p>"},{"location":"testing/mutatest/#raising-exceptions-for-survivor-tolerances","title":"Raising exceptions for survivor tolerances","text":"<p>By default, <code>mutatest</code> will only display output and not raise any final exceptions if there are survivors in the trial results. You can set a tolerance number using the <code>--exception</code> or <code>-x</code> argument that will raise an exception if that number if met or exceeded for the count of survivors after the trials. This argument is included for use in automated running of <code>mutatest</code> e.g. as a stage in continuous integration.</p> <p>When combined with the random seed and category selection you can have targeted stages for important sections of code where you want a low count of surviving mutations enforced.</p> <p>.. code-block::</p> <pre><code>$ mutatest --exception 5\n\n# using shorthand arguments\n$ mutatest -x 5\n</code></pre> <p>The exception type is a <code>SurvivingMutantException</code>:</p> <p>.. code-block::</p> <pre><code>... prior output from trial...\n\nmutatest.cli.SurvivingMutantException: Survivor tolerance breached: 8 / 2\n</code></pre>"},{"location":"testing/mutatest/#controlling-trial-timeout-behavior","title":"Controlling trial timeout behavior","text":"<p>.. versionadded:: 1.2     The <code>--timeout_factor</code> argument.</p> <p>Typically mutation trials take approximately the same time as the first clean trial with some small variance. There are instances where a mutation could cause source code to enter an infinite loop, such as changing a <code>while</code> statement using a comparison operation like <code>&lt;</code> to <code>&gt;</code> or <code>==</code>. To protect against these effects a <code>--timeout_factor</code> controls a multiplier of the first clean run that will act as the timeout cap for any mutation trials. For example, if the clean trial takes 2 seconds, and the <code>--timeout_factor</code> is set to 5 (the default value), the maximum run time for a mutation trial before being stopped and logged as a <code>TIMEOUT</code> is 10 seconds (2 seconds * 5).</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --timeout_factor=1.5\n</code></pre> <p>Note that if you set the <code>--timeout_factor</code> to be exactly 1 you will likely get timeout trials by natural variance in logging success vs. failure.</p> <p>.. _Parallelization:</p>"},{"location":"testing/mutatest/#parallelization","title":"Parallelization","text":"<p>.. versionadded:: 3.0.0     Support for multiprocessing parallelization in Python 3.8.</p> <p>The <code>--parallel</code> argument can be used if you are running with Python 3.8 to enable multiprocessing of mutation trials. This argument has no effect if you are running Python 3.7. Parallelism is achieved by creating parallel cache directories in a <code>.mutatest_cache/</code> folder in the current working directory. Unique folders for each trial are created and the subprocess command sets <code>PYTHONPYCACHEPREFIX</code> per trial. These sub-folders, and the top level <code>.mutatest_cache/</code> directory, are removed when the trials are complete. Multiprocessing uses all CPUs detected by <code>os.cpu_count()</code> in the pool.</p> <p>The parallel cache adds some IO overhead to the trial process. You will get the most benefit from multiprocessing if you are running a longer test suite or a high number of trials. All trials get an additional 10 seconds added to the maximum timeout calculation as a buffer for gathering results. If you notice excessive false positive timeouts try running without parallelization.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --parallel\n</code></pre>"},{"location":"testing/mutatest/#putting-it-all-together","title":"Putting it all together","text":"<p>If you want to run 5 trials, in fast <code>sd</code> mode, with a random seed of 345 and an output file name of <code>mutation_345.rst</code>, you would do the following if your directory structure has a Python package folder and tests that are auto-discoverable and run by <code>pytest</code>.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest -n 5 -m sd -r 345 -o mutation_345.rst\n</code></pre> <p>With <code>coverage</code> optimization if your <code>pytest.ini</code> file does not already specify it:</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest -n 5 -m sd -r 345 -o mutation_345.rst -t \"pytest --cov=mypackage\"\n</code></pre>"},{"location":"testing/mutatest/#getting-help","title":"Getting help","text":"<p>Run <code>mutatest --help</code> to see command line arguments and supported operations:</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --help\n\nusage: Mutatest [-h] [-b [STR [STR ...]]] [-e PATH] [-m {f,s,d,sd}] [-n INT]\n                [-o PATH] [-r INT] [-s PATH] [-t STR_CMDS]\n                [-w [STR [STR ...]]] [-x INT] [--debug] [--nocov] [--parallel]\n                [--timeout_factor FLOAT &gt; 1]\n\nPython mutation testing. Mutatest will manipulate local __pycache__ files.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -k [STR [STR ...]], --skip [STR [STR ...]]\n                        Mutation categories to skip for trials. (default: empty list)\n  -e PATH, --exclude PATH\n                        Path to .py file to exclude, multiple -e entries supported. (default: None)\n  -m {f,s,d,sd}, --mode {f,s,d,sd}\n                        Running modes, see the choice option descriptions below. (default: s)\n  -n INT, --nlocations INT\n                        Number of locations in code to randomly select for mutation from possible targets. (default: 10)\n  -o PATH, --output PATH\n                        Output RST file location for results. (default: No output written)\n  -r INT, --rseed INT   Random seed to use for sample selection.\n  -s PATH, --src PATH   Source code (file or directory) for mutation testing. (default: auto-detection attempt).\n  -t STR_CMDS, --testcmds STR_CMDS\n                        Test command string to execute. (default: 'pytest')\n  -y [STR [STR ...]], --only [STR [STR ...]]\n                        Only mutation categories to use for trials. (default: empty list)\n  -x INT, --exception INT\n                        Count of survivors to raise Mutation Exception for system exit.\n  --debug               Turn on DEBUG level logging output.\n  --nocov               Ignore coverage files for optimization.\n  --parallel            Run with multiprocessing (Py3.8 only).\n  --timeout_factor FLOAT &gt; 1\n                        If a mutation trial running time is beyond this factor multiplied by the first\n                        clean trial running time then that mutation trial is aborted and logged as a timeout.\n</code></pre>"},{"location":"testing/mutatest/#using-a-config-file","title":"Using a config file","text":"<p>.. versionadded:: 2.2.0     Support for <code>setup.cfg</code> as an optional settings file.</p> <p>Arguments for <code>mutatest</code> can be stored in a <code>mutatest.ini</code> config file in the directory where you run the command. Use the full argument names and either spaces or newlines to separate multiple values for a given argument. The flag commands (<code>--debug</code> and <code>--nocov</code>) are given boolean flags that can be interpreted by the Python <code>ConfigParser</code>. Command line arguments passed to <code>mutatest</code> will override the values in the <code>ini</code> file. Any command line arguments that are not in the <code>ini</code> file will be added to the execution parameters along with the config file values.</p> <p>Alternatively, you may use <code>setup.cfg</code> with either a <code>[mutatest]</code> or <code>[tool:mutatest]</code> entry. The <code>mutatest.ini</code> file will be used first if it is present, skipping <code>setup.cfg</code>. <code>setup.cfg</code> will honor the <code>[mutatest]</code> and <code>[tool:mutatest]</code> in that order. Entries are not combined if both are present.</p> <p>Example config file ~~~~~~~~~~~~~~~~~~~</p> <p>The contents of an example <code>mutatest.ini</code> or entry in <code>setup.cfg</code>:</p> <p>.. code-block:: ini</p> <p>[mutatest]</p> <p>skip = nc su ix    exclude =        mutatest/init.py        mutatest/_devtools.py    mode = sd    rseed = 567    testcmds = pytest -m 'not slow'    debug = no    nocov = no</p> <p>.. target-notes:: .. _Pytest Test Layout: https://docs.pytest.org/en/latest/goodpractices.html#choosing-a-test-layout-import-rules .. _Python AST grammar: https://docs.python.org/3/library/ast.html#abstract-grammar</p>"},{"location":"testing/mutatest/#file-docscontributingrst","title":"File: /docs/contributing.rst","text":"<p>.. _contributing:</p> <p>.. include:: ../CONTRIBUTING.rst</p>"},{"location":"testing/mutatest/#file-docsindexrst","title":"File: /docs/index.rst","text":""},{"location":"testing/mutatest/#mutatest-python-mutation-testing","title":"Mutatest: Python mutation testing","text":"<p>|  |py-versions| |license| |ci-azure| |ci-travis| |docs| |coverage| |black| |  |pypi-version| |pypi-status| |pypi-format| |pypi-downloads| |  |conda-version| |conda-recipe| |conda-platform| |conda-downloads|</p> <p>Are you confident in your tests? Try out <code>mutatest</code> and see if your tests will detect small modifications (mutations) in the code. Surviving mutations represent subtle changes that are undetectable by your tests. These mutants are potential modifications in source code that continuous integration checks would miss.</p>"},{"location":"testing/mutatest/#features","title":"Features:","text":"<pre><code>- Simple command line tool with `multiple configuration options &lt;https://mutatest.readthedocs.io/en/latest/commandline.html&gt;`_.\n- Built on Python's Abstract Syntax Tree (AST) grammar to ensure `mutants are valid &lt;https://mutatest.readthedocs.io/en/latest/mutants.html&gt;`_.\n- `No source code modification &lt;https://mutatest.readthedocs.io/en/latest/install.html#mutation-trial-process&gt;`_,\n  only the ``__pycache__`` is changed.\n- Uses ``coverage`` to create `only meaningful mutants &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#coverage-filtering&gt;`_.\n- Built for efficiency with `multiple running modes &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#selecting-a-running-mode&gt;`_\n  and `random sampling of mutation targets &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#controlling-randomization-behavior-and-trial-number&gt;`_.\n- Capable of running `parallel mutation trials &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#parallelization&gt;`_\n  with multiprocessing on Python 3.8.\n- Flexible enough to run on a `whole package &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#auto-detected-package-structures&gt;`_\n  or a `single file &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#specifying-source-files-and-test-commands&gt;`_.\n- Includes an `API for custom mutation controls &lt;https://mutatest.readthedocs.io/en/latest/modules.html&gt;`_.\n- Tested on Linux, Windows, and MacOS with `Azure pipelines &lt;https://dev.azure.com/evankepner/mutatest/_build/latest?definitionId=1&amp;branchName=master&gt;`_.\n- Full strict static type annotations throughout the source code and the API.\n</code></pre>"},{"location":"testing/mutatest/#quick-start","title":"Quick Start","text":"<p><code>mutatest</code> requires Python 3.7 or 3.8.</p> <p>Install from <code>PyPI &lt;https://pypi.org/project/mutatest/&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ pip install mutatest\n</code></pre> <p>Install from <code>conda-forge &lt;https://anaconda.org/conda-forge/mutatest&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ conda install -c conda-forge mutatest\n</code></pre> <p>Alternatively, clone the repo from <code>GitHub &lt;https://github.com/EvanKepner/mutatest&gt;</code>_ and install from the source code:</p> <p>.. code-block:: bash</p> <pre><code>$ cd mutatest\n$ pip install .\n</code></pre> <p><code>mutatest</code> is designed to work when your test files are separated from your source directory and are prefixed with <code>test_</code>. See <code>Pytest Test Layout &lt;https://docs.pytest.org/en/latest/goodpractices.html#choosing-a-test-layout-import-rules&gt;</code>_ for more details.</p> <p><code>mutatest</code> is a diagnostic command line tool for your test coverage assessment. If you have a Python package in with an associated <code>tests/</code> folder, or internal <code>test_</code> prefixed files, that are auto-detected with <code>pytest</code>, then you can run <code>mutatest</code> without any arguments.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest\n</code></pre> <p>See more examples with additional configuration options in :ref:<code>Command Line Controls</code>.</p> <p>Help ~~~~</p> <p>Run <code>mutatest --help</code> to see command line arguments and supported operations:</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest --help\n\nusage: Mutatest [-h] [-b [STR [STR ...]]] [-e PATH] [-m {f,s,d,sd}] [-n INT]\n                [-o PATH] [-r INT] [-s PATH] [-t STR_CMDS]\n                [-w [STR [STR ...]]] [-x INT] [--debug] [--nocov] [--parallel]\n                [--timeout_factor FLOAT &gt; 1]\n\nPython mutation testing. Mutatest will manipulate local __pycache__ files.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -k [STR [STR ...]], --skip [STR [STR ...]]\n                        Mutation categories to skip for trials. (default: empty list)\n  -e PATH, --exclude PATH\n                        Path to .py file to exclude, multiple -e entries supported. (default: None)\n  -m {f,s,d,sd}, --mode {f,s,d,sd}\n                        Running modes, see the choice option descriptions below. (default: s)\n  -n INT, --nlocations INT\n                        Number of locations in code to randomly select for mutation from possible targets. (default: 10)\n  -o PATH, --output PATH\n                        Output RST file location for results. (default: No output written)\n  -r INT, --rseed INT   Random seed to use for sample selection.\n  -s PATH, --src PATH   Source code (file or directory) for mutation testing. (default: auto-detection attempt).\n  -t STR_CMDS, --testcmds STR_CMDS\n                        Test command string to execute. (default: 'pytest')\n  -y [STR [STR ...]], --only [STR [STR ...]]\n                        Only mutation categories to use for trials. (default: empty list)\n  -x INT, --exception INT\n                        Count of survivors to raise Mutation Exception for system exit.\n  --debug               Turn on DEBUG level logging output.\n  --nocov               Ignore coverage files for optimization.\n  --parallel            Run with multiprocessing (Py3.8 only).\n  --timeout_factor FLOAT &gt; 1\n                        If a mutation trial running time is beyond this factor multiplied by the first\n                        clean trial running time then that mutation trial is aborted and logged as a timeout.\n</code></pre> <p>Example Output ~~~~~~~~~~~~~~</p> <p>This is an output example running mutation trials against the :ref:<code>API Tutorial</code> example folder.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest -s example/ -t \"pytest\" -r 314\n\nRunning clean trial\n2 mutation targets found in example/a.py AST.\n1 mutation targets found in example/b.py AST.\nSetting random.seed to: 314\nTotal sample space size: 2\n10 exceeds sample space, using full sample: 2.\n\nStarting individual mutation trials!\nCurrent target location: a.py, LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;)\nDetected mutation at example/a.py: (6, 11)\nDetected mutation at example/a.py: (6, 11)\nSurviving mutation at example/a.py: (6, 11)\nBreak on survival: stopping further mutations at location.\n\nCurrent target location: b.py, LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;)\nDetected mutation at example/b.py: (6, 11)\nRunning clean trial\n\nMutatest diagnostic summary\n===========================\n - Source location: /home/user/Github/mutatest/docs/api_tutorial/example\n - Test commands: ['pytest']\n - Mode: s\n - Excluded files: []\n - N locations input: 10\n - Random seed: 314\n\nRandom sample details\n---------------------\n - Total locations mutated: 2\n - Total locations identified: 2\n - Location sample coverage: 100.00 %\n\n\nRunning time details\n--------------------\n - Clean trial 1 run time: 0:00:00.348999\n - Clean trial 2 run time: 0:00:00.350213\n - Mutation trials total run time: 0:00:01.389095\n\nTrial Summary Report:\n\nOverall mutation trial summary\n==============================\n - DETECTED: 3\n - SURVIVED: 1\n - TOTAL RUNS: 4\n - RUN DATETIME: 2019-10-17 16:57:08.645355\n\nDetected mutations:\n\nDETECTED\n--------\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Sub'&gt;\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Mod'&gt;\n - example/b.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Is'&gt; to &lt;class '_ast.IsNot'&gt;\n\nSurviving mutations:\n\nSURVIVED\n--------\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Mult'&gt;\n</code></pre>"},{"location":"testing/mutatest/#contents","title":"Contents","text":"<p>.. toctree::    :maxdepth: 4</p> <p>install    commandline    mutants    api_tutorial/api_tutorial    modules    license    changelog    contributing    GitHub https://github.com/EvanKepner/mutatest</p>"},{"location":"testing/mutatest/#indices-and-tables","title":"Indices and tables","text":"<ul> <li>:ref:<code>genindex</code></li> <li>:ref:<code>modindex</code></li> <li>:ref:<code>search</code></li> </ul> <p>.. |py-versions| image:: https://img.shields.io/pypi/pyversions/mutatest?color=green     :target: https://pypi.org/project/mutatest/     :alt: Python versions .. |license| image:: https://img.shields.io/pypi/l/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: License .. |pypi-version| image:: https://badge.fury.io/py/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI version .. |pypi-status| image:: https://img.shields.io/pypi/status/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI status .. |pypi-format| image:: https://img.shields.io/pypi/format/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI Format .. |pypi-downloads| image:: https://pepy.tech/badge/mutatest     :target: https://pepy.tech/project/mutatest     :alt: PyPI Downloads .. |ci-travis| image:: https://travis-ci.org/EvanKepner/mutatest.svg?branch=master     :target: https://travis-ci.org/EvanKepner/mutatest     :alt: TravisCI .. |ci-azure| image:: https://dev.azure.com/evankepner/mutatest/_apis/build/status/EvanKepner.mutatest?branchName=master     :target: https://dev.azure.com/evankepner/mutatest/_build/latest?definitionId=1&amp;branchName=master     :alt: Azure Pipelines .. |docs| image:: https://readthedocs.org/projects/mutatest/badge/?version=latest     :target: https://mutatest.readthedocs.io/en/latest/?badge=latest     :alt: RTD status .. |coverage| image:: https://codecov.io/gh/EvanKepner/mutatest/branch/master/graph/badge.svg     :target: https://codecov.io/gh/EvanKepner/mutatest     :alt: CodeCov .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg     :target: https://github.com/psf/black     :alt: Black .. |conda-recipe| image:: https://img.shields.io/badge/recipe-mutatest-green.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda recipe .. |conda-version| image:: https://img.shields.io/conda/vn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda version .. |conda-platform| image:: https://img.shields.io/conda/pn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda platforms .. |conda-azure| image:: https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/mutatest-feedstock?branchName=master     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda azure status .. |conda-downloads| image:: https://img.shields.io/conda/dn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda downloads</p>"},{"location":"testing/mutatest/#file-docsinstallrst","title":"File: /docs/install.rst","text":"<p>.. _Installation:</p>"},{"location":"testing/mutatest/#installation","title":"Installation","text":"<p><code>mutatest</code> requires Python 3.7 or Python 3.8.</p> <p>Install from <code>PyPI &lt;https://pypi.org/project/mutatest/&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ pip install mutatest\n</code></pre> <p>Install from <code>conda-forge &lt;https://anaconda.org/conda-forge/mutatest&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ conda install -c conda-forge mutatest\n</code></pre> <p>Alternatively, clone this repo and install locally:</p> <p>.. code-block:: bash</p> <pre><code>$ cd mutatest\n$ pip install .\n</code></pre> <p><code>mutatest</code> is designed to work when your test files are separated from your source directory and are prefixed with <code>test_</code>. See <code>Pytest Test Layout</code>_ for more details.</p> <p>.. _Mutation Trial Process:</p>"},{"location":"testing/mutatest/#mutation-trial-process","title":"Mutation Trial Process","text":"<p><code>mutatest</code> is designed to be a diagnostic command line tool for your test coverage assessment.</p> <p>The mutation trial process follows these steps when <code>mutatest</code> is run from the CLI:</p> <ol> <li>Scan for your existing Python package, or use the input source location.</li> <li>Create an abstract syntax tree (AST) from the source files.</li> <li>Identify locations in the code that may be mutated (line and column). If you are running with    <code>coverage</code> the sample is restricted only to lines that are marked as covered in the    <code>.coverage</code> file.</li> <li>Take a random sample of the identified locations.</li> <li>Apply a mutation at the location by modifying a copy of the AST and writing a new cache file    to the appropriate <code>__pycache__</code> location with the source file statistics.</li> <li>Run the test suite. This will use the mutated <code>__pycache__</code> file since the source statistics    are the same for modification time.</li> <li>See if the test suite detected the mutation by a failed test.</li> <li>Remove the modified <code>__pycache__</code> file.</li> <li>Repeat steps 5-9 for the remaining selected locations to mutate.</li> <li>Write an output report of the various mutation results.</li> </ol> <p>A \"clean trial\" of your tests is run before any mutations are applied. This same \"clean trial\" is run at the end of the mutation testing. This ensures that your original test suite passes before attempting to detect surviving mutations and that the <code>__pycache__</code> has been appropriately reset when the mutation trials are finished.</p> <p>.. _Motivation:</p>"},{"location":"testing/mutatest/#motivation-and-faqs","title":"Motivation and FAQs","text":""},{"location":"testing/mutatest/#mutation-testing-overview","title":"Mutation Testing Overview","text":"<p>Mutation testing is designed to assess the quality of other testing; typically, unit tests. The idea is that unit tests should fail given a specific mutation in a tested function. For example, if a new contributor were to submit a pull request for an important numerical library and accidentally typo a <code>&gt;</code> to be <code>&gt;=</code> in an existing tested function, the maintainer should expect that the change is detected through unit test failure. Mutation testing is a way to ensure this assumption is valid. Essentially, mutation testing is a test of the alarm system created by the unit tests.</p>"},{"location":"testing/mutatest/#why-random-sampling-instead-of-all-possible-mutants","title":"Why random sampling instead of all possible mutants?","text":"<p>By nature, mutation testing can be slow. You have to make a small modification in your source code and then see if your test suite fails. For fast tests and smaller projects running every possible mutation may be feasible. For larger projects, this could be prohibitively expensive in time. Random sampling of the target locations, as well as of the mutations to apply, takes advantage of the \"alarm testing\" nature of mutation testing. You do not need to exhaustively test every mutation to have a good understanding of whether or not your test suite is generally sufficient to detect these changes, and it provides a sense of the types of mutations that could slip past your unit tests. Using the source and test commands targeting, as well as the category filters, you can create specific mutation trials for important components of your code. Setting a <code>random seed &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#controlling-randomization-behavior-and-trial-number&gt;</code>_ on the command line ensures reproducibility for the same set of arguments.</p>"},{"location":"testing/mutatest/#why-modify-the-pycache","title":"Why modify the pycache?","text":"<p>In short, protection of source code. A goal of <code>mutatest</code> is to avoid source code modification so that mutations are not accidentally committed to version control. Writing the mutations from memory to the <code>__pycache__</code> is a safety mechanism to ensure that the worst-case scenario of a killed process in a trial is to clear you cache.</p>"},{"location":"testing/mutatest/#can-i-use-mutatest-in-cicd","title":"Can I use mutatest in CICD?","text":"<p>Yes, though because of the slow nature of running your test suite multiple times it is not something you would run across your entire codebase on every commit. <code>Mutatest</code> includes an option to <code>raise survivor exceptions &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#raising-exceptions-for-survivor-tolerances&gt;</code>_ based on a tolerance level e.g., you may tolerate up to 2 surviving mutants (you set the threshold) out of 20 with specific pieces of your source code. <code>Mutatest</code> is most useful as a diagnostic tool to determine weak spots in your overall test structure.</p>"},{"location":"testing/mutatest/#are-there-differences-in-running-with-python-37-vs-python-38","title":"Are there differences in running with Python 3.7 vs. Python 3.8?","text":"<p>.. versionadded:: 2.0.0     Support for Python 3.8 .. versionadded:: 3.0.0     Multiprocessing parallelization in Python 3.8</p> <p>Yes, though they do not impact the command line interface experience. In Python 3.8, the <code>NamedConstant</code> node type was deprecated in favor of <code>Constant</code>, and new location attributes were added to individual nodes: <code>end_lineno</code> and <code>end_col_offset</code>. These changes are accounted for in the <code>transformers</code> module. Running with Python 3.7 the <code>LocIndex.end_lineno</code> and <code>LocIndex.end_col_offset</code> will always be set to <code>None</code>, and in Python 3.8 these values are populated based on the AST. Additional information is on <code>Python 3.8 What's New Improved Modules</code>_.</p> <p>Python 3.8 also supports a parallel pycache directory. This is used to enable multiprocessing of mutation trials with the <code>--parallel</code> argument. Parallelization is not supported on Python 3.7.</p>"},{"location":"testing/mutatest/#known-limitations","title":"Known limitations","text":"<p>Since <code>mutatest</code> operates on the local <code>__pycache__</code> it is a serial execution process. This means it will take as long as running your test suite in series for the number of operations. It's designed as a diagnostic tool, and you should try to find the combination of test commands, source specifiers, and exclusions that generate meaningful diagnostics. For example, if you have 600 tests, running <code>mutatest</code> over the entire test suite may take some time. A better strategy would be:</p> <ol> <li>Select a subset of your tests and run <code>pytest</code> with <code>coverage</code> to see the    covered percentage per source file.</li> <li>Run <code>mutatest</code> with the same <code>pytest</code> command passed in with <code>-t</code> and generating    a coverage file. Use <code>-s</code> to pick the source file of interest to restrict the sample space,    or use <code>-e</code> to exclude files if you want to target multiple files.</li> </ol> <p>If you kill the <code>mutatest</code> process before the trials complete you may end up with partially mutated <code>__pycache__</code> files. If this happens the best fix is to remove the <code>__pycache__</code> directories and let them rebuild automatically the next time your package is imported (for instance, by re-running your test suite).</p> <p>The mutation status is based on the return code of the test suite e.g. 0 for success, 1 for failure. <code>mutatest</code> can theoretically be run with any test suite that you pass with the <code>--testcmds</code> argument; however, only <code>pytest</code> has been tested to date. The <code>mutatest.run.MutantTrialResult</code> contains the definitions for translating return codes into mutation trial statuses.</p> <p>.. target-notes:: .. _Pytest Test Layout: https://docs.pytest.org/en/latest/goodpractices.html#choosing-a-test-layout-import-rules .. _Python 3.8 What's New Improved Modules: https://docs.python.org/3/whatsnew/3.8.html#ast</p>"},{"location":"testing/mutatest/#file-docslicenserst","title":"File: /docs/license.rst","text":"<p>.. _license:</p>"},{"location":"testing/mutatest/#license","title":"License","text":"<p>Distributed under the terms of the <code>MIT</code>_ license, <code>mutatest</code> is free and open source software.</p> <p>.. literalinclude:: ../LICENSE</p> <p>.. target-notes:: .. _<code>MIT</code>: https://github.com/EvanKepner/mutatest/blob/master/LICENSE</p>"},{"location":"testing/mutatest/#file-docsmodulesrst","title":"File: /docs/modules.rst","text":"<p>.. _API Reference:</p>"},{"location":"testing/mutatest/#api-reference","title":"API Reference","text":"<p>.. automodule:: mutatest.api    :members:</p> <p>.. automodule:: mutatest.cache    :members:</p> <p>.. automodule:: mutatest.cli    :members:</p> <p>.. automodule:: mutatest.filters    :members:</p> <p>.. automodule:: mutatest.report    :members:</p> <p>.. automodule:: mutatest.run    :members:</p> <p>.. automodule:: mutatest.transformers    :members:</p>"},{"location":"testing/mutatest/#file-docsmutantsrst","title":"File: /docs/mutants.rst","text":"<p>.. _Mutations:</p>"},{"location":"testing/mutatest/#mutations","title":"Mutations","text":"<p><code>mutatest</code> supports the following mutation operations based on the <code>Python AST grammar</code>_:</p> <p>Supported operations:     - <code>AugAssign</code> mutations e.g. <code>+= -= *= /=</code>.     - <code>BinOp</code> mutations e.g. <code>+ - / *</code>.     - <code>BinOp Bitwise Comparison</code> mutations e.g. <code>x&amp;y x|y x^y</code>.     - <code>BinOp Bitwise Shift</code> mutations e.g. <code>&lt;&lt; &gt;&gt;</code>.     - <code>BoolOp</code> mutations e.g. <code>and or</code>.     - <code>Compare</code> mutations e.g. <code>== &gt;= &lt; &lt;= !=</code>.     - <code>Compare In</code> mutations e.g. <code>in, not in</code>.     - <code>Compare Is</code> mutations e.g. <code>is, is not</code>.     - <code>If</code> mutations e.g. <code>If x &gt; y</code> becomes <code>If True</code> or <code>If False</code>.     - <code>Index</code> mutations e.g. <code>i[0]</code> becomes <code>i[1]</code> and <code>i[-1]</code>.     - <code>NameConstant</code> mutations e.g. <code>True</code>, <code>False</code>, and <code>None</code>.     - <code>Slice</code> mutations e.g. changing <code>x[:2]</code> to <code>x[2:]</code>.</p> <p>These are the current operations that are mutated as compatible sets. The two-letter category code for white/black-list selection is beside the name in double quotes.</p>"},{"location":"testing/mutatest/#augassign-aa","title":"AugAssign - \"aa\"","text":"<p>Augmented assignment e.g. <code>+= -= /= *=</code>.</p> <p>Members:     - <code>AugAssign_Add</code>     - <code>AugAssign_Div</code>     - <code>AugAssign_Mult</code>     - <code>AugAssign_Sub</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx += y\n\n# mutations\nx -= y  # AugAssign_Sub\nx *= y  # AugAssign_Mult\nx /= y  # AugAssign_Div\n</code></pre>"},{"location":"testing/mutatest/#binop-bn","title":"BinOp - \"bn\"","text":"<p>Binary operations e.g. add, subtract, divide, etc.</p> <p>Members:     - <code>ast.Add</code>     - <code>ast.Div</code>     - <code>ast.FloorDiv</code>     - <code>ast.Mod</code>     - <code>ast.Mult</code>     - <code>ast.Pow</code>     - <code>ast.Sub</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx = a + b\n\n# mutations\nx = a / b  # ast.Div\nx = a - b  # ast.Sub\n</code></pre>"},{"location":"testing/mutatest/#binop-bit-comparison-bc","title":"BinOp Bit Comparison - \"bc\"","text":"<p>Bitwise comparison operations e.g. <code>x &amp; y, x | y, x ^ y</code>.</p> <p>Members:     - <code>ast.BitAnd</code>     - <code>ast.BitOr</code>     - <code>ast.BitXor</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx = a &amp; y\n\n# mutations\nx = a | y  # ast.BitOr\nx = a ^ y  # ast.BitXor\n</code></pre>"},{"location":"testing/mutatest/#binop-bit-shifts-bs","title":"BinOp Bit Shifts - \"bs\"","text":"<p>Bitwise shift operations e.g. <code>&lt;&lt; &gt;&gt;</code>.</p> <p>Members:     - <code>ast.LShift</code>     - <code>ast.RShift</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx &gt;&gt; y\n\n# mutation\nx &lt;&lt; y\n</code></pre>"},{"location":"testing/mutatest/#boolop-bl","title":"BoolOp - \"bl\"","text":"<p>Boolean operations e.g. <code>and or</code>.</p> <p>Members:     - <code>ast.And</code>     - <code>ast.Or</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nif x and y:\n\n# mutation\nif x or y:\n</code></pre>"},{"location":"testing/mutatest/#compare-cp","title":"Compare - \"cp\"","text":"<p>Comparison operations e.g. <code>== &gt;= &lt;= &gt; &lt;</code>.</p> <p>Members:     - <code>ast.Eq</code>     - <code>ast.Gt</code>     - <code>ast.GtE</code>     - <code>ast.Lt</code>     - <code>ast.LtE</code>     - <code>ast.NotEq</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx &gt;= y\n\n# mutations\nx &lt; y  # ast.Lt\nx &gt; y  # ast.Gt\nx != y  # ast.NotEq\n</code></pre>"},{"location":"testing/mutatest/#compare-in-cn","title":"Compare In - \"cn\"","text":"<p>Compare membership e.g. <code>in, not in</code>.</p> <p>Members:     - <code>ast.In</code>     - <code>ast.NotIn</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx in [1, 2, 3, 4]\n\n# mutation\nx not in [1, 2, 3, 4]\n</code></pre>"},{"location":"testing/mutatest/#compare-is-cs","title":"Compare Is - \"cs\"","text":"<p>Comapre identity e.g. <code>is, is not</code>.</p> <p>Members:     - <code>ast.Is</code>     - <code>ast.IsNot</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx is None\n\n# mutation\nx is not None\n</code></pre>"},{"location":"testing/mutatest/#if-if","title":"If - \"if\"","text":"<p>If mutations change <code>if</code> statements to always be <code>True</code> or <code>False</code>. The original statement is represented by the class <code>If_Statement</code> in reporting.</p> <p>Members:     - <code>If_False</code>     - <code>If_Statement</code>     - <code>If_True</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nif a &gt; b:   # If_Statement\n    ...\n\n# Mutations\nif True:   # If_True\n    ...\n\nif False:  # If_False\n    ...\n</code></pre>"},{"location":"testing/mutatest/#index-ix","title":"Index - \"ix\"","text":"<p>Index values for iterables e.g. <code>i[-1], i[0], i[0][1]</code>. It is worth noting that this is a unique mutation form in that any index value that is positive will be marked as <code>Index_NumPos</code> and the same relative behavior will happen for negative index values to <code>Index_NumNeg</code>. During the mutation process there are three possible outcomes: the index is set to 0, -1 or 1. The alternate values are chosen as potential mutations e.g. if the original operation is classified as <code>Index_NumPos</code> such as <code>x[10]</code> then valid mutations are to <code>x[0]</code> or <code>x[-1]</code>.</p> <p>Members:     - <code>Index_NumNeg</code>     - <code>Index_NumPos</code>     - <code>Index_NumZero</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx = [a[10], a[-4], a[0]]\n\n# mutations\nx = [a[-1], a[-4], a[0]]  # a[10] mutated to Index_NumNeg\nx = [a[10], a[0], a[0]]  # a[-4] mutated to Index_NumZero\nx = [a[10], a[1], a[0]]  # a[-4] mutated to Index_NumPos\nx = [a[10], a[-4], a[1]]  # a[0] mutated to Index_NumPos\n</code></pre>"},{"location":"testing/mutatest/#nameconstant-nc","title":"NameConstant - \"nc\"","text":"<p>Named constant mutations e.g. <code>True, False, None</code>.</p> <p>Members:     - <code>False</code>     - <code>None</code>     - <code>True</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nx = True\n\n# mutations\nx = False\nX = None\n</code></pre>"},{"location":"testing/mutatest/#slices-su","title":"Slices - \"su\"","text":"<p>Slice mutations to swap lower/upper values, or change range e.g. <code>x[2:] to x[:2]</code>. This is a unique mutation. If the upper or lower bound is set to <code>None</code> then the bound values are swapped. This is represented by the operations of <code>Slice_UnboundedUpper</code> for swap None to the \"upper\" value  from \"lower\". The category code for this type of mutation is \"su\".</p> <p>Members:     - <code>Slice_Unbounded</code>     - <code>Slice_UnboundedLower</code>     - <code>Slice_UnboundedUpper</code></p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code># source code\nw = a[:2]\nx = a[4:]\n\n# mutation\nw = a[2:]  # Slice_UnboundedUpper, upper is now unbounded and lower has a value\nx = a[4:]\n\n# mutation\nw = a[:2]\nx = a[:4]  # Slice_UnboundedLower, lower is now unbounded and upper has a value\n\n# mutation\nw = a[:2]\nx = a[:]  # Slice_Unbounded, both upper and lower are unbounded\n</code></pre> <p>.. target-notes:: .. _Python AST grammar: https://docs.python.org/3/library/ast.html#abstract-grammar</p>"},{"location":"testing/mutatest/#file-authorsrst","title":"File: /AUTHORS.rst","text":""},{"location":"testing/mutatest/#authors","title":"Authors","text":"<p><code>mutatest</code> is written and maintained by Evan Kepner.</p> <p>See the <code>Contributing Guidelines &lt;https://mutatest.readthedocs.io/en/latest/contributing.html&gt;</code> if you are interested in submitting code in the form of pull requests. Contributors are listed here, and can be seen on the <code>GitHub contribution graph &lt;https://github.com/EvanKepner/mutatest/graphs/contributors&gt;</code>.</p>"},{"location":"testing/mutatest/#contributors","title":"Contributors","text":"<ul> <li>David Li-Bland</li> <li>Alireza Aghamohammadi</li> </ul>"},{"location":"testing/mutatest/#file-changelogrst","title":"File: /CHANGELOG.rst","text":""},{"location":"testing/mutatest/#changelog","title":"Changelog","text":""},{"location":"testing/mutatest/#stable-releases","title":"Stable Releases","text":"<p>3.1.0 ~~~~~</p> <ul> <li>Maintenance patches and API changes to skip/only category selection.</li> </ul> <p>3.0.2 ~~~~~</p> <pre><code>- `Maintenance patch #27 &lt;https://github.com/EvanKepner/mutatest/pull/27&gt;`_ updating source\n  code conditional logic in the CLI argument parsing.\n- Minor fixes for the most updated CI checks.\n</code></pre> <p>3.0.1 ~~~~~</p> <pre><code>- `Bug fix #24 &lt;https://github.com/EvanKepner/mutatest/issues/24&gt;`_ where the bit-shift\n  operators where not being applied during mutation trials and raised ``KeyError``.\n- A new ``test_all_op_types.py`` ensures all mutation substitutions work as intended.\n</code></pre> <p>3.0.0 ~~~~~</p> <pre><code>- ``Mutatest`` has reached a level of maturity to warrant a stable release.\n  With the addition of the multiprocessing capabilities, support for ``coverage`` versions\n  4.x and 5.x, support for Python 3.7 and 3.8, being installable through ``pip`` or\n  ``conda``, and with Azure Pipelines CI for platform tests, the tool and API are\n  unlikely to change in a major way without moving to ``4.0.0``.\n\nNew in this release:\n\n- Multiprocessing support on Python 3.8!\n    - The new ``--parallel`` command argument will instruct ``mutatest`` to use\n      multiprocessing for mutation trials. See the documentation for complete details.\n\n- Bug fix in ``mutatest.cache.create_cache_dirs()`` where the cache directory did not\n  include \"parents\" in case of packages with nested directories without existing pycache.\n- Removal of the ``sr`` subcategory of slice mutations (``Slice_RC`` for range changes).\n  These were rare, and supporting both Python 3.7 and 3.8 required excessive complexity.\n  The ``su`` category remains valid as the primary slice mutation set.\n</code></pre>"},{"location":"testing/mutatest/#beta-releases","title":"Beta Releases","text":"<p>2.2.0 ~~~~~</p> <pre><code>- Added support for specifying settings in ``setup.cfg`` using either ``[mutatest]`` or\n  ``[tool:mutatest]`` sections in addition to the ``mutatest.ini`` file.\n</code></pre> <p>2.1.3 ~~~~~</p> <pre><code>- Addressing test issues on Windows platform in the coverage tests by adding a\n  ``resolve_source`` flag to the ``CoverageFilter.filter`` method.\n</code></pre> <p>2.1.2 ~~~~~</p> <pre><code>- Moved the ``tests`` directory to be within the package of ``mutatest``.\n  This enabled the installation to be tested with ``pytest --pyargs mutatest`` as well\n  as ``pytest`` from local source files.\n  Test dependencies are still installed with ``pip install .[tests]``.\n</code></pre> <p>2.1.1 ~~~~~</p> <pre><code>- Includes specific test environments for ``coverage`` versions 4 and 5 with appropriate mocked\n  ``.coverage`` data outputs (JSON or SQL based on version).\n- A new ``tox`` test environment called ``cov4`` is added, with a new ``pytest`` marker\n  ``pytest.mark.coverage`` for test selection.\n</code></pre> <p>2.1.0 ~~~~~</p> <pre><code>- ``Coverage`` version 5.0 has moved to a SQLite database instead of a flat file. To support\n  both 4x and 5x versions of ``Coverage`` the ``filters`` source code has been updated.\n  The test suite includes mocked coverage data parsing tests of 4x only for now.\n</code></pre> <p>2.0.1 ~~~~~</p> <pre><code>- Explicit including of ``typing-extensions`` in ``setup.py`` requirements to fix breaking\n  documentation builds on Python version 3.7 vs. 3.8.\n</code></pre> <p>2.0.0 ~~~~~</p> <pre><code>- Python 3.8 support! There are breaking changes with the ``LocIndex`` and other components\n  of the ``transformers`` from prior versions of ``mutatest``. Python 3.8 introduces a new\n  AST structure - including additional node attributes ``end_lineno`` and ``end_col_offset``\n  that have to be accounted for. ``transformers.MutateAST`` is now build from a base class\n  and a mixin class depending on the Python version (3.7 vs. 3.8) for the appropriate AST\n  treatment. There are no changes in the CLI usage.\n</code></pre> <p>1.2.1 ~~~~~</p> <pre><code>- Bugfix to ensure ``exclude`` path processing in ``GenomeGroup.add_folder`` always uses full\n  resolved paths for files.\n</code></pre> <p>1.2.0 ~~~~~</p> <pre><code>- `Feature #18 &lt;https://github.com/EvanKepner/mutatest/pull/18&gt;`_: Allow mutation trials to time out.\n  There are cases where a mutation could cause an infinite loop, such as changing the comparator in\n  a ``while`` statement e.g., ``while x &lt; 5`` becomes ``while x &gt;= 5``. A new ``--timeout_factor``\n  argument is added to set a cap on the maximum trial time as a multiplier of the clean-trial run.\n- Bugfix on using ``exclude`` where files were logged but still becoming part of the sample.\n</code></pre> <p>1.1.1 ~~~~~</p> <pre><code>- `Bug Fix #15 &lt;https://github.com/EvanKepner/mutatest/pull/15&gt;`_: Fix ``LocIndex.ast_class`` setting for ``Index`` node mutations.\n</code></pre> <p>1.1.0 ~~~~~</p> <pre><code>- Add support for a ``mutatest.ini`` configuration file for command line arguments.\n</code></pre> <p>1.0.1 ~~~~~</p> <pre><code>- Documentation updates, including the API tutorial.\n- Fix on latest ``mypy`` errors related to ``strict`` processing of ``run`` and ``cache``.\n</code></pre> <p>1.0.0 ~~~~~</p> <pre><code>- Moving from the alpha to the beta version with an API design. The alpha releases were focused\n  on defining the functionality of the CLI. In the beta version, the CLI remains unchanged; however,\n  a full internal design has been applied to create a coherent API. The ``controller``, ``optimizers``,\n  and ``maker`` modules have been fully replaced by ``run``, ``api``, and ``filters``. See\n  the new full API documentation for details on using these modules outside of the CLI.\n- Additionally, ``pytest`` was removed from the installation requirements since it is assumed\n  for the default running modes but not required for the API or installation.\n</code></pre>"},{"location":"testing/mutatest/#alpha-releases","title":"Alpha Releases","text":"<p>0.9.2 ~~~~~</p> <pre><code>- Added ``--exception`` and ``-x`` as a survivor tolerance to raise an exception\n  after the trial completes if the count of surviving mutants is greater than or equal to the\n  specified value.\n</code></pre> <p>0.9.1 ~~~~~</p> <pre><code>- Added ``--only`` and ``--skip`` with category codes for mutation families.\n- Provides CLI selection of mutation types to be used during the trials.\n</code></pre> <p>0.9.0 ~~~~~</p> <pre><code>- Added new ``If`` mutation:\n    1. Original statements are represented by ``If_Statement`` and mutated to be either\n       ``If_True`` where the statement always passes, or ``If_False`` where the statement\n       is never passed.\n</code></pre> <p>0.8.0 ~~~~~</p> <pre><code>- Breaking changes to the CLI arguments and new defaults:\n    1. Output files are now optional, the default behavior has changed from always writing an RST\n       file using the ``-o`` option on the command line.\n    2. Exclusions are still marked as ``-e``; however, now multiple ``-e`` arguments are\n       supported and arguments must point to a Python file. The argument used to be:\n       ``mutatest -e \"__init__.py _devtools.py\"`` and now it is\n       ``mutatest -e src/__init__.py -e src/_devtools.py``. There are no longer default\n       exclusions applied.\n\n- Improved CLI reporting, including selected test counts and line/col locations\n  for trial results while processing.\n</code></pre> <p>0.7.1 ~~~~~</p> <pre><code>- Internal changes to ``Slice`` mutations for clearer categorization and report output.\n- Includes clearing names to ``Slice_Swap`` and ``Slice_RangeChange`` for categories.\n- Updates operation names to ``Slice_Unbounded...`` with \"lower\" or \"upper\".\n</code></pre> <p>0.7.0 ~~~~~</p> <pre><code>- Added new slice mutations:\n    1. ``Slice_SwapNoneUL`` and ``Slice_SwapNoneLU`` for swapping the upper and lower\n       bound values when only one is specified e.g. ``x[1:]`` to ``x[:1]``.\n    2. ``Slice_UPosToZero`` and ``Slice_UNegToZero`` for moving the upper bound of a\n       slice by 1 unit e.g. ``x[1:5]`` becomes ``x[1:4]``.\n</code></pre> <p>0.6.1 ~~~~~</p> <pre><code>- Added explicit tests for ``argparse`` cli options.\n- Added mechanism to sort reporting mutations by source file, then line number, then column\n  number.\n</code></pre> <p>0.6.0 ~~~~~</p> <pre><code>- Including ``pytest`` in the installation requirements. Technically, any test runner can\n  be used but with all base package assumptions being built around ``pytest`` this feels\n  like the right assumption to call out as an install dependency. It is the default behavior.\n- Updated ``controller`` for test file exclusion to explicitly match prefix or suffix cases\n  for ``\"test_\"`` and ``\"_test\"`` per ``pytest`` conventions.\n- Changed error and unknown status results to console color as yellow instead of red.\n- Added multiple invariant property tests, primarily to ``controller`` and ``cache``.\n- Added ``hypothesis`` to the test components of ``extras_require``.\n- Moved to ``@property`` decorators for internal class properties that should only\n  be set at initialization, may add custom ``setters`` at a later time.\n- Fixed a zero-division bug in the ``cli`` when reporting coverage percentage.\n</code></pre> <p>0.5.0 ~~~~~</p> <pre><code>- Addition of ``optimizers``, including the new class ``CoverageOptimizer``.\n- This optimizer restricts the full sample space only to source locations that are marked\n  as covered in the ``.coverage`` file. If you have a ``pytest.ini`` that includes\n  the ``--cov=`` command it will automatically generate during the clean-trial run.\n</code></pre> <p>0.4.2 ~~~~~</p> <pre><code>- More behind the scenes maintenance: updated debug level logging to include source file\n  names and line numbers for all visit operations and separated colorized output to a new\n  function.\n</code></pre> <p>0.4.1 ~~~~~</p> <pre><code>- Updated the reporting functions to return colorized display results to CLI.\n</code></pre> <p>0.4.0 ~~~~~</p> <pre><code>- Added new mutation support for:\n    1. ``AugAssign`` in AST e.g. ``+= -= *= /=``.\n    2. ``Index`` substitution in lists e.g. take a positive number like ``i[1]`` and\n       mutate to zero and a negative number e.g. ``i[-1] i[0]``.\n\n- Added a ``desc`` attribute to ``transformers.MutationOpSet`` that is used in the\n  cli help display.\n- Updated the cli help display to show the description and valid members.\n</code></pre> <p>0.3.0 ~~~~~</p> <pre><code>- Added new mutation support for ``NameConstant`` in AST.\n- This includes substitutions for singleton assignments such as: ``True``, ``False``,\n  and ``None``.\n- This is the first non-type mutation and required adding a ``readonly`` parameter\n  to the ``transformers.MutateAST`` class. Additionally, the type-hints for the\n  ``LocIndex`` and ``MutationOpSet`` were updated to ``Any`` to support\n  the mixed types. This was more flexible than a series of ``overload`` signatures.\n</code></pre> <p>0.2.0 ~~~~~</p> <pre><code>- Added new compare mutation support for:\n    1. ``Compare Is`` mutations e.g. ``is, is not``.\n    2. ``Compare In`` mutations e.g. ``in, not in``.\n</code></pre> <p>0.1.0 ~~~~~</p> <pre><code>- Initial release!\n- Requires Python 3.7 due to the ``importlib`` internal references for manipulating cache.\n- Run mutation tests using the ``mutatest`` command line interface.\n- Supported operations:\n\n    1. ``BinOp`` mutations e.g. ``+ - / *`` including bit-operations.\n    2. ``Compare`` mutations e.g. ``== &gt;= &lt; &lt;= !=``.\n    3. ``BoolOp`` mutations e.g. ``and or``.\n</code></pre>"},{"location":"testing/mutatest/#file-contributingrst","title":"File: /CONTRIBUTING.rst","text":""},{"location":"testing/mutatest/#contributing","title":"Contributing","text":"<p>Up top, thanks for considering a contribution! New features that align to the vision are welcome. You can either open an issue to discuss the idea first, or if you have working code, submit a pull-request.</p>"},{"location":"testing/mutatest/#vision","title":"Vision","text":"<p>The goal of <code>mutatest</code> is to provide a simple tool and API for mutation testing. The top level priorities for the project are:</p> <ol> <li>Collect useful mutation patterns without modifying the target source code.</li> <li>Make it fast.</li> </ol> <p>Open questions I'm working through on the design:</p> <ol> <li> <p>Fancy test selection? If there is a way to not only test coverage, but only select tests based    on the mutation location (a form of \"who tests what\") that could make trials more efficient.</p> </li> <li> <p>Local database? Keeping a local database of mutations and trial results would allow for re-running    failed mutations quickly. Providing the ability to log false positives to skip on future samples    would also be valuable.</p> </li> <li> <p>Clustered mutations? There could be room for specifying a number of mutations to run simultaneously.</p> </li> <li> <p>More API options? If you add two Genomes together should it create a GenomeGroup automatically?</p> </li> <li> <p>More reporting options? HTML etc.</p> </li> </ol>"},{"location":"testing/mutatest/#development-guidelines","title":"Development Guidelines","text":"<p>The following guidelines are used in the style formatting of this package. Many are enforced through <code>pre-commit</code> Git hooks and in the test running configuration of <code>tox</code>.</p> <p>Development environment setup ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>Here is how to get up and running for development on <code>mutatest</code>. Referenced tools are included in the development dependencies as part of the set up procedure.</p> <ol> <li>Fork this repo, then clone your fork locally.</li> <li>Create a new Python virtual environment using Python 3.7 and activate it.</li> <li>Change to the local directory of your clone. All commands are run in the top level directory    where the <code>setup.py</code> file is located.</li> <li>Install <code>mutatest</code> in edit mode with all development dependencies using <code>pip</code>.</li> </ol> <p>.. code-block:: bash</p> <pre><code>$ pip install -e .[dev]\n</code></pre> <ol> <li>Run a clean <code>tox</code> trial to ensure you're starting from a correct installation:</li> </ol> <p>.. code-block:: bash</p> <pre><code>$ tox\n\n# expected output ...\n\npy37: commands succeeded\nlint: commands succeeded\ntyping: commands succeeded\npypi-description: commands succeeded\nmanifest: commands succeeded\nhelp: commands succeeded\ncongratulations :)\n</code></pre> <ol> <li>Install <code>pre-commit</code> for the cloned repo. This ensures that every commit runs the    formatting checks including <code>black</code> and <code>flake8</code>.</li> </ol> <p>.. code-block:: bash</p> <pre><code>$ pre-commit install\n</code></pre> <ol> <li>Start developing!</li> <li>Run <code>tox</code> one more time before you open the PR to make sure your functionality passes the    original tests (and any new ones you have added).</li> </ol> <p>Style: all files ~~~~~~~~~~~~~~~~</p> <pre><code>- Generally hard-wrap at 100 characters for all files, including text files or RST.\n- Prefer RST over markdown or plaintext for explanations and outputs.\n- Accept the edits from the ``pre-commit`` configuration e.g. to trim trailing\n  whitespace.\n</code></pre> <p>Style: Package Python code ~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>Many of these points are automated with <code>pre-commit</code> and the existing configuration settings for <code>black</code> and <code>flake8</code>. In general:</p> <pre><code>- Use ``isort`` for ordering ``import`` statements in Python files.\n- Run ``black`` for formatting all Python files.\n- Use \"Google Style\" doc-string formatting for functions.\n- Type-hints are strictly enforced with ``mypy --strct``.\n- Adhere to PEP-8 for naming conventions and general style defaults.\n- All code is hard-wrapped at 100 characters.\n- If you are adding a new development tool instead of a feature, prefix the module name\n  with an underscore.\n- Provide justification for any new install requirements.\n- All tests are stored in the ``tests/`` directory.\n- Accept the edits from the ``pre-commit`` configuration.\n</code></pre> <p>Style: Test Python code ~~~~~~~~~~~~~~~~~~~~~~~</p> <p><code>Pytest</code> is used to manage unit tests, and <code>tox</code> is used to run various environment tests. <code>Hypothesis</code> is used for property testing in addition to the unit tests. If you are adding a new feature ensure that tests are added to cover the functionality. Some style enforcing is relaxed on the test files:</p> <pre><code>- Use ``isort`` for ordering ``import`` statements in Python files.\n- Run ``black`` for formatting all Python files.\n- Use \"Google Style\" doc-string formatting for functions, though single-line descriptions can be\n  appropriate for unit test descriptions.\n- Test files are all in the ``mutatest/tests/`` directory so tests are distributed with the package.\n- Tests do not require type-hints for the core test function or fixtures. Use as appropriate to\n  add clarity with custom classes or mocking.\n- Prefer to use ``pytest`` fixtures such as ``tmp_path`` and ``monkeypatch``.\n- All test files are prefixed with ``test_``.\n- All test functions are prefixed with ``test_`` and are descriptive.\n- Shared fixtures are stored in ``tests/conftest.py``.\n- Accept the edits from the ``pre-commit`` configuration.\n</code></pre> <p>Commits ~~~~~~~</p> <pre><code>- Use descriptive commit messages in \"action form\". Messages should be read as, \"If applied,\n  this commit will... &lt;&lt;your commit message&gt;&gt;\" e.g. \"add tests for coverage of bool_op visit\".\n- Squash commits as appropriate.\n</code></pre>"},{"location":"testing/mutatest/#file-readmerst","title":"File: /README.rst","text":""},{"location":"testing/mutatest/#mutatest-python-mutation-testing_1","title":"<code>mutatest</code>: Python mutation testing","text":"<p>|  |py-versions| |license| |ci-azure| |docs| |coverage| |black| |  |pypi-version| |pypi-status| |pypi-format| |pypi-downloads| |  |conda-version| |conda-recipe| |conda-platform| |conda-downloads|</p> <p>Are you confident in your tests? Try out <code>mutatest</code> and see if your tests will detect small modifications (mutations) in the code. Surviving mutations represent subtle changes that are undetectable by your tests. These mutants are potential modifications in source code that continuous integration checks would miss.</p>"},{"location":"testing/mutatest/#features_1","title":"Features","text":"<pre><code>- Simple command line tool with `multiple configuration options &lt;https://mutatest.readthedocs.io/en/latest/commandline.html&gt;`_.\n- Built on Python's Abstract Syntax Tree (AST) grammar to ensure `mutants are valid &lt;https://mutatest.readthedocs.io/en/latest/mutants.html&gt;`_.\n- `No source code modification &lt;https://mutatest.readthedocs.io/en/latest/install.html#mutation-trial-process&gt;`_,\n  only the ``__pycache__`` is changed.\n- Uses ``coverage`` to create `only meaningful mutants &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#coverage-filtering&gt;`_.\n- Built for efficiency with `multiple running modes &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#selecting-a-running-mode&gt;`_\n  and `random sampling of mutation targets &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#controlling-randomization-behavior-and-trial-number&gt;`_.\n- Capable of running `parallel mutation trials &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#parallelization&gt;`_\n  with multiprocessing on Python 3.8.\n- Flexible enough to run on a `whole package &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#auto-detected-package-structures&gt;`_\n  or a `single file &lt;https://mutatest.readthedocs.io/en/latest/commandline.html#specifying-source-files-and-test-commands&gt;`_.\n- Includes an `API for custom mutation controls &lt;https://mutatest.readthedocs.io/en/latest/modules.html&gt;`_.\n- Tested on Linux, Windows, and MacOS with `Azure pipelines &lt;https://dev.azure.com/evankepner/mutatest/_build/latest?definitionId=1&amp;branchName=master&gt;`_.\n- Full strict static type annotations throughout the source code and the API.\n</code></pre>"},{"location":"testing/mutatest/#install","title":"Install","text":"<p>Install from <code>PyPI &lt;https://pypi.org/project/mutatest/&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ pip install mutatest\n</code></pre> <p>Install from <code>conda-forge &lt;https://anaconda.org/conda-forge/mutatest&gt;</code>_:</p> <p>.. code-block:: bash</p> <pre><code>$ conda install -c conda-forge mutatest\n</code></pre>"},{"location":"testing/mutatest/#example-output","title":"Example Output","text":"<p>This is an output example running mutation trials against the <code>API Tutorial example folder &lt;https://mutatest.readthedocs.io/en/latest/api_tutorial/api_tutorial.html&gt;</code>_ example folder.</p> <p>.. code-block:: bash</p> <pre><code>$ mutatest -s example/ -t \"pytest\" -r 314\n\nRunning clean trial\n2 mutation targets found in example/a.py AST.\n1 mutation targets found in example/b.py AST.\nSetting random.seed to: 314\nTotal sample space size: 2\n10 exceeds sample space, using full sample: 2.\n\nStarting individual mutation trials!\nCurrent target location: a.py, LocIndex(ast_class='BinOp', lineno=6, col_offset=11, op_type=&lt;class '_ast.Add'&gt;)\nDetected mutation at example/a.py: (6, 11)\nDetected mutation at example/a.py: (6, 11)\nSurviving mutation at example/a.py: (6, 11)\nBreak on survival: stopping further mutations at location.\n\nCurrent target location: b.py, LocIndex(ast_class='CompareIs', lineno=6, col_offset=11, op_type=&lt;class '_ast.Is'&gt;)\nDetected mutation at example/b.py: (6, 11)\nRunning clean trial\n\nMutatest diagnostic summary\n===========================\n - Source location: /home/user/Github/mutatest/docs/api_tutorial/example\n - Test commands: ['pytest']\n - Mode: s\n - Excluded files: []\n - N locations input: 10\n - Random seed: 314\n\nRandom sample details\n---------------------\n - Total locations mutated: 2\n - Total locations identified: 2\n - Location sample coverage: 100.00 %\n\n\nRunning time details\n--------------------\n - Clean trial 1 run time: 0:00:00.348999\n - Clean trial 2 run time: 0:00:00.350213\n - Mutation trials total run time: 0:00:01.389095\n\nTrial Summary Report:\n\nOverall mutation trial summary\n==============================\n - DETECTED: 3\n - SURVIVED: 1\n - TOTAL RUNS: 4\n - RUN DATETIME: 2019-10-17 16:57:08.645355\n\nDetected mutations:\n\nDETECTED\n--------\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Sub'&gt;\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Mod'&gt;\n - example/b.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Is'&gt; to &lt;class '_ast.IsNot'&gt;\n\nSurviving mutations:\n\nSURVIVED\n--------\n - example/a.py: (l: 6, c: 11) - mutation from &lt;class '_ast.Add'&gt; to &lt;class '_ast.Mult'&gt;\n</code></pre>"},{"location":"testing/mutatest/#documentation","title":"Documentation","text":"<p>For full documentation, including installation, CLI references, API references, and tutorials, please see https://mutatest.readthedocs.io/en/latest/. The project is hosted on PyPI at https://pypi.org/project/mutatest/.</p>"},{"location":"testing/mutatest/#bugsrequests","title":"Bugs/Requests","text":"<p>Please use the <code>GitHub issue tracker &lt;https://github.com/EvanKepner/mutatest/issues&gt;</code> to submit bugs or request features. See the <code>Contributing Guidelines &lt;https://mutatest.readthedocs.io/en/latest/contributing.html&gt;</code> if you are interested in submitting code in the form of pull requests.</p>"},{"location":"testing/mutatest/#changelog_1","title":"ChangeLog","text":"<p>Consult the <code>Changelog &lt;https://mutatest.readthedocs.io/en/latest/changelog.html&gt;</code>_ page for fixes and enhancements of each version.</p>"},{"location":"testing/mutatest/#license_1","title":"License","text":"<p>Copyright Evan Kepner 2018-2020.</p> <p>Distributed under the terms of the <code>MIT &lt;https://github.com/pytest-dev/pytest/blob/master/LICENSE&gt;</code>_ license, <code>mutatest</code> is free and open source software.</p> <p>.. |py-versions| image:: https://img.shields.io/pypi/pyversions/mutatest?color=green     :target: https://pypi.org/project/mutatest/     :alt: Python versions .. |license| image:: https://img.shields.io/pypi/l/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: License .. |pypi-version| image:: https://badge.fury.io/py/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI version .. |pypi-status| image:: https://img.shields.io/pypi/status/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI status .. |pypi-format| image:: https://img.shields.io/pypi/format/mutatest.svg     :target: https://pypi.org/project/mutatest/     :alt: PyPI Format .. |pypi-downloads| image:: https://pepy.tech/badge/mutatest     :target: https://pepy.tech/project/mutatest     :alt: PyPI Downloads .. |ci-travis| image:: https://travis-ci.org/EvanKepner/mutatest.svg?branch=master     :target: https://travis-ci.org/EvanKepner/mutatest     :alt: TravisCI .. |ci-azure| image:: https://dev.azure.com/evankepner/mutatest/_apis/build/status/EvanKepner.mutatest?branchName=master     :target: https://dev.azure.com/evankepner/mutatest/_build/latest?definitionId=1&amp;branchName=master     :alt: Azure Pipelines .. |docs| image:: https://readthedocs.org/projects/mutatest/badge/?version=latest     :target: https://mutatest.readthedocs.io/en/latest/?badge=latest     :alt: RTD status .. |coverage| image:: https://codecov.io/gh/EvanKepner/mutatest/branch/master/graph/badge.svg     :target: https://codecov.io/gh/EvanKepner/mutatest     :alt: CodeCov .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg     :target: https://github.com/psf/black     :alt: Black .. |conda-recipe| image:: https://img.shields.io/badge/recipe-mutatest-green.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda recipe .. |conda-version| image:: https://img.shields.io/conda/vn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda version .. |conda-platform| image:: https://img.shields.io/conda/pn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda platforms .. |conda-azure| image:: https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/mutatest-feedstock?branchName=master     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda azure status .. |conda-downloads| image:: https://img.shields.io/conda/dn/conda-forge/mutatest.svg     :target: https://anaconda.org/conda-forge/mutatest     :alt: Conda downloads</p>"},{"location":"testing/mutation_testing/","title":"Mutation Testing with Mutatest","text":"<p>Mutation testing is a technique to evaluate the quality of your test suite by introducing small changes (mutations) to your code and checking if your tests can detect these changes.</p>"},{"location":"testing/mutation_testing/#what-is-mutation-testing","title":"What is Mutation Testing?","text":"<p>Mutation testing works by: 1. Making small changes to your code (mutations) 2. Running your tests against the mutated code 3. If your tests fail, the mutation is \"killed\" (good) 4. If your tests pass, the mutation \"survives\" (bad)</p> <p>Surviving mutations indicate areas where your tests might not be thorough enough.</p>"},{"location":"testing/mutation_testing/#using-mutatest-in-rna_predict","title":"Using Mutatest in RNA_PREDICT","text":"<p>We use mutatest for mutation testing in the RNA_PREDICT project.</p>"},{"location":"testing/mutation_testing/#installation","title":"Installation","text":"<pre><code>pip install mutatest coverage==5.5 pytest-cov==2.12.1\n</code></pre> <p>Note: Mutatest requires specific versions of coverage and pytest-cov to work correctly.</p>"},{"location":"testing/mutation_testing/#configuration","title":"Configuration","text":"<p>The project includes a <code>mutatest.ini</code> configuration file with the following settings:</p> <pre><code>[mutatest]\nexclude =\n    tests/\n    */__init__.py\n    */scripts/*\nmode = s\ntestcmds = pytest -n auto --cov=rna_predict tests -k 'not slow'\nnocov = no\n# Parallel execution is not available in this version\ntimeout_factor = 3.0\n</code></pre>"},{"location":"testing/mutation_testing/#running-mutation-tests","title":"Running Mutation Tests","text":"<p>We provide a script to run mutation testing:</p> <pre><code>./run_mutation_testing.sh\n</code></pre>"},{"location":"testing/mutation_testing/#script-options","title":"Script Options","text":"<ul> <li><code>-n, --nlocations NUMBER</code>: Number of locations to mutate (default: 20)</li> <li><code>-m, --mode MODE</code>: Running mode: f, s, d, sd (default: s)</li> <li><code>-o, --output FILE</code>: Output file for report (default: mutation_report.rst)</li> <li><code>-y, --only CATEGORIES</code>: Only use these mutation categories (space separated)</li> <li><code>-k, --skip CATEGORIES</code>: Skip these mutation categories (space separated) Note: Parallel execution is not available in this version of mutatest</li> </ul>"},{"location":"testing/mutation_testing/#mutation-categories","title":"Mutation Categories","text":"<ul> <li><code>aa</code>: AugAssign (e.g., +=, -=, *=)</li> <li><code>bn</code>: BinOp (e.g., +, -, *, /)</li> <li><code>bc</code>: BinOpBC (e.g., &amp;, |, ^)</li> <li><code>bs</code>: BinOpBS (e.g., &gt;&gt;, &lt;&lt;)</li> <li><code>bl</code>: BoolOp (e.g., and, or)</li> <li><code>cp</code>: Compare (e.g., &lt;, &gt;, &lt;=, &gt;=, ==, !=)</li> <li><code>cn</code>: CompareIn (e.g., in, not in)</li> <li><code>cs</code>: CompareIs (e.g., is, is not)</li> <li><code>if</code>: If (e.g., if statements)</li> <li><code>ix</code>: Index (e.g., list indexing)</li> <li><code>nc</code>: NameConstant (e.g., True, False, None)</li> <li><code>su</code>: SliceUS (e.g., list slicing)</li> </ul>"},{"location":"testing/mutation_testing/#examples","title":"Examples","text":"<p>Run mutation testing on 10 random locations: <pre><code>./run_mutation_testing.sh -n 10\n</code></pre></p> <p>Run mutation testing only on comparison operators: <pre><code>./run_mutation_testing.sh -y cp\n</code></pre></p> <p>Run mutation testing in full mode (test all possible mutations): <pre><code>./run_mutation_testing.sh -m f\n</code></pre></p>"},{"location":"testing/mutation_testing/#interpreting-results","title":"Interpreting Results","text":"<p>The results are saved to <code>mutation_report.rst</code> by default. Look for:</p> <ol> <li>SURVIVED mutations: These are changes that your tests didn't detect. You should improve your tests to catch these.</li> <li>DETECTED mutations: These are changes that your tests successfully caught.</li> <li>TIMEOUT mutations: These are changes that caused your tests to run too long.</li> </ol>"},{"location":"testing/mutation_testing/#best-practices","title":"Best Practices","text":"<ol> <li>Start with a small number of locations (<code>-n 10</code>) to get quick feedback</li> <li>Focus on critical modules first</li> <li>Use the coverage filter to focus on code that's actually executed</li> <li>Gradually increase the scope as your tests improve</li> <li>Add tests for any surviving mutations</li> </ol>"},{"location":"testing/mutation_testing/#comparison-with-other-tools","title":"Comparison with Other Tools","text":"<p>We previously used Cosmic Ray but switched to mutatest because:</p> <ol> <li>Mutatest modifies the <code>__pycache__</code> files rather than source code</li> <li>It has better isolation for parallel execution</li> <li>It integrates well with coverage</li> <li>It's simpler to configure and use</li> </ol>"},{"location":"testing/mutation_testing_guide/","title":"Comprehensive Guide: Using Mutation Testing with Cosmic Ray","text":""},{"location":"testing/mutation_testing_guide/#1-introduction","title":"1. Introduction","text":"<ul> <li>What is Mutation Testing?</li> <li>Mutation testing is a technique used to evaluate the quality of your software tests. It works by making small, specific changes (called \"mutations\") to your production code.</li> <li>For each mutation, it runs your existing test suite.</li> <li>If a test fails: The mutant is considered \"killed\". This is good! It means your tests detected the change.</li> <li>If all tests pass: The mutant \"survived\". This indicates a potential weakness in your tests \u2013 they didn't notice the code change. This could mean the test coverage is insufficient, the assertions aren't strong enough, or the mutated code is \"equivalent\" (doesn't actually change behavior, though this is less common).</li> <li>What is Cosmic Ray?</li> <li>Cosmic Ray is the mutation testing tool we use in this project. It automates the process of creating mutants, running tests, and reporting results.</li> <li>Why Use It?</li> <li>To ensure our test suite is robust and catches potential bugs.</li> <li>To identify areas where our tests might be weak or missing assertions.</li> <li>To gain confidence in the reliability of our codebase.</li> </ul>"},{"location":"testing/mutation_testing_guide/#2-prerequisites","title":"2. Prerequisites","text":"<p>Before you start, make sure you have:</p> <ul> <li>Python Environment: A working Python 3 environment (check project requirements for specific versions).</li> <li>Cosmic Ray Installation: Install Cosmic Ray with:   <pre><code>pip install cosmic-ray\n</code></pre></li> <li>Project Tests: Familiarity with how to run the project's main test suite. This is typically done using pytest:   <pre><code>pytest tests/\n</code></pre>   Ensure this command runs successfully before starting mutation testing.</li> <li>Git: Basic understanding of Git for potentially using git-related filters later.</li> </ul>"},{"location":"testing/mutation_testing_guide/#3-core-workflow-step-by-step","title":"3. Core Workflow: Step-by-Step","text":"<p>Mutation testing with Cosmic Ray follows a standard workflow: Configure -&gt; Initialize -&gt; Baseline -&gt; Execute -&gt; Report.</p> <ul> <li>Step 1: Configuration (<code>cosmic-ray.toml</code>)</li> <li>Goal: Tell Cosmic Ray what code to mutate, how to run the tests, and other parameters.</li> <li>Action: Create a configuration file. Let's name it <code>cosmic-ray.toml</code> and place it in the project's root directory.</li> <li>Key Settings:<ul> <li><code>module-path</code>: The code to mutate. For our project, this will be <code>rna_predict</code>.</li> <li><code>test-command</code>: The exact command to run the test suite. Use the command identified in Prerequisites, adding <code>-x</code> to stop on the first failure (makes mutation testing faster): <code>\"pytest tests/ -x\"</code>.</li> <li><code>timeout</code>: A maximum time (in seconds) allowed for a single test run against one mutant. Start with a reasonable value like <code>60</code> or <code>120</code> and adjust if needed. This prevents infinite loops caused by mutations.</li> <li><code>excluded-modules</code>: A list of file patterns (globs) to exclude from mutation. Crucially, exclude test files themselves and potentially utility scripts or examples if they aren't the primary target. Example: <code>[\"tests/**\", \"docs/**\"]</code>.</li> <li><code>distributor</code>: How to run the mutation jobs. Start with the simplest: <code>name = \"local\"</code>.</li> </ul> </li> <li>Example <code>cosmic-ray.toml</code>: <pre><code># cosmic-ray.toml\n[cosmic-ray]\nmodule-path = \"rna_predict\"\ntest-command = \"pytest tests/ -x\"\ntimeout = 120.0 # 2 minutes, adjust as needed\nexcluded-modules = [\n    \"tests/**\",       # Exclude the main test directory\n    \"docs/**\",        # Exclude documentation files\n    # Add more specific exclusions if needed\n]\n\n# Distributor config - start with local\n[cosmic-ray.distributor]\nname = \"local\"\n\n# Optional: Operator configuration (usually defaults are fine)\n# [cosmic-ray.operators]\n# \"core/NumberReplacer\" = [{}, {\"offset\": 10}] # Example syntax if needed\n\n# Optional: Filter configuration (see Advanced Topics)\n# [cosmic-ray.filters.operators-filter]\n# exclude-operators = [\"core/SomeOperatorToSkip\"]\n# [cosmic-ray.filters.git-filter]\n# branch = \"main\" # Or \"master\"\n\n# Optional: Badge configuration (see Reporting)\n# [cosmic-ray.badge]\n# label = \"Mutation Score\"\n# value_format = \"%.1f%%\"\n# [cosmic-ray.badge.thresholds]\n# 50 = \"red\"\n# 75 = \"orange\"\n# 90 = \"yellow\"\n# 100 = \"green\"\n</code></pre></li> <li> <p>Location: Place this file at the root of the project.</p> </li> <li> <p>Step 2: Initialization (<code>cosmic-ray init</code>)</p> </li> <li>Goal: Scan the <code>module-path</code> (<code>rna_predict</code>) for all possible mutation points using the available operators and create a database of \"work items\".</li> <li>Action: Run the following command from the project root:     <pre><code>cosmic-ray init cosmic-ray.toml session.sqlite\n</code></pre></li> <li>Output: This creates a <code>session.sqlite</code> file. This file stores the plan (which mutations to apply where) and will later store the results.</li> <li> <p>When to Re-run <code>init</code>:</p> <ul> <li>If you change the code in <code>rna_predict</code>.</li> <li>If you change the <code>module-path</code>, <code>excluded-modules</code>, or operators in <code>cosmic-ray.toml</code>.</li> <li>If you add/remove operators (e.g., by installing plugins).</li> <li>Running <code>init</code> again overwrites the existing session file. If you have partial results you want to keep, don't re-run <code>init</code> unless necessary. Use <code>--force</code> to overwrite if the session file already contains results.</li> </ul> </li> <li> <p>Step 3: Baselining (<code>cosmic-ray baseline</code>)</p> </li> <li>Goal: Verify that your <code>test-command</code> (from <code>cosmic-ray.toml</code>) passes successfully on the unmutated code within the specified <code>timeout</code>. If the baseline fails, mutation results are meaningless.</li> <li>Action: Run:     <pre><code>cosmic-ray baseline cosmic-ray.toml\n</code></pre></li> <li> <p>Output: Should report success. If it fails, fix your test suite or the <code>test-command</code> in the config before proceeding.</p> </li> <li> <p>Step 4: Execution (<code>cosmic-ray exec</code>)</p> </li> <li>Goal: Execute the mutation testing run. For each pending work item in <code>session.sqlite</code>:<ol> <li>Apply the mutation to the code on disk (temporarily).</li> <li>Run the <code>test-command</code>.</li> <li>Record the outcome (killed, survived, incompetent, timeout) in <code>session.sqlite</code>.</li> <li>Revert the code change.</li> </ol> </li> <li>Action: Run:     <pre><code>cosmic-ray exec cosmic-ray.toml session.sqlite\n</code></pre></li> <li> <p>Important:</p> <ul> <li>Commit your code first! While Cosmic Ray should always revert changes, it's safest to have a clean Git state before running <code>exec</code>.</li> <li>This step can take a long time, depending on the number of mutants and the speed of your test suite.</li> <li>You can usually stop (<code>Ctrl+C</code>) and resume <code>exec</code> later; it picks up from the <code>session.sqlite</code> file.</li> <li>Use <code>--verbosity INFO</code> or <code>DEBUG</code> for more detailed progress output.</li> </ul> </li> <li> <p>Step 5: Reporting (<code>cr-report</code>, <code>cr-html</code>, <code>cr-badge</code>)</p> </li> <li>Goal: Analyze the results stored in <code>session.sqlite</code>.</li> <li>Actions:<ul> <li>Text Summary: Get a detailed list of each mutation and its outcome.   <pre><code>cr-report session.sqlite\n# Useful options:\n# cr-report session.sqlite --show-diff  # See code changes for each mutant\n# cr-report session.sqlite --show-output # See test output for killed/incompetent mutants\n# cr-report session.sqlite --show-pending # Include items not yet executed\n</code></pre></li> <li>HTML Report: Generate a browsable report, often including diffs.   <pre><code>cr-html session.sqlite &gt; cosmic-ray-report.html\n</code></pre>   Then open <code>cosmic-ray-report.html</code> in your web browser.</li> <li>Survival Rate: Get the percentage of mutants that survived. Lower is better.   <pre><code>cr-rate session.sqlite\n</code></pre></li> <li>Badge (Optional): Generate an SVG badge showing the mutation score (kill rate). Requires configuration in <code>cosmic-ray.toml</code> (see example above).   <pre><code>cr-badge cosmic-ray.toml badge.svg session.sqlite\n</code></pre></li> </ul> </li> <li>Interpretation: Focus on the surviving mutants. Each survivor represents a gap in your test suite. Analyze the code change (<code>--show-diff</code> or HTML report) and figure out why your tests didn't fail. Then, improve your tests (add new ones, strengthen assertions) and re-run the relevant parts of the mutation testing process.</li> </ul>"},{"location":"testing/mutation_testing_guide/#4-advanced-topics-getting-the-most-out-of-it","title":"4. Advanced Topics: Getting the Most Out Of It","text":"<ul> <li>Filtering Mutations:</li> <li>Sometimes you want to skip certain mutations before execution.</li> <li>Why? To ignore known equivalent mutants, mutations in comments/docstrings, or code explicitly marked as not needing mutation coverage. To speed up runs by focusing on specific areas.</li> <li>How? Filters modify the <code>session.sqlite</code> file after <code>init</code> but before <code>exec</code>, marking items as <code>SKIPPED</code>.</li> <li> <p>Available Filters (run after <code>init</code>):</p> <ul> <li><code>cr-filter-pragma session.sqlite</code>: Skips mutations on lines containing <code># pragma: no mutate</code>. Add this comment to your source code (<code>rna_predict/...</code>) where needed.</li> <li><code>cr-filter-operators cosmic-ray.toml session.sqlite</code>: Skips mutations based on operator names matching regex patterns defined in <code>cosmic-ray.toml</code> under <code>[cosmic-ray.filters.operators-filter]</code>. Useful for excluding entire categories of mutations.</li> <li><code>cr-filter-git session.sqlite</code>: Skips mutations on lines not changed relative to a specific Git branch (e.g., <code>main</code> or <code>master</code>). Configure the branch in <code>cosmic-ray.toml</code> under <code>[cosmic-ray.filters.git-filter]</code>. Great for faster feedback on pull requests.</li> </ul> </li> <li> <p>Distributed Execution (Parallelism):</p> </li> <li>Why? Mutation testing can be slow. Running jobs in parallel significantly speeds it up.</li> <li>How? Use the <code>http</code> distributor.</li> <li>Setup:<ol> <li>Modify <code>cosmic-ray.toml</code>:</li> <li>Set <code>[cosmic-ray.distributor]</code> <code>name = \"http\"</code>.</li> <li>Define <code>[cosmic-ray.distributor.http]</code> <code>worker-urls = [\"http://localhost:9876\", \"http://localhost:9877\", ...]</code>. List the addresses where worker processes will listen.</li> <li>Workers: Each worker needs its own isolated copy of the codebase (because mutations modify files).</li> <li>Manual: Clone the repository multiple times (e.g., into <code>worker1</code>, <code>worker2</code>). Start a worker in each clone directory:     <pre><code># In terminal 1, inside worker1 clone:\ncosmic-ray http-worker --port 9876\n\n# In terminal 2, inside worker2 clone:\ncosmic-ray http-worker --port 9877\n</code></pre></li> <li>Helper Tool (<code>cr-http-workers</code>): This tool automates cloning and starting local workers based on your config file. Run it from the original project root:     <pre><code># Reads worker-urls from config, clones '.', starts workers\ncr-http-workers cosmic-ray.toml .\n</code></pre>     Run <code>cosmic-ray exec</code> in another terminal while <code>cr-http-workers</code> manages the workers. Stop <code>cr-http-workers</code> (<code>Ctrl+C</code>) when done to clean up clones.</li> <li>Run <code>cosmic-ray init</code> and <code>cosmic-ray exec</code> as usual from the original project root. <code>exec</code> will send jobs to the running workers listed in the config.</li> </ol> </li> </ul>"},{"location":"testing/mutation_testing_guide/#5-integrating-into-your-workflow","title":"5. Integrating into Your Workflow","text":"<ul> <li>Local Development:</li> <li>Run mutation testing periodically on your feature branches.</li> <li>Use <code>cr-filter-git</code> to focus only on changed code for faster feedback loops.</li> <li>Analyze surviving mutants before merging to improve test quality.</li> <li>Continuous Integration (CI/CD):</li> <li>Goal: Automatically run mutation testing on pull requests or merges.</li> <li>Steps in Workflow:<ol> <li>Checkout code.</li> <li>Set up Python.</li> <li>Install dependencies.</li> <li>Run <code>cosmic-ray init</code>.</li> <li>(Optional) Run filters (e.g., <code>cr-filter-pragma</code>).</li> <li>Run <code>cosmic-ray baseline</code>.</li> <li>Run <code>cosmic-ray exec</code>. (Consider using <code>cr-http-workers</code> if your CI runner has enough resources, or configure external workers if needed).</li> <li>Run <code>cr-report</code> and <code>cr-html</code>.</li> <li>Upload the HTML report as a build artifact.</li> <li>(Optional) Use <code>cr-rate</code> with the <code>--fail-over</code> option to fail the build if the survival rate is too high.</li> <li>(Optional) Generate and upload a badge (<code>cr-badge</code>).</li> </ol> </li> <li>Challenges: CI runs can be time-consuming. Use filters aggressively, consider parallel execution, or run mutation testing less frequently (e.g., nightly) instead of on every commit.</li> </ul>"},{"location":"testing/mutation_testing_guide/#6-best-practices-and-tips","title":"6. Best Practices and Tips","text":"<ul> <li>Start Small: Don't try to mutate the entire codebase at once. Configure <code>module-path</code> or use exclusions/filters to target specific critical modules first.</li> <li>Ensure Baseline Passes: Always run <code>cosmic-ray baseline</code> first.</li> <li>Commit Code: Ensure your working directory is clean before running <code>cosmic-ray exec</code>.</li> <li>Use Filters: Employ <code># pragma: no mutate</code> and potentially operator/git filters to reduce noise and runtime.</li> <li>Analyze Survivors: Don't just look at the survival rate number. Investigate why mutants survived and improve your tests accordingly.</li> <li>Iterate: Mutation testing is a cycle: test, analyze, improve tests, repeat.</li> <li>Parallelize: Use the <code>http</code> distributor and multiple workers (<code>cr-http-workers</code> locally or configured workers in CI) for significant speedups on larger runs.</li> <li>Timeouts: Set a reasonable <code>timeout</code>. Too short, and valid tests might fail (incompetent mutants); too long, and runs drag unnecessarily.</li> </ul>"},{"location":"testing/mutation_testing_guide/#7-troubleshooting-common-issues","title":"7. Troubleshooting Common Issues","text":"<ul> <li>Baseline Fails: Your normal test suite is failing. Fix the tests or the <code>test-command</code> in your config.</li> <li>High Timeout Rate: Mutants are causing tests to hang. Increase the <code>timeout</code> value in your config, or investigate the specific mutations causing hangs (they might indicate fragile code).</li> <li>High Survival Rate: Your tests aren't catching the mutations. Analyze the survivors using reports (<code>cr-report --show-diff</code>, <code>cr-html</code>) and improve your tests (add assertions, cover more edge cases).</li> <li>Configuration Errors: Double-check the syntax and paths in your <code>cosmic-ray.toml</code>.</li> <li>Worker Errors (HTTP Distributor): Ensure workers are running, accessible at the configured URLs, and have the correct code version and dependencies installed in their isolated environments. Check worker logs for errors.</li> </ul>"},{"location":"testing/mutation_testing_guide/#8-checklists","title":"8. Checklists","text":"<ul> <li>Initial Setup Checklist:</li> <li>[ ] Install Cosmic Ray: <code>pip install cosmic-ray</code></li> <li>[ ] Verify project tests pass: <code>pytest tests/</code></li> <li>[ ] Create <code>cosmic-ray.toml</code> at project root.</li> <li>[ ] Configure <code>module-path = \"rna_predict\"</code>.</li> <li>[ ] Configure <code>test-command = \"pytest tests/ -x\"</code>.</li> <li>[ ] Configure <code>timeout</code>.</li> <li>[ ] Configure <code>excluded-modules</code> (especially <code>tests/**</code>).</li> <li>[ ] Configure <code>distributor.name = \"local\"</code>.</li> <li>Basic Run Checklist:</li> <li>[ ] Ensure code is committed or stashed.</li> <li>[ ] Run <code>cosmic-ray init cosmic-ray.toml session.sqlite</code>.</li> <li>[ ] (Optional) Add <code># pragma: no mutate</code> to source files.</li> <li>[ ] (Optional) Run <code>cr-filter-pragma session.sqlite</code>.</li> <li>[ ] Run <code>cosmic-ray baseline cosmic-ray.toml</code>. Fix issues if it fails.</li> <li>[ ] Run <code>cosmic-ray exec cosmic-ray.toml session.sqlite</code>.</li> <li>[ ] Run <code>cr-report session.sqlite</code> and/or <code>cr-html session.sqlite &gt; report.html</code>.</li> <li>[ ] Analyze surviving mutants.</li> <li>[ ] Improve tests based on analysis.</li> <li>Distributed Execution Checklist (HTTP):</li> <li>[ ] Configure <code>distributor.name = \"http\"</code> in TOML.</li> <li>[ ] Configure <code>distributor.http.worker-urls</code> in TOML.</li> <li>[ ] Start workers:<ul> <li>Manually (clones + <code>cosmic-ray http-worker --port ...</code> in each) OR</li> <li>Automatically (<code>cr-http-workers cosmic-ray.toml .</code>)</li> </ul> </li> <li>[ ] Run <code>init</code>, <code>baseline</code>, <code>exec</code> from the original project root.</li> <li>[ ] Stop workers when done (kill manual processes or <code>cr-http-workers</code>).</li> </ul>"},{"location":"testing/mutation_testing_guide/#9-conclusion","title":"9. Conclusion","text":"<p>Mutation testing is a powerful way to enhance the quality and reliability of our codebase. By systematically introducing small changes and checking if our tests detect them, we can uncover blind spots and build a more robust test suite. Start with the basic workflow, gradually incorporate filters and parallel execution, and integrate it into your development process to continuously improve test coverage and overall quality. Remember to focus on analyzing surviving mutants \u2013 that's where the real value lies!</p>"}]}