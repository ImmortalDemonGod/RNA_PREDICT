RNA Pipeline Combined Output
Generated on: 2025-04-07 00:49:10
================================================================================


================================================================================
Output from: rna_predict/pipeline/stageA/run_stageA.py
Timestamp: 2025-04-07 00:49:10
================================================================================

STDOUT:
[INFO] Adjacency shape: (352, 352)
[VARNA] Running: java -cp RFold/VARNAv3-93.jar fr.orsay.lri.varna.applications.VARNAcmd -i test_seq.ct -o test_seq.png -resolution 8.0
[VARNA] Visualization saved to test_seq.png


================================================================================
Output from: rna_predict/pipeline/stageB/main.py
Timestamp: 2025-04-07 00:49:14
================================================================================

STDOUT:
[2025-04-07 00:49:15,397] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[Stage A] adjacency shape = torch.Size([8, 8])
[Stage B] angles shape = torch.Size([8, 7])
[Stage C] coords shape = torch.Size([24, 3]), #atoms = 24

=== Now Running Gradient Flow Test ===
Running gradient flow test on device: cpu
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
Loss: 0.3785651922225952

--- Grad Check ---
  [final_head_s] weight, grad sum=2.2674e+01
  [final_head_s] bias, grad sum=2.0147e-01
  [final_head_angles] weight, grad sum=0.0000e+00
  [final_head_angles] bias, grad sum=2.0147e-01
  [final_head_z] weight, grad sum=4.3685e+00
  [final_head_z] bias, grad sum=2.0147e-01
  [pairformer] stack.blocks.0.tri_mul_out.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_z.weight, grad sum=1.9894e+01
  [pairformer] stack.blocks.0.tri_mul_out.linear_z.bias, grad sum=2.7338e-01
  [pairformer] stack.blocks.0.tri_mul_out.layer_norm_in.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.layer_norm_in.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.layer_norm_out.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.layer_norm_out.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_a_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_a_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_a_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_a_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_b_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_b_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_b_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_out.linear_b_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_z.weight, grad sum=1.8291e+01
  [pairformer] stack.blocks.0.tri_mul_in.linear_z.bias, grad sum=2.7683e-01
  [pairformer] stack.blocks.0.tri_mul_in.layer_norm_in.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.layer_norm_in.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.layer_norm_out.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.layer_norm_out.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_a_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_a_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_a_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_a_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_b_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_b_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_b_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_mul_in.linear_b_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.layer_norm.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.layer_norm.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_o.weight, grad sum=1.5064e+01
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_o.bias, grad sum=3.8680e-01
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_start.mha.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.layer_norm.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.layer_norm.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_o.weight, grad sum=5.5345e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_o.bias, grad sum=4.4498e-01
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.tri_att_end.mha.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.pair_transition.layernorm1.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.pair_transition.layernorm1.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.pair_transition.linear_no_bias_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.pair_transition.linear_no_bias_b.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.pair_transition.linear_no_bias.weight, grad sum=5.0246e+00
  [pairformer] stack.blocks.0.attention_pair_bias.layernorm_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.layernorm_a.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.layernorm_z.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.layernorm_z.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.linear_nobias_z.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.gating_bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.to_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.to_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.to_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.to_out.weight, grad sum=1.2141e+01
  [pairformer] stack.blocks.0.attention_pair_bias.attention.to_out.bias, grad sum=3.6640e-01
  [pairformer] stack.blocks.0.attention_pair_bias.attention.gating_linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.attention_pair_bias.attention.gating_linear.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.single_transition.layernorm1.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.single_transition.layernorm1.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.single_transition.linear_no_bias_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.single_transition.linear_no_bias_b.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.0.single_transition.linear_no_bias.weight, grad sum=3.8889e+01
  [pairformer] stack.blocks.1.tri_mul_out.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_z.weight, grad sum=1.8326e+01
  [pairformer] stack.blocks.1.tri_mul_out.linear_z.bias, grad sum=2.7606e-01
  [pairformer] stack.blocks.1.tri_mul_out.layer_norm_in.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.layer_norm_in.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.layer_norm_out.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.layer_norm_out.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_a_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_a_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_a_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_a_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_b_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_b_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_b_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_out.linear_b_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_z.weight, grad sum=1.8104e+01
  [pairformer] stack.blocks.1.tri_mul_in.linear_z.bias, grad sum=2.6810e-01
  [pairformer] stack.blocks.1.tri_mul_in.layer_norm_in.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.layer_norm_in.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.layer_norm_out.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.layer_norm_out.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_a_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_a_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_a_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_a_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_b_p.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_b_p.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_b_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_mul_in.linear_b_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.layer_norm.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.layer_norm.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_o.weight, grad sum=1.5922e+01
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_o.bias, grad sum=3.8738e-01
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_start.mha.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.layer_norm.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.layer_norm.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_o.weight, grad sum=5.8834e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_o.bias, grad sum=5.1860e-01
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_g.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.tri_att_end.mha.linear_g.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.pair_transition.layernorm1.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.pair_transition.layernorm1.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.pair_transition.linear_no_bias_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.pair_transition.linear_no_bias_b.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.pair_transition.linear_no_bias.weight, grad sum=4.7978e+00
  [pairformer] stack.blocks.1.attention_pair_bias.layernorm_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.layernorm_a.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.layernorm_z.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.layernorm_z.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.linear_nobias_z.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.gating_bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.to_q.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.to_k.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.to_v.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.to_out.weight, grad sum=1.4474e+01
  [pairformer] stack.blocks.1.attention_pair_bias.attention.to_out.bias, grad sum=3.6640e-01
  [pairformer] stack.blocks.1.attention_pair_bias.attention.gating_linear.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.attention_pair_bias.attention.gating_linear.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.single_transition.layernorm1.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.single_transition.layernorm1.bias, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.single_transition.linear_no_bias_a.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.single_transition.linear_no_bias_b.weight, grad sum=0.0000e+00
  [pairformer] stack.blocks.1.single_transition.linear_no_bias.weight, grad sum=3.8098e+01
STDERR:
W0407 00:49:16.043000 8676640832 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.


================================================================================
Output from: rna_predict/pipeline/stageC/stage_c_reconstruction.py
Timestamp: 2025-04-07 00:49:18
================================================================================

STDOUT:
RNA coords shape: torch.Size([4, 21, 3])  total atoms: 84
STDERR:
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/mp_nerf/rna.py:204: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Cross.cpp:66.)
  perpendicular = torch.cross(


================================================================================
Output from: rna_predict/pipeline/stageD/run_stageD.py
Timestamp: 2025-04-07 00:49:18
================================================================================

STDOUT:
[2025-04-07 00:49:19,798] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[DEBUG] s_trunk shape: torch.Size([1, 25, 384])
[DEBUG] s_inputs shape: torch.Size([1, 25, 449])
[DEBUG] Before sample_diffusion:
  coords_init shape: torch.Size([1, 25, 3])
  s_trunk shape: torch.Size([1, 25, 384])
  s_inputs shape: torch.Size([1, 25, 449])
[DEBUG] Determined true_batch_shape: torch.Size([1])
[DEBUG][Generator Loop 0] Before denoise_net - x_noisy: torch.Size([1, 1, 25, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 10000 (from torch.Size([1, 1, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 25, 25, 3])
[DEBUG][Generator Loop 1] Before denoise_net - x_noisy: torch.Size([1, 25, 25, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 25, 25, 3])
[DEBUG][Generator Loop 2] Before denoise_net - x_noisy: torch.Size([1, 25, 25, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 25, 25, 3])
[DEBUG][Generator Loop 3] Before denoise_net - x_noisy: torch.Size([1, 25, 25, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 25, 25, 3])
[DEBUG][Generator Loop 4] Before denoise_net - x_noisy: torch.Size([1, 25, 25, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 256, actual size: 250000 (from torch.Size([1, 25, 16, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 16, actual size: 2500 (from torch.Size([1, 1, 4, 25, 25])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 25, 25, 3])
[DEBUG][sample_diffusion] Returning _chunk_sample_diffusion output shape: torch.Size([1, 25, 25, 3])
[DEBUG] After sample_diffusion:
  coords_final shape before squeeze: torch.Size([1, 25, 25, 3])
[DEBUG] Final coords_final shape (handling N_sample=1): torch.Size([1, 25, 3])
[DEBUG] After inference call: 's_inputs' in internal? True, 's_inputs' in original? True
STDERR:
W0407 00:49:20.333000 8676640832 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:165: UserWarning: input_features not provided, using basic fallback based on partial_coords.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/encoder_components/feature_processing.py:131: UserWarning: ref_space_uid has wrong shape torch.Size([1, 25]), expected [..., 3]. Setting to zeros.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 25, 128]), shift=torch.Size([1, 25, 128]), a=torch.Size([1, 1, 25, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 25, 128]), s shape: torch.Size([1, 25, 384])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 25, 768]), shift=torch.Size([1, 25, 768]), a=torch.Size([1, 1, 25, 768])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 25, 768]), s shape: torch.Size([1, 25, 384])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/atom_attention_decoder.py:169: UserWarning: Conditioning signal 's' has incorrect feature dim 128, expected 384. Adapting.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 1, 25, 128]), shift=torch.Size([1, 1, 25, 128]), a=torch.Size([1, 25, 25, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 25, 25, 128]), s shape: torch.Size([1, 1, 25, 384])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:476: UserWarning: Broadcasting t_hat (torch.Size([1, 1])) to x_noisy (torch.Size([1, 25, 25, 3])) might be ambiguous.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/protenix_diffusion_manager.py:289: UserWarning: Observed unexpected shape torch.Size([1, 25, 25, 3]) for N_sample=1. Expected shape like [B, 1, N_atom, 3]. Selecting first element along dimension 1.
  warnings.warn(


================================================================================
Output from: rna_predict/interface.py
Timestamp: 2025-04-07 00:49:27
================================================================================

STDOUT:
   ID resname  resid       x_1  ...  z_4       x_5       y_5  z_5
0   1       A      1  0.000000  ...  0.0  0.000000  0.000000  0.0
1   2       C      2  1.000000  ...  0.0  1.000000  0.000000  0.0
2   3       G      3  0.874002  ...  0.0  0.874002  1.309680  0.0
3   4       U      4  0.182341  ...  0.0  0.182341  0.961230  0.0
4   5       A      5  0.726211  ...  0.0  0.726211 -0.704627  0.0

[5 rows x 18 columns]
STDERR:
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/mp_nerf/rna.py:204: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Cross.cpp:66.)
  perpendicular = torch.cross(


================================================================================
Output from: rna_predict/main.py
Timestamp: 2025-04-07 00:49:30
================================================================================

STDOUT:
Running demo_run_input_embedding()...
Now streaming the bprna-spot dataset...
Showing the full dataset structure for the first row...
Demo completed successfully.


================================================================================
Output from: rna_predict/print_rna_pipeline_output.py
Timestamp: 2025-04-07 00:49:30
================================================================================

STDOUT:
[2025-04-07 00:49:31,490] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
Running RNA prediction pipeline on sequence: AUGCAUGG
Stage D available: True
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
[Debug] Tensor shapes for merger:
  adjacency: torch.Size([8, 8])
  torsion_angles: torch.Size([8, 14])
  s_emb: torch.Size([8, 64])
  z_emb: torch.Size([8, 8, 32])
[DEBUG] s_trunk shape: torch.Size([1, 8, 64])
[DEBUG] s_inputs is invalid or None!
[DEBUG] z_trunk shape: torch.Size([1, 8, 8, 32])
[DEBUG] Before sample_diffusion:
  coords_init shape: torch.Size([1, 24, 3])
  s_trunk shape: torch.Size([1, 8, 64])
  z_trunk shape: torch.Size([1, 8, 8, 32])
[DEBUG] Determined true_batch_shape: torch.Size([1])
[DEBUG][Generator Loop 0] Before denoise_net - x_noisy: torch.Size([1, 1, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 128 (from torch.Size([1, 1, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 1] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 2] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 3] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 4] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 5] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 6] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 7] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 8] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 9] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 10] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 11] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 12] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 13] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 14] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 15] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 16] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 17] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 18] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 19] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][sample_diffusion] Returning _chunk_sample_diffusion output shape: torch.Size([1, 24, 24, 3])
[DEBUG] After sample_diffusion:
  coords_final shape before squeeze: torch.Size([1, 24, 24, 3])
[DEBUG] Final coords_final shape (handling N_sample=1): torch.Size([1, 24, 3])
[DEBUG] After inference call: 's_inputs' in internal? False, 's_inputs' in original? False

--- Pipeline Output with Examples ---
  adjacency: shape=(8, 8)
  Example values: [[1.  0.8 0.  0.  0. , ...],
 [0.8 1.  0.8 0.  0. , ...],
 [0.  0.8 1.  0.8 0. , ...],
 [0.  0.  0.8 1.  0.8, ...],
 [0.  0.  0.  0.8 1. , ...],
  ...]

  torsion_angles: shape=(8, 14)
  Example values: [[0. 0. 0. 0. 0., ...],
 [0. 0. 0. 0. 0., ...],
 [0. 0. 0. 0. 0., ...],
 [0. 0. 0. 0. 0., ...],
 [0. 0. 0. 0. 0., ...],
  ...]

  s_embeddings: shape=(8, 64)
  Example values: [[-0.1666  0.1335 -0.6008  0.2845 -0.3827, ...],
 [-0.6246 -0.1639  0.1052 -0.4817  0.3954, ...],
 [ 0.4531  0.2407 -0.0378  0.2626  1.8427, ...],
 [-0.2714 -1.127  -0.018  -0.2442  0.0147, ...],
 [-0.6343 -0.6394 -1.6589  0.1607  2.2174, ...],
  ...]

  z_embeddings: shape=(8, 8, 32)
  Example values: First slice: [[ 9.9805212e-01  9.9349099e-01  9.9172014e-01  1.0038146e+00
   1.0179801e+00]
 [ 7.9472911e-01  7.9153287e-01  8.0168766e-01  8.0742830e-01
   7.8446329e-01]
 [-1.4291719e-02 -9.0734651e-03 -8.0050565e-03 -5.3094202e-03
  -5.2196216e-03]
 [ 1.1459215e-02 -2.0848649e-02  2.8669229e-03 -1.9915698e-02
   9.5198266e-05]
 [-1.3927281e-03  1.3033076e-02 -1.4831028e-02  4.6895989e-03
   1.3219562e-02]]

  partial_coords: shape=(1, 24, 3)
  Example values: First slice: [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]

  unified_latent: shape=(8, 128)
  Example values: [[-0.1532  0.0878 -0.1435  0.1721 -0.0109, ...],
 [ 0.2607  0.0868 -0.034  -0.2112 -0.0417, ...],
 [-0.2441  0.0252 -0.1619  0.4523 -0.2037, ...],
 [ 0.0035  0.2701 -0.3386  0.003  -0.1068, ...],
 [-0.0322 -0.0189 -0.1143 -0.1901  0.1907, ...],
  ...]

  final_coords: shape=(1, 24, 3)
  Example values: First slice: [[  3648.532     -861.98865   1030.851  ]
 [ -5717.0503    1293.2711    2756.975  ]
 [  8785.246    -4718.088    13372.416  ]
 [ -1075.2733    1589.1508     654.94495]
 [-11379.0625   -4241.426    21530.56   ]]

Done.
STDERR:
W0407 00:49:32.167000 8676640832 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:165: UserWarning: input_features not provided, using basic fallback based on partial_coords.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:68: UserWarning: Adjusting sequence length for s_trunk from 8 to 24
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:88: UserWarning: Adjusting sequence lengths for pair from (8, 8) to (24, 24)
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/encoder_components/feature_processing.py:131: UserWarning: ref_space_uid has wrong shape torch.Size([1, 24]), expected [..., 3]. Setting to zeros.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils.py:344: UserWarning: Clipping atom_to_token_idx: max index 23 >= N_token 8. Original index shape: torch.Size([1, 24]), Token shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 8, 128]), shift=torch.Size([1, 8, 128]), a=torch.Size([1, 1, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 24, 128]), s shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:380: UserWarning: Shape mismatch & broadcast failed between a_token (torch.Size([1, 1, 24, 768])) and projected s_single (torch.Size([1, 8, 768])). Skipping addition. Error: The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 2
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 8, 768]), shift=torch.Size([1, 8, 768]), a=torch.Size([1, 1, 24, 768])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 24, 768]), s shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/atom_attention_decoder.py:169: UserWarning: Conditioning signal 's' has incorrect feature dim 128, expected 64. Adapting.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 1, 24, 128]), shift=torch.Size([1, 1, 24, 128]), a=torch.Size([1, 24, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 128]), s shape: torch.Size([1, 1, 24, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:476: UserWarning: Broadcasting t_hat (torch.Size([1, 1])) to x_noisy (torch.Size([1, 24, 24, 3])) might be ambiguous.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils.py:344: UserWarning: Clipping atom_to_token_idx: max index 23 >= N_token 8. Original index shape: torch.Size([1, 24, 24]), Token shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 24, 8, 128]), shift=torch.Size([1, 24, 8, 128]), a=torch.Size([1, 24, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 128]), s shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:380: UserWarning: Shape mismatch & broadcast failed between a_token (torch.Size([1, 24, 24, 768])) and projected s_single (torch.Size([1, 24, 8, 768])). Skipping addition. Error: The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 2
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 24, 8, 768]), shift=torch.Size([1, 24, 8, 768]), a=torch.Size([1, 24, 24, 768])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 768]), s shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/protenix_diffusion_manager.py:289: UserWarning: Observed unexpected shape torch.Size([1, 24, 24, 3]) for N_sample=1. Expected shape like [B, 1, N_atom, 3]. Selecting first element along dimension 1.
  warnings.warn(


================================================================================
Output from: rna_predict/run_full_pipeline.py
Timestamp: 2025-04-07 00:49:35
================================================================================

STDOUT:
[2025-04-07 00:49:35,947] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
Example usage of run_full_pipeline with dummy config.
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
q.shape: torch.Size([16, 8, 4]), k.shape: torch.Size([16, 8, 4]), v.shape: torch.Size([16, 8, 4])
[Debug] Tensor shapes for merger:
  adjacency: torch.Size([8, 8])
  torsion_angles: torch.Size([8, 14])
  s_emb: torch.Size([8, 64])
  z_emb: torch.Size([8, 8, 32])
[Debug] Creating MLP with dimensions: 110 -> 128
[DEBUG] s_trunk shape: torch.Size([1, 8, 64])
[DEBUG] s_inputs is invalid or None!
[DEBUG] z_trunk shape: torch.Size([1, 8, 8, 32])
[DEBUG] Before sample_diffusion:
  coords_init shape: torch.Size([1, 24, 3])
  s_trunk shape: torch.Size([1, 8, 64])
  z_trunk shape: torch.Size([1, 8, 8, 32])
[DEBUG] Determined true_batch_shape: torch.Size([1])
[DEBUG][Generator Loop 0] Before denoise_net - x_noisy: torch.Size([1, 1, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 128 (from torch.Size([1, 1, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 1] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 2] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 3] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 4] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 5] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 6] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 7] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 8] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 9] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 10] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 11] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 12] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 13] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 14] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 15] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 16] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 17] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 18] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][Generator Loop 19] Before denoise_net - x_noisy: torch.Size([1, 24, 24, 3]), t_hat: torch.Size([1, 1])
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 3072 (from torch.Size([1, 24, 2, 8, 8])). Skipping bias application for stability.
Warning: Selected bias shape mismatch in _process_small_tensors. Expected size: 4, actual size: 1152 (from torch.Size([1, 1, 2, 24, 24])). Skipping bias application for stability.
[DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 24, 24, 3])
[DEBUG][sample_diffusion] Returning _chunk_sample_diffusion output shape: torch.Size([1, 24, 24, 3])
[DEBUG] After sample_diffusion:
  coords_final shape before squeeze: torch.Size([1, 24, 24, 3])
[DEBUG] Final coords_final shape (handling N_sample=1): torch.Size([1, 24, 3])
[DEBUG] After inference call: 's_inputs' in internal? False, 's_inputs' in original? False

--- Pipeline Output ---
  adjacency: shape=(8, 8)
  torsion_angles: shape=(8, 14)
  s_embeddings: shape=(8, 64)
  z_embeddings: shape=(8, 8, 32)
  partial_coords: shape=(1, 24, 3)
  unified_latent: shape=(8, 128)
  final_coords: shape=(1, 24, 3)
Done.
STDERR:
W0407 00:49:36.465000 8676640832 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:165: UserWarning: input_features not provided, using basic fallback based on partial_coords.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:68: UserWarning: Adjusting sequence length for s_trunk from 8 to 24
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/run_stageD_unified.py:88: UserWarning: Adjusting sequence lengths for pair from (8, 8) to (24, 24)
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/encoder_components/feature_processing.py:131: UserWarning: ref_space_uid has wrong shape torch.Size([1, 24]), expected [..., 3]. Setting to zeros.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils.py:344: UserWarning: Clipping atom_to_token_idx: max index 23 >= N_token 8. Original index shape: torch.Size([1, 24]), Token shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 8, 128]), shift=torch.Size([1, 8, 128]), a=torch.Size([1, 1, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 24, 128]), s shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:380: UserWarning: Shape mismatch & broadcast failed between a_token (torch.Size([1, 1, 24, 768])) and projected s_single (torch.Size([1, 8, 768])). Skipping addition. Error: The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 2
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 8, 768]), shift=torch.Size([1, 8, 768]), a=torch.Size([1, 1, 24, 768])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 1, 24, 768]), s shape: torch.Size([1, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/atom_attention_decoder.py:169: UserWarning: Conditioning signal 's' has incorrect feature dim 128, expected 64. Adapting.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 1, 24, 128]), shift=torch.Size([1, 1, 24, 128]), a=torch.Size([1, 24, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 128]), s shape: torch.Size([1, 1, 24, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:476: UserWarning: Broadcasting t_hat (torch.Size([1, 1])) to x_noisy (torch.Size([1, 24, 24, 3])) might be ambiguous.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils.py:344: UserWarning: Clipping atom_to_token_idx: max index 23 >= N_token 8. Original index shape: torch.Size([1, 24, 24]), Token shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 24, 8, 128]), shift=torch.Size([1, 24, 8, 128]), a=torch.Size([1, 24, 24, 128])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 128]), s shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/components/diffusion_module.py:380: UserWarning: Shape mismatch & broadcast failed between a_token (torch.Size([1, 24, 24, 768])) and projected s_single (torch.Size([1, 24, 8, 768])). Skipping addition. Error: The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 2
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:114: UserWarning: WARNING: Skipping adaptive layernorm conditioning due to shape mismatch: Shape mismatch after unsqueeze/projections: scale=torch.Size([1, 24, 8, 768]), shift=torch.Size([1, 24, 8, 768]), a=torch.Size([1, 24, 24, 768])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/attention_base.py:117: UserWarning:          a shape (original): torch.Size([1, 24, 24, 768]), s shape: torch.Size([1, 24, 8, 64])
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD/diffusion/protenix_diffusion_manager.py:289: UserWarning: Observed unexpected shape torch.Size([1, 24, 24, 3]) for N_sample=1. Expected shape like [B, 1, N_atom, 3]. Selecting first element along dimension 1.
  warnings.warn(

