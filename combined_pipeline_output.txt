RNA Pipeline Combined Output
Generated on: 2025-05-08 14:11:29
Using standardized test sequence: ACGUACGU
================================================================================


================================================================================
Output from: pipeline/stageA/run_stageA.py
Timestamp: 2025-05-08 14:11:29
================================================================================

STDOUT:
[2025-05-08 14:11:30,480][rna_predict.pipeline.stageA.run_stageA][INFO] - Starting Stage A pipeline with Hydra config
[2025-05-08 14:11:30,499][rna_predict.pipeline.stageA.run_stageA][INFO] - Config:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-08 14:11:30,543][rna_predict.pipeline.stageA.run_stageA][INFO] - [Hydra Config] Full config:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-08 14:11:30,543][rna_predict.pipeline.stageA.run_stageA][INFO] - [Hydra Config] Loaded Stage A config:
{'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}
[2025-05-08 14:11:30,873][rna_predict.pipeline.stageA.run_stageA][INFO] - [Info] File already exists and is valid zip, skipping download: RFold/checkpoints.zip
[2025-05-08 14:11:30,873][rna_predict.pipeline.stageA.run_stageA][INFO] - [Unzip] Extracting RFold/checkpoints.zip into RFold
[2025-05-08 14:11:31,271][rna_predict.pipeline.stageA.run_stageA][INFO] - [HYDRA-DEBUG][StageA] Resolved stage_cfg.device: cpu
[2025-05-08 14:11:31,272][rna_predict.pipeline.stageA.run_stageA][INFO] - [HYDRA-DEBUG][StageA] Global cfg.device: cpu
[2025-05-08 14:11:31,274][rna_predict.pipeline.stageA.run_stageA][INFO] - [Device] Using device: cpu
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-08 14:11:31,277][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-08 14:11:31,351][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-08 14:11:31,351][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-08 14:11:31,353][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:31,382][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-08 14:11:31,385][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-08 14:11:31,385][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-08 14:11:31,385][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 244.60 MB
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.run_stageA][INFO] - [Example] Running inference on standardized test sequence: ACGUACGU (length: 8)
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-08 14:11:31,386][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-08 14:11:31,391][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-08 14:11:31,391][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:31,391][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-08 14:11:31,391][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-08 14:11:31,393][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:31,534][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-08 14:11:31,535][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-08 14:11:31,535][rna_predict.pipeline.stageA.run_stageA][INFO] - [Example] Adjacency shape: (8, 8)
[2025-05-08 14:11:31,536][rna_predict.pipeline.stageA.run_stageA][INFO] - [Visualization] Attempting with JAR: tools/varna-3-93.jar
[2025-05-08 14:11:31,536][rna_predict.pipeline.stageA.run_stageA][WARNING] - [Warning] VARNA JAR not found at: tools/varna-3-93.jar -> skipping visualization.
STDERR:
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 244.60 MB
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-08 14:11:31][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32


================================================================================
Output from: pipeline/stageB/main.py
Timestamp: 2025-05-08 14:11:31
================================================================================

STDOUT:
[2025-05-08 14:11:32,972] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-08 14:11:36,233][rna_predict.pipeline.stageB.main][INFO] - [DEBUG-SEQUENCE-ENTRY-STAGEB] type=<class 'str'>, value=ACGUACGU
[2025-05-08 14:11:36,237][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-08 14:11:36,240][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-08 14:11:36,241][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-08 14:11:36,241][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-08 14:11:36,241][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-08 14:11:36,241][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-08 14:11:36,241][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-08 14:11:36,441][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-08 14:11:36,441][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-08 14:11:36,442][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:36,483][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 353.21 MB
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-08 14:11:36,484][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-08 14:11:36,485][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-08 14:11:36,485][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-08 14:11:36,485][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:36,485][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-08 14:11:36,485][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-08 14:11:36,486][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:36,551][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-08 14:11:36,553][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-08 14:11:36,555][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: True
[2025-05-08 14:11:36,555][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: True, None, None
[2025-05-08 14:11:36,555][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-08 14:11:36,555][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-08 14:11:36,555][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-INST-STAGEB-002] Full config received in StageBTorsionBertPredictor: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-08 14:11:36,557][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.device
[2025-05-08 14:11:36,557][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:36,558][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using direct attributes
[2025-05-08 14:11:36,558][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-08 14:11:36,560][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 398.75 MB
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-08 14:11:36,561][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-08 14:11:38,571][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:11:38,571][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:11:38,572][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:11:38,572][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:11:38,578][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-08 14:11:38,578][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:11:38,579][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:11:38,579][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-08 14:11:38,579][rna_predict.pipeline.stageB.main][INFO] - [DEBUG-SEQUENCE-BEFORE-STAGEB] type=<class 'str'>, value=ACGUACGU
[CASCADE-DEBUG] BEFORE STAGE B: type=<class 'str'>, value=ACGUACGU
[2025-05-08 14:11:38,579][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Received adjacency matrix with shape torch.Size([8, 8]), but it is not used
[2025-05-08 14:11:38,579][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-08 14:11:38,581][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-08 14:11:38,584][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-08 14:11:38,584][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-08 14:11:39,629][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-08 14:11:39,629][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[CASCADE-DEBUG] AFTER STAGE B: type=<class 'str'>, value=ACGUACGU
[CASCADE-DEBUG] BEFORE STAGE C: type=<class 'str'>, value=ACGUACGU
[2025-05-08 14:11:39,630][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-08 14:11:39,631][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 732.38 MB
[2025-05-08 14:11:39,631][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-08 14:11:39,631][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 732.45 MB
[2025-05-08 14:11:39,652][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: False
[2025-05-08 14:11:39,652][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: None, None, None
[2025-05-08 14:11:39,652][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-08 14:11:39,652][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-08 14:11:39,653][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.stageB.torsion_bert.device
[2025-05-08 14:11:39,653][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:39,654][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using cfg.stageB.torsion_bert
[2025-05-08 14:11:39,654][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-08 14:11:39,655][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:39,655][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 742.80 MB
[2025-05-08 14:11:41,142][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:11:41,143][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 699.61 MB
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][WARNING] - [UNIQUE-WARN-PAIRFORMER-DUMMYMODE] Config missing or incomplete, entering dummy mode
[2025-05-08 14:11:41,145][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=2, c_z=32, c_s=64, dropout=0.1
[2025-05-08 14:11:41,145][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-08 14:11:41,166][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-08 14:11:41,166][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 708.51 MB
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 8, 64]), s.shape=torch.Size([1, 8, 64]), z.shape=torch.Size([1, 8, 8, 32])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 8, 64]), s.shape=torch.Size([1, 8, 64])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=True, a_norm.shape=torch.Size([1, 8, 64])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=False, a_norm.grad_fn=<NativeLayerNormBackward0 object at 0x14719a3e0>
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Error in stack forward pass: [AdaLN] s.shape[-1] (64) does not match layernorm_s.normalized_shape (384)
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Input shapes: s=torch.Size([1, 8, 64]), z_adjusted=torch.Size([1, 8, 8, 32]), pair_mask=torch.Size([1, 8, 8])
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Stack config: c_z=32, c_s=64
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.main][ERROR] - Error in pairformer_model: [AdaLN] s.shape[-1] (64) does not match layernorm_s.normalized_shape (384)
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.main][ERROR] - Stack trace:
Traceback (most recent call last):
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageB/main.py", line 153, in run_stageB_combined
    pairformer_output = pairformer_model(init_s, init_z_tensor, pair_mask)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageB/pairwise/pairformer_wrapper.py", line 312, in forward
    s_updated, z_updated = self.stack(
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageB/pairwise/pairformer.py", line 392, in forward
    s, z = checkpoint_blocks(
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 89, in checkpoint_blocks
    return exec(blocks, args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageB/pairwise/pairformer.py", line 241, in forward
    s = s + self.attention_pair_bias(
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/attention.py", line 640, in forward
    a = self.layernorm_a(a=a, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm.py", line 179, in forward
    assert s.shape[-1] == self.layernorm_s.normalized_shape[0], (
AssertionError: [AdaLN] s.shape[-1] (64) does not match layernorm_s.normalized_shape (384)
STDERR:
W0508 14:11:33.568000 8696171584 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 353.21 MB
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 398.75 MB
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-08 14:11:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Received adjacency matrix with shape torch.Size([8, 8]), but it is not used
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-08 14:11:38][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-08 14:11:39][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-08 14:11:39][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 732.38 MB
[2025-05-08 14:11:39][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-08 14:11:39][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 732.45 MB
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: False
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: None, None, None
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.stageB.torsion_bert.device
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using cfg.stageB.torsion_bert
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 742.80 MB
[2025-05-08 14:11:41][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:11:41][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 699.61 MB
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}
[2025-05-08 14:11:41,144][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][WARNING] - [UNIQUE-WARN-PAIRFORMER-DUMMYMODE] Config missing or incomplete, entering dummy mode
[2025-05-08 14:11:41,145][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=2, c_z=32, c_s=64, dropout=0.1
[2025-05-08 14:11:41,145][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:41,146][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-08 14:11:41,166][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-08 14:11:41,166][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 708.51 MB
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Error in stack forward pass: [AdaLN] s.shape[-1] (64) does not match layernorm_s.normalized_shape (384)
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Input shapes: s=torch.Size([1, 8, 64]), z_adjusted=torch.Size([1, 8, 8, 32]), pair_mask=torch.Size([1, 8, 8])
[2025-05-08 14:11:41,758][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][ERROR] - Stack config: c_z=32, c_s=64


================================================================================
Output from: pipeline/stageC/stage_c_reconstruction.py
Timestamp: 2025-05-08 14:11:42
================================================================================

STDOUT:
[2025-05-08 14:11:44,688][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Debug logging is enabled for StageC.
[2025-05-08 14:11:44,688][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44,688][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44,688][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44,688][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44,690][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - Running Stage C with Hydra configuration:
[2025-05-08 14:11:44,703][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-08 14:11:44,704][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using standardized test sequence: ACGUACGU with 7 torsion angles
[2025-05-08 14:11:44,705][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Running Stage C for sequence: ACGUACGU
[2025-05-08 14:11:44,705][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using dummy torsions shape: torch.Size([8, 7])
[2025-05-08 14:11:44,705][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44,705][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44,706][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44,706][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: False
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: None
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44,707][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44,708][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: ${device}
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 7])
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-08 14:11:44,709][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 7])
[2025-05-08 14:11:44,712][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: False
[2025-05-08 14:11:44,712][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: None
[2025-05-08 14:11:44,715][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: False
[2025-05-08 14:11:44,715][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: None
[2025-05-08 14:11:44,715][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-08 14:11:44,720][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-3.4510, -5.7782,  5.2362, -1.0664,  3.8396,  0.8487, -3.1863])
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-08 14:11:44,721][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-08 14:11:44,737][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.2242, 37.2459, 24.6995],
        [17.2699, 38.4976, 24.9452],
        [16.7420, 37.1763, 25.1673],
        [18.2265, 37.0274, 25.4002],
        [18.0343, 38.4501, 25.6243],
        [16.7834, 37.6553, 25.8556],
        [17.8739, 36.7679, 26.0751],
        [18.2774, 38.1940, 26.3065],
        [16.8857, 38.0832, 26.5244],
        [17.4136, 36.6984, 26.7557]])
[2025-05-08 14:11:44,737][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: False
[2025-05-08 14:11:44,737][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: None
[2025-05-08 14:11:44,737][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-08 14:11:44,737][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: False, grad_fn: None
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: False, grad_fn: None
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: False, grad_fn: None
[2025-05-08 14:11:44,799][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: False
[2025-05-08 14:11:44,799][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: None
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: False
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: None
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.02s
[2025-05-08 14:11:44,802][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.02s
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Stage C Output:
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords shape: torch.Size([170, 3])
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords 3D shape: torch.Size([8, 23, 3])
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Atom count: 170
[2025-05-08 14:11:44,802][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Output device: cpu
STDERR:
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - Running Stage C with Hydra configuration:
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using standardized test sequence: ACGUACGU with 7 torsion angles
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Running Stage C for sequence: ACGUACGU
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using dummy torsions shape: torch.Size([8, 7])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: ${device}
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 7])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 7])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-3.4510, -5.7782,  5.2362, -1.0664,  3.8396,  0.8487, -3.1863])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.2242, 37.2459, 24.6995],
        [17.2699, 38.4976, 24.9452],
        [16.7420, 37.1763, 25.1673],
        [18.2265, 37.0274, 25.4002],
        [18.0343, 38.4501, 25.6243],
        [16.7834, 37.6553, 25.8556],
        [17.8739, 36.7679, 26.0751],
        [18.2774, 38.1940, 26.3065],
        [16.8857, 38.0832, 26.5244],
        [17.4136, 36.6984, 26.7557]])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: False, grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: False
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: None
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.02s
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Stage C Output:
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords shape: torch.Size([170, 3])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords 3D shape: torch.Size([8, 23, 3])
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Atom count: 170
[2025-05-08 14:11:44][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Output device: cpu


================================================================================
Output from: pipeline/stageD/run_stageD.py
Timestamp: 2025-05-08 14:11:45
================================================================================

STDOUT:
[HYDRA DEBUG] CWD: /Users/tomriddle1/RNA_PREDICT/rna_predict
[HYDRA DEBUG] SCRIPT DIR: /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD
[HYDRA DEBUG] sys.path: ['/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python310.zip', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/lib-dynload', '/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages', '__editable__.rna_predict-1.0.0.finder.__path_hook__', '/Users/tomriddle1/RNA_PREDICT/auto-type-annotate']
[CONFIG DEBUG] Top-level keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'data', 'model', 'stageD_diffusion', 'test_data']
[CONFIG DEBUG] model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-08 14:11:46,201][__main__][DEBUG] - [hydra_main] cfg.model: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-08 14:11:46,201][__main__][DEBUG] - [hydra_main] cfg.model.stageD: {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}
[2025-05-08 14:11:46,202][__main__][INFO] - Running Stage D Standalone Demo
[2025-05-08 14:11:46,202][__main__][DEBUG] - [UNIQUE-DEBUG-STAGED-TEST] Stage D runner started.
[2025-05-08 14:11:46,202][__main__][DEBUG] - Using standardized test sequence: ACGU with 44 atoms per residue
[2025-05-08 14:11:46,202][__main__][DEBUG] - [run_stageD] ENTRY: z_trunk.shape = torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] s_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] z_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] s_inputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict type: <class 'dict'>
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] atom_metadata type: <class 'dict'>
[2025-05-08 14:11:46,204][__main__][DEBUG] - [run_stageD] trunk_embeddings type: <class 'NoneType'>
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] features type: <class 'NoneType'>
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] stage_cfg type: <class 'NoneType'>
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] diffusion_cfg type: <class 'NoneType'>
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] device: None
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] mode: None
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] debug_logging: True
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] s_trunk shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] s_inputs shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] num_atoms: 176
[MEMORY-LOG][StageD ENTRY] Memory usage: 171.78 MB
[2025-05-08 14:11:46,205][__main__][DEBUG] - [DEBUG][run_stageD] Starting Stage D implementation
[DEBUG][run_stageD] Starting Stage D implementation
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict at Stage D entry:
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[DEBUG][run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[DEBUG][run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[DEBUG][run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:46,205][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[DEBUG][run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:46,206][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(atom_metadata): <class 'dict'>, keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-08 14:11:46,206][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(residue_indices): <class 'torch.Tensor'>, len: 176
[2025-05-08 14:11:46,206][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices.shape: torch.Size([176])
[2025-05-08 14:11:46,206][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices (first 20): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:11:46,206][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] Dumping config values before diffusion:
[2025-05-08 14:11:46,207][__main__][DEBUG] -   n_atoms: 176, n_residues: 4
[2025-05-08 14:11:46,207][__main__][DEBUG] -   s_trunk.shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,207][__main__][DEBUG] -   s_inputs.shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:46,207][__main__][DEBUG] -   z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:46,207][__main__][DEBUG] -   atom_metadata keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-08 14:11:46,207][__main__][DEBUG] -   context.device: None
[2025-05-08 14:11:46,207][__main__][DEBUG] -   context.mode: None
[2025-05-08 14:11:46,207][__main__][DEBUG] -   context.debug_logging: True
[2025-05-08 14:11:46,207][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] END CONFIG DUMP
[2025-05-08 14:11:46,310][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2025-05-08 14:11:46,393][rna_predict.utils.tensor_utils.types][DEBUG] - Expanded residue embeddings torch.Size([4, 8]) to atom embeddings torch.Size([176, 8])
[2025-05-08 14:11:46,393][__main__][DEBUG] - [HYDRA-CONF-BRIDGE][StageD] Bridged s_trunk to atom-level: torch.Size([1, 176, 8])
[MEMORY-LOG][Before bridging residue-to-atom] Memory usage: 209.12 MB
[MEMORY-LOG][After bridging residue-to-atom] Memory usage: 209.52 MB
[MEMORY-LOG][Before diffusion] Memory usage: 209.52 MB
[2025-05-08 14:11:46,396][__main__][DEBUG] - [run_stageD] Calling unified Stage D runner with DiffusionConfig.
[2025-05-08 14:11:46,397][__main__][INFO] - [HYDRA-DEBUG][StageD] stage_cfg.device: cpu
[2025-05-08 14:11:46,397][__main__][INFO] - [HYDRA-DEBUG][StageD] global cfg.device: cpu
[2025-05-08 14:11:46,542] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-08 14:11:48,634][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Initializing ProtenixDiffusionManager
[2025-05-08 14:11:48,634][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage: 383.17 MB
[2025-05-08 14:11:48,635][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: cfg.model.stageD.diffusion.device=cpu
[2025-05-08 14:11:48,637][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: full cfg.model.stageD.diffusion={'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': True, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[2025-05-08 14:11:48,638][rna_predict.pipeline.stageD.diffusion.utils.config_utils][DEBUG] - [DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg.keys(): ['enabled', 'mode', 'device', 'debug_logging', 'ref_element_size', 'ref_atom_name_chars_size', 'profile_size', 'test_residues_per_batch', 'model_architecture', 'transformer', 'atom_encoder', 'atom_decoder', 'noise_schedule', 'inference', 'use_memory_efficient_kernel', 'use_deepspeed_evo_attention', 'use_lma', 'inplace_safe', 'chunk_size', 'init_from_scratch', 'feature_dimensions']
[DEBUG][DiffusionModule.__init__] cfg.model_architecture: {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}
[DEBUG] DiffusionModule.__init__ kwargs: {}
[2025-05-08 14:11:48,638][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] self.debug_logging resolved to: True
[2025-05-08 14:11:48,638][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] config subtree used: True, None, None
[2025-05-08 14:11:48,638][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] full config: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg attributes: ['atom_decoder', 'atom_encoder', 'chunk_size', 'debug_logging', 'device', 'enabled', 'feature_dimensions', 'inference', 'init_from_scratch', 'inplace_safe', 'mode', 'model_architecture', 'noise_schedule', 'profile_size', 'ref_atom_name_chars_size', 'ref_element_size', 'test_residues_per_batch', 'transformer', 'use_deepspeed_evo_attention', 'use_lma', 'use_memory_efficient_kernel']
[DiffusionModule][__init__] Using OUTER ref_element_size: 128
[DiffusionModule][__init__] Using OUTER ref_atom_name_chars_size: 256
[2025-05-08 14:11:48,639][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - Instantiating DiffusionConditioning with: sigma_data=1.0, c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4, debug_logging=True
[DEBUG][DiffusionConditioning.__init__] c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4
[DEBUG][DiffusionConditioning] expected_z_dim: 8
[DEBUG][DiffusionConditioning] layernorm_s normalized_shape: 16
[DEBUG][DiffusionConditioning] layernorm_n normalized_shape: 4
[2025-05-08 14:11:48,640][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - encoder_config_dict: {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}
[2025-05-08 14:11:48,641][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-08 14:11:48,642][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[2025-05-08 14:11:48,643][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-08 14:11:48,645][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - DiffusionModule config: enabled: true
mode: inference
device: ${device}
debug_logging: true
ref_element_size: ${shared.ref_element_size}
ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
profile_size: ${shared.profile_size}
test_residues_per_batch: 2
model_architecture:
  c_token: 8
  c_s: 8
  c_z: 4
  c_s_inputs: 8
  c_atom: 4
  c_atompair: 4
  c_noise_embedding: 4
  sigma_data: 1.0
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  coord_eps: 1.0e-06
  coord_min: -10000.0
  coord_max: 10000.0
  coord_similarity_rtol: 0.001
  test_residues_per_batch: 25
transformer:
  n_blocks: 2
  n_heads: 2
  blocks_per_ckpt: null
atom_encoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
atom_decoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
noise_schedule:
  schedule_type: linear
  s_max: 1.0
  s_min: 0.01
  p: 0.5
  p_mean: 0.0
  p_std: 1.0
inference:
  num_steps: 2
  temperature: 1.0
  use_ddim: true
  sampling:
    num_samples: 1
    gamma0: 0.8
    gamma_min: 1.0
    noise_scale_lambda: 1.003
    step_scale_eta: 1.5
use_memory_efficient_kernel: false
use_deepspeed_evo_attention: false
use_lma: false
inplace_safe: false
chunk_size: null
init_from_scratch: false
feature_dimensions:
  c_s: 8
  c_s_inputs: 8
  c_sing: 8
  s_trunk: 8
  s_inputs: 8

[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] After super().__init__
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage after initialization: 384.22 MB
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][coords_init] original shape: torch.Size([1, 176, 3]), patched shape: torch.Size([1, 4096, 3]), max_atoms=4096
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][s_trunk] original shape: torch.Size([1, 176, 8]), patched shape: torch.Size([1, 4096, 8]), max_atoms=4096
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][s_inputs] original shape: torch.Size([1, 176, 8]), patched shape: torch.Size([1, 4096, 8]), max_atoms=4096
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [HYDRA-CONF-DEBUG][StageD] Using num_steps from config: 2
[2025-05-08 14:11:48,646][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [DEBUG][get_s_inputs] trunk_embeddings keys: ['s_trunk', 's_inputs', 'pair']
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [EmbeddingContext] Initial z_trunk shape: torch.Size([1, 4, 4, 4])
[MEMORY-LOG][After get_z_trunk return] Memory usage: 366.72 MB
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] z_trunk shape at entry: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] coords_init shape: torch.Size([1, 4096, 3])
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,647][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [INSTRUMENT][FeaturePreparationContext] max_atoms=4096, coords_init.shape=torch.Size([1, 4096, 3])
[2025-05-08 14:11:48,648][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Before sample_diffusion:
[2025-05-08 14:11:48,648][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   coords_init shape: torch.Size([1, 4096, 3])
[2025-05-08 14:11:48,648][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,649][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   noise_schedule (len=3): tensor([1.0000, 0.5000, 0.0000])
[2025-05-08 14:11:48,649][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   num_steps from config: 2
[2025-05-08 14:11:48,649][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   schedule_type from config: linear
[MEMORY-LOG][Before diffusion step 0] Memory usage: 365.59 MB
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-08 14:11:48,714][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:48,714][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,715][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,716][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-08 14:11:48,716][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:48,716][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:48,716][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,716][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 4096, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 4096, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 4096, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:48,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:48,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] encoder.input_feature expected keys: ['ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars']
[DEBUG][extract_atom_features] encoder.input_feature dict: {'ref_pos': 3, 'ref_charge': 1, 'ref_mask': 1, 'ref_element': 128, 'ref_atom_name_chars': 256}
[DEBUG][extract_atom_features] SUM expected feature dim: 389
[DEBUG][extract_atom_features] input_feature_dict actual keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
[DEBUG][extract_atom_features] key='ref_charge' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_pos' shape=torch.Size([1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='atom_to_token_idx' shape=torch.Size([1, 176]) dtype=torch.int64
[DEBUG][extract_atom_features] key='ref_mask' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_element' shape=torch.Size([1, 176, 128]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_atom_name_chars' shape=torch.Size([1, 176, 256]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_space_uid' shape=torch.Size([1, 1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='profile' shape=torch.Size([1, 176, 32]) dtype=torch.float32
[DEBUG][extract_atom_features] key='expected_n_tokens' type=<class 'int'>
[DEBUG][extract_atom_features] SUM actual feature last dims: 600
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:48,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-08 14:11:50,051][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-08 14:11:50,055][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:50,061][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-08 14:11:50,061][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:50,061][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-08 14:11:50,061][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:50,066][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[2025-05-08 14:11:50,067][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x304ca5c60>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x304ca5c60>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:50,137][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:50,138][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:50,138][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:50,139][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:50,140][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:50,140][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:50,143][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:50,178][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad=True, shape=torch.Size([1, 176, 8])
[DEBUG][aggregate_atom_to_token] x_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), n_token=4
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,238][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,239][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:50,239][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-08 14:11:50,239][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,239][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-08 14:11:50,239][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[DEBUG][scatter_mean-call] a_token.shape[-2] = 4
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].min() = 0
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].max() = 3
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] index shape: torch.Size([176]), values: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] dim_size (N_token): 4
[DEBUG][scatter_mean-fallback] dim_size = 4
[DEBUG][scatter_mean-fallback] index.min() = 0
[DEBUG][scatter_mean-fallback] index.max() = 3
[DEBUG][scatter_mean-fallback] index = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[2025-05-08 14:11:50,240][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x31e66b610>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b610>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x31e66b610>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:50,249][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,249][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:50,249][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:50,249][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:50,249][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:50,250][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b6a0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b6a0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b700>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:50,276][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,276][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:50,276][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:50,277][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:50,277][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:50,277][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b730>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b730>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:50,293][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,293][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-08 14:11:50,293][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x31e66b7c0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66b7c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x31e66b7c0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:50,296][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66b820>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66b820>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-08 14:11:50,297][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-08 14:11:50,298][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 0.4843774139881134
[MEMORY-LOG][After diffusion step 0] Memory usage: 331.84 MB
[MEMORY-LOG][Before diffusion step 1] Memory usage: 330.28 MB
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-08 14:11:50,605][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-08 14:11:50,605][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,606][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:50,607][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,607][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 4096, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 4096, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,609][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 4096, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:50,610][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:50,610][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] encoder.input_feature expected keys: ['ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars']
[DEBUG][extract_atom_features] encoder.input_feature dict: {'ref_pos': 3, 'ref_charge': 1, 'ref_mask': 1, 'ref_element': 128, 'ref_atom_name_chars': 256}
[DEBUG][extract_atom_features] SUM expected feature dim: 389
[DEBUG][extract_atom_features] input_feature_dict actual keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
[DEBUG][extract_atom_features] key='ref_charge' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_pos' shape=torch.Size([1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='atom_to_token_idx' shape=torch.Size([1, 176]) dtype=torch.int64
[DEBUG][extract_atom_features] key='ref_mask' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_element' shape=torch.Size([1, 176, 128]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_atom_name_chars' shape=torch.Size([1, 176, 256]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_space_uid' shape=torch.Size([1, 1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='profile' shape=torch.Size([1, 176, 32]) dtype=torch.float32
[DEBUG][extract_atom_features] key='expected_n_tokens' type=<class 'int'>
[DEBUG][extract_atom_features] SUM actual feature last dims: 600
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046],
         [-0.0148,  0.0262, -0.0423, -0.0046]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:50,614][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-08 14:11:51,548][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[2025-05-08 14:11:51,549][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x31e66b820>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x31e66b820>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:51,559][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:51,559][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:51,559][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:51,559][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:51,559][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:51,560][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:51,560][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:51,580][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad=True, shape=torch.Size([1, 176, 8])
[DEBUG][aggregate_atom_to_token] x_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), n_token=4
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,599][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,599][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:51,599][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-08 14:11:51,600][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,600][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-08 14:11:51,600][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[DEBUG][scatter_mean-call] a_token.shape[-2] = 4
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].min() = 0
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].max() = 3
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] index shape: torch.Size([176]), values: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] dim_size (N_token): 4
[DEBUG][scatter_mean-fallback] dim_size = 4
[DEBUG][scatter_mean-fallback] index.min() = 0
[DEBUG][scatter_mean-fallback] index.max() = 3
[DEBUG][scatter_mean-fallback] index = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[2025-05-08 14:11:51,600][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x31e66ba60>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66ba60>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x31e66ba60>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:51,606][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b1c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66b1c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66baf0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:51,625][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,626][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:51,626][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:51,626][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:51,626][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:51,626][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66bb80>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x31e66bb80>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:51,640][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,641][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,641][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x31e66bac0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66bac0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x31e66bac0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:51,642][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66bbb0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x31e66bbb0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-08 14:11:51,643][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 8.132561683654785
[MEMORY-LOG][After diffusion step 1] Memory usage: 554.64 MB
[2025-05-08 14:11:51,709][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [PATCHED] Found multi-sample coords with shape torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,710][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [PATCHED] Squeezed singleton sample dimension: new shape torch.Size([1, 176, 3])
[2025-05-08 14:11:51,710][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Final coords shape: torch.Size([1, 176, 3])
[MEMORY-LOG][After diffusion] Memory usage: 554.64 MB
[2025-05-08 14:11:51,710][__main__][INFO] - Successfully refined coordinates: torch.Size([1, 176, 3])
[2025-05-08 14:11:51,711][__main__][INFO] - Running Stage D Standalone Demo
[2025-05-08 14:11:51,711][__main__][DEBUG] - [UNIQUE-DEBUG-STAGED-TEST] Stage D runner started.
[2025-05-08 14:11:51,711][__main__][DEBUG] - Using standardized test sequence: ACGU with 44 atoms per residue
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] ENTRY: z_trunk.shape = torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] s_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] z_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] s_inputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict type: <class 'dict'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] atom_metadata type: <class 'dict'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] trunk_embeddings type: <class 'NoneType'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] features type: <class 'NoneType'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] stage_cfg type: <class 'NoneType'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] diffusion_cfg type: <class 'NoneType'>
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] device: None
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] mode: None
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] debug_logging: True
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] s_trunk shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,711][__main__][DEBUG] - [run_stageD] s_inputs shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] num_atoms: 176
[MEMORY-LOG][StageD ENTRY] Memory usage: 554.91 MB
[2025-05-08 14:11:51,712][__main__][DEBUG] - [DEBUG][run_stageD] Starting Stage D implementation
[DEBUG][run_stageD] Starting Stage D implementation
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict at Stage D entry:
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[DEBUG][run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[DEBUG][run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[DEBUG][run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:51,712][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[DEBUG][run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:51,713][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(atom_metadata): <class 'dict'>, keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-08 14:11:51,713][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(residue_indices): <class 'torch.Tensor'>, len: 176
[2025-05-08 14:11:51,713][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices.shape: torch.Size([176])
[2025-05-08 14:11:51,713][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices (first 20): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:11:51,713][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] Dumping config values before diffusion:
[2025-05-08 14:11:51,713][__main__][DEBUG] -   n_atoms: 176, n_residues: 4
[2025-05-08 14:11:51,713][__main__][DEBUG] -   s_trunk.shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,713][__main__][DEBUG] -   s_inputs.shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:51,713][__main__][DEBUG] -   z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,713][__main__][DEBUG] -   atom_metadata keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-08 14:11:51,713][__main__][DEBUG] -   context.device: None
[2025-05-08 14:11:51,713][__main__][DEBUG] -   context.mode: None
[2025-05-08 14:11:51,713][__main__][DEBUG] -   context.debug_logging: True
[2025-05-08 14:11:51,713][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] END CONFIG DUMP
[2025-05-08 14:11:51,714][rna_predict.utils.tensor_utils.types][DEBUG] - Expanded residue embeddings torch.Size([4, 8]) to atom embeddings torch.Size([176, 8])
[2025-05-08 14:11:51,714][__main__][DEBUG] - [HYDRA-CONF-BRIDGE][StageD] Bridged s_trunk to atom-level: torch.Size([1, 176, 8])
[MEMORY-LOG][Before bridging residue-to-atom] Memory usage: 555.03 MB
[MEMORY-LOG][After bridging residue-to-atom] Memory usage: 555.16 MB
[MEMORY-LOG][Before diffusion] Memory usage: 555.16 MB
[2025-05-08 14:11:51,717][__main__][DEBUG] - [run_stageD] Calling unified Stage D runner with DiffusionConfig.
[2025-05-08 14:11:51,717][__main__][INFO] - [HYDRA-DEBUG][StageD] stage_cfg.device: cpu
[2025-05-08 14:11:51,718][__main__][INFO] - [HYDRA-DEBUG][StageD] global cfg.device: cpu
[2025-05-08 14:11:51,718][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Initializing ProtenixDiffusionManager
[2025-05-08 14:11:51,718][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage: 582.12 MB
[2025-05-08 14:11:51,718][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: cfg.model.stageD.diffusion.device=cpu
[2025-05-08 14:11:51,719][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: full cfg.model.stageD.diffusion={'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': True, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[2025-05-08 14:11:51,721][rna_predict.pipeline.stageD.diffusion.utils.config_utils][DEBUG] - [DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg.keys(): ['enabled', 'mode', 'device', 'debug_logging', 'ref_element_size', 'ref_atom_name_chars_size', 'profile_size', 'test_residues_per_batch', 'model_architecture', 'transformer', 'atom_encoder', 'atom_decoder', 'noise_schedule', 'inference', 'use_memory_efficient_kernel', 'use_deepspeed_evo_attention', 'use_lma', 'inplace_safe', 'chunk_size', 'init_from_scratch', 'feature_dimensions']
[DEBUG][DiffusionModule.__init__] cfg.model_architecture: {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}
[DEBUG] DiffusionModule.__init__ kwargs: {}
[2025-05-08 14:11:51,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] self.debug_logging resolved to: True
[2025-05-08 14:11:51,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] config subtree used: True, None, None
[2025-05-08 14:11:51,721][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] full config: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg attributes: ['atom_decoder', 'atom_encoder', 'chunk_size', 'debug_logging', 'device', 'enabled', 'feature_dimensions', 'inference', 'init_from_scratch', 'inplace_safe', 'mode', 'model_architecture', 'noise_schedule', 'profile_size', 'ref_atom_name_chars_size', 'ref_element_size', 'test_residues_per_batch', 'transformer', 'use_deepspeed_evo_attention', 'use_lma', 'use_memory_efficient_kernel']
[DiffusionModule][__init__] Using OUTER ref_element_size: 128
[DiffusionModule][__init__] Using OUTER ref_atom_name_chars_size: 256
[2025-05-08 14:11:51,722][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - Instantiating DiffusionConditioning with: sigma_data=1.0, c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4, debug_logging=True
[DEBUG][DiffusionConditioning.__init__] c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4
[DEBUG][DiffusionConditioning] expected_z_dim: 8
[DEBUG][DiffusionConditioning] layernorm_s normalized_shape: 16
[DEBUG][DiffusionConditioning] layernorm_n normalized_shape: 4
[2025-05-08 14:11:51,722][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - encoder_config_dict: {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}
[2025-05-08 14:11:51,723][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-08 14:11:51,724][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[2025-05-08 14:11:51,725][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-08 14:11:51,727][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - DiffusionModule config: enabled: true
mode: inference
device: ${device}
debug_logging: true
ref_element_size: ${shared.ref_element_size}
ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
profile_size: ${shared.profile_size}
test_residues_per_batch: 2
model_architecture:
  c_token: 8
  c_s: 8
  c_z: 4
  c_s_inputs: 8
  c_atom: 4
  c_atompair: 4
  c_noise_embedding: 4
  sigma_data: 1.0
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  coord_eps: 1.0e-06
  coord_min: -10000.0
  coord_max: 10000.0
  coord_similarity_rtol: 0.001
  test_residues_per_batch: 25
transformer:
  n_blocks: 2
  n_heads: 2
  blocks_per_ckpt: null
atom_encoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
atom_decoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
noise_schedule:
  schedule_type: linear
  s_max: 1.0
  s_min: 0.01
  p: 0.5
  p_mean: 0.0
  p_std: 1.0
inference:
  num_steps: 2
  temperature: 1.0
  use_ddim: true
  sampling:
    num_samples: 1
    gamma0: 0.8
    gamma_min: 1.0
    noise_scale_lambda: 1.003
    step_scale_eta: 1.5
use_memory_efficient_kernel: false
use_deepspeed_evo_attention: false
use_lma: false
inplace_safe: false
chunk_size: null
init_from_scratch: false
feature_dimensions:
  c_s: 8
  c_s_inputs: 8
  c_sing: 8
  s_trunk: 8
  s_inputs: 8

[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] After super().__init__
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage after initialization: 582.66 MB
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][coords_init] original shape: torch.Size([1, 176, 3]), patched shape: torch.Size([1, 4096, 3]), max_atoms=4096
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][s_trunk] original shape: torch.Size([1, 176, 8]), patched shape: torch.Size([1, 4096, 8]), max_atoms=4096
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [PATCH][s_inputs] original shape: torch.Size([1, 176, 8]), patched shape: torch.Size([1, 4096, 8]), max_atoms=4096
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [HYDRA-CONF-DEBUG][StageD] Using num_steps from config: 2
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [DEBUG][get_s_inputs] trunk_embeddings keys: ['s_trunk', 's_inputs', 'pair']
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [EmbeddingContext] Initial z_trunk shape: torch.Size([1, 4, 4, 4])
[MEMORY-LOG][After get_z_trunk return] Memory usage: 555.72 MB
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] z_trunk shape at entry: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,728][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] coords_init shape: torch.Size([1, 4096, 3])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [INSTRUMENT][FeaturePreparationContext] max_atoms=4096, coords_init.shape=torch.Size([1, 4096, 3])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Before sample_diffusion:
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   coords_init shape: torch.Size([1, 4096, 3])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   noise_schedule (len=3): tensor([1.0000, 0.5000, 0.0000])
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   num_steps from config: 2
[2025-05-08 14:11:51,729][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   schedule_type from config: linear
[MEMORY-LOG][Before diffusion step 0] Memory usage: 555.80 MB
[2025-05-08 14:11:51,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-08 14:11:51,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-08 14:11:51,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-08 14:11:51,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-08 14:11:51,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-08 14:11:51,794][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-08 14:11:51,795][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:51,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,796][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 4096, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 4096, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,798][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 4096, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:51,798][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:51,798][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] encoder.input_feature expected keys: ['ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars']
[DEBUG][extract_atom_features] encoder.input_feature dict: {'ref_pos': 3, 'ref_charge': 1, 'ref_mask': 1, 'ref_element': 128, 'ref_atom_name_chars': 256}
[DEBUG][extract_atom_features] SUM expected feature dim: 389
[DEBUG][extract_atom_features] input_feature_dict actual keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
[DEBUG][extract_atom_features] key='ref_charge' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_pos' shape=torch.Size([1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='atom_to_token_idx' shape=torch.Size([1, 176]) dtype=torch.int64
[DEBUG][extract_atom_features] key='ref_mask' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_element' shape=torch.Size([1, 176, 128]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_atom_name_chars' shape=torch.Size([1, 176, 256]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_space_uid' shape=torch.Size([1, 1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='profile' shape=torch.Size([1, 176, 32]) dtype=torch.float32
[DEBUG][extract_atom_features] key='expected_n_tokens' type=<class 'int'>
[DEBUG][extract_atom_features] SUM actual feature last dims: 600
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:51,801][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-08 14:11:52,707][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[2025-05-08 14:11:52,708][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x31e684220>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x31e684220>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:52,714][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:52,714][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:52,714][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:52,714][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:52,714][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:52,715][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:52,715][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:52,734][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad=True, shape=torch.Size([1, 176, 8])
[DEBUG][aggregate_atom_to_token] x_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), n_token=4
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,752][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,752][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:52,752][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-08 14:11:52,752][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,753][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-08 14:11:52,753][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[DEBUG][scatter_mean-call] a_token.shape[-2] = 4
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].min() = 0
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].max() = 3
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] index shape: torch.Size([176]), values: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] dim_size (N_token): 4
[DEBUG][scatter_mean-fallback] dim_size = 4
[DEBUG][scatter_mean-fallback] index.min() = 0
[DEBUG][scatter_mean-fallback] index.max() = 3
[DEBUG][scatter_mean-fallback] index = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[2025-05-08 14:11:52,753][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x304ca71c0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca71c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x304ca71c0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:52,758][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6380>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6380>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca63b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:52,777][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,777][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:52,777][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:52,777][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:52,777][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:52,778][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6410>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6410>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:52,793][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-08 14:11:52,794][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x304ca6470>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca6470>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x304ca6470>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:52,795][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca63e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca63e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-08 14:11:52,796][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 0.46560895442962646
[MEMORY-LOG][After diffusion step 0] Memory usage: 644.08 MB
[MEMORY-LOG][Before diffusion step 1] Memory usage: 644.08 MB
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-08 14:11:52,926][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-08 14:11:52,926][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-08 14:11:52,927][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,928][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:52,928][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,928][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 4096, 8]), s_inputs.shape=torch.Size([1, 4096, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 4096, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 4096, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,930][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 4096, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-08 14:11:52,930][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:52,930][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] encoder.input_feature expected keys: ['ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars']
[DEBUG][extract_atom_features] encoder.input_feature dict: {'ref_pos': 3, 'ref_charge': 1, 'ref_mask': 1, 'ref_element': 128, 'ref_atom_name_chars': 256}
[DEBUG][extract_atom_features] SUM expected feature dim: 389
[DEBUG][extract_atom_features] input_feature_dict actual keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
[DEBUG][extract_atom_features] key='ref_charge' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_pos' shape=torch.Size([1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='atom_to_token_idx' shape=torch.Size([1, 176]) dtype=torch.int64
[DEBUG][extract_atom_features] key='ref_mask' shape=torch.Size([1, 176, 1]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_element' shape=torch.Size([1, 176, 128]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_atom_name_chars' shape=torch.Size([1, 176, 256]) dtype=torch.float32
[DEBUG][extract_atom_features] key='ref_space_uid' shape=torch.Size([1, 1, 176, 3]) dtype=torch.float32
[DEBUG][extract_atom_features] key='profile' shape=torch.Size([1, 176, 32]) dtype=torch.float32
[DEBUG][extract_atom_features] key='expected_n_tokens' type=<class 'int'>
[DEBUG][extract_atom_features] SUM actual feature last dims: 600
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279],
         [-0.0307,  0.0479, -0.0371,  0.0279]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 4096, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[2025-05-08 14:11:52,933][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-08 14:11:53,959][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-08 14:11:53,959][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[2025-05-08 14:11:53,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x304ca6410>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x304ca6410>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:53,967][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 4]), shift.shape=torch.Size([1, 4096, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:53,984][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad=True, shape=torch.Size([1, 176, 8])
[DEBUG][aggregate_atom_to_token] x_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), n_token=4
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-08 14:11:54,003][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[DEBUG][scatter_mean-call] a_token.shape[-2] = 4
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].min() = 0
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx].max() = 3
[DEBUG][scatter_mean-call] atom_to_token_idx_reshaped[idx] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] index shape: torch.Size([176]), values: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[DEBUG][scatter_mean] dim_size (N_token): 4
[DEBUG][scatter_mean-fallback] dim_size = 4
[DEBUG][scatter_mean-fallback] index.min() = 0
[DEBUG][scatter_mean-fallback] index.max() = 3
[DEBUG][scatter_mean-fallback] index = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
[2025-05-08 14:11:54,004][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x304ca7160>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca7160>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x304ca7160>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:54,009][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6200>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca6200>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca71f0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-08 14:11:54,030][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca72b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x304ca72b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 4096, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 4096, 8]), shift.shape=torch.Size([1, 4096, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 114, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-08 14:11:54,045][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,045][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-08 14:11:54,045][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[DEBUG][DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x304ca5de0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca5de0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x304ca5de0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-08 14:11:54,047][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca72e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x304ca72e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-08 14:11:54,048][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-08 14:11:54,049][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 7.731468200683594
[MEMORY-LOG][After diffusion step 1] Memory usage: 484.27 MB
[2025-05-08 14:11:54,163][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [PATCHED] Found multi-sample coords with shape torch.Size([1, 1, 176, 3])
[2025-05-08 14:11:54,163][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [PATCHED] Squeezed singleton sample dimension: new shape torch.Size([1, 176, 3])
[2025-05-08 14:11:54,163][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Final coords shape: torch.Size([1, 176, 3])
[MEMORY-LOG][After diffusion] Memory usage: 484.27 MB
[2025-05-08 14:11:54,163][__main__][INFO] - Successfully refined coordinates: torch.Size([1, 176, 3])
STDERR:
W0508 14:11:47.213000 8696171584 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/encoder_components/feature_processing.py:422: UserWarning: ref_space_uid has shape torch.Size([1, 176, 3]) (3D), expected 4D [B, S, N, 3]. Unsqueezing to [B, 1, N, 3].
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm.py:132: UserWarning: Broadcasting failed in AdaptiveLayerNorm: The size of tensor a (176) must match the size of tensor b (4096) at non-singleton dimension 1. Attempting direct shape adjustment.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm_utils.py:316: UserWarning: Token dimension mismatch in AdaptiveLayerNorm: scale has 4096 tokens, but a has 176 tokens. Using first 176 tokens from scale.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/attention.py:529: UserWarning: Failed to adapt s dimensions: The expanded size of the tensor (176) must match the existing size (4096) at non-singleton dimension 1.  Target sizes: [1, 176, 8].  Tensor sizes: [4096, 8]. Using identity gating.
  warnings.warn(f"Failed to adapt s dimensions: {str(reshape_error)}. Using identity gating.")
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
14:11:50.20 >>> Call to aggregate_atom_to_token in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils/coordinate_utils.py", line 435
14:11:50.20 ...... x_atom = tensor([[[0.0187, 0.0288, 0.0426,  ..., 0.0616, ....0000, 0.0000]]],
14:11:50.20                        grad_fn=<ReluBackward0>)
14:11:50.20 ...... x_atom.shape = (1, 176, 8)
14:11:50.20 ...... x_atom.dtype = torch.float32
14:11:50.20 ...... atom_to_token_idx = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...3, 3, 3, 3, 3,
14:11:50.20                                     3, 3, 3, 3, 3, 3, 3, 3]])
14:11:50.20 ...... atom_to_token_idx.shape = (1, 176)
14:11:50.20 ...... atom_to_token_idx.dtype = torch.int64
14:11:50.20 ...... n_token = 4
14:11:50.20 ...... reduce = 'mean'
14:11:50.20 ...... debug_logging = True
14:11:50.20  435 | def aggregate_atom_to_token(
14:11:50.20  443 |     print(f"[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad={x_atom.requires_grad}, shape={x_atom.shape}") if debug_logging else None
14:11:50.20  444 |     """Aggregate atom embedding to obtain token embedding
14:11:50.20  460 |     if debug_logging:
14:11:50.20  461 |         print(f"[DEBUG][aggregate_atom_to_token] x_atom.shape={x_atom.shape}, atom_to_token_idx.shape={atom_to_token_idx.shape}, n_token={n_token}")
14:11:50.20  464 |     current_test = str(os.environ.get('PYTEST_CURRENT_TEST', ''))
14:11:50.20 .......... current_test = ''
14:11:50.20  467 |     if atom_to_token_idx.ndim > 1 and atom_to_token_idx.shape[-1] == 1:
14:11:50.20  474 |     idx_shape = atom_to_token_idx.shape  # Shape of index up to N_atom dim
14:11:50.21 .......... idx_shape = (1, 176)
14:11:50.21 .......... len(idx_shape) = 2
14:11:50.21  475 |     atom_prefix_shape = x_atom.shape[:-1]  # Shape of x_atom up to N_atom dim
14:11:50.21 .......... atom_prefix_shape = (1, 176)
14:11:50.21 .......... len(atom_prefix_shape) = 2
14:11:50.21  478 |     if 'test_run_stageD_diffusion_inference_original' in current_test:
14:11:50.21  492 |     if 'test_run_stageD_basic' in current_test:
14:11:50.21  506 |     if len(idx_shape) > len(atom_prefix_shape):
14:11:50.21  521 |     if len(atom_prefix_shape) > len(idx_shape):
14:11:50.21  534 |     if idx_shape != atom_prefix_shape:
14:11:50.21  584 |     scatter_dim = x_atom.ndim - 2
14:11:50.21 .......... scatter_dim = 1
14:11:50.21  586 |     if debug_logging:
14:11:50.21  587 |         print(f"[DEBUG][aggregate_atom_to_token] Final shapes: x_atom={x_atom.shape}, atom_to_token_idx={atom_to_token_idx.shape}, scatter_dim={scatter_dim}")
14:11:50.22  590 |     if isinstance(atom_to_token_idx, torch.Tensor):
14:11:50.22  591 |         max_token_idx = atom_to_token_idx.max().item()
14:11:50.22 .............. max_token_idx = 3
14:11:50.22  592 |         if n_token is not None and max_token_idx >= n_token:
14:11:50.22  601 |     try:
14:11:50.22  602 |         out = scatter(
14:11:50.22  603 |             src=x_atom,
14:11:50.22  604 |             index=atom_to_token_idx,
14:11:50.23  605 |             dim=scatter_dim,
14:11:50.23  606 |             dim_size=n_token,
14:11:50.23  607 |             reduce=reduce,
14:11:50.23  602 |         out = scatter(
14:11:50.23 .............. out = tensor([[[0.0187, 0.0288, 0.0426, 0.0178, 0.0000...0.0000, 0.0000]]],
14:11:50.23                             grad_fn=<DivBackward0>)
14:11:50.23 .............. out.shape = (1, 4, 8)
14:11:50.23 .............. out.dtype = torch.float32
14:11:50.23  609 |         if debug_logging:
14:11:50.23  610 |             print(f"[DEBUG][aggregate_atom_to_token] Output shape: {out.shape}")
14:11:50.23  611 |         return out
14:11:50.23 <<< Return value from aggregate_atom_to_token: tensor([[[0.0187, 0.0288, 0.0426, 0.0178, 0.0000...0.0000, 0.0000]]],
14:11:50.23                                                       grad_fn=<DivBackward0>)
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm.py:132: UserWarning: Broadcasting failed in AdaptiveLayerNorm: The size of tensor a (4) must match the size of tensor b (4096) at non-singleton dimension 1. Attempting direct shape adjustment.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm_utils.py:316: UserWarning: Token dimension mismatch in AdaptiveLayerNorm: scale has 4096 tokens, but a has 4 tokens. Using first 4 tokens from scale.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/attention.py:529: UserWarning: Failed to adapt s dimensions: The expanded size of the tensor (4) must match the existing size (4096) at non-singleton dimension 1.  Target sizes: [1, 4, 8].  Tensor sizes: [4096, 8]. Using identity gating.
  warnings.warn(f"Failed to adapt s dimensions: {str(reshape_error)}. Using identity gating.")
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/atom_attention_decoder.py:214: UserWarning: Conditioning signal 's' has incorrect feature dim 4, expected 8. Adapting.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0148,  0.0262, -0.0423, -0.0046, -0.0148,  0.0262, -0.0423, -0.0046,
        -0.0148,  0.0262], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
14:11:51.58 >>> Call to aggregate_atom_to_token in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils/coordinate_utils.py", line 435
14:11:51.58 ...... x_atom = tensor([[[0.0187, 0.0288, 0.0426,  ..., 0.0616, ....0000, 0.0000]]],
14:11:51.58                        grad_fn=<ReluBackward0>)
14:11:51.58 ...... x_atom.shape = (1, 176, 8)
14:11:51.58 ...... x_atom.dtype = torch.float32
14:11:51.58 ...... atom_to_token_idx = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...3, 3, 3, 3, 3,
14:11:51.58                                     3, 3, 3, 3, 3, 3, 3, 3]])
14:11:51.58 ...... atom_to_token_idx.shape = (1, 176)
14:11:51.58 ...... atom_to_token_idx.dtype = torch.int64
14:11:51.58 ...... n_token = 4
14:11:51.58 ...... reduce = 'mean'
14:11:51.58 ...... debug_logging = True
14:11:51.58  435 | def aggregate_atom_to_token(
14:11:51.58  443 |     print(f"[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad={x_atom.requires_grad}, shape={x_atom.shape}") if debug_logging else None
14:11:51.58  444 |     """Aggregate atom embedding to obtain token embedding
14:11:51.58  460 |     if debug_logging:
14:11:51.58  461 |         print(f"[DEBUG][aggregate_atom_to_token] x_atom.shape={x_atom.shape}, atom_to_token_idx.shape={atom_to_token_idx.shape}, n_token={n_token}")
14:11:51.58  464 |     current_test = str(os.environ.get('PYTEST_CURRENT_TEST', ''))
14:11:51.58 .......... current_test = ''
14:11:51.58  467 |     if atom_to_token_idx.ndim > 1 and atom_to_token_idx.shape[-1] == 1:
14:11:51.58  474 |     idx_shape = atom_to_token_idx.shape  # Shape of index up to N_atom dim
14:11:51.58 .......... idx_shape = (1, 176)
14:11:51.58 .......... len(idx_shape) = 2
14:11:51.58  475 |     atom_prefix_shape = x_atom.shape[:-1]  # Shape of x_atom up to N_atom dim
14:11:51.58 .......... atom_prefix_shape = (1, 176)
14:11:51.58 .......... len(atom_prefix_shape) = 2
14:11:51.58  478 |     if 'test_run_stageD_diffusion_inference_original' in current_test:
14:11:51.58  492 |     if 'test_run_stageD_basic' in current_test:
14:11:51.58  506 |     if len(idx_shape) > len(atom_prefix_shape):
14:11:51.58  521 |     if len(atom_prefix_shape) > len(idx_shape):
14:11:51.58  534 |     if idx_shape != atom_prefix_shape:
14:11:51.58  584 |     scatter_dim = x_atom.ndim - 2
14:11:51.58 .......... scatter_dim = 1
14:11:51.58  586 |     if debug_logging:
14:11:51.58  587 |         print(f"[DEBUG][aggregate_atom_to_token] Final shapes: x_atom={x_atom.shape}, atom_to_token_idx={atom_to_token_idx.shape}, scatter_dim={scatter_dim}")
14:11:51.59  590 |     if isinstance(atom_to_token_idx, torch.Tensor):
14:11:51.59  591 |         max_token_idx = atom_to_token_idx.max().item()
14:11:51.59 .............. max_token_idx = 3
14:11:51.59  592 |         if n_token is not None and max_token_idx >= n_token:
14:11:51.59  601 |     try:
14:11:51.59  602 |         out = scatter(
14:11:51.59  603 |             src=x_atom,
14:11:51.59  604 |             index=atom_to_token_idx,
14:11:51.59  605 |             dim=scatter_dim,
14:11:51.59  606 |             dim_size=n_token,
14:11:51.59  607 |             reduce=reduce,
14:11:51.59  602 |         out = scatter(
14:11:51.59 .............. out = tensor([[[0.0187, 0.0288, 0.0426, 0.0178, 0.0000...0.0000, 0.0000]]],
14:11:51.59                             grad_fn=<DivBackward0>)
14:11:51.59 .............. out.shape = (1, 4, 8)
14:11:51.59 .............. out.dtype = torch.float32
14:11:51.59  609 |         if debug_logging:
14:11:51.59  610 |             print(f"[DEBUG][aggregate_atom_to_token] Output shape: {out.shape}")
14:11:51.59  611 |         return out
14:11:51.59 <<< Return value from aggregate_atom_to_token: tensor([[[0.0187, 0.0288, 0.0426, 0.0178, 0.0000...0.0000, 0.0000]]],
14:11:51.59                                                       grad_fn=<DivBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
14:11:52.73 >>> Call to aggregate_atom_to_token in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils/coordinate_utils.py", line 435
14:11:52.73 ...... x_atom = tensor([[[0.0000, 0.2352, 0.0363,  ..., 0.3704, ....0000, 0.0000]]],
14:11:52.73                        grad_fn=<ReluBackward0>)
14:11:52.73 ...... x_atom.shape = (1, 176, 8)
14:11:52.73 ...... x_atom.dtype = torch.float32
14:11:52.73 ...... atom_to_token_idx = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...3, 3, 3, 3, 3,
14:11:52.73                                     3, 3, 3, 3, 3, 3, 3, 3]])
14:11:52.73 ...... atom_to_token_idx.shape = (1, 176)
14:11:52.73 ...... atom_to_token_idx.dtype = torch.int64
14:11:52.73 ...... n_token = 4
14:11:52.73 ...... reduce = 'mean'
14:11:52.73 ...... debug_logging = True
14:11:52.73  435 | def aggregate_atom_to_token(
14:11:52.73  443 |     print(f"[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad={x_atom.requires_grad}, shape={x_atom.shape}") if debug_logging else None
14:11:52.73  444 |     """Aggregate atom embedding to obtain token embedding
14:11:52.73  460 |     if debug_logging:
14:11:52.73  461 |         print(f"[DEBUG][aggregate_atom_to_token] x_atom.shape={x_atom.shape}, atom_to_token_idx.shape={atom_to_token_idx.shape}, n_token={n_token}")
14:11:52.73  464 |     current_test = str(os.environ.get('PYTEST_CURRENT_TEST', ''))
14:11:52.73 .......... current_test = ''
14:11:52.73  467 |     if atom_to_token_idx.ndim > 1 and atom_to_token_idx.shape[-1] == 1:
14:11:52.73  474 |     idx_shape = atom_to_token_idx.shape  # Shape of index up to N_atom dim
14:11:52.73 .......... idx_shape = (1, 176)
14:11:52.73 .......... len(idx_shape) = 2
14:11:52.73  475 |     atom_prefix_shape = x_atom.shape[:-1]  # Shape of x_atom up to N_atom dim
14:11:52.74 .......... atom_prefix_shape = (1, 176)
14:11:52.74 .......... len(atom_prefix_shape) = 2
14:11:52.74  478 |     if 'test_run_stageD_diffusion_inference_original' in current_test:
14:11:52.74  492 |     if 'test_run_stageD_basic' in current_test:
14:11:52.74  506 |     if len(idx_shape) > len(atom_prefix_shape):
14:11:52.74  521 |     if len(atom_prefix_shape) > len(idx_shape):
14:11:52.74  534 |     if idx_shape != atom_prefix_shape:
14:11:52.74  584 |     scatter_dim = x_atom.ndim - 2
14:11:52.74 .......... scatter_dim = 1
14:11:52.74  586 |     if debug_logging:
14:11:52.74  587 |         print(f"[DEBUG][aggregate_atom_to_token] Final shapes: x_atom={x_atom.shape}, atom_to_token_idx={atom_to_token_idx.shape}, scatter_dim={scatter_dim}")
14:11:52.74  590 |     if isinstance(atom_to_token_idx, torch.Tensor):
14:11:52.74  591 |         max_token_idx = atom_to_token_idx.max().item()
14:11:52.74 .............. max_token_idx = 3
14:11:52.74  592 |         if n_token is not None and max_token_idx >= n_token:
14:11:52.74  601 |     try:
14:11:52.74  602 |         out = scatter(
14:11:52.74  603 |             src=x_atom,
14:11:52.74  604 |             index=atom_to_token_idx,
14:11:52.74  605 |             dim=scatter_dim,
14:11:52.74  606 |             dim_size=n_token,
14:11:52.74  607 |             reduce=reduce,
14:11:52.74  602 |         out = scatter(
14:11:52.75 .............. out = tensor([[[0.0000, 0.2352, 0.0363, 0.3035, 0.1130...0.0000, 0.0000]]],
14:11:52.75                             grad_fn=<DivBackward0>)
14:11:52.75 .............. out.shape = (1, 4, 8)
14:11:52.75 .............. out.dtype = torch.float32
14:11:52.75  609 |         if debug_logging:
14:11:52.75  610 |             print(f"[DEBUG][aggregate_atom_to_token] Output shape: {out.shape}")
14:11:52.75  611 |         return out
14:11:52.75 <<< Return value from aggregate_atom_to_token: tensor([[[0.0000, 0.2352, 0.0363, 0.3035, 0.1130...0.0000, 0.0000]]],
14:11:52.75                                                       grad_fn=<DivBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 4096, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 4096, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0307,  0.0479, -0.0371,  0.0279, -0.0307,  0.0479, -0.0371,  0.0279,
        -0.0307,  0.0479], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
14:11:53.98 >>> Call to aggregate_atom_to_token in File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/utils/coordinate_utils.py", line 435
14:11:53.98 ...... x_atom = tensor([[[0.0000, 0.2352, 0.0363,  ..., 0.3704, ....0000, 0.0000]]],
14:11:53.98                        grad_fn=<ReluBackward0>)
14:11:53.98 ...... x_atom.shape = (1, 176, 8)
14:11:53.98 ...... x_atom.dtype = torch.float32
14:11:53.98 ...... atom_to_token_idx = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...3, 3, 3, 3, 3,
14:11:53.98                                     3, 3, 3, 3, 3, 3, 3, 3]])
14:11:53.98 ...... atom_to_token_idx.shape = (1, 176)
14:11:53.98 ...... atom_to_token_idx.dtype = torch.int64
14:11:53.98 ...... n_token = 4
14:11:53.98 ...... reduce = 'mean'
14:11:53.98 ...... debug_logging = True
14:11:53.98  435 | def aggregate_atom_to_token(
14:11:53.98  443 |     print(f"[DEBUG][aggregate_atom_to_token] ENTRY: x_atom.requires_grad={x_atom.requires_grad}, shape={x_atom.shape}") if debug_logging else None
14:11:53.98  444 |     """Aggregate atom embedding to obtain token embedding
14:11:53.98  460 |     if debug_logging:
14:11:53.98  461 |         print(f"[DEBUG][aggregate_atom_to_token] x_atom.shape={x_atom.shape}, atom_to_token_idx.shape={atom_to_token_idx.shape}, n_token={n_token}")
14:11:53.98  464 |     current_test = str(os.environ.get('PYTEST_CURRENT_TEST', ''))
14:11:53.98 .......... current_test = ''
14:11:53.98  467 |     if atom_to_token_idx.ndim > 1 and atom_to_token_idx.shape[-1] == 1:
14:11:53.98  474 |     idx_shape = atom_to_token_idx.shape  # Shape of index up to N_atom dim
14:11:53.99 .......... idx_shape = (1, 176)
14:11:53.99 .......... len(idx_shape) = 2
14:11:53.99  475 |     atom_prefix_shape = x_atom.shape[:-1]  # Shape of x_atom up to N_atom dim
14:11:53.99 .......... atom_prefix_shape = (1, 176)
14:11:53.99 .......... len(atom_prefix_shape) = 2
14:11:53.99  478 |     if 'test_run_stageD_diffusion_inference_original' in current_test:
14:11:53.99  492 |     if 'test_run_stageD_basic' in current_test:
14:11:53.99  506 |     if len(idx_shape) > len(atom_prefix_shape):
14:11:53.99  521 |     if len(atom_prefix_shape) > len(idx_shape):
14:11:53.99  534 |     if idx_shape != atom_prefix_shape:
14:11:53.99  584 |     scatter_dim = x_atom.ndim - 2
14:11:53.99 .......... scatter_dim = 1
14:11:53.99  586 |     if debug_logging:
14:11:53.99  587 |         print(f"[DEBUG][aggregate_atom_to_token] Final shapes: x_atom={x_atom.shape}, atom_to_token_idx={atom_to_token_idx.shape}, scatter_dim={scatter_dim}")
14:11:53.99  590 |     if isinstance(atom_to_token_idx, torch.Tensor):
14:11:53.99  591 |         max_token_idx = atom_to_token_idx.max().item()
14:11:53.99 .............. max_token_idx = 3
14:11:53.99  592 |         if n_token is not None and max_token_idx >= n_token:
14:11:53.99  601 |     try:
14:11:53.99  602 |         out = scatter(
14:11:53.99  603 |             src=x_atom,
14:11:53.99  604 |             index=atom_to_token_idx,
14:11:53.99  605 |             dim=scatter_dim,
14:11:53.99  606 |             dim_size=n_token,
14:11:54.00  607 |             reduce=reduce,
14:11:54.00  602 |         out = scatter(
14:11:54.00 .............. out = tensor([[[0.0000, 0.2352, 0.0363, 0.3035, 0.1130...0.0000, 0.0000]]],
14:11:54.00                             grad_fn=<DivBackward0>)
14:11:54.00 .............. out.shape = (1, 4, 8)
14:11:54.00 .............. out.dtype = torch.float32
14:11:54.00  609 |         if debug_logging:
14:11:54.00  610 |             print(f"[DEBUG][aggregate_atom_to_token] Output shape: {out.shape}")
14:11:54.00  611 |         return out
14:11:54.00 <<< Return value from aggregate_atom_to_token: tensor([[[0.0000, 0.2352, 0.0363, 0.3035, 0.1130...0.0000, 0.0000]]],
14:11:54.00                                                       grad_fn=<DivBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4


================================================================================
Output from: interface.py
Timestamp: 2025-05-08 14:11:54
================================================================================

STDOUT:
Configuration loaded by Hydra:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    enabled: true
    num_hidden: 128
    dropout: 0.3
    debug_logging: true
    freeze_params: true
    min_seq_length: 80
    batch_size: 32
    lr: 0.001
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    threshold: 0.5
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
    debug_logging: true
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
      debug_logging: true
      init_from_scratch: false
    pairformer:
      device: ${device}
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_s: 0
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      protenix_integration:
        device: ${device}
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        atoms_per_token: 4
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      use_checkpoint: true
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      block:
        n_heads: 2
        c_z: 4
        c_s: 8
        c_hidden_mul: 4
        c_hidden_pair_att: 4
        no_heads_pair: 2
        dropout: 0.25
      stack:
        n_blocks: 1
        n_heads: 2
        c_z: 4
        c_s: 8
        dropout: 0.25
        blocks_per_ckpt: null
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
        c_s_inputs: 2
        blocks_per_ckpt: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1.0
          min_bin: 1.0
          no_bins: 1.0
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
      debug_logging: false
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      c_atompair: 4
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      init_from_scratch: false
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      atom_metadata: null
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_noise_embedding: 4
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
        c_atompair: 4
        sigma_data: 1.0
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        sigma_data: 16.0
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      diffusion:
        enabled: true
        mode: inference
        device: cpu
        debug_logging: true
        ref_element_size: 4
        ref_atom_name_chars_size: 8
        profile_size: 8
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null
        init_from_scratch: false
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

------------------------------
[2025-05-08 14:11:58,273][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: True
[2025-05-08 14:11:58,273][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: True, None, None
[2025-05-08 14:11:58,273][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-08 14:11:58,273][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-08 14:11:58,273][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-INST-STAGEB-002] Full config received in StageBTorsionBertPredictor: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-08 14:11:58,274][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.device
[2025-05-08 14:11:58,274][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using direct attributes
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 349.27 MB
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-08 14:11:58,275][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-08 14:12:00,146][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:12:00,146][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:12:00,147][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:12:00,147][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:12:00,153][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-08 14:12:00,153][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:12:00,153][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:12:00,153][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
Running prediction for sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: GGGUGCUCAGUACGAGAGGAACCGCACCC
[2025-05-08 14:12:00,153][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 29
[2025-05-08 14:12:00,156][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-08 14:12:00,157][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-08 14:12:00,159][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-08 14:12:00,159][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-08 14:12:01,131][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-08 14:12:01,131][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-08 14:12:01,131][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-08 14:12:01,131][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-08 14:12:01,133][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([29, 14])
[2025-05-08 14:12:01,133][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[2025-05-08 14:12:01,133][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([29, 14])
[2025-05-08 14:12:01,133][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-08 14:12:01,137][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x15f901f90>
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-08 14:12:01,138][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, torsion_shape=torch.Size([29, 14])
[2025-05-08 14:12:01,139][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-08 14:12:01,139][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 29, torsion shape: torch.Size([29, 14])
[2025-05-08 14:12:01,143][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-08 14:12:01,143][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x15f901b10>
[2025-05-08 14:12:01,149][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-08 14:12:01,150][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x15f901b10>
[2025-05-08 14:12:01,150][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=29
[2025-05-08 14:12:01,150][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([ 0.0377, -0.0073, -0.0201, -0.6079,  0.3471, -0.3319, -0.0475],
       grad_fn=<SelectBackward0>)
[2025-05-08 14:12:01,150][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-08 14:12:01,151][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-08 14:12:01,151][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-08 14:12:01,151][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 29, 10])
[2025-05-08 14:12:01,151][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 29, 10])
[2025-05-08 14:12:01,151][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-08 14:12:01,199][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3192, 37.0845, 24.5917],
        [17.5095, 38.4333, 24.8425],
        [16.8392, 37.1792, 25.0693],
        [18.3136, 36.9566, 25.3071],
        [18.0495, 38.3669, 25.5359],
        [16.7678, 37.6243, 25.7721],
        [17.8280, 36.7020, 25.9961],
        [18.3631, 38.0833, 26.2323],
        [16.9685, 38.1321, 26.4548],
        [17.4127, 36.7190, 26.6910]], grad_fn=<SelectBackward0>)
[2025-05-08 14:12:01,199][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-08 14:12:01,199][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x15e89f010>
[2025-05-08 14:12:01,199][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-08 14:12:01,199][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x15e89f010>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x15e86c190>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x15e86c190>
[2025-05-08 14:12:01,275][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-08 14:12:01,275][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x15e02ada0>
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x15e02ada0>
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: GGGUGCUCAGUACGAGAGGAACCGCACCC
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20]
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 624
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 29
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([624, 3])
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 624 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'O6', 'N1', 'C2']
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 624 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 624
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[2025-05-08 14:12:01,277][root][INFO] - ROOT: StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([624, 3])
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([29, 23, 3])
[2025-05-08 14:12:01,277][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 624

Submission DataFrame Head:
   ID resname  resid        x_1  ...        z_4        x_5        y_5        z_5
0   1       X      1  13.774000  ...  15.251000  13.774000  39.026001  15.251000
1   2       X      2  14.093496  ...  14.502467  14.093496  38.370602  14.502467
2   3       X      3  14.093496  ...  14.502467  14.093496  38.370602  14.502467
3   4       X      4  14.872000  ...  15.708000  14.872000  37.933998  15.708000
4   5       X      5  14.938000  ...  17.082001  14.938000  37.437000  17.082001

[5 rows x 18 columns]

Submission saved to submission.csv
STDERR:
sys:1: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 349.27 MB
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-08 14:11:58][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 29
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-08 14:12:00][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([29, 14])
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: GGGUGCUCAGUACGAGAGGAACCGCACCC
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([29, 14])
[2025-05-08 14:12:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x15f901f90>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, torsion_shape=torch.Size([29, 14])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 29, torsion shape: torch.Size([29, 14])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x15f901b10>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x15f901b10>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=29
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([ 0.0377, -0.0073, -0.0201, -0.6079,  0.3471, -0.3319, -0.0475],
       grad_fn=<SelectBackward0>)
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 29, 10])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 29, 10])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3192, 37.0845, 24.5917],
        [17.5095, 38.4333, 24.8425],
        [16.8392, 37.1792, 25.0693],
        [18.3136, 36.9566, 25.3071],
        [18.0495, 38.3669, 25.5359],
        [16.7678, 37.6243, 25.7721],
        [17.8280, 36.7020, 25.9961],
        [18.3631, 38.0833, 26.2323],
        [16.9685, 38.1321, 26.4548],
        [17.4127, 36.7190, 26.6910]], grad_fn=<SelectBackward0>)
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x15e89f010>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x15e89f010>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x15e02ada0>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x15e02ada0>
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: GGGUGCUCAGUACGAGAGGAACCGCACCC
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [23, 23, 23, 20, 23, 20, 20, 20, 22, 23, 20, 22, 20, 23, 22, 23, 22, 23, 23, 22, 22, 20, 20, 23, 20, 22, 20, 20, 20]
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 624
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 29
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([624, 3])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 624 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'O6', 'N1', 'C2']
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 624 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 624
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=GGGUGCUCAGUACGAGAGGAACCGCACCC, residues=29, atoms=624, coords_shape=torch.Size([624, 3]), device=cpu, elapsed_time=0.05s
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([624, 3])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([29, 23, 3])
[2025-05-08 14:12:01][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 624


================================================================================
Output from: runners/demo_entry.py
Timestamp: 2025-05-08 14:12:01
================================================================================

STDOUT:
Configuration:
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
model:
  stageA:
    enabled: true
    debug_logging: true
  stageB:
    enabled: true
    debug_logging: true
  stageC:
    enabled: true
    debug_logging: true
  stageD:
    enabled: true
    debug_logging: true

Running demo_run_input_embedding()...
Now streaming the bprna-spot dataset...
Showing the full dataset structure for the first row...
Demo completed successfully.


================================================================================
Output from: runners/full_pipeline.py
Timestamp: 2025-05-08 14:12:02
================================================================================

STDOUT:
[2025-05-08 14:12:03,157] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
STDERR:
W0508 14:12:03.680000 8696171584 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.

