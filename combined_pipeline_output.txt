RNA Pipeline Combined Output
Generated on: 2025-05-19 13:20:52
Using standardized test sequence: ACGUACGU
================================================================================


================================================================================
Output from: pipeline/stageA/run_stageA.py
Timestamp: 2025-05-19 13:20:52
================================================================================

STDOUT:
[2025-05-19 13:20:54,337][rna_predict.pipeline.stageA.run_stageA][INFO] - Starting Stage A pipeline with Hydra config
[2025-05-19 13:20:54,351][rna_predict.pipeline.stageA.run_stageA][INFO] - Config:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-19 13:20:54,365][rna_predict.pipeline.stageA.run_stageA][INFO] - [Hydra Config] Full config:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-19 13:20:54,365][rna_predict.pipeline.stageA.run_stageA][INFO] - [Hydra Config] Loaded Stage A config:
{'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}
[2025-05-19 13:20:54,581][rna_predict.pipeline.stageA.run_stageA][INFO] - [Info] File already exists and is valid zip, skipping download: RFold/checkpoints.zip
[2025-05-19 13:20:54,581][rna_predict.pipeline.stageA.run_stageA][INFO] - [Unzip] Extracting RFold/checkpoints.zip into RFold
[2025-05-19 13:20:54,910][rna_predict.pipeline.stageA.run_stageA][INFO] - [HYDRA-DEBUG][StageA] Resolved stage_cfg.device: cpu
[2025-05-19 13:20:54,911][rna_predict.pipeline.stageA.run_stageA][INFO] - [HYDRA-DEBUG][StageA] Global cfg.device: cpu
[2025-05-19 13:20:54,912][rna_predict.pipeline.stageA.run_stageA][INFO] - [Device] Using device: cpu
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-19 13:20:54,914][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 13:20:54,915][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 13:20:54,965][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 13:20:54,965][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 13:20:54,966][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:54,985][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 251.51 MB
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.run_stageA][INFO] - [Example] Running inference on standardized test sequence: ACGUACGU (length: 8)
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-19 13:20:54,986][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-19 13:20:54,989][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-19 13:20:54,989][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:54,990][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-19 13:20:54,990][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-19 13:20:54,990][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:55,043][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-19 13:20:55,044][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-19 13:20:55,044][rna_predict.pipeline.stageA.run_stageA][INFO] - [Example] Adjacency shape: (8, 8)
[2025-05-19 13:20:55,044][rna_predict.pipeline.stageA.run_stageA][INFO] - [Visualization] Attempting with JAR: tools/varna-3-93.jar
[2025-05-19 13:20:55,044][rna_predict.pipeline.stageA.run_stageA][WARNING] - [Warning] VARNA JAR not found at: tools/varna-3-93.jar -> skipping visualization.
STDERR:
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 251.51 MB
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-19 13:20:54][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:55][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-19 13:20:55][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32


================================================================================
Output from: pipeline/stageB/main.py
Timestamp: 2025-05-19 13:20:55
================================================================================

STDOUT:
[2025-05-19 13:20:56,223] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:20:59,262][rna_predict.pipeline.stageB.main][INFO] - [DEBUG-SEQUENCE-ENTRY-STAGEB] type=<class 'str'>, value=ACGUACGU
[2025-05-19 13:20:59,264][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 13:20:59,264][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 13:20:59,264][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 13:20:59,265][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 13:20:59,301][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 13:20:59,301][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 13:20:59,301][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:59,325][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 447.46 MB
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-19 13:20:59,326][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-19 13:20:59,327][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-19 13:20:59,327][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:59,327][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-19 13:20:59,327][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-19 13:20:59,327][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:59,358][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-19 13:20:59,359][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-19 13:20:59,360][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: True
[2025-05-19 13:20:59,361][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: True, None, None
[2025-05-19 13:20:59,361][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-19 13:20:59,361][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-19 13:20:59,361][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-INST-STAGEB-002] Full config received in StageBTorsionBertPredictor: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}
[2025-05-19 13:20:59,362][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.device
[2025-05-19 13:20:59,362][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-19 13:20:59,362][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using direct attributes
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 492.73 MB
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-19 13:20:59,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-19 13:21:01,282][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:21:01,282][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:21:01,283][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:21:01,283][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:21:01,289][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.main][INFO] - [DEBUG-SEQUENCE-BEFORE-STAGEB] type=<class 'str'>, value=ACGUACGU
[CASCADE-DEBUG] BEFORE STAGE B: type=<class 'str'>, value=ACGUACGU
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Received adjacency matrix with shape torch.Size([8, 8]), but it is not used
[2025-05-19 13:21:01,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:21:01,292][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01,292][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01,292][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01,292][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:21:01,293][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:21:01,293][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:21:01,293][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:21:01,295][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:21:01,295][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:21:02,362][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:21:02,362][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[CASCADE-DEBUG] AFTER STAGE B: type=<class 'str'>, value=ACGUACGU
[CASCADE-DEBUG] BEFORE STAGE C: type=<class 'str'>, value=ACGUACGU
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 789.09 MB
[2025-05-19 13:21:02,363][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-19 13:21:02,364][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 789.12 MB
[2025-05-19 13:21:02,388][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: False
[2025-05-19 13:21:02,388][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: None, None, None
[2025-05-19 13:21:02,389][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-19 13:21:02,389][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-19 13:21:02,390][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.stageB.torsion_bert.device (legacy)
[2025-05-19 13:21:02,390][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-19 13:21:02,390][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using legacy cfg.stageB.torsion_bert
[2025-05-19 13:21:02,391][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-19 13:21:02,392][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:21:02,392][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 796.70 MB
[2025-05-19 13:21:05,531][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:21:05,531][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:21:05,532][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 488.33 MB
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Using direct stageB_pairformer config
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=1, c_z=2, c_s=0, dropout=0.0
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-19 13:21:05,548][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-19 13:21:05,549][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 496.86 MB
STDERR:
W0519 13:20:56.938000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Initializing StageARFoldPredictor...
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Checkpoint path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Example sequence length: 352
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Freeze params: True
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] -   Run example: True
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-001] Effective debug_logging in StageARFoldPredictor.__init__: True
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-002] logger.level: 10
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-003] logger.handlers: [<StreamHandler <stderr> (DEBUG)>]
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-INST-STAGEA-004] Handler 0 level: 10
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] stage_cfg.model: {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA-DIAG] model_args (with device): {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}, 'device': device(type='cpu')}
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Instantiated RFoldModel: <class 'rna_predict.pipeline.stageA.adjacency.RFold_code.RFoldModel'>
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] Model device after instantiation and .to(device): cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Loading checkpoint from RFold/checkpoints/RNAStralign_trainset_pretrained.pth
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [Load] Checkpoint loaded successfully.
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [StageA] All model parameters frozen (requires_grad=False) per freeze_params config.
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] After super().__init__
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - [MEMORY-LOG][StageA] Memory usage: 447.46 MB
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - rna_sequence type: <class 'str'>, value (first 50): ACGUACGU
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 1: Received RNA sequence of length 8
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 2: seq_idxs (len=8): [0, 2, 3, 1, 0, 2, 3, 1]
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 3: original_len=8, padded_len=80
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR] Entered _create_sequence_tensor with device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Creating tensor with device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-SEQ-TENSOR-GENERIC] Tensor device after creation and .to(self.device): cpu, shape: torch.Size([1, 80]), dtype: torch.int64
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 4: seq_tensor type: <class 'torch.Tensor'>, shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] seq_tensor.device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][DEBUG] - [DEBUG-PREDICT-ADJACENCY] self.model.device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 5: About to call model with seq_tensor shape: torch.Size([1, 80]), dtype: torch.int64, device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 6: Model output shape: torch.Size([1, 80, 80]), dtype: torch.float32, device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageA.adjacency.rfold_predictor][INFO] - Step 7: Final adjacency matrix shape: (8, 8), dtype: float32
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 492.73 MB
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-19 13:20:59][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Received adjacency matrix with shape torch.Size([8, 8]), but it is not used
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:21:01][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:21:02][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Initializing StageCReconstruction
[2025-05-19 13:21:02][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 789.09 MB
[2025-05-19 13:21:02][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] After super().__init__
[2025-05-19 13:21:02][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [MEMORY-LOG][StageC] Memory usage: 789.12 MB
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: False
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: None, None, None
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.stageB.torsion_bert.device (legacy)
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using legacy cfg.stageB.torsion_bert
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:21:02][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 796.70 MB
[2025-05-19 13:21:05][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:21:05][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:21:05,532][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing PairformerWrapper...
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 488.33 MB
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] self.debug_logging resolved to: False
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] config subtree used: False, None
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-PROPAGATION][StageB-Pairformer] full config: {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Using direct stageB_pairformer config
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved configuration: n_blocks=1, c_z=2, c_s=0, dropout=0.0
[2025-05-19 13:21:05,533][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [UNIQUE-INFO-STAGEB-PAIRFORMER-TEST] PairformerWrapper initialized
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Used cfg.device
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [DEBUG-STAGEB-PAIRFORMER-CONFIG] Resolved device in config: cpu
[2025-05-19 13:21:05,534][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - Initializing Pairformer wrapper with device: cpu
[2025-05-19 13:21:05,548][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] After super().__init__
[2025-05-19 13:21:05,549][rna_predict.pipeline.stageB.pairwise.pairformer_wrapper][INFO] - [MEMORY-LOG][StageB-Pairformer] Memory usage: 496.86 MB
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")


================================================================================
Output from: pipeline/stageC/stage_c_reconstruction.py
Timestamp: 2025-05-19 13:21:07
================================================================================

STDOUT:
[2025-05-19 13:21:08,625][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Debug logging is enabled for StageC.
[2025-05-19 13:21:08,626][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08,626][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08,626][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08,626][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08,627][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - Running Stage C with Hydra configuration:
[2025-05-19 13:21:08,642][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-19 13:21:08,642][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using standardized test sequence: ACGUACGU with 7 torsion angles
[2025-05-19 13:21:08,643][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Running Stage C for sequence: ACGUACGU
[2025-05-19 13:21:08,643][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using dummy torsions shape: torch.Size([8, 7])
[2025-05-19 13:21:08,644][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08,644][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08,644][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08,644][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08,645][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: False
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: None
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08,646][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: ${device}
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 7])
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:21:08,648][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 7])
[2025-05-19 13:21:08,650][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: False
[2025-05-19 13:21:08,651][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: None
[2025-05-19 13:21:08,652][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: False
[2025-05-19 13:21:08,652][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: None
[2025-05-19 13:21:08,652][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:21:08,656][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-6.2667,  1.2026, -2.2331, -4.2016, -1.0661,  0.0090, -0.3273])
[2025-05-19 13:21:08,656][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:21:08,657][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:21:08,657][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:21:08,657][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:21:08,657][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:21:08,657][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:21:08,672][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3966, 37.4926, 24.8878],
        [17.3082, 38.6299, 25.1319],
        [16.9313, 37.2577, 25.3525],
        [18.4224, 37.2024, 25.5839],
        [17.9721, 38.5658, 25.8065],
        [16.7660, 37.7041, 26.0363],
        [17.8564, 36.8161, 26.2544],
        [18.3809, 38.2025, 26.4842],
        [16.9849, 38.2321, 26.7007],
        [17.4420, 36.8220, 26.9305]])
[2025-05-19 13:21:08,672][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: False
[2025-05-19 13:21:08,672][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: None
[2025-05-19 13:21:08,672][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:21:08,672][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: False, grad_fn: None
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: False, grad_fn: None
[2025-05-19 13:21:08,849][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: False, grad_fn: None
[2025-05-19 13:21:08,996][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: False
[2025-05-19 13:21:08,996][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: None
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: False
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: None
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.01s
[2025-05-19 13:21:08,997][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.01s
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:21:08,997][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:21:08,998][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Stage C Output:
[2025-05-19 13:21:08,998][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords shape: torch.Size([170, 3])
[2025-05-19 13:21:08,998][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords 3D shape: torch.Size([8, 23, 3])
[2025-05-19 13:21:08,998][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Atom count: 170
[2025-05-19 13:21:08,998][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Output device: cpu
STDERR:
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - Running Stage C with Hydra configuration:
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    num_hidden: 128
    dropout: 0.3
    min_seq_length: 80
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    batch_size: 32
    lr: 0.001
    threshold: 0.5
    debug_logging: true
    freeze_params: true
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    debug_logging: true
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      debug_logging: true
      init_from_scratch: false
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
    pairformer:
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      device: ${device}
      protenix_integration:
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
        device: ${device}
        atoms_per_token: 4
      c_s: 0
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        c_s_inputs: 2
        blocks_per_ckpt: 1
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1
          min_bin: 1
          no_bins: 1
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      debug_logging: false
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      init_from_scratch: false
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using standardized test sequence: ACGUACGU with 7 torsion angles
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Running Stage C for sequence: ACGUACGU
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Using dummy torsions shape: torch.Size([8, 7])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'shared': {'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32}, 'sequence': '${test_data.sequence}', 'device': '${oc.env:DEVICE, cpu}', 'seed': 42, 'atoms_per_residue': 44, 'extraction_backend': 'dssr', 'run_stageD': True, 'enable_stageC': True, 'merge_latent': True, 'pipeline': {'verbose': True, 'save_intermediates': True, 'output_dir': 'outputs', 'ignore_nan_values': True, 'nan_replacement_value': 0.0}, 'training': {'checkpoint_dir': 'outputs/checkpoints', 'accelerator': 'cpu', 'devices': 1, 'epochs': 10, 'batch_size': 32, 'limit_train_batches': 1, 'streamline_mode': True}, 'prediction': {'repeats': 5, 'residue_atom_choice': 0, 'enable_stochastic_inference_for_submission': False}, 'data': {'index_csv': './data/index.csv', 'root_dir': './data/', 'max_residues': 512, 'max_atoms': 4096, 'C_element': 128, 'C_char': 256, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'batch_size': 4, 'num_workers': 8, 'load_adj': False, 'load_ang': True, 'coord_fill_value': 'nan', 'coord_dtype': 'float32'}, 'model': {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}, 'stageD_diffusion': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}, 'test_data': {'sequence': 'ACGUACGU', 'sequence_length': 8, 'atoms_per_residue': 44, 'adjacency_fill_value': 1.0, 'target_dim': 3, 'torsion_angle_dim': 7, 'embedding_dims': {'s_trunk': 384, 'z_trunk': 128, 's_inputs': 449}, 'sequence_path': './data/kaggle/stanford-rna-3d-folding/train_sequences.csv', 'data_index': './rna_predict/dataset/examples/kaggle_minimal_index.csv', 'target_id': '1SCL_A', 'model': {'stageD': {'enabled': True, 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'diffusion': {'init_from_scratch': False, 'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}}}}}
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: ${device}
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 7])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 7])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-6.2667,  1.2026, -2.2331, -4.2016, -1.0661,  0.0090, -0.3273])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3966, 37.4926, 24.8878],
        [17.3082, 38.6299, 25.1319],
        [16.9313, 37.2577, 25.3525],
        [18.4224, 37.2024, 25.5839],
        [17.9721, 38.5658, 25.8065],
        [16.7660, 37.7041, 26.0363],
        [17.8564, 36.8161, 26.2544],
        [18.3809, 38.2025, 26.4842],
        [16.9849, 38.2321, 26.7007],
        [17.4420, 36.8220, 26.9305]])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: False, grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: False
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: None
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.01s
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - 
Stage C Output:
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords shape: torch.Size([170, 3])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Coords 3D shape: torch.Size([8, 23, 3])
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Atom count: 170
[2025-05-19 13:21:08][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] -   Output device: cpu


================================================================================
Output from: pipeline/stageD/run_stageD.py
Timestamp: 2025-05-19 13:21:09
================================================================================

STDOUT:
[HYDRA DEBUG] CWD: /Users/tomriddle1/RNA_PREDICT/rna_predict
[HYDRA DEBUG] SCRIPT DIR: /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD
[HYDRA DEBUG] sys.path: ['/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageD', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python310.zip', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10', '/Users/tomriddle1/.local/share/uv/python/cpython-3.10.12-macos-aarch64-none/lib/python3.10/lib-dynload', '/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages', '__editable__.rna_predict-2.0.3.finder.__path_hook__', '/Users/tomriddle1/RNA_PREDICT/auto-type-annotate']
[CONFIG DEBUG] Top-level keys: ['shared', 'sequence', 'device', 'seed', 'atoms_per_residue', 'extraction_backend', 'run_stageD', 'enable_stageC', 'merge_latent', 'pipeline', 'training', 'prediction', 'data', 'model', 'stageD_diffusion', 'test_data']
[CONFIG DEBUG] model keys: ['stageA', 'stageC', 'stageB', 'stageD', 'protenix_integration']
[2025-05-19 13:21:10,348][__main__][DEBUG] - [hydra_main] cfg.model: {'stageA': {'num_hidden': 128, 'dropout': 0.3, 'min_seq_length': 80, 'device': '${device}', 'checkpoint_path': 'RFold/checkpoints/RNAStralign_trainset_pretrained.pth', 'checkpoint_url': 'https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1', 'checkpoint_zip_path': 'RFold/checkpoints.zip', 'batch_size': 32, 'lr': 0.001, 'threshold': 0.5, 'debug_logging': True, 'freeze_params': True, 'run_example': True, 'example_sequence': 'AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC', 'visualization': {'enabled': True, 'varna_jar_path': 'tools/varna-3-93.jar', 'resolution': 8.0, 'output_path': 'test_seq.png'}, 'model': {'conv_channels': [64, 128, 256, 512], 'residual': True, 'c_in': 1, 'c_out': 1, 'c_hid': 32, 'seq2map': {'input_dim': 4, 'max_length': 3000, 'attention_heads': 8, 'attention_dropout': 0.1, 'positional_encoding': True, 'query_key_dim': 128, 'expansion_factor': 2.0, 'heads': 1}, 'decoder': {'up_conv_channels': [256, 128, 64], 'skip_connections': True}}}, 'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': '${device}', 'debug_logging': True, 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None}, 'stageB': {'torsion_bert': {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'debug_logging': True, 'init_from_scratch': False, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}}, 'pairformer': {'n_blocks': 1, 'n_heads': 1, 'c_z': 2, 'c_token': 384, 'c_atom': 128, 'c_pair': 32, 'dropout': 0.0, 'freeze_params': False, 'device': '${device}', 'protenix_integration': {'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False, 'device': '${device}', 'atoms_per_token': 4}, 'c_s': 0, 'msa': {'c_m': 2, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'n_blocks': 1, 'enable': False, 'strategy': 'random', 'train_cutoff': 1, 'test_cutoff': 1, 'train_lowerb': 1, 'test_lowerb': 1, 'n_heads': 1, 'pair_dropout': 0.0, 'c_s_inputs': 2, 'blocks_per_ckpt': 1, 'input_feature_dims': {'msa': 2, 'has_deletion': 1, 'deletion_value': 1}}, 'template': {'n_blocks': 1, 'c': 2, 'c_z': 2, 'dropout': 0.0, 'blocks_per_ckpt': None, 'input_feature_dims': {'feature1': {'template_distogram': 1, 'b_template_backbone_frame_mask': 1, 'template_unit_vector': 1, 'b_template_pseudo_beta_mask': 1}, 'feature2': {'template_restype_i': 1, 'template_restype_j': 1}}, 'distogram': {'max_bin': 1, 'min_bin': 1, 'no_bins': 1}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'c_hidden_mul': 1, 'c_hidden_pair_att': 2, 'no_heads_pair': 1, 'init_z_from_adjacency': False, 'debug_logging': False, 'lora': {'enabled': False, 'r': 1, 'alpha': 1, 'dropout': 0.0, 'target_modules': ['query', 'value']}}}, 'stageD': {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}, 'protenix_integration': {'device': '${device}', 'c_token': 449, 'restype_dim': 32, 'profile_dim': 32, 'c_atom': 128, 'c_pair': 32, 'atoms_per_token': 4, 'num_heads': 4, 'num_layers': 3, 'r_max': 32, 's_max': 2, 'use_optimized': False}}
[2025-05-19 13:21:10,348][__main__][DEBUG] - [hydra_main] cfg.model.stageD: {'enabled': True, 'mode': '${stageD_diffusion.diffusion.mode}', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25, 'sigma_data': 1.0}, 'transformer': {'n_blocks': '${stageD_diffusion.diffusion.transformer.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.transformer.n_heads}', 'blocks_per_ckpt': '${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}'}, 'atom_encoder': {'c_in': '${stageD_diffusion.diffusion.atom_encoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_encoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_encoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_encoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_encoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_encoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_encoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_encoder.n_keys}'}, 'atom_decoder': {'c_in': '${stageD_diffusion.diffusion.atom_decoder.c_in}', 'c_hidden': '${stageD_diffusion.diffusion.atom_decoder.c_hidden}', 'c_out': '${stageD_diffusion.diffusion.atom_decoder.c_out}', 'dropout': '${stageD_diffusion.diffusion.atom_decoder.dropout}', 'n_blocks': '${stageD_diffusion.diffusion.atom_decoder.n_blocks}', 'n_heads': '${stageD_diffusion.diffusion.atom_decoder.n_heads}', 'n_queries': '${stageD_diffusion.diffusion.atom_decoder.n_queries}', 'n_keys': '${stageD_diffusion.diffusion.atom_decoder.n_keys}'}, 'diffusion': {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}, 'input_features': None}
[2025-05-19 13:21:10,349][__main__][INFO] - Running Stage D Standalone Demo
[2025-05-19 13:21:10,349][__main__][DEBUG] - [UNIQUE-DEBUG-STAGED-TEST] Stage D runner started.
[2025-05-19 13:21:10,349][__main__][DEBUG] - Using standardized test sequence: ACGU with 44 atoms per residue
[2025-05-19 13:21:10,349][__main__][DEBUG] - [run_stageD] ENTRY: z_trunk.shape = torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] s_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] z_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] s_inputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict type: <class 'dict'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] atom_metadata type: <class 'dict'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] trunk_embeddings type: <class 'NoneType'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] features type: <class 'NoneType'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] stage_cfg type: <class 'NoneType'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] diffusion_cfg type: <class 'NoneType'>
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] device: None
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] mode: None
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] debug_logging: True
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] s_trunk shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] s_inputs shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,351][__main__][DEBUG] - [run_stageD] num_atoms: 176
[MEMORY-LOG][StageD ENTRY] Memory usage: 172.44 MB
[2025-05-19 13:21:10,351][__main__][DEBUG] - [DEBUG][run_stageD] Starting Stage D implementation
[DEBUG][run_stageD] Starting Stage D implementation
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict at Stage D entry:
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[DEBUG][run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[DEBUG][run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[DEBUG][run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:10,352][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[DEBUG][run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:10,353][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(atom_metadata): <class 'dict'>, keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-19 13:21:10,353][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(residue_indices): <class 'torch.Tensor'>, len: 176
[2025-05-19 13:21:10,353][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices.shape: torch.Size([176])
[2025-05-19 13:21:10,353][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices (first 20): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:21:10,353][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] Dumping config values before diffusion:
[2025-05-19 13:21:10,354][__main__][DEBUG] -   n_atoms: 176, n_residues: 4
[2025-05-19 13:21:10,354][__main__][DEBUG] -   s_trunk.shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,354][__main__][DEBUG] -   s_inputs.shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:10,354][__main__][DEBUG] -   z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:10,354][__main__][DEBUG] -   atom_metadata keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-19 13:21:10,354][__main__][DEBUG] -   context.device: None
[2025-05-19 13:21:10,354][__main__][DEBUG] -   context.mode: None
[2025-05-19 13:21:10,354][__main__][DEBUG] -   context.debug_logging: True
[2025-05-19 13:21:10,354][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] END CONFIG DUMP
[2025-05-19 13:21:10,450][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2025-05-19 13:21:10,541][rna_predict.utils.tensor_utils.types][DEBUG] - Expanded residue embeddings torch.Size([4, 8]) to atom embeddings torch.Size([176, 8])
[2025-05-19 13:21:10,541][__main__][DEBUG] - [HYDRA-CONF-BRIDGE][StageD] Bridged s_trunk to atom-level: torch.Size([1, 176, 8])
[MEMORY-LOG][Before bridging residue-to-atom] Memory usage: 219.52 MB
[MEMORY-LOG][After bridging residue-to-atom] Memory usage: 219.89 MB
[MEMORY-LOG][Before diffusion] Memory usage: 219.89 MB
[2025-05-19 13:21:10,544][__main__][DEBUG] - [run_stageD] Calling unified Stage D runner with DiffusionConfig.
[2025-05-19 13:21:10,545][__main__][INFO] - [HYDRA-DEBUG][StageD] stage_cfg.device: cpu
[2025-05-19 13:21:10,545][__main__][INFO] - [HYDRA-DEBUG][StageD] global cfg.device: cpu
[2025-05-19 13:21:10,675] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
[2025-05-19 13:21:12,817][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Initializing ProtenixDiffusionManager
[2025-05-19 13:21:12,817][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage: 399.11 MB
[2025-05-19 13:21:12,818][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: cfg.model.stageD.diffusion.device=cpu
[2025-05-19 13:21:12,819][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: full cfg.model.stageD.diffusion={'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': True, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[2025-05-19 13:21:12,821][rna_predict.pipeline.stageD.diffusion.utils.config_utils][DEBUG] - [DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg.keys(): ['enabled', 'mode', 'device', 'debug_logging', 'ref_element_size', 'ref_atom_name_chars_size', 'profile_size', 'test_residues_per_batch', 'model_architecture', 'transformer', 'atom_encoder', 'atom_decoder', 'noise_schedule', 'inference', 'use_memory_efficient_kernel', 'use_deepspeed_evo_attention', 'use_lma', 'inplace_safe', 'chunk_size', 'init_from_scratch', 'feature_dimensions']
[DEBUG][DiffusionModule.__init__] cfg.model_architecture: {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}
[DEBUG] DiffusionModule.__init__ kwargs: {}
[2025-05-19 13:21:12,821][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] self.debug_logging resolved to: True
[2025-05-19 13:21:12,821][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] config subtree used: True, None, None
[2025-05-19 13:21:12,821][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] full config: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg attributes: ['atom_decoder', 'atom_encoder', 'chunk_size', 'debug_logging', 'device', 'enabled', 'feature_dimensions', 'inference', 'init_from_scratch', 'inplace_safe', 'mode', 'model_architecture', 'noise_schedule', 'profile_size', 'ref_atom_name_chars_size', 'ref_element_size', 'test_residues_per_batch', 'transformer', 'use_deepspeed_evo_attention', 'use_lma', 'use_memory_efficient_kernel']
[DiffusionModule][__init__] Using OUTER ref_element_size: 128
[DiffusionModule][__init__] Using OUTER ref_atom_name_chars_size: 256
[2025-05-19 13:21:12,822][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - Instantiating DiffusionConditioning with: sigma_data=1.0, c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4, debug_logging=True
[DEBUG][DiffusionConditioning.__init__] c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4
[DEBUG][DiffusionConditioning] expected_z_dim: 8
[DEBUG][DiffusionConditioning] layernorm_s normalized_shape: 16
[DEBUG][DiffusionConditioning] layernorm_n normalized_shape: 4
[2025-05-19 13:21:12,823][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - encoder_config_dict: {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}
[2025-05-19 13:21:12,824][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 13:21:12,825][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[2025-05-19 13:21:12,826][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 13:21:12,828][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - DiffusionModule config: enabled: true
mode: inference
device: ${device}
debug_logging: true
ref_element_size: ${shared.ref_element_size}
ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
profile_size: ${shared.profile_size}
test_residues_per_batch: 2
model_architecture:
  c_token: 8
  c_s: 8
  c_z: 4
  c_s_inputs: 8
  c_atom: 4
  c_atompair: 4
  c_noise_embedding: 4
  sigma_data: 1.0
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  coord_eps: 1.0e-06
  coord_min: -10000.0
  coord_max: 10000.0
  coord_similarity_rtol: 0.001
  test_residues_per_batch: 25
transformer:
  n_blocks: 2
  n_heads: 2
  blocks_per_ckpt: null
atom_encoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
atom_decoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
noise_schedule:
  schedule_type: linear
  s_max: 1.0
  s_min: 0.01
  p: 0.5
  p_mean: 0.0
  p_std: 1.0
inference:
  num_steps: 2
  temperature: 1.0
  use_ddim: true
  sampling:
    num_samples: 1
    gamma0: 0.8
    gamma_min: 1.0
    noise_scale_lambda: 1.003
    step_scale_eta: 1.5
use_memory_efficient_kernel: false
use_deepspeed_evo_attention: false
use_lma: false
inplace_safe: false
chunk_size: null
init_from_scratch: false
feature_dimensions:
  c_s: 8
  c_s_inputs: 8
  c_sing: 8
  s_trunk: 8
  s_inputs: 8

[2025-05-19 13:21:12,829][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] After super().__init__
[2025-05-19 13:21:12,829][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage after initialization: 400.15 MB
[2025-05-19 13:21:12,829][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [DEBUG][NO-PAD] coords shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:12,829][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [HYDRA-CONF-DEBUG][StageD] Using num_steps from config: 2
[2025-05-19 13:21:12,829][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [DEBUG][get_s_inputs] trunk_embeddings keys: ['s_trunk', 's_inputs', 'pair']
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [EmbeddingContext] Initial z_trunk shape: torch.Size([1, 4, 4, 4])
[MEMORY-LOG][After get_z_trunk return] Memory usage: 381.61 MB
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] z_trunk shape at entry: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] coords_init shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,830][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [INSTRUMENT][FeaturePreparationContext] max_atoms=4096, coords_init.shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Before sample_diffusion:
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   coords_init shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   noise_schedule (len=3): tensor([1.0000, 0.5000, 0.0000])
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   num_steps from config: 2
[2025-05-19 13:21:12,832][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   schedule_type from config: linear
[MEMORY-LOG][Before diffusion step 0] Memory usage: 383.36 MB
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-19 13:21:12,901][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-19 13:21:12,901][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-19 13:21:12,902][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:12,903][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:12,903][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 176, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 176, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:12,905][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 176, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:12,907][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:12,907][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:12,910][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-19 13:21:13,876][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-19 13:21:13,877][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:13,878][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:13,878][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,878][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-19 13:21:13,878][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:13,879][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[2025-05-19 13:21:13,879][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,880][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098323e0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098323e0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:13,882][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:13,883][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,883][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,883][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,883][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,883][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,884][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] Initial shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176])
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,884][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,884][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:13,885][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ec280>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec280>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ec280>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,919][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,919][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:13,919][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:13,919][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:13,919][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:13,920][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,928][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,928][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec310>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec310>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:13,940][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,940][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,940][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec4c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:13,945][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,950][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,950][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec460>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec460>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:13,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,960][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:13,960][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,960][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-19 13:21:13,960][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[2025-05-19 13:21:13,961][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,962][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:13,962][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:13,962][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,963][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec5e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec5e0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-19 13:21:13,964][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 0.6300473809242249
[MEMORY-LOG][After diffusion step 0] Memory usage: 531.59 MB
[MEMORY-LOG][Before diffusion step 1] Memory usage: 531.61 MB
[2025-05-19 13:21:14,100][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-19 13:21:14,100][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-19 13:21:14,101][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-19 13:21:14,101][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,102][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:14,103][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-19 13:21:14,103][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:14,103][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:14,103][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:14,103][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 176, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 176, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:14,104][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 176, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:14,104][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:14,104][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427],
         [-0.0096,  0.0212,  0.0102,  0.0427]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:14,107][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-19 13:21:15,077][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-19 13:21:15,077][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,078][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ec4f0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ec4f0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,079][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,080][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] Initial shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176])
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,081][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ec580>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,086][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,087][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:15,087][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:15,087][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:15,087][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:15,087][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,092][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,092][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec610>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec610>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:15,102][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,102][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,102][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec6d0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,107][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,107][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:15,107][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:15,107][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:15,107][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:15,108][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,113][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,113][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec700>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ec700>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[2025-05-19 13:21:15,123][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ec790>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec790>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ec790>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:15,124][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:15,125][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,125][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,125][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec7f0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ec7f0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,125][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,125][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-19 13:21:15,126][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 10.012060165405273
[MEMORY-LOG][After diffusion step 1] Memory usage: 654.28 MB
[2025-05-19 13:21:15,194][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [PATCHED] Found multi-sample coords with shape torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,194][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [PATCHED] Squeezed singleton sample dimension: new shape torch.Size([1, 176, 3])
[2025-05-19 13:21:15,194][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Final coords shape: torch.Size([1, 176, 3])
[MEMORY-LOG][After diffusion] Memory usage: 654.34 MB
[2025-05-19 13:21:15,194][__main__][INFO] - Successfully refined coordinates: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,196][__main__][INFO] - Running Stage D Standalone Demo
[2025-05-19 13:21:15,196][__main__][DEBUG] - [UNIQUE-DEBUG-STAGED-TEST] Stage D runner started.
[2025-05-19 13:21:15,196][__main__][DEBUG] - Using standardized test sequence: ACGU with 44 atoms per residue
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] ENTRY: z_trunk.shape = torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] s_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] z_trunk type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] s_inputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict type: <class 'dict'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] atom_metadata type: <class 'dict'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] trunk_embeddings type: <class 'NoneType'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] features type: <class 'NoneType'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] stage_cfg type: <class 'NoneType'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] diffusion_cfg type: <class 'NoneType'>
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] device: None
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] mode: None
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] debug_logging: True
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] s_trunk shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] s_inputs shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,196][__main__][DEBUG] - [run_stageD] num_atoms: 176
[MEMORY-LOG][StageD ENTRY] Memory usage: 654.50 MB
[2025-05-19 13:21:15,197][__main__][DEBUG] - [DEBUG][run_stageD] Starting Stage D implementation
[DEBUG][run_stageD] Starting Stage D implementation
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict at Stage D entry:
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_trunk'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[DEBUG][run_stageD] input_feature_dict['s_inputs'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[DEBUG][run_stageD] input_feature_dict['pair'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[DEBUG][run_stageD] input_feature_dict['atom_metadata'] type: <class 'dict'>, shape: None
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[DEBUG][run_stageD] input_feature_dict['sequence'] type: <class 'str'>, shape: None
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][run_stageD] input_feature_dict['atom_to_token_idx'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:15,197][__main__][DEBUG] - [run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[DEBUG][run_stageD] input_feature_dict['ref_space_uid'] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,198][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(atom_metadata): <class 'dict'>, keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-19 13:21:15,198][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] type(residue_indices): <class 'torch.Tensor'>, len: 176
[2025-05-19 13:21:15,198][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices.shape: torch.Size([176])
[2025-05-19 13:21:15,198][rna_predict.pipeline.stageD.stage_d_utils.feature_utils][INFO] - [DEBUG][validate_atom_metadata] residue_indices (first 20): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:21:15,198][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] Dumping config values before diffusion:
[2025-05-19 13:21:15,198][__main__][DEBUG] -   n_atoms: 176, n_residues: 4
[2025-05-19 13:21:15,198][__main__][DEBUG] -   s_trunk.shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,198][__main__][DEBUG] -   s_inputs.shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:15,198][__main__][DEBUG] -   z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,198][__main__][DEBUG] -   atom_metadata keys: ['residue_indices', 'atom_type', 'is_backbone']
[2025-05-19 13:21:15,198][__main__][DEBUG] -   context.device: None
[2025-05-19 13:21:15,198][__main__][DEBUG] -   context.mode: None
[2025-05-19 13:21:15,198][__main__][DEBUG] -   context.debug_logging: True
[2025-05-19 13:21:15,198][__main__][DEBUG] - [HYDRA-CONF-DEBUG][StageD] END CONFIG DUMP
[2025-05-19 13:21:15,199][rna_predict.utils.tensor_utils.types][DEBUG] - Expanded residue embeddings torch.Size([4, 8]) to atom embeddings torch.Size([176, 8])
[2025-05-19 13:21:15,199][__main__][DEBUG] - [HYDRA-CONF-BRIDGE][StageD] Bridged s_trunk to atom-level: torch.Size([1, 176, 8])
[MEMORY-LOG][Before bridging residue-to-atom] Memory usage: 654.62 MB
[MEMORY-LOG][After bridging residue-to-atom] Memory usage: 654.70 MB
[MEMORY-LOG][Before diffusion] Memory usage: 654.70 MB
[2025-05-19 13:21:15,202][__main__][DEBUG] - [run_stageD] Calling unified Stage D runner with DiffusionConfig.
[2025-05-19 13:21:15,203][__main__][INFO] - [HYDRA-DEBUG][StageD] stage_cfg.device: cpu
[2025-05-19 13:21:15,203][__main__][INFO] - [HYDRA-DEBUG][StageD] global cfg.device: cpu
[2025-05-19 13:21:15,203][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Initializing ProtenixDiffusionManager
[2025-05-19 13:21:15,203][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage: 686.52 MB
[2025-05-19 13:21:15,204][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: cfg.model.stageD.diffusion.device=cpu
[2025-05-19 13:21:15,205][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] ProtenixDiffusionManager.__init__: full cfg.model.stageD.diffusion={'enabled': True, 'mode': 'inference', 'device': 'cpu', 'debug_logging': True, 'ref_element_size': 128, 'ref_atom_name_chars_size': 256, 'profile_size': 32, 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[2025-05-19 13:21:15,206][rna_predict.pipeline.stageD.diffusion.utils.config_utils][DEBUG] - [DEBUG][_parse_diffusion_module_args] stage_cfg: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg.keys(): ['enabled', 'mode', 'device', 'debug_logging', 'ref_element_size', 'ref_atom_name_chars_size', 'profile_size', 'test_residues_per_batch', 'model_architecture', 'transformer', 'atom_encoder', 'atom_decoder', 'noise_schedule', 'inference', 'use_memory_efficient_kernel', 'use_deepspeed_evo_attention', 'use_lma', 'inplace_safe', 'chunk_size', 'init_from_scratch', 'feature_dimensions']
[DEBUG][DiffusionModule.__init__] cfg.model_architecture: {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}
[DEBUG] DiffusionModule.__init__ kwargs: {}
[2025-05-19 13:21:15,206][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] self.debug_logging resolved to: True
[2025-05-19 13:21:15,207][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] config subtree used: True, None, None
[2025-05-19 13:21:15,207][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][INFO] - [DEBUG-PROPAGATION][StageD-Diffusion] full config: {'enabled': True, 'mode': 'inference', 'device': '${device}', 'debug_logging': True, 'ref_element_size': '${shared.ref_element_size}', 'ref_atom_name_chars_size': '${shared.ref_atom_name_chars_size}', 'profile_size': '${shared.profile_size}', 'test_residues_per_batch': 2, 'model_architecture': {'c_token': 8, 'c_s': 8, 'c_z': 4, 'c_s_inputs': 8, 'c_atom': 4, 'c_atompair': 4, 'c_noise_embedding': 4, 'sigma_data': 1.0, 'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'coord_eps': 1e-06, 'coord_min': -10000.0, 'coord_max': 10000.0, 'coord_similarity_rtol': 0.001, 'test_residues_per_batch': 25}, 'transformer': {'n_blocks': 2, 'n_heads': 2, 'blocks_per_ckpt': None}, 'atom_encoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'atom_decoder': {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}, 'noise_schedule': {'schedule_type': 'linear', 's_max': 1.0, 's_min': 0.01, 'p': 0.5, 'p_mean': 0.0, 'p_std': 1.0}, 'inference': {'num_steps': 2, 'temperature': 1.0, 'use_ddim': True, 'sampling': {'num_samples': 1, 'gamma0': 0.8, 'gamma_min': 1.0, 'noise_scale_lambda': 1.003, 'step_scale_eta': 1.5}}, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'init_from_scratch': False, 'feature_dimensions': {'c_s': 8, 'c_s_inputs': 8, 'c_sing': 8, 's_trunk': 8, 's_inputs': 8}}
[DEBUG][DiffusionModule.__init__] type(cfg): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG][DiffusionModule.__init__] cfg attributes: ['atom_decoder', 'atom_encoder', 'chunk_size', 'debug_logging', 'device', 'enabled', 'feature_dimensions', 'inference', 'init_from_scratch', 'inplace_safe', 'mode', 'model_architecture', 'noise_schedule', 'profile_size', 'ref_atom_name_chars_size', 'ref_element_size', 'test_residues_per_batch', 'transformer', 'use_deepspeed_evo_attention', 'use_lma', 'use_memory_efficient_kernel']
[DiffusionModule][__init__] Using OUTER ref_element_size: 128
[DiffusionModule][__init__] Using OUTER ref_atom_name_chars_size: 256
[2025-05-19 13:21:15,207][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - Instantiating DiffusionConditioning with: sigma_data=1.0, c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4, debug_logging=True
[DEBUG][DiffusionConditioning.__init__] c_z=4, c_s=8, c_s_inputs=8, c_noise_embedding=4
[DEBUG][DiffusionConditioning] expected_z_dim: 8
[DEBUG][DiffusionConditioning] layernorm_s normalized_shape: 16
[DEBUG][DiffusionConditioning] layernorm_n normalized_shape: 4
[2025-05-19 13:21:15,208][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - encoder_config_dict: {'c_in': 4, 'c_hidden': [8], 'c_out': 4, 'dropout': 0.1, 'n_blocks': 1, 'n_heads': 2, 'n_queries': 2, 'n_keys': 2}
[2025-05-19 13:21:15,209][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 13:21:15,210][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[2025-05-19 13:21:15,211][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[2025-05-19 13:21:15,213][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - DiffusionModule config: enabled: true
mode: inference
device: ${device}
debug_logging: true
ref_element_size: ${shared.ref_element_size}
ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
profile_size: ${shared.profile_size}
test_residues_per_batch: 2
model_architecture:
  c_token: 8
  c_s: 8
  c_z: 4
  c_s_inputs: 8
  c_atom: 4
  c_atompair: 4
  c_noise_embedding: 4
  sigma_data: 1.0
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  coord_eps: 1.0e-06
  coord_min: -10000.0
  coord_max: 10000.0
  coord_similarity_rtol: 0.001
  test_residues_per_batch: 25
transformer:
  n_blocks: 2
  n_heads: 2
  blocks_per_ckpt: null
atom_encoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
atom_decoder:
  c_in: 4
  c_hidden:
  - 8
  c_out: 4
  dropout: 0.1
  n_blocks: 1
  n_heads: 2
  n_queries: 2
  n_keys: 2
noise_schedule:
  schedule_type: linear
  s_max: 1.0
  s_min: 0.01
  p: 0.5
  p_mean: 0.0
  p_std: 1.0
inference:
  num_steps: 2
  temperature: 1.0
  use_ddim: true
  sampling:
    num_samples: 1
    gamma0: 0.8
    gamma_min: 1.0
    noise_scale_lambda: 1.003
    step_scale_eta: 1.5
use_memory_efficient_kernel: false
use_deepspeed_evo_attention: false
use_lma: false
inplace_safe: false
chunk_size: null
init_from_scratch: false
feature_dimensions:
  c_s: 8
  c_s_inputs: 8
  c_sing: 8
  s_trunk: 8
  s_inputs: 8

[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [StageD] After super().__init__
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] Memory usage after initialization: 687.00 MB
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.stage_d_utils.output_utils][DEBUG] - [DEBUG][NO-PAD] coords shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [HYDRA-CONF-DEBUG][StageD] Using num_steps from config: 2
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [DEBUG][get_s_inputs] trunk_embeddings keys: ['s_trunk', 's_inputs', 'pair']
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [EmbeddingContext] Initial z_trunk shape: torch.Size([1, 4, 4, 4])
[MEMORY-LOG][After get_z_trunk return] Memory usage: 655.17 MB
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [StageD] z_trunk shape at entry: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] coords_init shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,214][rna_predict.pipeline.stageD.diffusion.context_objects][DEBUG] - [INSTRUMENT][FeaturePreparationContext] max_atoms=4096, coords_init.shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Before sample_diffusion:
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   coords_init shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   noise_schedule (len=3): tensor([1.0000, 0.5000, 0.0000])
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   num_steps from config: 2
[2025-05-19 13:21:15,215][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] -   schedule_type from config: linear
[MEMORY-LOG][Before diffusion step 0] Memory usage: 655.19 MB
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-19 13:21:15,284][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:15,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:15,285][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:15,286][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:15,286][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 176, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 176, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:15,287][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 176, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:15,287][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:15,287][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:15,290][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[2025-05-19 13:21:16,279][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,280][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ee020>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ee020>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:16,281][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:16,282][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,282][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,282][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] Initial shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176])
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,283][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-19 13:21:16,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[2025-05-19 13:21:16,284][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:16,284][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ee200>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee200>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ee200>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:16,289][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,295][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,295][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee260>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee260>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:16,305][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,305][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,305][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee290>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,310][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,310][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:16,310][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:16,310][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:16,310][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:16,311][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,316][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,316][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee320>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee320>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[2025-05-19 13:21:16,328][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ee350>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee350>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ee350>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,330][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee3b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee3b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-19 13:21:16,331][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 0.5494630932807922
[MEMORY-LOG][After diffusion step 0] Memory usage: 514.33 MB
[MEMORY-LOG][Before diffusion step 1] Memory usage: 495.53 MB
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] DiffusionModule.forward: x_noisy device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] t_hat_noise_level device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_trunk device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] s_inputs device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] z_trunk device=cpu
[2025-05-19 13:21:16,479][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_charge] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_pos] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[atom_to_token_idx] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_mask] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_element] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_atom_name_chars] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[ref_space_uid] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEVICE-DEBUG][StageD] processed_input_dict[profile] device=cpu
[2025-05-19 13:21:16,479][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] x_noisy.shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] t_hat_noise_level.shape: torch.Size([1, 1])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_trunk.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] s_inputs.shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [SHAPE-DEBUG][StageD] z_trunk.shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict keys: ['s_trunk', 's_inputs', 'pair', 'atom_metadata', 'sequence', 'atom_to_token_idx', 'ref_space_uid', 'ref_pos', 'ref_charge', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'profile']
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_trunk]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[s_inputs]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[pair]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_metadata]: type=<class 'dict'>, is_tensor=False, shape=None
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[sequence]: type=<class 'str'>, is_tensor=False, shape=None
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[atom_to_token_idx]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_space_uid]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_pos]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 3])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_charge]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_mask]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 1])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_element]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 128])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[ref_atom_name_chars]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 256])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] input_feature_dict[profile]: type=<class 'torch.Tensor'>, is_tensor=True, shape=torch.Size([1, 176, 32])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:16,480][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] atom_to_token_idx tensor shape: torch.Size([1, 176])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Calling f_forward with input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed x_noisy shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed t_hat_noise_level shape: torch.Size([1, 1])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_trunk shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed s_inputs shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Processed z_trunk shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] EDM factors shapes - c_in: torch.Size([1, 1, 1, 1]), c_skip: torch.Size([1, 1, 1, 1]), c_out: torch.Size([1, 1, 1, 1])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_noisy shape after scaling: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:16,481][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[STAGED DEBUG] DiffusionConditioning.forward: c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:16,481][root][INFO] - [DEVICE-DEBUG][StageD] _ensure_input_feature_dict: using device cpu
[DEBUG][_process_pair_features] z_trunk.shape=torch.Size([1, 4, 4, 4]), relpe_output.shape=torch.Size([1, 4, 4, 4])
[DEBUG][_process_pair_features] pair_z.shape before layernorm_z=torch.Size([1, 4, 4, 8])
[STAGED DEBUG] _process_single_features: s_trunk.shape=torch.Size([1, 176, 8]), s_inputs.shape=torch.Size([1, 176, 8]), c_s=8, c_s_inputs=8, expected_in_features=16
[STAGED DEBUG] After concat: single_s.shape=torch.Size([1, 176, 16])
[STAGED DEBUG] After linear_no_bias_s: single_s.shape=torch.Size([1, 176, 8])
[STAGED DEBUG] After transitions: single_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:16,482][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After conditioning - s_single shape: torch.Size([1, 176, 8]), z_pair shape: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:16,482][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] Before _run_with_checkpointing: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:16,482][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.f_forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][AtomAttentionEncoder.forward] Entry: input_feature_dict['atom_to_token_idx']: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][AtomAttentionEncoder.forward] type: <class 'torch.Tensor'>, shape: torch.Size([1, 176]), value: 
[DEBUG][extract_atom_features] input_feature_dict keys: ['ref_charge', 'ref_pos', 'atom_to_token_idx', 'ref_mask', 'ref_element', 'ref_atom_name_chars', 'ref_space_uid', 'profile', 'expected_n_tokens']
    key: ref_charge, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_pos, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 3])
    key: atom_to_token_idx, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
    key: ref_mask, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 1])
    key: ref_element, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 128])
    key: ref_atom_name_chars, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 256])
    key: ref_space_uid, type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 176, 3])
    key: profile, type: <class 'torch.Tensor'>, shape: torch.Size([1, 176, 32])
    key: expected_n_tokens, type: <class 'int'>, shape: None
[DEBUG][extract_atom_features] returning c_l type: <class 'torch.Tensor'> shape: torch.Size([1, 176, 4]) value: tensor([[[-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100],
         [-0.0293, -0.0391,  0.0168,  0.0100]]], grad_fn=<UnsafeViewBackward0>)
[DEBUG][AtomAttentionEncoder.forward_debug] Extracted c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] atom_to_token_idx before process_inputs_with_coords: type=<class 'torch.Tensor'>, shape=torch.Size([1, 176]), is_tensor=True
[DEBUG][forward_debug] c_l shape: torch.Size([1, 176, 4])
[DEBUG][forward_debug] s shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] z shape: torch.Size([1, 176, 8])
[DEBUG][forward_debug] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][forward_debug] t_hat_noise_level shape: torch.Size([1, 1])
[DEBUG][forward_debug] restype shape: None
[DEBUG][process_inputs_with_coords] params.input_feature_dict['atom_to_token_idx']: <class 'torch.Tensor'> torch.Size([1, 176])
[DEBUG][process_inputs_with_coords] atom_to_token_idx argument: <class 'NoneType'> None
[2025-05-19 13:21:16,485][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[2025-05-19 13:21:16,485][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[2025-05-19 13:21:16,486][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][process_inputs_with_coords] restype shape: None
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[2025-05-19 13:21:17,460][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,461][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=False, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ee380>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<UnsafeViewBackward0 object at 0x3098ee380>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,462][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=True, s.grad_fn=None
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic][DEBUG] - [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][_aggregate_to_token_level] a_atom.shape=torch.Size([1, 176, 8]), atom_to_token_idx.shape=torch.Size([1, 176]), num_tokens=4
[DEBUG][aggregate_atom_to_token] Initial shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176])
[DEBUG][aggregate_atom_to_token] Final shapes: x_atom=torch.Size([1, 176, 8]), atom_to_token_idx=torch.Size([1, 176]), scatter_dim=1
[DEBUG][aggregate_atom_to_token] Output shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] After encoder - a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] q_skip shape: torch.Size([1, 176, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] p_skip shape: torch.Size([1, 1, 176, 176, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] a_token shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] atom_to_token_idx shape: torch.Size([1, 176])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] num_tokens: 4
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] Expanded z_pair shape for transformer: torch.Size([1, 4, 4, 4])
[2025-05-19 13:21:17,463][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ee560>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee560>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<AddBackward0 object at 0x3098ee560>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:17,469][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,474][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,474][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee5c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee5c0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:17,483][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,484][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,484][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] ENTRY: a.requires_grad=False, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 4, 4, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee5f0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=4
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 8
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 4, 4]), k_orig=torch.Size([1, 2, 4, 4]), v_orig=torch.Size([1, 2, 4, 4]), bias_orig=torch.Size([1, 2, 4, 4]), num_heads=2
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 2, 4, 4]), num_heads=2
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 4, 4])
DEBUG: After final reshape: result.shape=torch.Size([2, 4, 4])
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[2025-05-19 13:21:17,489][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,494][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,494][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 4, 8]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee680>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 4, 8]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 4, 16]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 4, 16]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<AddBackward0 object at 0x3098ee680>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 4, 8]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 8]), shift.shape=torch.Size([1, 176, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after adjust_tensor_shapes: scale.shape=torch.Size([1, 4, 8]), shift.shape=torch.Size([1, 4, 8]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after direct conditioning: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 4, 8])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 4, 8])
[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. b.shape=torch.Size([1, 4, 16]), b.requires_grad=False, b.is_leaf=True
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 77, in exec_sliced
    return exec(blocks[s:e], a)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/checkpointing.py", line 72, in exec
    a = wrap(block(*a))
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/diffusion.py", line 119, in forward
    ff_out = self.conditioned_transition_block(a=attn_out, s=s)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/transition.py", line 150, in forward
    print(''.join(traceback.format_stack(limit=12)))

[INSTRUMENT][Fallback] ConditionedTransitionBlock fallback triggered. result.requires_grad=False, result.is_leaf=True
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token_transformed shape: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] a_token shape after layernorm: torch.Size([1, 4, 8])
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG AGG] Adapting q_skip feature dimension from 4 to 8
[2025-05-19 13:21:17,504][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][APB] ENTRY: a.requires_grad=True, s.requires_grad=True, z.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8]), z.shape=torch.Size([1, 1, 176, 176, 4])
[DEBUG][AdaLN] ENTRY: a.requires_grad=True, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ee6b0>
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee6b0>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=False, a.grad_fn=<MulBackward0 object at 0x3098ee6b0>
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[DEBUG][APB] after layernorm_a: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,505][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[2025-05-19 13:21:17,505][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] num_heads=2, head_dim=2
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [prep_qkv] expected last dim: 4
[DEBUG][BatchMatmul] q_orig=torch.Size([1, 2, 176, 2]), k_orig=torch.Size([1, 2, 176, 2]), v_orig=torch.Size([1, 2, 176, 2]), bias_orig=torch.Size([1, 1, 2, 176, 176]), num_heads=2
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
DEBUG: input shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2
DEBUG: Squeezing bias from shape=[1, 1, 2, 176, 176]
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing][DEBUG] - Squeezed bias to shape=torch.Size([1, 2, 176, 176])
DEBUG: Before final reshape: bias.shape=torch.Size([1, 2, 176, 176])
DEBUG: After final reshape: result.shape=torch.Size([2, 176, 176])
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights][DEBUG] - q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal][DEBUG] - [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][APB] after standard_multihead_attention: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] after _apply_gating: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][APB] RETURN: a.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,506][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a.shape=torch.Size([1, 176, 4]), a.requires_grad=False, a.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee710>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[INSTRUMENT][CTB.forward] a_norm.shape=torch.Size([1, 176, 4]), a_norm.requires_grad=False, a_norm.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a1.shape=torch.Size([1, 176, 8]), linear_a1.requires_grad=False, linear_a1.is_leaf=True
[INSTRUMENT][CTB.forward] linear_a2.shape=torch.Size([1, 176, 8]), linear_a2.requires_grad=False, linear_a2.is_leaf=True
[INSTRUMENT][CTB.forward] b.shape=torch.Size([1, 176, 8]), b.requires_grad=False, b.is_leaf=True
[DEBUG][AdaLN] ENTRY: a.requires_grad=False, s.requires_grad=True, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] ENTRY: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] ENTRY: s.device=cpu, s.dtype=torch.float32, s.is_leaf=False, s.grad_fn=<CatBackward0 object at 0x3098ee710>
[DEBUG][AdaLN] layernorm_a.weight.device=cpu, layernorm_a.weight.dtype=torch.float32
[DEBUG][AdaLN] layernorm_s.weight.device=cpu, layernorm_s.weight.dtype=torch.float32
[DEBUG][AdaLN] after adjust_tensor_feature_dim: a.device=cpu, a.dtype=torch.float32, a.is_leaf=True, a.grad_fn=None
[DEBUG][AdaLN] after layernorm_a: a_norm.requires_grad=False, a_norm.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] after layernorm_a: a_norm.device=cpu, a_norm.dtype=torch.float32, a_norm.is_leaf=True, a_norm.grad_fn=None
[DEBUG][AdaLN] after layernorm_s: s_norm.requires_grad=False, s_norm.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN] after layernorm_s: s_norm.device=cpu, s_norm.dtype=torch.float32, s_norm.is_leaf=True, s_norm.grad_fn=None
[DEBUG][AdaLN][_apply_conditioning] ENTRY: a.requires_grad=False, s.requires_grad=False, a.shape=torch.Size([1, 176, 4]), s.shape=torch.Size([1, 176, 8])
[DEBUG][AdaLN][_apply_conditioning] after _prepare_scale_and_shift: scale.shape=torch.Size([1, 176, 4]), shift.shape=torch.Size([1, 176, 4]), scale.requires_grad=False, shift.requires_grad=False
[DEBUG][AdaLN][_apply_conditioning] after _try_broadcasting: conditioned_a.requires_grad=False, conditioned_a.shape=torch.Size([1, 176, 4])
[DEBUG][AdaLN] RETURN: out.requires_grad=False, out.shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion][DEBUG] - [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] r_update shape: torch.Size([1, 176, 3])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG] x_denoised shape: torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] mask.shape=torch.Size([1, 176, 1]), x_denoised.shape=torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][forward] After adjustment: mask.shape=torch.Size([1, 1, 176, 1])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][_compute_loss] mask.shape=torch.Size([1, 1, 176, 1]), squared_error.shape=torch.Size([1, 1, 176]), t_hat_noise_level.shape=torch.Size([1, 1])
[2025-05-19 13:21:17,507][rna_predict.pipeline.stageD.diffusion.components.diffusion_module][DEBUG] - [DEBUG][DiffusionModule.forward] Returning x_denoised shape: torch.Size([1, 1, 176, 3]), loss: 8.824686050415039
[MEMORY-LOG][After diffusion step 1] Memory usage: 688.98 MB
[2025-05-19 13:21:17,590][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [PATCHED] Found multi-sample coords with shape torch.Size([1, 1, 176, 3])
[2025-05-19 13:21:17,590][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][INFO] - [PATCHED] Squeezed singleton sample dimension: new shape torch.Size([1, 176, 3])
[2025-05-19 13:21:17,590][rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager][DEBUG] - [multi_step_inference] Final coords shape: torch.Size([1, 176, 3])
[MEMORY-LOG][After diffusion] Memory usage: 688.98 MB
[2025-05-19 13:21:17,590][__main__][INFO] - Successfully refined coordinates: torch.Size([1, 176, 3])
STDERR:
W0519 13:21:11.321000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/encoder_components/feature_processing.py:401: UserWarning: ref_space_uid has shape torch.Size([1, 176, 3]) (3D), expected 4D [B, S, N, 3]. Unsqueezing to [B, 1, N, 3].
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm.py:133: UserWarning: Broadcasting failed in AdaptiveLayerNorm: The size of tensor a (4) must match the size of tensor b (176) at non-singleton dimension 1. Attempting direct shape adjustment.
  warnings.warn(
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/primitives/adaptive_layer_norm_utils.py:337: UserWarning: Token dimension mismatch in AdaptiveLayerNorm: scale has 176 tokens, but a has 4 tokens. Using first 4 tokens from scale.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/attention.py:529: UserWarning: Failed to adapt s dimensions: The expanded size of the tensor (4) must match the existing size (176) at non-singleton dimension 1.  Target sizes: [1, 4, 8].  Tensor sizes: [176, 8]. Using identity gating.
  warnings.warn(f"Failed to adapt s dimensions: {str(reshape_error)}. Using identity gating.")
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
/Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageA/input_embedding/current/transformer/atom_attention_decoder.py:214: UserWarning: Conditioning signal 's' has incorrect feature dim 4, expected 8. Adapting.
  warnings.warn(
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0096,  0.0212,  0.0102,  0.0427, -0.0096,  0.0212,  0.0102,  0.0427,
        -0.0096,  0.0212], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 2 blocks, 2 heads, dimensions: c_a=8, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] Initialized DiffusionTransformer with 1 blocks, 2 heads, dimensions: c_a=4, c_s=8, c_z=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx extracted from params.input_feature_dict: <class 'torch.Tensor'>, shape: torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx type: <class 'torch.Tensor'>, is_tensor: True, shape: torch.Size([1, 176]), dtype: torch.int64
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] atom_to_token_idx contents: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
         3, 3, 3, 3, 3, 3, 3, 3]])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] q_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] c_l shape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] r_l shape: torch.Size([1, 1, 176, 3])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] s shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] z shape: torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][process_inputs_with_coords] restype shape: None
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] p_lm is 4D, unsqueezing to 5D. Shape before: torch.Size([1, 176, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][CALL] _process_style_embedding c_l.shape=torch.Size([1, 176, 4]) s.shape=torch.Size([1, 176, 8]) atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][PRE-ADD] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4]), broadcasted_s.shape=torch.Size([1, 176, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.shape=torch.Size([1, 176, 4]), x.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l type=<class 'torch.Tensor'>, x type=<class 'torch.Tensor'>
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] atom_to_token_idx.shape=torch.Size([1, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [DEBUG][ENCODER][_process_style_embedding] c_l.flatten()[:10]=tensor([-0.0293, -0.0391,  0.0168,  0.0100, -0.0293, -0.0391,  0.0168,  0.0100,
        -0.0293, -0.0391], grad_fn=<SliceBackward0>)
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] q_l.shape=torch.Size([1, 176, 4]) c_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.encoder_components.forward_logic] [process_inputs_with_coords] POST-TRANSFORMER q_l.shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 2, 4, 4]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 4, 4]), k.shape: torch.Size([2, 4, 4]), v.shape: torch.Size([2, 4, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 4, 2, 4]), c_hidden=8
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 4, 8])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ENTRY: a.requires_grad=True, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] q.shape before reshape: torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] num_heads=2, head_dim=2
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [prep_qkv] expected last dim: 4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] _reshape_attention_bias input: shape=torch.Size([1, 1, 2, 176, 176]), num_heads=2, dtype=torch.float32
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_processing] Squeezed bias to shape=torch.Size([1, 2, 176, 176])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_weights] q.shape: torch.Size([2, 176, 2]), k.shape: torch.Size([2, 176, 2]), v.shape: torch.Size([2, 176, 2])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.primitives.attention_utils_internal] [wrap_up] o.shape=torch.Size([1, 176, 2, 2]), c_hidden=4
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after attention).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] attn_out(after residual).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] ff_out.requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])
[DEBUG][rna_predict.pipeline.stageA.input_embedding.current.transformer.diffusion] [DTB] out_a(final).requires_grad=False, shape=torch.Size([1, 176, 4])


================================================================================
Output from: interface.py
Timestamp: 2025-05-19 13:21:18
================================================================================

STDOUT:
--- [DEBUG] Hydra Config at startup ---
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    enabled: true
    num_hidden: 128
    dropout: 0.3
    debug_logging: true
    freeze_params: true
    min_seq_length: 80
    batch_size: 32
    lr: 0.001
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    threshold: 0.5
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
    debug_logging: true
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
      debug_logging: true
      init_from_scratch: false
    pairformer:
      device: ${device}
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_s: 0
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      protenix_integration:
        device: ${device}
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        atoms_per_token: 4
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      use_checkpoint: true
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      block:
        n_heads: 2
        c_z: 4
        c_s: 8
        c_hidden_mul: 4
        c_hidden_pair_att: 4
        no_heads_pair: 2
        dropout: 0.25
      stack:
        n_blocks: 1
        n_heads: 2
        c_z: 4
        c_s: 8
        dropout: 0.25
        blocks_per_ckpt: null
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
        c_s_inputs: 2
        blocks_per_ckpt: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1.0
          min_bin: 1.0
          no_bins: 1.0
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
      debug_logging: false
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      c_atompair: 4
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      init_from_scratch: false
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      atom_metadata: null
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_noise_embedding: 4
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
        c_atompair: 4
        sigma_data: 1.0
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        sigma_data: 16.0
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      diffusion:
        enabled: true
        mode: inference
        device: cpu
        debug_logging: true
        ref_element_size: 4
        ref_atom_name_chars_size: 8
        profile_size: 8
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null
        init_from_scratch: false
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

--- [END DEBUG] ---
Configuration loaded by Hydra:
shared:
  ref_element_size: 128
  ref_atom_name_chars_size: 256
  profile_size: 32
sequence: ${test_data.sequence}
device: ${oc.env:DEVICE, cpu}
seed: 42
atoms_per_residue: 44
extraction_backend: dssr
run_stageD: true
enable_stageC: true
merge_latent: true
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
  ignore_nan_values: true
  nan_replacement_value: 0.0
training:
  checkpoint_dir: outputs/checkpoints
  accelerator: cpu
  devices: 1
  epochs: 10
  batch_size: 32
  limit_train_batches: 1
  streamline_mode: true
prediction:
  repeats: 5
  residue_atom_choice: 0
  enable_stochastic_inference_for_submission: false
data:
  index_csv: ./data/index.csv
  root_dir: ./data/
  max_residues: 512
  max_atoms: 4096
  C_element: 128
  C_char: 256
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  batch_size: 4
  num_workers: 8
  load_adj: false
  load_ang: true
  coord_fill_value: 'nan'
  coord_dtype: float32
model:
  stageA:
    enabled: true
    num_hidden: 128
    dropout: 0.3
    debug_logging: true
    freeze_params: true
    min_seq_length: 80
    batch_size: 32
    lr: 0.001
    device: ${device}
    checkpoint_path: RFold/checkpoints/RNAStralign_trainset_pretrained.pth
    checkpoint_url: https://www.dropbox.com/s/l04l9bf3v6z2tfd/checkpoints.zip?dl=1
    checkpoint_zip_path: RFold/checkpoints.zip
    threshold: 0.5
    run_example: true
    example_sequence: AAGUCUGGUGGACAUUGGCGUCCUGAGGUGUUAAAACCUCUUAUUGCUGACGCCAGAAAGAGAAGAACUUCGGUUCUACUAGUCGACUAUACUACAAGCUUUGGGUGUAUAGCGGCAAGACAACCUGGAUCGGGGGAGGCUAAGGGCGCAAGCCUAUGCUAACCCCGAGCCGAGCUACUGGAGGGCAACCCCCAGAUAGCCGGUGUAGAGCGCGGAAAGGUGUCGGUCAUCCUAUCUGAUAGGUGGCUUGAGGGACGUGCCGUCUCACCCGAAAGGGUGUUUCUAAGGAGGAGCUCCCAAAGGGCAAAUCUUAGAAAAGGGUGUAUACCCUAUAAUUUAACGGCCAGCAGCC
    visualization:
      enabled: true
      varna_jar_path: tools/varna-3-93.jar
      resolution: 8.0
      output_path: test_seq.png
    model:
      conv_channels:
      - 64
      - 128
      - 256
      - 512
      residual: true
      c_in: 1
      c_out: 1
      c_hid: 32
      seq2map:
        input_dim: 4
        max_length: 3000
        attention_heads: 8
        attention_dropout: 0.1
        positional_encoding: true
        query_key_dim: 128
        expansion_factor: 2.0
        heads: 1
      decoder:
        up_conv_channels:
        - 256
        - 128
        - 64
        skip_connections: true
  stageC:
    enabled: true
    method: mp_nerf
    do_ring_closure: false
    place_bases: true
    sugar_pucker: C3'-endo
    device: ${device}
    angle_representation: degrees
    use_metadata: false
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
    debug_logging: true
  stageB:
    torsion_bert:
      model_name_or_path: sayby/rna_torsionbert
      device: ${device}
      angle_mode: sin_cos
      num_angles: 7
      max_length: 512
      checkpoint_path: null
      lora:
        enabled: false
        r: 8
        alpha: 16
        dropout: 0.1
        target_modules:
        - query
        - value
      debug_logging: true
      init_from_scratch: false
    pairformer:
      device: ${device}
      n_blocks: 1
      n_heads: 1
      c_z: 2
      c_s: 0
      c_token: 384
      c_atom: 128
      c_pair: 32
      dropout: 0.0
      freeze_params: false
      protenix_integration:
        device: ${device}
        c_token: 449
        restype_dim: 32
        profile_dim: 32
        c_atom: 128
        c_pair: 32
        atoms_per_token: 4
        num_heads: 4
        num_layers: 3
        r_max: 32
        s_max: 2
        use_optimized: false
      c_hidden_mul: 1
      c_hidden_pair_att: 2
      no_heads_pair: 1
      init_z_from_adjacency: false
      use_checkpoint: true
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      block:
        n_heads: 2
        c_z: 4
        c_s: 8
        c_hidden_mul: 4
        c_hidden_pair_att: 4
        no_heads_pair: 2
        dropout: 0.25
      stack:
        n_blocks: 1
        n_heads: 2
        c_z: 4
        c_s: 8
        dropout: 0.25
        blocks_per_ckpt: null
      msa:
        c_m: 2
        c: 2
        c_z: 2
        dropout: 0.0
        n_blocks: 1
        enable: false
        strategy: random
        train_cutoff: 1
        test_cutoff: 1
        train_lowerb: 1
        test_lowerb: 1
        n_heads: 1
        pair_dropout: 0.0
        input_feature_dims:
          msa: 2
          has_deletion: 1
          deletion_value: 1
        c_s_inputs: 2
        blocks_per_ckpt: 1
      template:
        n_blocks: 1
        c: 2
        c_z: 2
        dropout: 0.0
        blocks_per_ckpt: null
        input_feature_dims:
          feature1:
            template_distogram: 1
            b_template_backbone_frame_mask: 1
            template_unit_vector: 1
            b_template_pseudo_beta_mask: 1
          feature2:
            template_restype_i: 1
            template_restype_j: 1
        distogram:
          max_bin: 1.0
          min_bin: 1.0
          no_bins: 1.0
      lora:
        enabled: false
        r: 1
        alpha: 1
        dropout: 0.0
        target_modules:
        - query
        - value
      debug_logging: false
  stageD:
    enabled: true
    mode: ${stageD_diffusion.diffusion.mode}
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_noise_embedding: 4
      num_layers: 6
      num_heads: 8
      dropout: 0.1
      coord_eps: 1.0e-06
      coord_min: -10000.0
      coord_max: 10000.0
      coord_similarity_rtol: 0.001
      test_residues_per_batch: 25
      c_atompair: 4
      sigma_data: 1.0
    transformer:
      n_blocks: ${stageD_diffusion.diffusion.transformer.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.transformer.n_heads}
      blocks_per_ckpt: ${stageD_diffusion.diffusion.transformer.blocks_per_ckpt}
    atom_encoder:
      c_in: ${stageD_diffusion.diffusion.atom_encoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_encoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_encoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_encoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_encoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_encoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_encoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_encoder.n_keys}
    atom_decoder:
      c_in: ${stageD_diffusion.diffusion.atom_decoder.c_in}
      c_hidden: ${stageD_diffusion.diffusion.atom_decoder.c_hidden}
      c_out: ${stageD_diffusion.diffusion.atom_decoder.c_out}
      dropout: ${stageD_diffusion.diffusion.atom_decoder.dropout}
      n_blocks: ${stageD_diffusion.diffusion.atom_decoder.n_blocks}
      n_heads: ${stageD_diffusion.diffusion.atom_decoder.n_heads}
      n_queries: ${stageD_diffusion.diffusion.atom_decoder.n_queries}
      n_keys: ${stageD_diffusion.diffusion.atom_decoder.n_keys}
    diffusion:
      init_from_scratch: false
      enabled: true
      mode: inference
      device: ${device}
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      atom_metadata: null
      feature_dimensions:
        c_s: 8
        c_s_inputs: 8
        c_sing: 8
        s_trunk: 8
        s_inputs: 8
      test_residues_per_batch: 2
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_noise_embedding: 4
        num_layers: 6
        num_heads: 8
        dropout: 0.1
        coord_eps: 1.0e-06
        coord_min: -10000.0
        coord_max: 10000.0
        coord_similarity_rtol: 0.001
        test_residues_per_batch: 25
        c_atompair: 4
        sigma_data: 1.0
      transformer:
        n_blocks: 2
        n_heads: 2
        blocks_per_ckpt: null
      atom_encoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      atom_decoder:
        c_in: 4
        c_hidden:
        - 8
        c_out: 4
        dropout: 0.1
        n_blocks: 1
        n_heads: 2
        n_queries: 2
        n_keys: 2
      noise_schedule:
        schedule_type: linear
        s_max: 1.0
        s_min: 0.01
        p: 0.5
        sigma_data: 16.0
        p_mean: 0.0
        p_std: 1.0
      inference:
        num_steps: 2
        temperature: 1.0
        use_ddim: true
        sampling:
          num_samples: 1
          gamma0: 0.8
          gamma_min: 1.0
          noise_scale_lambda: 1.003
          step_scale_eta: 1.5
      use_memory_efficient_kernel: false
      use_deepspeed_evo_attention: false
      use_lma: false
      inplace_safe: false
      chunk_size: null
      diffusion:
        enabled: true
        mode: inference
        device: cpu
        debug_logging: true
        ref_element_size: 4
        ref_atom_name_chars_size: 8
        profile_size: 8
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null
        init_from_scratch: false
    input_features: null
  protenix_integration:
    device: ${device}
    c_token: 449
    restype_dim: 32
    profile_dim: 32
    c_atom: 128
    c_pair: 32
    atoms_per_token: 4
    num_heads: 4
    num_layers: 3
    r_max: 32
    s_max: 2
    use_optimized: false
stageD_diffusion:
  enabled: true
  debug_logging: true
  ref_element_size: ${shared.ref_element_size}
  ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
  profile_size: ${shared.profile_size}
  model_architecture:
    c_token: 8
    c_s: 8
    c_z: 4
    c_s_inputs: 8
    c_atom: 4
    c_atompair: 4
    c_noise_embedding: 4
    sigma_data: 1.0
  diffusion:
    init_from_scratch: false
    enabled: true
    mode: inference
    device: ${device}
    debug_logging: true
    ref_element_size: ${shared.ref_element_size}
    ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
    profile_size: ${shared.profile_size}
    feature_dimensions:
      c_s: 8
      c_s_inputs: 8
      c_sing: 8
      s_trunk: 8
      s_inputs: 8
    test_residues_per_batch: 2
    model_architecture:
      c_token: 8
      c_s: 8
      c_z: 4
      c_s_inputs: 8
      c_atom: 4
      c_atompair: 4
      c_noise_embedding: 4
      sigma_data: 1.0
    transformer:
      n_blocks: 2
      n_heads: 2
      blocks_per_ckpt: null
    atom_encoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    atom_decoder:
      c_in: 4
      c_hidden:
      - 8
      c_out: 4
      dropout: 0.1
      n_blocks: 1
      n_heads: 2
      n_queries: 2
      n_keys: 2
    noise_schedule:
      schedule_type: linear
      s_max: 1.0
      s_min: 0.01
      p: 0.5
      p_mean: 0.0
      p_std: 1.0
    inference:
      num_steps: 2
      temperature: 1.0
      use_ddim: true
      sampling:
        num_samples: 1
        gamma0: 0.8
        gamma_min: 1.0
        noise_scale_lambda: 1.003
        step_scale_eta: 1.5
    use_memory_efficient_kernel: false
    use_deepspeed_evo_attention: false
    use_lma: false
    inplace_safe: false
    chunk_size: null
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
  adjacency_fill_value: 1.0
  target_dim: 3
  torsion_angle_dim: 7
  embedding_dims:
    s_trunk: 384
    z_trunk: 128
    s_inputs: 449
  sequence_path: ./data/kaggle/stanford-rna-3d-folding/train_sequences.csv
  data_index: ./rna_predict/dataset/examples/kaggle_minimal_index.csv
  target_id: 1SCL_A
  model:
    stageD:
      enabled: true
      debug_logging: true
      ref_element_size: ${shared.ref_element_size}
      ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
      profile_size: ${shared.profile_size}
      model_architecture:
        c_token: 8
        c_s: 8
        c_z: 4
        c_s_inputs: 8
        c_atom: 4
        c_atompair: 4
        c_noise_embedding: 4
        sigma_data: 1.0
      diffusion:
        init_from_scratch: false
        enabled: true
        mode: inference
        device: ${device}
        debug_logging: true
        ref_element_size: ${shared.ref_element_size}
        ref_atom_name_chars_size: ${shared.ref_atom_name_chars_size}
        profile_size: ${shared.profile_size}
        feature_dimensions:
          c_s: 8
          c_s_inputs: 8
          c_sing: 8
          s_trunk: 8
          s_inputs: 8
        test_residues_per_batch: 2
        model_architecture:
          c_token: 8
          c_s: 8
          c_z: 4
          c_s_inputs: 8
          c_atom: 4
          c_atompair: 4
          c_noise_embedding: 4
          sigma_data: 1.0
        transformer:
          n_blocks: 2
          n_heads: 2
          blocks_per_ckpt: null
        atom_encoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        atom_decoder:
          c_in: 4
          c_hidden:
          - 8
          c_out: 4
          dropout: 0.1
          n_blocks: 1
          n_heads: 2
          n_queries: 2
          n_keys: 2
        noise_schedule:
          schedule_type: linear
          s_max: 1.0
          s_min: 0.01
          p: 0.5
          p_mean: 0.0
          p_std: 1.0
        inference:
          num_steps: 2
          temperature: 1.0
          use_ddim: true
          sampling:
            num_samples: 1
            gamma0: 0.8
            gamma_min: 1.0
            noise_scale_lambda: 1.003
            step_scale_eta: 1.5
        use_memory_efficient_kernel: false
        use_deepspeed_evo_attention: false
        use_lma: false
        inplace_safe: false
        chunk_size: null

------------------------------
[2025-05-19 13:21:50,633][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] self.debug_logging resolved to: True
[2025-05-19 13:21:50,641][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] config subtree used: True, None, None
[2025-05-19 13:21:50,643][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-PROPAGATION][StageB-TorsionBert] full config: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-19 13:21:50,643][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG-FULL] Full cfg received: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-19 13:21:50,643][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-INST-STAGEB-002] Full config received in StageBTorsionBertPredictor: {'model_name_or_path': 'sayby/rna_torsionbert', 'device': '${device}', 'angle_mode': 'sin_cos', 'num_angles': 7, 'max_length': 512, 'checkpoint_path': None, 'lora': {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}, 'debug_logging': True, 'init_from_scratch': False}
[2025-05-19 13:21:50,650][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Used cfg.device
[2025-05-19 13:21:50,650][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved device in config: cpu
[2025-05-19 13:21:50,651][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Using direct attributes
[2025-05-19 13:21:50,652][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEBUG-STAGEB-TORSIONBERT-CONFIG] Resolved configuration: model_name_or_path=sayby/rna_torsionbert, angle_mode=sin_cos, num_angles=7, max_length=512
[2025-05-19 13:21:50,654][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:21:50,669][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 56.84 MB
[2025-05-19 13:21:50,670][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-19 13:21:50,670][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-19 13:21:50,670][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-19 13:21:50,670][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-19 13:21:50,670][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-19 13:22:11,457][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:22:11,584][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:22:11,705][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:22:11,707][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:22:11,751][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-19 13:22:11,796][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:22:11,797][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:22:11,801][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
Running prediction for sequence: ACGUACGU
[2025-05-19 13:22:11,822][rna_predict.predict_submission][INFO] - [DEBUG-CONFIG-PROPAGATION] enable_stochastic_inference_for_submission=False (from config), passing as stochastic_pass=False
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: ACGUACGU
[2025-05-19 13:22:11,833][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:12,005][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12,006][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12,006][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12,007][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:12,007][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:12,007][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:12,007][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:12,116][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:12,116][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:22:23,687][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:22:23,705][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:22:23,795][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:22:23,796][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:22:23,829][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:22:23,834][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:22:23,834][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:22:23,834][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-19 13:22:24,425][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:24,428][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:24,428][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:24,428][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:24,428][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:22:24,429][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:22:24,429][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:22:24,448][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156d628f0>
[2025-05-19 13:22:24,450][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:24,457][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:24,457][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:24,457][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:24,457][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:22:24,458][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:22:24,458][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:22:24,458][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:22:24,458][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:22:24,458][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:22:25,173][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:22:25,182][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156d63ca0>
[2025-05-19 13:22:25,408][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:22:25,414][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156d63ca0>
[2025-05-19 13:22:25,414][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:25,581][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:22:25,582][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:22:25,582][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:25,584][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:22:25,585][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:22:25,585][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:22:25,587][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:22:25,959][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:22:25,961][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:22:25,961][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x156acdbd0>
[2025-05-19 13:22:25,961][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:22:25,961][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156acdbd0>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156d62410>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x156d62ef0>
[2025-05-19 13:22:26,319][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:22:26,322][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:22:26,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:22:26,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:22:26,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:22:26,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:22:26,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:22:26,330][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:22:26,331][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.37s
[2025-05-19 13:22:26,333][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.37s
[2025-05-19 13:22:26,333][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:22:26,333][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:22:26,333][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: ACGUACGU
[2025-05-19 13:22:26,356][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:26,484][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:26,485][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:26,489][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:26,489][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:22:54,592][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:22:54,723][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:22:54,745][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:22:54,751][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:22:54,854][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:22:54,855][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:22:54,855][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:22:54,855][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-19 13:22:55,858][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:55,864][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:55,865][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:55,865][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:55,865][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:22:55,865][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:22:55,865][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:22:55,878][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156d61de0>
[2025-05-19 13:22:55,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:55,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:55,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:55,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:55,884][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:22:55,884][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:22:55,889][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:22:55,890][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:22:55,890][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:22:55,891][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:22:55,892][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:22:55,895][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156d63b50>
[2025-05-19 13:22:55,957][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:22:55,957][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156d63b50>
[2025-05-19 13:22:55,958][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:56,206][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:22:56,206][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:22:56,206][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:56,207][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:22:56,207][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:22:56,207][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:22:56,207][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:22:56,604][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:22:56,604][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:22:56,604][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:22:56,604][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:22:56,604][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156d63b50>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x156fa2ef0>
[2025-05-19 13:22:56,872][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:22:56,873][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:22:56,897][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:22:56,899][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:22:56,900][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:22:56,902][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:22:56,903][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.40s
[2025-05-19 13:22:56,903][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.40s
[2025-05-19 13:22:56,903][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:22:56,904][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:22:56,904][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: ACGUACGU
[2025-05-19 13:22:57,190][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:57,290][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:57,296][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:57,298][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:15,875][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:15,952][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:15,962][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:15,972][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:15,998][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:16,007][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:16,008][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:16,018][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-19 13:23:17,717][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:18,090][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:18,092][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:18,093][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:18,108][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:18,108][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:18,108][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:18,124][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa2d10>
[2025-05-19 13:23:18,125][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:18,126][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:18,126][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:18,126][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:18,131][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:18,132][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:18,139][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:18,142][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:18,142][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:18,143][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:18,148][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:18,153][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa0b20>
[2025-05-19 13:23:18,220][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:18,220][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa0b20>
[2025-05-19 13:23:18,221][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:18,322][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:18,325][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:18,325][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:18,328][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:18,328][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:18,329][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:18,333][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:18,596][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:18,596][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:18,597][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:18,597][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:18,597][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156fa0b20>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x156fa2e30>
[2025-05-19 13:23:18,773][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:18,773][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:18,793][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:18,796][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:18,797][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:18,797][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:18,797][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:18,797][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:18,797][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:18,798][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:18,798][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:18,798][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:18,798][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:18,798][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.26s
[2025-05-19 13:23:18,798][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.26s
[2025-05-19 13:23:18,799][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:18,799][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:18,799][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: ACGUACGU
[2025-05-19 13:23:19,159][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:23:19,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19,239][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19,240][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19,240][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:23:19,240][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:23:19,240][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:23:19,240][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:23:19,249][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:23:19,249][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:36,770][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:36,820][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:36,860][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:36,867][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:36,980][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:36,987][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:36,987][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:36,987][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-19 13:23:38,030][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:38,030][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:38,030][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:38,030][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:38,034][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:38,034][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:38,034][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:38,045][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa2cb0>
[2025-05-19 13:23:38,045][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:38,045][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:38,046][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:38,047][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:38,048][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:38,050][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa1a80>
[2025-05-19 13:23:38,116][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:38,117][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa1a80>
[2025-05-19 13:23:38,117][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:38,185][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:38,185][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:38,185][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:38,189][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:38,189][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:38,189][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:38,189][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:38,529][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:38,529][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:38,529][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:38,529][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:38,529][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156fa1a80>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x156fa0b80>
[2025-05-19 13:23:38,857][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:38,857][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:38,869][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:38,875][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:38,875][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:38,875][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:38,880][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:38,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:38,881][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.34s
[2025-05-19 13:23:38,881][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.34s
[2025-05-19 13:23:38,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:38,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:38,881][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[DEBUG][predict_3d_structure] sequence type: <class 'str'>, value: ACGUACGU
[2025-05-19 13:23:39,074][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:23:39,235][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:23:39,236][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:23:39,242][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:23:39,250][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:54,790][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:54,797][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:54,797][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:54,797][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:54,807][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:54,807][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:54,807][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:54,807][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[DEBUG] type(stageC_full_config): <class 'omegaconf.dictconfig.DictConfig'>
[DEBUG] stageC_full_config keys: ['model']
[DEBUG] stageC_full_config['model'] keys: ['stageC']
[2025-05-19 13:23:54,934][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:54,935][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:54,935][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:54,935][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:54,936][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:54,936][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:54,936][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:54,936][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa28c0>
[2025-05-19 13:23:54,938][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:54,938][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:54,938][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:54,938][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:54,939][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:54,939][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:54,940][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:54,940][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:54,940][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:54,940][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:54,942][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:54,943][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa1ab0>
[2025-05-19 13:23:54,955][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:54,956][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa1ab0>
[2025-05-19 13:23:54,956][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:54,971][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:54,971][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:54,971][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:54,979][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:54,979][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:54,979][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:54,979][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:55,213][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:55,215][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:55,216][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:55,216][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:55,216][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[DEBUG-GRAD-PRINT] backbone_coords (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156fa1ab0>
[DEBUG-GRAD-PRINT-BASE-PLACEMENT] full_coords.requires_grad: True, grad_fn: <CopySlices object at 0x156de8040>
[2025-05-19 13:23:55,407][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:55,407][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:55,433][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:55,433][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:55,433][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:55,433][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:55,434][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:55,435][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.23s
[2025-05-19 13:23:55,435][root][INFO] - ROOT: StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.23s
[2025-05-19 13:23:55,435][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:55,435][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:55,435][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170

Submission DataFrame Head:
   ID resname  resid        x_1  ...        z_4        x_5        y_5        z_5
0   1       A      1  13.774000  ...  15.251000  13.774000  39.026001  15.251000
1   2       A      1  14.093496  ...  14.502467  14.093496  38.370602  14.502467
2   3       A      1  14.093496  ...  14.502467  14.093496  38.370602  14.502467
3   4       A      1  14.872000  ...  15.708000  14.872000  37.933998  15.708000
4   5       A      1  14.938000  ...  17.082001  14.938000  37.437000  17.082001

[5 rows x 18 columns]

Submission saved to submission.csv
STDERR:
sys:1: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
sys:1: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'data/default' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageA' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageC' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_torsion' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageB_pairformer' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/protenix_integration' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/Users/tomriddle1/RNA_PREDICT/.venv/lib/python3.10/site-packages/hydra/main.py:94: UserWarning: 
'model/stageD' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing StageBTorsionBertPredictor...
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [MEMORY-LOG][StageB] Memory usage: 56.84 MB
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Initializing TorsionBERT predictor with device: cpu
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Model path: sayby/rna_torsionbert
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Angle mode: sin_cos
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Max length: 512
[2025-05-19 13:21:50][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - LoRA config: {'enabled': False, 'r': 8, 'alpha': 16, 'dropout': 0.1, 'target_modules': ['query', 'value']}
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class before to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config before to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model class after to(device): <class 'transformers_modules.sayby.rna_torsionbert.fe64e43f94249f68c7e50b18f1befe8290492d91.rna_torsionbert_model.RNATorsionBERTModel'>
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Model config after to(device): RNATorsionBertConfig {
  "_attn_implementation_autoset": true,
  "angles": "BACKBONE",
  "architectures": [
    "RNATorsionBERTModel"
  ],
  "auto_map": {
    "AutoConfig": "sayby/rna_torsionbert--rna_torsionbert_config.RNATorsionBertConfig",
    "AutoModel": "sayby/rna_torsionbert--rna_torsionbert_model.RNATorsionBERTModel"
  },
  "hidden_size": 1024,
  "k": 3,
  "model_type": "rna_torsionbert",
  "torch_dtype": "float32",
  "transformers_version": "4.50.0"
}

[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [DEVICE-DEBUG][stageB_torsion] Parameter device summary: {'dnabert.embeddings.word_embeddings.weight': 'cpu', 'dnabert.embeddings.position_embeddings.weight': 'cpu', 'dnabert.embeddings.token_type_embeddings.weight': 'cpu', 'dnabert.embeddings.LayerNorm.weight': 'cpu', 'dnabert.embeddings.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.0.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.0.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.dense.weight': 'cpu', 'dnabert.encoder.layer.0.output.dense.bias': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.0.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.1.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.1.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.dense.weight': 'cpu', 'dnabert.encoder.layer.1.output.dense.bias': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.1.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.2.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.2.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.dense.weight': 'cpu', 'dnabert.encoder.layer.2.output.dense.bias': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.2.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.3.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.3.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.dense.weight': 'cpu', 'dnabert.encoder.layer.3.output.dense.bias': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.3.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.4.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.4.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.dense.weight': 'cpu', 'dnabert.encoder.layer.4.output.dense.bias': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.4.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.5.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.5.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.dense.weight': 'cpu', 'dnabert.encoder.layer.5.output.dense.bias': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.5.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.6.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.6.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.dense.weight': 'cpu', 'dnabert.encoder.layer.6.output.dense.bias': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.6.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.7.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.7.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.dense.weight': 'cpu', 'dnabert.encoder.layer.7.output.dense.bias': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.7.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.8.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.8.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.dense.weight': 'cpu', 'dnabert.encoder.layer.8.output.dense.bias': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.8.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.9.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.9.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.dense.weight': 'cpu', 'dnabert.encoder.layer.9.output.dense.bias': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.9.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.10.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.10.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.dense.weight': 'cpu', 'dnabert.encoder.layer.10.output.dense.bias': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.10.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.query.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.key.bias': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.weight': 'cpu', 'dnabert.encoder.layer.11.attention.self.value.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.attention.output.LayerNorm.bias': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.weight': 'cpu', 'dnabert.encoder.layer.11.intermediate.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.dense.weight': 'cpu', 'dnabert.encoder.layer.11.output.dense.bias': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.weight': 'cpu', 'dnabert.encoder.layer.11.output.LayerNorm.bias': 'cpu', 'dnabert.pooler.dense.weight': 'cpu', 'dnabert.pooler.dense.bias': 'cpu', 'regressor.0.weight': 'cpu', 'regressor.0.bias': 'cpu', 'regressor.1.weight': 'cpu', 'regressor.1.bias': 'cpu', 'regressor.3.weight': 'cpu', 'regressor.3.bias': 'cpu'}
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - [LoRA] LoRA not applied (missing PEFT, config, or disabled). All params trainable.
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - TorsionBERT model and tokenizer loaded successfully.
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][INFO] - Expected model output dimension: 14
[2025-05-19 13:22:11][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:12][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:22:23][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156d628f0>
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:22:24][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156d63ca0>
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156d63ca0>
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x156acdbd0>
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:22:25][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x156acdbd0>
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.37s
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:22:26][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:26][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:22:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156d61de0>
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156d63b50>
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156d63b50>
[2025-05-19 13:22:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.40s
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:22:56][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:22:57][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:15][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:15][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:15][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:15][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:15][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:16][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:16][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:16][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:23:17][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa2d10>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa0b20>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa0b20>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.26s
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:18][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:23:19][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:36][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa2cb0>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa1a80>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa1a80>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.34s
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:38][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [UNIQUE-DEBUG-STAGEB-TORSIONBERT-PREDICT] Predicting angles for sequence of length 8 | stochastic_pass=False | seed=None
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'input_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'token_type_ids' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG-PREPROCESS] Output tensor 'attention_mask' device: cpu (should match self.device: cpu)
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Model device: cpu
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'input_ids' device: cpu
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'token_type_ids' device: cpu
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] Input tensor 'attention_mask' device: cpu
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Inputs to model: {'input_ids': tensor([[2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
[2025-05-19 13:23:39][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Calling model with inputs: {'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512])}
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model outputs type: <class 'dict'>
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Model output (logits/last_hidden_state) shape: torch.Size([1, 512, 32])
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] angle_preds device before projection: cpu, contiguous: True
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG] output_projection.weight device: cpu
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEBUG-PREDICTOR] Raw predictions shape: torch.Size([8, 14])
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] sequence: ACGUACGU
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [TorsionBERT] output: torch.Size([8, 14])
[2025-05-19 13:23:54][rna_predict.pipeline.stageB.torsion.torsion_bert_predictor][DEBUG] - [DEVICE-DEBUG][StageBTorsionBertPredictor.__call__] Output 'torsion_angles' device: cpu
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-ENTRY] Entered run_stageC_rna_mpnerf in /Users/tomriddle1/RNA_PREDICT/rna_predict/pipeline/stageC/stage_c_reconstruction.py
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Entered run_stageC_rna_mpnerf
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.requires_grad: True
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions.grad_fn: <SqueezeBackward1 object at 0x156fa28c0>
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg: {'model': {'stageC': {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}}}
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg keys: ['model']
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [validate_stageC_config] cfg.model keys: ['stageC']
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [UNIQUE-DEBUG-STAGEC-TEST] Stage C config validated.
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] stage_cfg.device: cpu (type: <class 'str'>)
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] Full stage_cfg: {'enabled': True, 'method': 'mp_nerf', 'do_ring_closure': False, 'place_bases': True, 'sugar_pucker': "C3'-endo", 'device': 'cpu', 'angle_representation': 'degrees', 'use_metadata': False, 'use_memory_efficient_kernel': False, 'use_deepspeed_evo_attention': False, 'use_lma': False, 'inplace_safe': False, 'chunk_size': None, 'debug_logging': True}
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - [DEBUG][StageC] OmegaConf resolved device: cpu
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - This should always appear if logger is working. sequence=ACGUACGU, torsion_shape=torch.Size([8, 14])
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Running MP-NeRF with device=cpu, do_ring_closure=False
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence length: 8, torsion shape: torch.Size([8, 14])
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions requires_grad: True
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - predicted_torsions grad_fn: <SliceBackward0 object at 0x156fa1ab0>
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] requires_grad: True
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - scaffolds['torsions'] grad_fn: <SliceBackward0 object at 0x156fa1ab0>
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] seq: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] torsions[3] = tensor([-0.3415, -0.5903,  0.0688,  0.2767,  0.4661, -0.2179,  0.1524],
       grad_fn=<SelectBackward0>)
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] device: type=<class 'str'>, shape/len=3
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] sugar_pucker: type=<class 'str'>, shape/len=8
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] bond_mask[3] = tensor([1.5930, 1.6070, 1.5000, 1.5000, 1.4800, 1.4800, 1.4400, 1.5100, 1.4530,
        1.5240])
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] angles_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([2, 8, 10])
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] point_ref_mask: type=<class 'torch.Tensor'>, shape/len=torch.Size([3, 8, 10])
[2025-05-19 13:23:54][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-SCAFFOLDS-RES3] cloud_mask[3] = tensor([True, True, True, True, True, True, True, True, True, True])
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-BB-COORDS-RES3] coords_bb[3] = tensor([[18.3160, 37.0958, 24.5705],
        [17.4975, 38.4399, 24.8182],
        [16.8349, 37.1811, 25.0421],
        [18.3098, 36.9580, 25.2768],
        [18.0604, 38.3715, 25.5027],
        [16.7717, 37.6402, 25.7359],
        [17.8382, 36.7245, 25.9572],
        [18.3616, 38.1107, 26.1904],
        [16.9662, 38.1448, 26.4101],
        [17.4303, 36.7376, 26.6433]], grad_fn=<SelectBackward0>)
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb requires_grad: True
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_bb grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-PLACE_BASES] place_bases: True
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - [DEBUG-GRAD-PRINT-CALLSITE] coords_bb (input to place_rna_bases) requires_grad: True, grad_fn: <StackBackward0 object at 0x154459780>
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) requires_grad: True
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full (after place_bases) grad_fn: <CopySlices object at 0x156acc5b0>
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat requires_grad: True
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat grad_fn: <IndexBackward0 object at 0x156acc5b0>
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Sequence used for atom metadata: ACGUACGU
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Atom counts for each residue: [22, 20, 23, 20, 22, 20, 23, 20]
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - Total atom count: 170
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - sequence length: 8
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - coords_full_flat.shape: torch.Size([170, 3])
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - atom_names (len): 170 ['P', 'OP1', 'OP2', "O5'", "C5'", "C4'", "O4'", "C3'", "O3'", "C2'", "O2'", "C1'", 'N9', 'C8', 'N7', 'C5', 'C6', 'N6', 'N1', 'C2']
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - residue_indices (len): 170 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - valid_atom_mask (sum): 170
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - place_bases: True
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][INFO] - StageC completed: sequence=ACGUACGU, residues=8, atoms=170, coords_shape=torch.Size([170, 3]), device=cpu, elapsed_time=0.23s
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords'] shape: torch.Size([170, 3])
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['coords_3d'] shape: torch.Size([8, 23, 3])
[2025-05-19 13:23:55][rna_predict.pipeline.stageC.stage_c_reconstruction][DEBUG] - output['atom_count']: 170


================================================================================
Output from: runners/demo_entry.py
Timestamp: 2025-05-19 13:24:04
================================================================================

STDOUT:
Configuration:
test_data:
  sequence: ACGUACGU
  sequence_length: 8
  atoms_per_residue: 44
pipeline:
  verbose: true
  save_intermediates: true
  output_dir: outputs
model:
  stageA:
    enabled: true
    debug_logging: true
  stageB:
    enabled: true
    debug_logging: true
  stageC:
    enabled: true
    debug_logging: true
  stageD:
    enabled: true
    debug_logging: true

Running demo_run_input_embedding()...
Now streaming the bprna-spot dataset...
Showing the full dataset structure for the first row...
Demo completed successfully.


================================================================================
Output from: runners/full_pipeline.py
Timestamp: 2025-05-19 13:24:07
================================================================================

STDOUT:
[2025-05-19 13:24:17,966] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to mps (auto detect)
STDERR:
W0519 13:24:26.462000 8760659008 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.

