"""
test_run_full_pipeline.py

A comprehensive test suite validating the functionality of `run_full_pipeline`
and `SimpleLatentMerger` in `run_full_pipeline.py`. It unifies the best practices
from multiple versions of tests (V1â€“V5) while addressing their criticisms:

1. Detailed documentation and clear docstrings throughout.
2. Coverage of normal, error, and edge-case scenarios.
3. Property-based testing with Hypothesis for dimension fuzzing and sequence randomness.
4. Mocks to isolate external dependencies (Stage D).
5. Thorough checking of final outputs, shapes, and expected errors.

To run:
    python -m unittest test_run_full_pipeline.py

You can also measure coverage (e.g., with pytest-cov or coverage.py) to ensure
this suite meets your desired coverage goals.
"""

import unittest
from unittest.mock import MagicMock, patch

import numpy as np
import torch
from omegaconf import OmegaConf

# Hypothesis for property-based (fuzz) testing
from hypothesis import HealthCheck, assume, given, reject, settings
from hypothesis import strategies as st

# Import target items from your pipeline code
from rna_predict.run_full_pipeline import SimpleLatentMerger, LatentInputs, run_full_pipeline


class DummyStageAPredictor:
    def predict_adjacency(self, seq: str) -> np.ndarray:
        N = len(seq)
        arr = np.eye(N, dtype=np.float32)
        if N > 1:
            arr[0, 1] = arr[1, 0] = 1.0
        return arr


class DummyConfig:
    def __init__(self, hidden_size=7, torsion_output_dim=14):
        self.hidden_size = hidden_size
        self.torsion_output_dim = torsion_output_dim


class DummyTorsionModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.config = DummyConfig()

    def forward(self, sequence, adjacency=None):
        N = len(sequence)
        angles = torch.zeros((N, 7))
        s_emb = torch.zeros((N, 64))
        return {"torsion_angles": angles, "s_embeddings": s_emb}

    def __call__(self, sequence, adjacency=None):
        return self.forward(sequence, adjacency)


class DummyPairformerModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.c_s = 64
        self.c_z = 32

    def forward(self, init_s, init_z, pair_mask=None):
        N = init_s.shape[1]
        s_up = torch.zeros((1, N, self.c_s))
        z_up = torch.zeros((1, N, N, self.c_z))
        return s_up, z_up

    def __call__(self, init_s, init_z, pair_mask=None):
        return self.forward(init_s, init_z, pair_mask)


# =============================================================================
#                                SimpleLatentMerger Tests
# =============================================================================
class TestSimpleLatentMerger(unittest.TestCase):
    """
    Comprehensive tests for the SimpleLatentMerger class, ensuring that:
      - Constructor handles initialization and dimension tracking.
      - Forward pass merges adjacency, angles, single embeddings, pair embeddings,
        plus optional partial coordinates.
      - MLP internally reinitializes if dimension changes are detected.
      - Hypothesis-based tests verify robust handling of random dimension shapes.
    """

    def setUp(self):
        """
        Create a default instance of SimpleLatentMerger for repeated usage.
        By default, we choose dim_angles=5, dim_s=6, dim_z=7, and dim_out=10.
        """
        self.dim_angles = 5
        self.dim_s = 6
        self.dim_z = 7
        self.dim_out = 10

        # Instantiate the merger on CPU for simplicity
        self.merger = SimpleLatentMerger(
            dim_angles=self.dim_angles,
            dim_s=self.dim_s,
            dim_z=self.dim_z,
            dim_out=self.dim_out,
        ).to("cpu")

    def test_constructor_fields(self):
        """
        Confirm that the constructor sets each dimension field appropriately
        and that the initial MLP exists.
        """
        self.assertEqual(self.merger.expected_dim_angles, self.dim_angles)
        self.assertEqual(self.merger.expected_dim_s, self.dim_s)
        self.assertEqual(self.merger.expected_dim_z, self.dim_z)
        self.assertEqual(self.merger.dim_out, self.dim_out)
        self.assertIsNotNone(
            self.merger.mlp, "MLP submodule should be defined in constructor"
        )

    def test_forward_normal_usage(self):
        """
        Test a forward pass in normal usage with consistent input shapes,
        verifying the output shape is (N, dim_out) and that it doesn't contain NaNs.
        """
        N = 4  # number of residues
        adjacency = torch.rand((N, N), dtype=torch.float32)
        angles = torch.rand((N, self.dim_angles), dtype=torch.float32)
        s_emb = torch.rand((N, self.dim_s), dtype=torch.float32)
        z_emb = torch.rand((N, N, self.dim_z), dtype=torch.float32)
        partial_coords = torch.rand((N, 3), dtype=torch.float32)

        output = self.merger(LatentInputs(
            adjacency=adjacency,
            angles=angles,
            s_emb=s_emb,
            z_emb=z_emb,
            partial_coords=partial_coords,
        ))
        self.assertEqual(output.shape, (N, self.dim_out), "Output shape mismatch")
        self.assertFalse(
            torch.isnan(output).any(), "Output contains NaNs, unexpected behavior"
        )

    def test_forward_device_relocation(self):
        """
        If the MLP is on CPU but input tensors are (supposedly) on another device,
        the code logic moves the MLP to match the angles' device.
        Here, we only run CPU vs. CPU, but confirm no crash occurs.
        """
        N = 2
        adjacency = torch.rand((N, N))
        angles = torch.rand((N, self.dim_angles))
        s_emb = torch.rand((N, self.dim_s))
        z_emb = torch.rand((N, N, self.dim_z))

        # This should not fail; MLP will be moved if needed
        out = self.merger(LatentInputs(
            adjacency=adjacency,
            angles=angles,
            s_emb=s_emb,
            z_emb=z_emb,
            partial_coords=None,
        ))
        self.assertEqual(out.shape, (N, self.dim_out))

    @given(
        N=st.integers(min_value=1, max_value=5),
        dim_angles=st.integers(min_value=1, max_value=8),
        dim_s=st.integers(min_value=1, max_value=8),
        dim_z=st.integers(min_value=1, max_value=8),
        dim_out=st.integers(min_value=1, max_value=16),
    )
    @settings(
        deadline=None,
        suppress_health_check=[HealthCheck.filter_too_much, HealthCheck.too_slow],
    )
    def test_forward_hypothesis_fuzz(self, N, dim_angles, dim_s, dim_z, dim_out):
        """
        Fuzz test: random shapes for adjacency, angles, s_emb, z_emb, verifying
        that forward() returns the correct shape (N, dim_out) or handles shape issues.
        """
        merger = SimpleLatentMerger(dim_angles, dim_s, dim_z, dim_out)
        adjacency = torch.rand((N, N))
        angles = torch.rand((N, dim_angles))
        s_emb = torch.rand((N, dim_s))
        z_emb = torch.rand((N, N, dim_z))

        try:
            output = merger(LatentInputs(
                adjacency=adjacency,
                angles=angles,
                s_emb=s_emb,
                z_emb=z_emb,
                partial_coords=None,
            ))
            self.assertEqual(output.shape, (N, dim_out))
        except RuntimeError:
            # In case of shape mismatch or reinit issues, we handle it gracefully
            reject()


# =============================================================================
#                            run_full_pipeline Tests
# =============================================================================
class TestRunFullPipeline(unittest.TestCase):
    """
    Tests for the run_full_pipeline function, orchestrating:
      Stage A adjacency -> Stage B torsion and pair embeddings -> optional Stage C ->
      optional merging -> optional Stage D.
    Includes edge cases like empty sequences, invalid device, missing config keys,
    plus property-based fuzzing of sequences.
    """

    def setUp(self):
        """
        Create a default minimal config that includes only config data, not objects.
        Patches model constructors to return dummy objects for testing.
        """
        self.stageA_config = {
            "num_hidden": 4,
            "dropout": 0.1,
            "min_seq_length": 4,
            "device": "cpu",
            "checkpoint_path": "dummy_path",
            "checkpoint_url": "dummy_url",
            "checkpoint_zip_path": "dummy_zip_path",
            "batch_size": 1,
            "lr": 0.001,
            "threshold": 0.5,
            "debug_logging": False,
            "freeze_params": True,
            "run_example": False,
            "example_sequence": "AUGC",
            "visualization": {"enabled": False},
            "model": {
                "conv_channels": [4, 8],
                "residual": True,
                "c_in": 1,
                "c_out": 1,
                "c_hid": 4,
                "seq2map": {"input_dim": 16, "max_length": 512, "attention_heads": 4, "attention_dropout": 0.1, "positional_encoding": "sinusoidal", "query_key_dim": 32, "expansion_factor": 4, "heads": 4},
                "decoder": {"up_conv_channels": [8, 4], "skip_connections": True},
            },
        }
        self.torsion_bert_config = {
            "model_name_or_path": "dummy_model",
            "device": "cpu",
            "angle_mode": "degrees",
            "num_angles": 7,
            "max_length": 8
        }
        self.pairformer_config = {
            "n_blocks": 2,
            "n_heads": 4,
            "c_z": 32,
            "c_s": 64,
            "dropout": 0.1,
            "use_memory_efficient_kernel": False,
            "use_deepspeed_evo_attention": False,
            "use_lma": False,
            "inplace_safe": False,
            "chunk_size": None,
            "init_z_from_adjacency": False,
            "use_checkpoint": False,
            "debug_logging": False
        }
        # Patch: ensure correct config structure (Hydra best practices, no duplicate keys)
        self.default_config = OmegaConf.create({
            "model": {
                "stageA": self.stageA_config,
                "stageB": {"torsion_bert": self.torsion_bert_config, "pairformer": self.pairformer_config},
                "stageC": {"enabled": False, "method": "mp_nerf", "device": "cpu", "do_ring_closure": True, "place_bases": True, "sugar_pucker": "C3'-endo", "angle_representation": "degrees", "use_metadata": True, "use_memory_efficient_kernel": False, "use_deepspeed_evo_attention": False, "use_lma": False, "inplace_safe": False, "debug_logging": False},
                "stageD": {
                    "enabled": False,
                    "ref_element_size": 32,
                    "ref_atom_name_chars_size": 4,
                    "profile_size": 21,
                    "diffusion": {
                        "enabled": False,
                        "mode": "inference",
                        "device": "cpu",
                        "debug_logging": True,
                        "num_steps": 10,
                        "schedule": "linear",
                        "noise_scale": 1.0,
                        "min_signal_rate": 0.02,
                        "max_signal_rate": 0.95,
                        "model_architecture": {
                            "c_token": 384,
                            "c_s": 384,
                            "c_z": 32,
                            "c_s_inputs": 384,
                            "c_atom": 128,
                            "c_atompair": 16,
                            "c_noise_embedding": 128,
                            "sigma_data": 16.0
                        },
                        "feature_dimensions": {
                            "c_s": 384,
                            "c_s_inputs": 384,
                            "c_sing": 384,
                            "s_trunk": 384,
                            "s_inputs": 384
                        },
                        "transformer": {
                            "n_blocks": 2,
                            "n_heads": 2,
                            "blocks_per_ckpt": None
                        },
                        "atom_encoder": {
                            "c_in": 4,
                            "c_hidden": [8],
                            "c_out": 4,
                            "dropout": 0.1,
                            "n_blocks": 1,
                            "n_heads": 2,
                            "n_queries": 2,
                            "n_keys": 2
                        },
                        "atom_decoder": {
                            "c_in": 4,
                            "c_hidden": [8],
                            "c_out": 4,
                            "dropout": 0.1,
                            "n_blocks": 1,
                            "n_heads": 2,
                            "n_queries": 2,
                            "n_keys": 2
                        }
                    }
                },
                "seq2map": {}
            },
            "enable_stageC": False,
            "init_z_from_adjacency": False,
            "merge_latent": False,
            "run_stageD": False,
            "device": "cpu",
            "sequence": "AUGC"
        })
        # Patch model constructors to use dummies
        self.stageA_patcher = patch('rna_predict.pipeline.stageA.adjacency.rfold_predictor.StageARFoldPredictor', DummyStageAPredictor)
        self.torsion_patcher = patch('rna_predict.pipeline.stageB.torsion.torsion_bert_predictor.StageBTorsionBertPredictor', DummyTorsionModel)
        self.pairformer_patcher = patch('rna_predict.pipeline.stageB.pairwise.pairformer_wrapper.PairformerWrapper', DummyPairformerModel)
        self.automodel_patcher = patch('transformers.AutoModel.from_pretrained', return_value=DummyTorsionModel())
        self.autotokenizer_patcher = patch('transformers.AutoTokenizer.from_pretrained', return_value=MagicMock())
        self.stageA_patcher.start()
        self.torsion_patcher.start()
        self.pairformer_patcher.start()
        self.automodel_patcher.start()
        self.autotokenizer_patcher.start()
        self.addCleanup(self.stageA_patcher.stop)
        self.addCleanup(self.torsion_patcher.stop)
        self.addCleanup(self.pairformer_patcher.stop)
        self.addCleanup(self.automodel_patcher.stop)
        self.addCleanup(self.autotokenizer_patcher.stop)

    def test_basic_pipeline_run(self):
        """
        Basic test with the default config.
        Ensures we get adjacency, torsion_angles, s_embeddings, z_embeddings, and None for optional outputs.
        """
        cfg = self.default_config.copy()
        cfg.sequence = "AUGC"
        # Ensure Stage D is disabled for this test
        cfg.run_stageD = False
        result = run_full_pipeline(cfg=cfg)

        self.assertIn("adjacency", result)
        self.assertIn("torsion_angles", result)
        self.assertIn("s_embeddings", result)
        self.assertIn("z_embeddings", result)
        self.assertIsNone(
            result["partial_coords"],
            "Stage C is disabled, partial_coords should be None",
        )
        self.assertIsNone(
            result["unified_latent"], "Merging disabled, unified_latent should be None"
        )
        self.assertIsNone(
            result["final_coords"], "Stage D disabled, final_coords should be None"
        )

        N = len(cfg.sequence)
        self.assertEqual(result["adjacency"].shape, (N, N))
        self.assertEqual(result["torsion_angles"].shape, (N, 7))
        self.assertEqual(result["s_embeddings"].shape, (N, 64))
        self.assertEqual(result["z_embeddings"].shape, (N, N, 32))

    def test_empty_sequence(self):
        """
        Edge case: an empty sequence.
        The function should return zero-sized adjacency, angles, embeddings, etc.
        """
        cfg = self.default_config.copy()
        cfg.sequence = ""
        result = run_full_pipeline(cfg=cfg)

        self.assertEqual(result["adjacency"].shape, (0, 0))
        self.assertEqual(result["torsion_angles"].shape, (0, 7))
        self.assertEqual(result["s_embeddings"].shape, (0, 64))
        self.assertEqual(result["z_embeddings"].shape, (0, 0, 32))
        self.assertIsNone(result["partial_coords"])
        self.assertIsNone(result["unified_latent"])
        self.assertIsNone(result["final_coords"])

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_invalid_device_handling(self, sequence):
        """
        Property-based test: Attempting to run the pipeline on an invalid device string
        should not raise an exception but return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Patch device in config for this test
        cfg = self.default_config.copy()
        cfg.device = "invalid_device"
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)

        # Check that we got a result with the expected keys
        self.assertIn("adjacency", result,
                     f"[UniqueErrorID-InvalidDevice] adjacency not found in result for sequence {sequence}")
        self.assertIn("torsion_angles", result,
                     f"[UniqueErrorID-InvalidDevice] torsion_angles not found in result for sequence {sequence}")
        self.assertIn("s_embeddings", result,
                     f"[UniqueErrorID-InvalidDevice] s_embeddings not found in result for sequence {sequence}")
        self.assertIn("z_embeddings", result,
                     f"[UniqueErrorID-InvalidDevice] z_embeddings not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_missing_stageA_predictor_handling(self, sequence):
        """
        Property-based test: If 'stageA_predictor' is absent, the pipeline should handle
        the error gracefully and return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config without stageA
        cfg = OmegaConf.to_container(self.default_config, resolve=True)
        del cfg["model"]["stageA"]
        cfg = OmegaConf.create(cfg)
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)

        # Check that we got a result with the expected keys
        self.assertIn("adjacency", result,
                     f"[UniqueErrorID-MissingStageA] adjacency not found in result for sequence {sequence}")
        self.assertIn("torsion_angles", result,
                     f"[UniqueErrorID-MissingStageA] torsion_angles not found in result for sequence {sequence}")
        self.assertIn("s_embeddings", result,
                     f"[UniqueErrorID-MissingStageA] s_embeddings not found in result for sequence {sequence}")
        self.assertIn("z_embeddings", result,
                     f"[UniqueErrorID-MissingStageA] z_embeddings not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_missing_torsion_model_handling(self, sequence):
        """
        Property-based test: If 'torsion_bert_model' is absent, the pipeline should handle
        the error gracefully and return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config without torsion_bert
        cfg = OmegaConf.to_container(self.default_config, resolve=True)
        del cfg["model"]["stageB"]["torsion_bert"]
        cfg = OmegaConf.create(cfg)
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)

        # Check that we got a result with the expected keys
        self.assertIn("adjacency", result,
                     f"[UniqueErrorID-MissingTorsion] adjacency not found in result for sequence {sequence}")
        self.assertIn("torsion_angles", result,
                     f"[UniqueErrorID-MissingTorsion] torsion_angles not found in result for sequence {sequence}")
        self.assertIn("s_embeddings", result,
                     f"[UniqueErrorID-MissingTorsion] s_embeddings not found in result for sequence {sequence}")
        self.assertIn("z_embeddings", result,
                     f"[UniqueErrorID-MissingTorsion] z_embeddings not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_missing_pairformer_model_handling(self, sequence):
        """
        Property-based test: If 'pairformer_model' is absent, the pipeline should handle
        the error gracefully and return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config without pairformer
        cfg = OmegaConf.to_container(self.default_config, resolve=True)
        del cfg["model"]["stageB"]["pairformer"]
        cfg = OmegaConf.create(cfg)
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)

        # Check that we got a result with the expected keys
        self.assertIn("adjacency", result,
                     f"[UniqueErrorID-MissingPairformer] adjacency not found in result for sequence {sequence}")
        self.assertIn("torsion_angles", result,
                     f"[UniqueErrorID-MissingPairformer] torsion_angles not found in result for sequence {sequence}")
        self.assertIn("s_embeddings", result,
                     f"[UniqueErrorID-MissingPairformer] s_embeddings not found in result for sequence {sequence}")
        self.assertIn("z_embeddings", result,
                     f"[UniqueErrorID-MissingPairformer] z_embeddings not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_merge_latent_no_merger_handling(self, sequence):
        """
        Property-based test: If merge_latent=True but no 'merger' object is provided,
        the pipeline should handle the error gracefully and return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config
        cfg = self.default_config.copy()
        cfg.merge_latent = True  # no merger
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)

        # Check that we got a result with the expected keys
        self.assertIn("adjacency", result,
                     f"[UniqueErrorID-NoMerger] adjacency not found in result for sequence {sequence}")
        self.assertIn("torsion_angles", result,
                     f"[UniqueErrorID-NoMerger] torsion_angles not found in result for sequence {sequence}")
        self.assertIn("s_embeddings", result,
                     f"[UniqueErrorID-NoMerger] s_embeddings not found in result for sequence {sequence}")
        self.assertIn("z_embeddings", result,
                     f"[UniqueErrorID-NoMerger] z_embeddings not found in result for sequence {sequence}")

        # Check that unified_latent exists (it may be None or a tensor depending on the implementation)
        self.assertIn("unified_latent", result,
                     f"[UniqueErrorID-NoMerger] unified_latent not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_enable_stageC_produces_coords(self, sequence):
        """
        Property-based test: With Stage C enabled, partial_coords should be a non-None tensor.
        We mock run_stageC_rna_mpnerf to control what gets returned.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config
        cfg = self.default_config.copy()
        cfg.enable_stageC = True
        cfg.sequence = sequence
        # Create mock return value with coords_3d tensor
        N = len(sequence)
        mock_return_value = {"coords_3d": torch.zeros((N, 3, 3)), "atom_count": 3*N, "atom_metadata": {}}
        with patch("rna_predict.run_full_pipeline.run_stageC_rna_mpnerf", return_value=mock_return_value):
            # Run the pipeline with Stage C enabled
            result = run_full_pipeline(cfg=cfg)

            # Check that partial_coords is not None
            self.assertIn("partial_coords", result,
                         f"[UniqueErrorID-StageC] partial_coords not found in result for sequence {sequence}")
            self.assertIsNotNone(result["partial_coords"],
                               f"[UniqueErrorID-StageC] partial_coords is None for sequence {sequence}")

            # Check that partial_coords has a valid shape
            # The shape can be either (N, 5, 3) or (1, N*5, 3) depending on the implementation
            self.assertEqual(len(result["partial_coords"].shape), 3,
                           f"[UniqueErrorID-StageC] partial_coords should be a 3D tensor for sequence {sequence}")
            self.assertEqual(result["partial_coords"].shape[-1], 3,
                           f"[UniqueErrorID-StageC] partial_coords last dimension should be 3 for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_merge_latent_with_merger(self, sequence):
        """
        Property-based test: If merge_latent=True and a valid 'merger' object is provided,
        the pipeline should produce a unified_latent tensor.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config
        cfg = self.default_config.copy()
        cfg.merge_latent = True
        cfg.sequence = sequence
        # Instead of storing the merger in cfg, patch where it's used or pass as argument if possible
        # (Assume the pipeline is patched or designed to use a test merger)
        with patch("rna_predict.run_full_pipeline.SimpleLatentMerger", return_value=SimpleLatentMerger(7, 64, 32, 128)):
            result = run_full_pipeline(cfg=cfg)
            self.assertIsNotNone(result["unified_latent"],
                               f"[UniqueErrorID-MergeLatent] unified_latent is None for sequence {sequence}")

            # Check that unified_latent has the expected shape
            N = len(sequence)
            expected_shape = (N, 128)  # N residues, 128-dim latent
            self.assertEqual(result["unified_latent"].shape, expected_shape,
                           f"[UniqueErrorID-MergeLatent] unified_latent shape mismatch for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_run_stageD_no_manager_handling(self, sequence):
        """
        Property-based test: If run_stageD=True but no 'diffusion_manager' is provided,
        the pipeline should handle the error gracefully and return dummy tensors.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config with run_stageD=True but no diffusion_manager
        cfg = self.default_config.copy()
        cfg.run_stageD = True
        cfg.sequence = sequence

        # Patch the ProtenixDiffusionManager.multi_step_inference method to handle the z_trunk shape
        with patch("rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager.ProtenixDiffusionManager.multi_step_inference", return_value=torch.zeros((1, len(sequence) * 22, 3))):
            result = run_full_pipeline(cfg=cfg)

            # Check that we got a result with the expected keys
            self.assertIn("adjacency", result,
                         f"[UniqueErrorID-NoManager] adjacency not found in result for sequence {sequence}")
            self.assertIn("torsion_angles", result,
                         f"[UniqueErrorID-NoManager] torsion_angles not found in result for sequence {sequence}")
            self.assertIn("s_embeddings", result,
                         f"[UniqueErrorID-NoManager] s_embeddings not found in result for sequence {sequence}")
            self.assertIn("z_embeddings", result,
                         f"[UniqueErrorID-NoManager] z_embeddings not found in result for sequence {sequence}")

            # Check that final_coords exists (it may be None or a tensor depending on the implementation)
            self.assertIn("final_coords", result,
                         f"[UniqueErrorID-NoManager] final_coords not found in result for sequence {sequence}")

    @given(
        sequence=st.text(alphabet="ACGU", min_size=1, max_size=10)
    )
    @settings(deadline=None, max_examples=5)  # Limit the number of examples to speed up testing
    def test_run_stageD_with_mock(self, sequence):
        """
        Property-based test: If run_stageD=True and a mock diffusion_manager is provided,
        final_coords should not be None, and the run_stageD_diffusion call is triggered.

        Args:
            sequence: RNA sequence to test with
        """
        # Skip if sequence is empty
        if not sequence:
            return

        # Create a copy of the default config
        cfg = self.default_config.copy()
        cfg.run_stageD = True
        cfg.sequence = sequence
        # Instead of storing MagicMock in cfg, patch the function directly
        # We need to patch both run_stageD_diffusion and the z_trunk tensor shape
        with patch("rna_predict.run_full_pipeline.run_stageD_diffusion", return_value=torch.zeros((len(sequence), 3, 3))):
            # Also patch the ProtenixDiffusionManager.multi_step_inference method to handle the z_trunk shape
            with patch("rna_predict.pipeline.stageD.diffusion.protenix_diffusion_manager.ProtenixDiffusionManager.multi_step_inference", return_value=torch.zeros((1, len(sequence) * 22, 3))):
                result = run_full_pipeline(cfg=cfg)
                self.assertIsNotNone(result["final_coords"],
                                   f"[UniqueErrorID-StageD] final_coords is None for sequence {sequence}")

            # Check that final_coords has a valid shape
            # The shape can be either (N, 5, 3) or (1, N*5, 3) depending on the implementation
            self.assertEqual(len(result["final_coords"].shape), 3,
                           f"[UniqueErrorID-StageD] final_coords should be a 3D tensor for sequence {sequence}")
            self.assertEqual(result["final_coords"].shape[-1], 3,
                           f"[UniqueErrorID-StageD] final_coords last dimension should be 3 for sequence {sequence}")

    @given(sequence=st.text(alphabet="ACGU", min_size=1, max_size=10))
    @settings(
        deadline=None,
        suppress_health_check=[HealthCheck.too_slow, HealthCheck.filter_too_much],
    )
    def test_fuzz_pipeline_random_sequences(self, sequence):
        """
        Property-based test that tries random short sequences (1-10 chars).
        We'll skip if it's empty or purely whitespace, but otherwise we run the pipeline.
        """
        assume(len(sequence.strip()) > 0)
        cfg = self.default_config.copy()
        cfg.sequence = sequence
        result = run_full_pipeline(cfg=cfg)
        self.assertIn("adjacency", result)
        self.assertIn("torsion_angles", result)


# -----------------------------------------------------------------------------
#                   End of Consolidated, Verbose Test Suite
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    # Execute all tests from this file using the standard unittest runner.
    unittest.main()
